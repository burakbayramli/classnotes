\documentclass[a4paper,11pt]{article}

\usepackage[latin5]{inputenc}
\usepackage{amsmath}
\usepackage{examplep}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphics}

\title{Kalman Filters: Derivation}

\author{Burak Bayramlý}

\begin{document}

\maketitle

Compiling an exhaustive list of fields where Kalman Filters (KF) are in use
would be a formiddable task: The list could start with navigation, radar
tracking, satellite orbit computation \cite{welling}, continue with sonar
ranging, stock price prediction (which hopefully this thesis will prove), and
many other fields where there is a dynamical system present. When the {\em
  Eagle} module of the Apollo 11 mission landed on the moon, it is widely known
that it did so with the help of a Kalman Filter. KF was first proposed in a
seminal paper by \cite{kalman}, later Kalman and Bucy \cite{kalman2} - the
method's first application setting was space tracking.

Kalman Filters, or in general, SSM methods try to model dynamic data using
two-level graphic model. In the first level, there are hidden states, one per
time step, which are random variables distributed as Gaussians. An outside
observer (modeler) does not see these states directly, instead sees them through
an ``observed'' variable whose outputs are assumed to be distorted by another
Gaussian distribution, refered to as white noise. In each time step, it is
assumed the hidden variables evolve linearly, in linear algebra this means the
hidden state is multiplied by a constant matrix $A$. Mathematically,
\begin{eqnarray*}
x_{t+1} &=& Ax_t + v_t 
\end{eqnarray*}
where
\begin{itemize}
   \item $v_0,v_1,...$ is white Gaussian noise with $\mathbf{E}v_t = 0$ and $cov
   \: v_t = Q$
   \item $x_t \sim N(\mu_t, \Sigma_t)$.
\end{itemize}
In order to model the output variables, we multiply hidden states with another
constant matrix and add another Gaussian white noise. All together our SSM looks
as follows:
\begin{eqnarray}
x_{t+1} = Ax_t + v_t \label{m1}\\
y_t = Cx_t + w_t \label{m2}
\end{eqnarray}
where $w_0,w_1,...$ are white Gaussian noise with $\mathbf{E}w_t = 0$ and $cov
\:\: w_t = R$. Random variables $w_t$ and $v_t$ are independent of $x_t$
and $y_t$, and from eachother, this assumption is put forth for simplicity. The
dimensions of each variable are:

\begin{itemize}
   \item $x(t) \in \mathbb{R}^n$ is the state
   \item $y(t) \in \mathbb{R}^p$ is the observed output
   \item $v(t) \in \mathbb{R}^n$ is process noise
   \item $w(t) \in \mathbb{R}^p$ is measurement noise
\end{itemize}

Kalman Filters carry the same conditional independence properties that
characterized HMMs - given a state for time $t$, the future is conditionally
independent from the past, hence KF also carries the first-order Markovian
property just like HMMs. Structurally there are many similarities between two
models as well and its methods for inference, as we shall soon see, the main
difference being that HMM uses multinomial distribution for hidden states
whereas KF uses Gaussians.

Inference with Kalman Filters means estimating the posterior probability given
an observed output sequence. There are two tasks here, filtering and
smoothing. Filtering is formulized by $p(x_t|y_0,...,y_t)$ which tries to
calculate posterior probability for hidden states using measurements up to time
$t$. In smoothing, we will calculate the same probability using measurements
taken after time $t$, which gives us $p(x_t|y_{t+1},...,y_T)$. We also hope to
formulize recursive formulas for both filtering and smoothing.

Both filtering and smoothing have direct consequences in terms of real-life
applications. KFs' natural roots are embedded in space tracking, the method
started as an ``online'' algorithm trying to track down targets using noisy
measurements, as each new measurement is received. In this sense, KF can be seen
as an estimator trying to guess a ``hidden'' position at time $t$ using
measurements up to time $t$. Smoothing, on the other hand, can be thought of a
global ``corrector'' - once we have better knowledge on hand, through smoothing
we can go back to old estimations and correct previous online estimates using
backward recursion.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Filtering}

Following the notation and the derivation steps used by Bishop and Jordan
\cite{jordan}, let's first denote

\begin{itemize}
   \item $\hat{x}_{t}^t \equiv \mathbf{E}[x_t|y_0,...,y_t]$
   \item $P_{t}^t \equiv \mathbf{E}[(x_t - \hat{x}_{t|t})(x_t -
   \hat{x}_{t|t})'|y_0,...,y_t]$ 
\end{itemize}
Our first goal is going to be trying to calculate $\hat{x}_{t+1}^{t+1}$ and
$P_{t+1}^{t+1}$ using a new measurement $y_{t+1}$. For this, we need to
``reverse the arrow'' so to speak, in other words formulize the distribution
where $x_t$ has conditional dependence on $y_t$ instead of other way around,
which is what the base equations state right now.

If we take expectation of both sides of \eqref{m1}
\begin{eqnarray*}
\mathbf{E}x_{t+1} = \hat{x}_{t+1} = A\mu_t = A\hat{x}_t
\end{eqnarray*}
Taking covariance of both sides of \eqref{m1} and denoting $P_t$ as $cov \: x(t)$
\begin{eqnarray*}
P_{t+1} = AP_{t}A' + Q
\end{eqnarray*}
This transition is called a ``time update''. It will allow us to propagate the
normal distribution at time $t$ to time $t_1$. The formulas that contain given
$y$'s use the same technique;
\begin{eqnarray*}
\hat{x}_{t+1}^t = Ax_{t}^t\\
P_{t+1}^t = AP_{t}^tA' + Q
\end{eqnarray*}
\begin{eqnarray*}
y_{t+1} &=& Cx_{t+1} + w_t\\
\mathbf{E}[y_{t+1}|y_0,...,y_t] &=& \mathbf{E}[Cx_{t+1}+w_t|y_0,...,y_t]\\
\hat{y}_{t+1}^t &=& C\hat{x}_{t+1}
\end{eqnarray*}
Similarly for covariance
\begin{eqnarray*}
E[(y_{t+1}-\hat{y}_{t+1}^t)(y_{t+1}-\hat{y}_{t+1}^t)'|y_0,...,y_t] =
CP_{t+1}^tC' + R
\end{eqnarray*}

Now the harder task of reversing the arrow: If our goal is formulizing
$p(x_t|y_t)$, then we need to derive the joint distribution between both
variables. Addition of Gaussians is another Gaussian, we can surmize the joint
probability $p(x_t,y_t)$ will be one huge Gaussian.

In order to define the joint normal distribution of $x_{t}$ and $y_{t}$, we need
to write the joint mean and covariance of this big Gaussian. Writing down the
mean will be easy, covariance will be little harder to derive. We can use a
trick \cite{lall} where we write $y_{t} = Cx_{t} + w_t$ as $z = Hu$
where
\begin{eqnarray*}
z =   \left[ \begin{array}{c}
      x_{t} \\
      y_{t}
      \end{array} \right],
H =   \left[ \begin{array}{cc}
      I & 0 \\
      C & I
      \end{array} \right]
u =   \left[ \begin{array}{c}
      x_{t} \\
      w_t
      \end{array} \right],      
\end{eqnarray*}
Now we need to take the covariance of the simpler equation:
\begin{eqnarray*}
cov(z) &=& H cov(u) H'\\
cov(u) &=& \left[ \begin{array}{cc}
      P_{t} & 0 \\
      0 & R
      \end{array} \right]     
\end{eqnarray*}
The full product is:
\begin{eqnarray*}
\left[ \begin{array}{cc}
      I & 0 \\
      C & I
      \end{array} \right]
\left[ \begin{array}{cc}
      P_{t} & 0 \\
      0 & R
      \end{array} \right]
\left[ \begin{array}{cc}
      I & C' \\
      0 & I
      \end{array} \right]           
\end{eqnarray*}
which results in
\begin{eqnarray*}
\left[ \begin{array}{cc}
      P_{t} & P_{t}C' \\
      CP_{t} & CP_{t}C'+R
      \end{array} \right]           
\end{eqnarray*}
We can also re-write this for conditional and with the mean;
\begin{eqnarray}
\left[ \begin{array}{c}
      \hat{x}_{t}^t \\
      C\hat{x}_{t}^t
      \end{array} \right]
\:and\:      
\left[ \begin{array}{cc}
      P_{t}^t & P_{t}^tC' \\
      CP_{t}^t & CP_{t}^tC'+R
      \end{array} \right] \label{jointxy}
\end{eqnarray}
Same for the joint distribution for $x_{t+1}, y_{t+1}$.
\begin{eqnarray}
\left[ \begin{array}{c}
      \hat{x}_{t+1}^t \\
      C\hat{x}_{t+1}^t
      \end{array} \right]
\:and\:      
\left[ \begin{array}{cc}
      P_{t+1}^t & P_{t+1}^tC' \\
      CP_{t+1}^t & CP_{t+1}^tC'+R
      \end{array} \right] \label{jointxy2}
\end{eqnarray}

Now in order to get statements for mean and variance for $x_{t+1}^{t+1}$, we
need to understand partitioned Gaussians. This will guide us when we use
portions of \eqref{jointxy2} and deriving the final equation \cite{jordan}. 

An $n$ dimensional Gaussian distribution can be partitioned into $p$ and $q$
dimensional sub-distributions where $n = p + q$. Hence we can say,

\begin{eqnarray}
\mu = 
\left[ \begin{array}{c}
      \mu_1 \\
      \mu_2
      \end{array} \right]           
\Sigma = 
\left[ \begin{array}{cc}
      \Sigma_{11} & \Sigma_{12}\\
      \Sigma_{21} & \Sigma_{22}
      \end{array} \right] \label{condgeneric}
\end{eqnarray}
\begin{eqnarray*}
p(x|\mu,\Sigma) &=& \frac{1}{(2\pi)^{(p+q)/2}|\Sigma|^{1/2}} \\
&& exp\bigg\{
-\frac{1}{2}
\left( \begin{array}{c}
      x_1 - \mu_1 \\
      x_2 - \mu_2
      \end{array} \right)'
\bigg\}
\left[ \begin{array}{cc}
      \Sigma_{11} & \Sigma_{12}\\
      \Sigma_{21} & \Sigma_{22}
      \end{array} \right]^{-1}
\left( \begin{array}{c}
      x_1 - \mu_1 \\
      x_2 - \mu_2
      \end{array} \right)
\bigg\} 
\end{eqnarray*}
After much algebra, we can obtain equation for $p(x_1|x_2)$. From this, we can
get {\em conditioned} $\mu$ and $\Sigma$, as
\begin{eqnarray}
\mu_{1|2} &=& \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2 - \mu_2) \label{condx} \\
\Sigma_{1|2} &=& \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \nonumber
\end{eqnarray}
Now plugging elements from  \eqref{condx} into \eqref{jointxy2} according
to placement in \eqref{condgeneric} we can formulize $\hat{x}_{t+1}^{t+1}$ and
$P_{t+1}^{t+1}$. 
\begin{eqnarray*}
\hat{x}_{t+1}^{t+1} &=& \hat{x}_{t+1}^t + P_{t+1}^tC'
                  (CP_{t+1}^tC'+ \Sigma_{w})^{-1} \\
               &&   (y_{t+1} - C\hat{x}_{t+1}^t) \\
P_{t+1}^{t+1} &=& P_{t+1}^t - P_{t+1}^tC' (CP_{t+1}^tC' + R)^{-1}CP_{t+1}^t
\end{eqnarray*}
If we declare $K_t \equiv P_{t+1}^tC' (CP_{t+1}^tC'+ \Sigma_{w})^{-1}$ then,
\begin{eqnarray}
\hat{x}_{t+1}^{t+1} &=& \hat{x}_{t+1}^t + K_t (y_{t+1} -
C\hat{x}_{t+1}^t) \label{xtplus1} \\
P_{t+1}^{t+1} &=& P_{t+1}^t - K_t C P_{t+1}^t
\end{eqnarray}

\begin{thebibliography}{99}
  
\bibitem{rabiner} Rabiner L. R. , 
  {\em A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition}, 
  Proceedings of IEEE vol. 77, no. 2, pp. 257-286, 
  1989.

\bibitem{ghahramani} Roweis S. and Z. Ghahramani,
  {\em  A Unifying Review of Linear Gaussian Models },
   Neural Computation 11(2):305--345, 
   1999.
   
\bibitem{ghahramani2} Ghahramani Z., H. E. Hinton, 
  {\em Parameter Estimation for Linear Dynamical Systems },
   Technical Report CRG-TR-96-2  
   \verb![ftp://ftp.cs.toronto.edu/pub/zoubin/tr96-2.ps.gz]!,
   Department of Computer Science, University of Toronto, 
   1996.

\bibitem{ghahramani3} Ghahramani Z., H. E. Hinton,
  {\em Switching State Space Models},
  Technical Report CRG-TR-96-3, Dept. Comp. Sci., Univ. Toronto, 
  1996.
   
\bibitem{shumway} Shumway R., H. S. Stoffer
  {\em Time series analysis and its applications 2nd Edition},
   New York, Springer, (Springer texts in statistics),
   2000.
   
\bibitem{jordan} Jordan M. I. , C. Bishop
  {\em An Introduction to Graphical Models},
   Not yet published, 
   2000.

\bibitem{kalman} Kalman R. E.,
  {\em A New Approach to Linear Filtering and Prediction Problems},
  Transactions of the ASME-Journal of Basic Engineering, 82 (Series D): 35-45, 
  1960.

\bibitem{kalman2} Kalman, R.E. and R.S. Bucy, 
  {\em New results in filtering and prediction theory},
  Trans. ASME J. Basic Eng., 83, 95-108,
  1961.
  
\bibitem{welling} Welling, M.,
  {\em The Kalman Filter - Lecture Tutorial},
   California Institute of Technology,
   2008.
   
\bibitem{lall} Lall, S.,
  {\em Modern Control 2 Lecture Notes},
   Stanford University,
   2006.
  
\bibitem{tsay} Tsay, R. S.,
  {\em Time Series and Forecasting: Brief History and Future Research,},
  Journal of the American Statistical Association, Vol. 95, No. 450., pp. 638-643.,
  Jun 2000.

\bibitem{shi} Shi S. and A. S. Weigend., {\em Taking Time Seriously: Hidden
  Markov Experts Applied to Financial Engineering}. In Proceedings of the
  IEEE/IAFE 1997 Conference on Computational Intelligence for Financial
  Engineering , pages 244-252. IEEE, 
  1997.
    
\bibitem{mandelbrot} Mandelbrot B. B., 
  {\em The variation of certain speculative prices, Journal of Business},
  XXXVI (1963), pp. 392-417, 
  1963.
  
\end{thebibliography}

\end{document}
