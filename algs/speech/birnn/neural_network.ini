[nn]
epochs = 100
network_type = BiRNN
decode_train = True
n_input = 26
n_context = 9
model_dir = nn/debug_models
SAVE_MODEL_EPOCH_NUM = 2
VALIDATION_EPOCH_NUM = 1
CURR_VALIDATION_LER_DIFF = 0.005
AVG_VALIDATION_LER_EPOCHS = 2
beam_search_decoder = default
shuffle_data_after_epoch = True
min_dev_ler = 100.0
tf_device = /gpu:0
simultaneous_users_count = 4

[data]
#If data_dir does not start with '/', then home_dir is prepended in set_dirs.py
data_dir = data/raw/librivox/LibriSpeech/
dir_pattern_train = train-clean-100-wav
dir_pattern_dev = dev-clean-wav
dir_pattern_test = test-clean-wav
n_train_limit = 5
n_dev_limit = 2
n_test_limit = 2
batch_size_train = 2
batch_size_dev = 2
batch_size_test = 2
start_idx_init_train = 0
start_idx_init_dev = 0
start_idx_init_test = 0
sort_train = filesize_low_high
sort_dev = random
sort_test = random

[optimizer]
# AdamOptimizer (http://arxiv.org/abs/1412.6980) parameters
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8
learning_rate = 0.001

[simplelstm]
n_character = 29
default_stddev = 0.046875
b1_stddev = %(default_stddev)s
h1_stddev = %(default_stddev)s
n_layers = 2
n_hidden_units = 512

[birnn]
n_character = 29
use_warpctc = False
dropout_rate = 0.05
dropout_rate2 = %(dropout_rate)s
dropout_rate3 = %(dropout_rate)s
dropout_rate4 = 0.0
dropout_rate5 = 0.0
dropout_rate6 = %(dropout_rate)s
dropout_rates = %(dropout_rate)s,%(dropout_rate2)s,%(dropout_rate3)s,%(dropout_rate4)s,%(dropout_rate5)s,%(dropout_rate6)s
relu_clip = 20
default_stddev = 0.046875
b1_stddev = %(default_stddev)s
h1_stddev = %(default_stddev)s
b2_stddev = %(default_stddev)s
h2_stddev = %(default_stddev)s
b3_stddev = %(default_stddev)s
h3_stddev = %(default_stddev)s
b5_stddev = %(default_stddev)s
h5_stddev = %(default_stddev)s
b6_stddev = %(default_stddev)s
h6_stddev = %(default_stddev)s
n_hidden = 1024
n_hidden_1 = %(n_hidden)s
n_hidden_2 = %(n_hidden)s
n_hidden_5 = %(n_hidden)s
n_cell_dim = %(n_hidden)s
n_hidden_3 = 2 * %(n_cell_dim)s
n_hidden_6 = %(n_character)s
