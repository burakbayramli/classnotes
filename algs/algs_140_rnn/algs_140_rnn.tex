\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Kendini Tekrarlayan Yapay Sinir Aðlarý (Recurrent Neural Network -RNN-)

RNN'ler zaman serilerini, sýralý olan verileri modellemek için
kullanýlýr. Mesela 2 3 1 2 3 1 2 3 1 2 3 gibi bir girdi olabilir, girdi
arka arkaya gelen her 3 karakter, hedef ise 4. karakter. Bu veri üzerinde
RNN eðitilebilir, ve mesela verili 2 3 1'den sonra hangi 4. sayý geldiði
tahmin edilmeye uðraþýlabilir. Ayrýksal olarak girdi bir harf dizisi de
olabilir. 

\includegraphics[width=30em]{rnn_01.png}

Daha önce iþlediðimiz Öne Doðru Beslemeli (Feed-Forward) YSA'lar en temel,
klasik yapýlardýr. Eðer bir $N$ boyutlu girdi alýyorlarsa bu verinin tüm
boyutlarýný ayný anda iþlerler. RNN için [3] yapý þöyle deðiþiyor (tek bir
nöron için),

$x_t$: $t$ anýndaki girdi. 

$s_t$: $t$ anýndaki gizli konum. 

$$ s_t = f(U x_t +  W s_{t-1})$$

$o_t$: $t$ anýndaki çýktý, $o_t = g(V s_t)$

Ýlginç olan $U,V,W$ aðýrlýk matrislerinin, parametrelerinin her zaman aný,
her veri noktasý için ayný olmasý. Yani farklý zaman dilimleri için farklý
aðýrlýklar atanmýyor. $t$ anýndaki gizli (hidden) konum $s_t$, bu bir nevi
``hafýza''. Bu fonksiyon $x_t$ girdisinin $W$ ile çarpýlmasý, artý bir
önceki konumun bir baþka $W$ ile çarpýlmasý sonucundan elde ediliyor. $W$
matrisi geçmiþe ne kadar önem verileceðini tanýmlýyorlar. Ardýndan tüm
hesap bir $\phi$ ile ``eziliyor'' yani belli aralýklara düþmesi zorlanýyor,
bunun için tipik olarak sigmoid, ya da $tanh$ kullanýlýr.

\includegraphics[width=30em]{rnn_07.jpg}

Bu kavramlar, konumlararasý geçiþ, $t$ anýndaki girdilerin ondan önceki
girdileri nasýl baðlý olduðunun aðýrlýklar üzerinden ayarlanmasý, yani
filtrelenmesi, aslýnda Markov zincirlerine benziyor (diðer bir açýdan
benzemiyor çünkü MZ matematiðinde bir zaman sadece bir öncekinden
etkilenir, RNN durumunda en baþtaki adým en sondakini etkileyebilir
[4]). Bu hesaplar sonucu elde edilen tahmin ve hata geriye yayma
(backpropagation) ile aðýrlýk matrislerini deðiþtirmek için kullanýlacak.

RNN ismindeki ``tekrarlanma'' $U,V,W$'nin her zaman adýmý için ayný
olmasýndan geliyor. Að bir bakýma tek bir seviye için, bir kez
tanýmlanýyor, ve geriye ne kadar gidileceði üzerinden o yöne doðru
kopyalanýyor, ya da ``açýlýyor (unfolding)''.  Bu açýlma iþlemini her zaman
adýmý için gösterebiliriz.  f Zaman Ýçinde Geriye Doðru Yayýlma
(Backpropagation Through Time -BPTT-)

RNN Çesitleri, Numaralarý

Ýlla her biriminin çýktýsýný kullanmak gerekmez. Her RNN biriminin $h$
formundaki gizli katman çýktýsý bir sonraki birime girdi kabul edildiði
için bu çýktýlar RNN'nin bütününü etkilerler, fakat etiket / düzeltme
baðlamýnda direk kullanýlmalarý þart deðildir. Mesela bir RNN'e bir veri
dizisi verip, çýktýlarýnýn en sonuncusu hariç geri kalanlarýný yok
sayabiliriz, o zaman bir dizi-vektör RNN'i elde ederiz. Ya da RNN'e bir
film hakkýndaki tüm yorumlarý kelime kelime veri dizisi olarak verebiliriz,
RNN'in tek çýktýsý -1/+1 þeklinde beðendi / beðenmedi skoru olabilir, bu
ünlü hissiyat analizi (sentiment analyis) örneðidir. Altta diyagramý
görüyoruz,

\includegraphics[width=15em]{rnn_05.png}

Eðer zaman serisi tahmin etmek istiyorsak $x_1,x_2,..$ serisini alýp bir
kaydýrarak ona karþýlýk olan ``etiketleri'' kendimiz yaratabiliriz. Bu
durumda $y_1,y_2,..$ serisi $x_2,x_3,..$ serisi haline gelir.

Bir RNN'in tekrar eden kýsým bir nöron yerine bir katman da olabilir, yani
bu katman içinde birden fazla nöron olur, ve bu katman zamanda geriye doðru
kopyalanýr. 

\includegraphics[width=24em]{rnn_06.png}

Eðer tüm çeþitleri göstermek gerekirse

\includegraphics[width=35em]{rnn_08.png}

Bire bir (one to one) çok basit, diðerleri bire çok (one to many), çoka bir
(many to one), ve iki çeþit çoka çok (many to many). RNN yapýsýndaki tek
sýnýr herhalde tekrarlanan hücrelerden daha fazla girdi hücresi olamayacak
olmasý, fakat onun haricinde neredeyse her tür olasýlýk mümkün. Mesela
soldan 4. örnekte üç boyutlu bir girdi sadece ilk üç tekrarlanan hücreye
veriliyor, geri kalanlara bir þey verilmiyor, çýktý ise son üç hücreden
alýnýyor.

Þimdi TensorFlow ile en basit RNN'yi kendimiz yaratalým. Tekrar eden bir
katman olacak, içinde 5 tane nöron,

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf

def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)

reset_graph()

n_inputs = 3
n_neurons = 5

X0 = tf.placeholder(tf.float32, [None, n_inputs])
X1 = tf.placeholder(tf.float32, [None, n_inputs])

W = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons],dtype=tf.float32))
U = tf.Variable(tf.random_normal(shape=[n_neurons,n_neurons],dtype=tf.float32))
b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))

Y0 = tf.tanh(tf.matmul(X0, W) + b)
Y1 = tf.tanh(tf.matmul(Y0, U) + tf.matmul(X1, W) + b)

init = tf.global_variables_initializer()
\end{minted}

Þimdi iki veri noktasý verip iki veri noktasý çýktýsýna bakalým. Bu 2-2
verisinden pek çok olacak, ki bu veriler ufak toptan setimizi (minibatch)
oluþturacak. Alttaki ufak veride 4 tane o þekilde veri noktasý var.

\begin{minted}[fontsize=\footnotesize]{python}
import numpy as np

# ufak toptan veri
X0_batch = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]]) # t = 0
X1_batch = np.array([[9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]]) # t = 1

with tf.Session() as sess:
    init.run()
    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict={X0: X0_batch, X1: X1_batch})

print 't=0'
print(Y0_val)
print 't=1'
print(Y1_val)
\end{minted}

\begin{verbatim}
t=0
[[-0.0664006   0.96257669  0.68105787  0.70918542 -0.89821595]
 [ 0.9977755  -0.71978885 -0.99657625  0.9673925  -0.99989718]
 [ 0.99999774 -0.99898815 -0.99999893  0.99677622 -0.99999988]
 [ 1.         -1.         -1.         -0.99818915  0.99950868]]
t=1
[[ 1.         -1.         -1.          0.40200216 -1.        ]
 [-0.12210433  0.62805319  0.96718419 -0.99371207 -0.25839335]
 [ 0.99999827 -0.9999994  -0.9999975  -0.85943311 -0.9999879 ]
 [ 0.99928284 -0.99999815 -0.99990582  0.98579615 -0.92205751]]
\end{verbatim}

Dikkat, eðitim yapmadýk, ayrýca çýktý da üretmedik, sadece her basamak için
gizli katmaný hesapladýk, ve RNN'e ileri yönde hesap yaptýrdýk, dört kez
$x_0,x_1$ verdik o da bize dört tane $y_0,y_1$ verdi. RNN'i eðitiyor
olsaydýk üretilen dört $y_0,y_1$ için $V$ ile çarpým, onu ``gerçek''
veriyle / etiketlerle karþýlaþtýrmamýz gerekecekti, onun üzerinden düzeltme
yapacaktýk, vs.

Üstteki kod kolaydý, fakat RNN iki yerine 100 zaman adýmý geriye gitsin
isteseydik, bu çizit çok daha büyük olurdu. O tür kodlarý kolaylaþtýrmak
için TF'in özel çaðrýlarý var, mesela \verb!static_rnn! bunlardan
biri. Üstteki aðý bu þekilde yaratabiliriz,

\begin{minted}[fontsize=\footnotesize]{python}
reset_graph()

X0 = tf.placeholder(tf.float32, [None, n_inputs])
X1 = tf.placeholder(tf.float32, [None, n_inputs])

basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1],
                                                dtype=tf.float32)
Y0, Y1 = output_seqs

print 't=0'
print(Y0_val)
print 't=1'
print(Y1_val)
\end{minted}

\begin{verbatim}
t=0
[[-0.0664006   0.96257669  0.68105787  0.70918542 -0.89821595]
 [ 0.9977755  -0.71978885 -0.99657625  0.9673925  -0.99989718]
 [ 0.99999774 -0.99898815 -0.99999893  0.99677622 -0.99999988]
 [ 1.         -1.         -1.         -0.99818915  0.99950868]]
t=1
[[ 1.         -1.         -1.          0.40200216 -1.        ]
 [-0.12210433  0.62805319  0.96718419 -0.99371207 -0.25839335]
 [ 0.99999827 -0.9999994  -0.9999975  -0.85943311 -0.9999879 ]
 [ 0.99928284 -0.99999815 -0.99990582  0.98579615 -0.92205751]]
\end{verbatim}

YSA'lar kendini tekrarlayan olsun ya da olmasýn aslýnda $f(g(h(x)))$
þeklinde basit içiçe fonksiyondurlar. Klasik YSA'da en sondaki hata
backprop ile aðýrlýklardaki deðiþim girdiler yönünde geriye doðru yayýlýr,
bunu yapmak için $-\frac{\partial E}{\partial w}$ hesaplanýr, böylece tüm
aðýrlýklar hataya yaptýklarý katký (!) baðlamýnda deðiþikliðe uðrarlar,
``düzeltilirler'', yani düzeltme Zincir Kanunu ile dýþ fonksiyonlardan içe
doðru aktarýlmýþ olur.

RNN'de içiçe olma durumu zaman faktöründen kaynaklanýyor, fonksiyonlar
önceki zaman dilimleri baðlamýnda içiçe geçmiþ durumdadýrlar, çünkü bir $t$
anýndaki tahmin önceki dilimlerdeki fonksiyonlarýn sonucudur, bir
geribesleme durumu vardýr, her gizli konum $h_t$ sadece bir önceki
$h_{t-1}$ deðil ondan önceki tüm gizli konumlardan da etkilenir. O zaman
eðitimin bunu gözönüne almasý gerekir.

Dikkat: RNN'lerde içiçe geçen fonksiyonlar sebebiyle hatalar ya çok büyüyüp
ya da çok küçülebiliyor, normal derin YSA'lerde de problem olabilir bu,
fakat RNN'lerde bu durum daha belirgin çünkü N adým geriye gitmek demek N
kadar içiçe geçen fonksiyon demek, ve sýralý veriyi tahmin için N'in büyük
olmasý gerekebilir.

Örnek

Alttaki kodda bir metin okunarak o metindeki harf sýrasý tahmin edilmeye
uðraþýlýyor. Metin tekrar sýfýrdan üretilmeye çabalanýyor. Otomatik türev
(automatic differentiation -AD-) alma ile içiçe geçmiþ fonksiyonlarýn
zincirleme türevinin alýnmasý saðlanýyor, \verb!rnn_predict! hesabý 40
geriye gider, AD tüm bu zinciri takip eder.

\inputminted[fontsize=\footnotesize]{python}{rnn.py}

\begin{minted}[fontsize=\footnotesize]{python}
import autograd.numpy as np
import autograd.numpy.random as npr
from autograd import grad
from autograd.optimizers import adam
import rnn

def build_dataset(filename, sequence_length, alphabet_size, max_lines=-1):
    with open(filename) as f:
        content = f.readlines()
    content = content[:max_lines]
    content = [line for line in content if len(line) > 2]   
    seqs = np.zeros((sequence_length, len(content), alphabet_size))
    for ix, line in enumerate(content):
        padded_line = (line + " " * sequence_length)[:sequence_length]
        seqs[:, ix, :] = string_to_one_hot(padded_line, alphabet_size)
    return seqs

num_chars = 128
text_filename = 'rnn.py'
train_inputs = build_dataset(text_filename, sequence_length=30,
                                 alphabet_size=num_chars, max_lines=60)

init_params = rnn.create_rnn_params(input_size=128, output_size=128,
                                    state_size=40, param_scale=0.01)
                                    
def print_training_prediction(weights):
    print("Training text                         Predicted text")
    logprobs = np.asarray(rnn_predict(weights, train_inputs))
    for t in range(logprobs.shape[1]):
        training_text  = one_hot_to_string(train_inputs[:,t,:])
        predicted_text = rnn.one_hot_to_string(logprobs[:,t,:])
        print(training_text.replace('\n', ' ') + "|" +
              predicted_text.replace('\n', ' '))

def callback(weights, iter, gradient):
    if iter % 10 == 0:
        print("Iteration", iter, "Train loss:", training_loss(weights, 0))
        #print_training_prediction(weights)

# Build gradient of loss function using autograd.
training_loss_grad = grad(training_loss)

print("Training RNN...")
trained_params = adam(training_loss_grad, init_params, step_size=0.1,
                      num_iters=280, callback=callback)
\end{minted}

\begin{verbatim}
Training RNN...
('Iteration', 0, 'Train loss:', 4.854500980126768)
('Iteration', 10, 'Train loss:', 3.069896973468059)
('Iteration', 20, 'Train loss:', 2.9564946588218)
('Iteration', 30, 'Train loss:', 2.590610887049078)
('Iteration', 40, 'Train loss:', 2.3255385285729027)
('Iteration', 50, 'Train loss:', 2.1211122619024696)
('Iteration', 60, 'Train loss:', 1.9691676257416404)
('Iteration', 70, 'Train loss:', 1.8868756780002685)
('Iteration', 80, 'Train loss:', 1.7455098359656291)
('Iteration', 90, 'Train loss:', 1.7750342336507772)
('Iteration', 100, 'Train loss:', 1.6059292555729703)
('Iteration', 110, 'Train loss:', 1.5077116694554635)
('Iteration', 120, 'Train loss:', 1.437485110908115)
('Iteration', 130, 'Train loss:', 1.4504849515039933)
('Iteration', 140, 'Train loss:', 1.3480379515887519)
('Iteration', 150, 'Train loss:', 1.4083643059429929)
('Iteration', 160, 'Train loss:', 1.2655987546227996)
('Iteration', 170, 'Train loss:', 1.2051278365327054)
('Iteration', 180, 'Train loss:', 1.1561998913079512)
('Iteration', 190, 'Train loss:', 1.1814640952544757)
('Iteration', 200, 'Train loss:', 1.3673188298901471)
('Iteration', 210, 'Train loss:', 1.1591863193874781)
('Iteration', 220, 'Train loss:', 1.056688128805028)
('Iteration', 230, 'Train loss:', 1.0465201536978259)
('Iteration', 240, 'Train loss:', 1.0373081053464259)
('Iteration', 250, 'Train loss:', 1.3591698106017474)
('Iteration', 260, 'Train loss:', 1.1556108786809474)
('Iteration', 270, 'Train loss:', 1.0323757883394502)
\end{verbatim}

Üretmek / eðitim için \verb!rnn.py! kodunun kendisi kullanýldý.

\begin{minted}[fontsize=\footnotesize]{python}
num_letters = 30
for t in range(20):
    text = ""
    for i in range(num_letters):
        seqs = rnn.string_to_one_hot(text, num_chars)[:, np.newaxis, :]
        logprobs = rnn.rnn_predict(trained_params, seqs)[-1].ravel()
        text += chr(npr.choice(len(logprobs), p=np.exp(logprobs)))
    print(text)
\end{minted}

\begin{verbatim}
    rs.lepugnunpdit - cenedili
def p.rnns, logs'ininpum, hid_
    ngan rrad_ti, feturn ns.sc
def catorrtar t  aut_re_strad.
            hiddens_numumut in
def hiddes = rhiddensdord.rato
    return ncan((ponddens_tram
ders minpperen(scnt_strt onut_
    returnts, utete, jIoutdati
    return oute_sthorn(pund_om
dershgline nigms, conteme_sco_
    return 0.5*(natorad.mincde
    contse, hiddens_mihrra opu
    [cran_put inhgto_tut= retu
  # Ite, wItre =        retat 
    ashis_lik[enutnoncam(nthen
      oonname, jItogput_pre_sp
    cet(fiddens = lot led_ome_
def rinnam_idt(parddens_nnd(on
    rs.leteqslind_tat  = [hipp
\end{verbatim}

Fena deðil; \verb!def! ile baþlanan satýr ardýndan sonraki satýr tab ile
boþluk býraktý, bunlar kolay þeyler deðil.  Altta karþýlaþtýrma amaçlý
olarak sadece frekans sayarak üretim yapan bir kod görüyoruz. O da fena
deðil, bu konu hakkýnda daha fazla detay için [2].

\begin{minted}[fontsize=\footnotesize]{python}
f = "../../stat/stat_naive/data/a1.txt"
print open(f).read()[:300]
\end{minted}

\begin{verbatim}
A well-known scientist (some say it was Bertrand Russell) once gave a
public lecture on astronomy. He described how the earth orbits around
the sun and how the sun, in turn, orbits around the center of a vast
collection of stars called our galaxy. At the end of the lecture, a
little old lady at the 
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
import lm
lmm = lm.train_char_lm(f, order=4)
res = lm.generate_text(lmm, 4)
print res[:400]
\end{minted}

\begin{verbatim}
A well-know
better? What the moon were caused by Ptolemy in more the picture only late the Greeks even had
been elongstanding that the sky what eclipses rather ridiculous, but why do we know about to someone looking the sun and the earth Star
lies one looking the North orbiting questimate thought spheres the superior smile before think we go back of the really a flat plater see then? What disk, th
\end{verbatim}


\begin{minted}[fontsize=\footnotesize]{python}
print lmm.keys()[:10]
print lmm.get('pla')
\end{minted}

\begin{verbatim}
['t w', 'Fir', 'all', 't t', 'sci', 'rom', 'ron', 'roo', 'thi', 'oss']
[('t', 0.5), ('n', 0.5)]
\end{verbatim}

Zaman Serisi Tahmini

Tensorflow ile zaman serisi tahmini yapalým. Eðitim verisinin formülünü
bildiðimiz bir sinüs eðrisinden alacaðýz, sanki eðriyi bilmiyormuþ gibi
yapalým, bu eðriyi sadece üretilen veriye bakarak ``öðreneceðiz''. Eðrinin
rasgele kýsýmlarýndan toptan veri parçalarý üretmek için bir çaðrý yazalým,

\begin{minted}[fontsize=\footnotesize]{python}
np.random.seed(1)

t_min, t_max = 0, 30
resolution = 0.1

def f(t):
    return t * np.sin(t) / 3 + 2 * np.sin(t*5)

def next_batch(batch_size, n_steps):
    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)
    Ts = t0 + np.arange(0., n_steps + 1) * resolution
    ys = f(Ts)
    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)
\end{minted}

Eðitim verisi zaman serisinin ufak bir parçasý, ve hedef verisi onun bir
ileri kaydýrýlmýþ hali. 

Zaman serisinin rasgele þekilde nasýl örneklendiðini göstermek için üstteki
çaðrý içindeki örnekleme kodunun benzerini altta tekrarlayalým ve
grafikleyelim. Kýrmýzý noktalar örneklenen veri noktalarý,

\begin{minted}[fontsize=\footnotesize]{python}
batch_size = 5
n_steps = 3
t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)
Ts = t0 + np.arange(0., n_steps+1) * resolution
ys = f(Ts)
print ys
\end{minted}

\begin{verbatim}
[[-2.3141651  -1.12233854  0.27200624  1.6253068 ]
 [ 4.31878159  4.63599743  4.61528818  4.112844  ]
 [ 0.03397153  0.99207952  1.71474727  2.02731967]
 [ 2.87376693  3.00044595  2.62695206  1.77847377]
 [-0.96970753 -2.03368272 -2.93926439 -3.47887785]]
\end{verbatim}

Grafiklersek,

\begin{minted}[fontsize=\footnotesize]{python}
t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))
y = f(t)
plt.plot(t,y)
plt.plot(Ts,ys,'r.')
plt.savefig('rnn_03.png')
\end{minted}

\includegraphics[width=20em]{rnn_03.png}

Öðrenme amacýyla daha büyük adým, iç nöron sayýsý tanýmlayalým,

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf

def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)

reset_graph()

n_steps = 30
n_inputs = 1
n_neurons = 200
n_outputs = 1

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])

cell = tf.contrib.rnn.OutputProjectionWrapper(
    tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),
    output_size=n_outputs)

outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)

learning_rate = 0.01

loss = tf.reduce_mean(tf.square(outputs - y)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()

n_iterations = 300
batch_size = 50

sess = tf.Session()

sess.run(tf.global_variables_initializer())

for iteration in range(n_iterations):
    X_batch, y_batch = next_batch(batch_size, n_steps)
    sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
    if iteration % 100 == 0:
        mse = loss.eval(feed_dict={X: X_batch, y: y_batch}, session=sess)
        print(iteration, "\tMSE:", mse)

\end{minted}

\begin{verbatim}
(0, '\tMSE:', 280.63486)
(100, '\tMSE:', 0.089582793)
(200, '\tMSE:', 0.044634577)
\end{verbatim}

Öðrenme tamamlandý ve MSE hata raporu fena deðil. Þimdi bu RNN'i hiç
görmediðimiz geleceði tahmin için kullanalým, \verb!n_more! adým ilerisini
tahmin edeceðiz, yanlýz \verb!n_steps! zaman serisi kaydýrýlmýþ
\verb!n_steps! ilerisini tahmin için kullanýlýyordu. Biz tahminleri
üretirken tahmin bloðunun sadece en sondaki öðesini alacaðýz. Sonra bu
ögeyi kaynak veriye dahil edip bir gelecek bloðu daha üreteceðiz (onun da
son öðesini alacaðýz, vs), ve böyle gide gide \verb!n_more! kadar geleceði
bitiþtirmiþ olacaðýz. 

\begin{minted}[fontsize=\footnotesize]{python}
n_more = 40

t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))
y = f(t)

newx = list(t[-n_steps:])
newy = list(y[-n_steps:])
for i in range(n_more): # bu kadar daha uret
   tst_input = np.array(newy[-n_steps:]).reshape(1,n_steps,1) 
   res = sess.run(outputs, feed_dict={X: tst_input})
   newy.append(res[0][0][0])
   newx.append(t_max + (i*resolution))

plt.plot(t,y)
plt.plot(newx[n_steps-1:],newy[n_steps-1:],'g')

plt.savefig('rnn_04.png')
\end{minted}

\includegraphics[width=20em]{rnn_04.png}

Yeþil renkli tahmin bölümü. 

Problemler

RNN problemlerinden biri þerisel olarak modellenen verinin að yapýsýnda
geriye doðru giderken eðitim sýrasýnda yokolan gradyan (vanishing gradient)
problemine sebep olabilmesi. Çünkü YSA yatay olarak derin, ve gradyan
geriye doðru yayýlma yaparken sayýsal olarak problemlere yol
açabiliyor. Çözümler adým sayýsýný azaltmak olabilir, ya da bu problemlerin
bazýlarýný düzelten LSTM kullanmak olabilir.

Kaynaklar

[1] {\em A Beginner's Guide to Recurrent Networks and LSTMs}, 
    \url{https://deeplearning4j.org/lstm#a-beginners-guide-to-recurrent-networks-and-lstms}

[2] Bayramlý, 
    {\em Derin Öðrenim ile Text Üretmek, RNN, LSTM}, 
    \url{https://burakbayramli.github.io/dersblog/sk/2017/01/derin-ogrenim-ile-text-uretmek-rnn-lstm.html}

[3] Britz, {\em Recurrent Neural Networks Tutorial, Part 1}, \url{http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/}

[4] Lipton, {\em A Critical Review of Recurrent Neural Networks for Sequence Learning},\url{https://arxiv.org/abs/1506.00019}


\end{document}
