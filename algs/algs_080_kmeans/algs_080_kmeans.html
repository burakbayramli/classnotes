<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>K-Means Kümeleme Metodu</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="k-means-kümeleme-metodu">K-Means Kümeleme Metodu</h1>
<p>Popüler kümeleme algoritmalarından biri k-means algoritması. Bu metotta kaç tane kümenin olması gerektiği baştan tanımlanır (<span class="math inline">\(k\)</span> parametresi ile), algoritma bunu kendisi bulmaz. Metotun geri kalanı basit - bir döngü (iteration) içinde her basamakta:</p>
<ol style="list-style-type: decimal">
<li><p>Her nokta için, eldeki küme merkezleri teker teker kontrol edilir ve o nokta en yakın olan kümeye atanır.</p></li>
<li><p>Atamalar tamamlandıktan sonra her küme içinde hangi noktaların olduğu bilindiği için her kümedeki noktaların ortalaması alınarak yeni küme merkezleri hesaplanır. Eski merkez hesapları atılır.</p></li>
<li><p>Başa dönülür. Döngü tekrar ilk adıma döndüğünde, bu sefer yeni küme merkezleri kullanılarak aynı adımlar tekrarlanacaktır.</p></li>
</ol>
<p>Fakat bir problem yok mu? Daha birinci döngü başlamadan küme merkezlerinin nerede olduğunu nereden bileceğiz? Burada bir tavuk-yumurta problemi var, küme merkezleri olmadan noktaları atayamayız, atama olmadan küme merkezlerini hesaplayamayız.</p>
<p>Bu probleme pratik bir çözüm ilk başta küme merkezlerini (ya da küme atamalarını) rasgele bir şekilde seçmektir. Pratikte bu yöntem çok iyi işliyor. Tabii bu rasgelelik yüzünden K-means'in doğru sonuca yaklaşması (convergence) garanti değildir, ama gerçek dünya uygulamalarında çoğunlukla kullanışlı kümeler bulunur. Bu potansiyel problemlerden kaçınmak için k-means pek çok kez işletilebilir (her seferinde yeni rasgele başlangıçlarla yani) ve aynı sonuca ulaşılıp ulaşılmadığı kontrol edilebilir.</p>
<p>Pek en iyi k nasıl bulunur? SVD kullanarak grafiğe bakmak (bu yazının sonunda anlatılıyor) mesela, fakat en iyisi K-Means yerine GMM kulanmak! Bkz. yazı sonundaki referans.</p>
<p>K-Means EM algoritmasının bir türevi olarak kabul edilebilir, EM kümeleri bir Gaussian (ya da Gaussian karışımı) gibi görür, ve her basamakta bu dağılımların merkezini, hem de kovaryansını hesaplar. Yani kümenin &quot;şekli&quot; de EM tarafından saptanır. Ayrıca EM her noktanın tüm kümelere olan üyeliklerini &quot;hafif (soft)&quot; olarak hesaplar (bir olasılık ölçütü üzerinden), fakat K-Means için bu atama nihai (hard membership). Nokta ya bir kümeye aittir, ya da değildir.</p>
<p>EM'in belli şartlarda yaklaşıksallığı için matematiksel ispat var. K-Means akıllı tahmin yaparak (heuristic) çalışan bir algoritma olarak biliniyor. Sonuca yaklaşması bu sebeple garanti değildir, ama daha önce belirttiğimiz gibi pratikte faydalıdır. Bir sürü alternatif kümeleme yöntemi olmasına rağmen hala K-Means kullanışlı. Burada bir etken de K-Means'in çok rahat paralelize edilebilmesi. Bu konu başka bir yazıda işlenecek.</p>
<p>Örnek test verisi altta</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
data <span class="op">=</span> pd.read_csv(<span class="st">&quot;synthetic.txt&quot;</span>,names<span class="op">=</span>[<span class="st">&#39;a&#39;</span>,<span class="st">&#39;b&#39;</span>],sep<span class="op">=</span><span class="st">&quot;   &quot;</span>)
<span class="bu">print</span> data.shape
data <span class="op">=</span> np.array(data)</code></pre></div>
<pre><code>(3000, 2)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.scatter(data[:,<span class="dv">0</span>],data[:,<span class="dv">1</span>])
plt.savefig(<span class="st">&#39;kmeans_1.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="kmeans_1.png" />

</div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> euc_to_clusters(x,y):
    <span class="cf">return</span> np.sqrt(np.<span class="bu">sum</span>((x<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>))

<span class="kw">class</span> KMeans():
    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,n_clusters,n_iter<span class="op">=</span><span class="dv">10</span>):
        <span class="va">self</span>.k <span class="op">=</span> n_clusters
        <span class="va">self</span>.<span class="bu">iter</span> <span class="op">=</span> n_iter
    <span class="kw">def</span> fit(<span class="va">self</span>,X):
        <span class="co"># her veri noktasi icin rasgele kume merkezi ata</span>
        labels <span class="op">=</span> [random.randint(<span class="dv">0</span>,<span class="va">self</span>.k<span class="dv">-1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">0</span>])]
        <span class="va">self</span>.labels_ <span class="op">=</span> np.array(labels)
        <span class="va">self</span>.centers_ <span class="op">=</span> np.zeros((<span class="va">self</span>.k,X.shape[<span class="dv">1</span>]))
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.<span class="bu">iter</span>):
            <span class="co"># yeni kume merkezleri uret</span>
            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.k):
                <span class="co"># eger kume j icinde hic nokta yoksa, ortalama (mean)</span>
                <span class="co"># hesabi yapma, cunku o zaman nan degeri geliyor, ve</span>
                <span class="co"># hesabin geri kalani bozuluyor.</span>
                <span class="cf">if</span> <span class="bu">len</span>(X[<span class="va">self</span>.labels_ <span class="op">==</span> j]) <span class="op">==</span> <span class="dv">0</span>: <span class="cf">continue</span>
                center <span class="op">=</span> np.mean(X[<span class="va">self</span>.labels_ <span class="op">==</span> j],axis<span class="op">=</span><span class="dv">0</span>)
                <span class="va">self</span>.centers_[j,:] <span class="op">=</span> center
            <span class="co"># her nokta icin kume merkezlerine gore kume atamasi yap</span>
            <span class="va">self</span>.labels_ <span class="op">=</span> []
            <span class="cf">for</span> point <span class="kw">in</span> X:
                c <span class="op">=</span> np.argmin(euc_to_clusters(<span class="va">self</span>.centers_, point))
                <span class="va">self</span>.labels_.append(<span class="bu">int</span>(c))

            <span class="va">self</span>.labels_ <span class="op">=</span> np.array(<span class="va">self</span>.labels_)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">cf <span class="op">=</span> KMeans(k<span class="op">=</span><span class="dv">5</span>,<span class="bu">iter</span><span class="op">=</span><span class="dv">20</span>)
cf.fit(data)
<span class="bu">print</span> cf.labels_</code></pre></div>
<pre><code>[3 3 3 ..., 2 2 2]</code></pre>
<p>Üstteki sonucun içinde iki ana vektör var, bu vektörlerden birincisi içinde 2,0, gibi sayılar görülüyor, bu sayılar her noktaya tekabül eden küme atamaları. İkinci vektör içinde iki boyutlu <span class="math inline">\(k\)</span> tane vektör var, bu vektörler de her kümenin merkez noktası. Merkez noktalarını ham veri üzerinde grafiklersek (kırmızı noktalar)</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.scatter(data[:,<span class="dv">0</span>],data[:,<span class="dv">1</span>])
plt.hold(<span class="va">True</span>)
plt.ylim([<span class="dv">30000</span>,<span class="dv">70000</span>])
<span class="cf">for</span> x <span class="kw">in</span> cf.centers_: plt.plot(x[<span class="dv">0</span>],x[<span class="dv">1</span>],<span class="st">&#39;rd&#39;</span>)
plt.savefig(<span class="st">&#39;kmeans_2.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="kmeans_2.png" />

</div>
<p>Görüldüğü gibi 5 tane küme için üstteki merkezler bulundu. Fena değil. Eğer 10 dersek</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">cf <span class="op">=</span> KMeans(k<span class="op">=</span><span class="dv">10</span>,<span class="bu">iter</span><span class="op">=</span><span class="dv">30</span>)
cf.fit(data)
plt.scatter(data[:,<span class="dv">0</span>],data[:,<span class="dv">1</span>])
plt.ylim([<span class="dv">30000</span>,<span class="dv">70000</span>])
plt.hold(<span class="va">True</span>)
<span class="cf">for</span> x <span class="kw">in</span> cf.centers_: plt.plot(x[<span class="dv">0</span>],x[<span class="dv">1</span>],<span class="st">&#39;rd&#39;</span>)
plt.savefig(<span class="st">&#39;kmeans_3.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="kmeans_3.png" />

</div>
<p>Kategorik ve Sayısal Öğeler İçeren Karışık Veriler</p>
<p>Bazen verimiz hem kategorik hem de sayısal (numeric) değerler içeriyor olabilir, KMeans yeni küme merkezlerini hesaplarken ortalama operasyonu kullandığı için sadece sayısal veriler üzerinde çalışabilir (kategorik verilerin nasıl ortalamasını alalım ki?). Bu durumda ne yapacağız?</p>
<p>Bir seçenek şu olabilir, kategorik her kolonu her değişik değeri bir yeni kolona tekabül edecek şekilde sağa doğru açarız, ve o değerin yeni kolonuna 1 değeri diğerlerine 0 değeri veririz. Bu kodlamaya 1-in-q kodlaması, 1-in-n kodlaması, ya da İngilizce 1-hot encoding ismi veriliyor.</p>
<p>Örnek olarak UCI veri bankasından Avustralya Kredi Verisine bakalım:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
df <span class="op">=</span> pd.read_csv(<span class="st">&quot;crx.csv&quot;</span>)
<span class="bu">print</span> df[:<span class="dv">2</span>]</code></pre></div>
<pre><code>  A1     A2    A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 A16
0  b  30.83  0.00  u  g  w  v  1.25  t   t    1   f   g  00202    0   +
1  a  58.67  4.46  u  g  q  h  3.04  t   t    6   f   g  00043  560   +</code></pre>
<p>Bu veride A1, A2, gibi kolon isimleri var, kategorik olanlarda 'g','w' gibi değerler görülüyor. Bu kolonları değiştirmek için</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.feature_extraction <span class="im">import</span> DictVectorizer
<span class="kw">def</span> one_hot_dataframe(data, cols):
    vec <span class="op">=</span> DictVectorizer()
    mkdict <span class="op">=</span> <span class="kw">lambda</span> row: <span class="bu">dict</span>((col, row[col]) <span class="cf">for</span> col <span class="kw">in</span> cols)
    vecData <span class="op">=</span> pd.DataFrame(vec.fit_transform(data[cols].to_dict(outtype<span class="op">=</span><span class="st">&#39;records&#39;</span>)).toarray())
    vecData.columns <span class="op">=</span> vec.get_feature_names()
    vecData.index <span class="op">=</span> data.index
    data <span class="op">=</span> data.drop(cols, axis<span class="op">=</span><span class="dv">1</span>)
    data <span class="op">=</span> data.join(vecData)
    <span class="cf">return</span> data

df2 <span class="op">=</span> one_hot_dataframe(df,[<span class="st">&#39;A1&#39;</span>,<span class="st">&#39;A4&#39;</span>,<span class="st">&#39;A5&#39;</span>,<span class="st">&#39;A6&#39;</span>,<span class="st">&#39;A7&#39;</span>,<span class="st">&#39;A9&#39;</span>,<span class="st">&#39;A10&#39;</span>,<span class="st">&#39;A12&#39;</span>,<span class="st">&#39;A13&#39;</span>])
<span class="bu">print</span> df2.ix[<span class="dv">0</span>]</code></pre></div>
<pre><code>A2       30.83
A3           0
A8        1.25
A11          1
A14      00202
A15          0
A16          +
A10=f        0
A10=t        1
A12=f        1
A12=t        0
A13=g        1
A13=p        0
A13=s        0
A1=?         0
A1=a         0
A1=b         1
A4=?         0
A4=l         0
A4=u         1
A4=y         0
A5=?         0
A5=g         1
A5=gg        0
A5=p         0
A6=?         0
A6=aa        0
A6=c         0
A6=cc        0
A6=d         0
A6=e         0
A6=ff        0
A6=i         0
A6=j         0
A6=k         0
A6=m         0
A6=q         0
A6=r         0
A6=w         1
A6=x         0
A7=?         0
A7=bb        0
A7=dd        0
A7=ff        0
A7=h         0
A7=j         0
A7=n         0
A7=o         0
A7=v         1
A7=z         0
A9=f         0
A9=t         1
Name: 0, Length: 52, dtype: object</code></pre>
<p>İşlem sonucunda A12=f mesela için 1 verilmiş, ama A12=t (ve diğer her mümkün değer için yani) 0 değeri verilmiş (sadece bu tek satır için). Böylece kategorik veriyi sayısal hale çevirmiş olduk.</p>
<p>Fakat işimiz bitti mi? Hayır. Şimdi KMeans bu tür veriyle acaba düzgün çalışır mıydı onu kendimize soralım. İçinde pek çok 0, bazen 1 içeren veri satırları arasında uzaklık hesabı yapmak ise yarar mı?</p>
<p>Yapay Öğrenim literatüründe bu tür veriler üzerinde kosinüs benzerliği (cosine similarity) kullanmak daha yaygındır. Bu konuyu {} yazısında daha iyi görülebilir. Kosinüs benzerliği bize 0 ile 1 arasında bir değer döndürür. Benzerliği uzaklığa çevirmek için basit bir şekilde 1-benzerlik formülünü kullanabiliriz. O zaman şöyle bir çözüm kullanabilir: normal sayısal değerler için Öklitsel, kategorik 1-hot kodlanmış kolonlar için Kosinüs uzaklığı kullanılır, bu uzaklıklar bazı ağırlıklar üzerinden birleştirilir, ve KMeans bu uzaklık ile iş yapar. Teknik olarak imkansız değil; KMeans merkez bulmak için ortalama alır ve Kosinüs uzaklığının verdiği aradaki açı, ortalama alma işlemi ile uyumludur. Yani içinde hem Öklitsel hem 1-hot kodlanmış verilerin olduğu vektörlerin ortalamasını alabiliriz, demek ki KMeans işleyebilir.</p>
<div class="figure">
<img src="kmeans_5.jpg" />

</div>
<p>Problem şudur, iki uzaklığı birleştiren ağırlıklar ne olmalıdır? Bu yöntemi denediğimizde bu ağırlıkların ne seçildiğinin çok önemli olduğunu farkettik, ve kümeleme gibi denetimsiz (unsupervised) bir yöntemde bu hiperparametreleri deneme / yanılma yöntemi ile bulma şansımız yoktur.</p>
<p>Bu durumda kullanılabilecek bir yöntem şudur: SVD kullanarak tüm matrisi azaltmak ve onun üzerinde pür Öklitsel uzaklıklar kullanmak. Sayısal ve kategorik karışık verileri içeren verileri kümelemek için tavsiye edilen yöntem şudur:</p>
<ol style="list-style-type: decimal">
<li><p>Kategorik veriler üzerinde 1-hot kodlama yap.</p></li>
<li><p>Önce kolonları sonra satırları normalize et.</p></li>
<li><p>Tüm matris üzerinde çok küçük olmayan bir <span class="math inline">\(k\)</span> ile SVD al (mesela alttaki veri seti için önce 10)</p></li>
<li><p><span class="math inline">\(S\)</span> vektörüne bak, ortalamadan büyük olan kaç tane hücre olduğunu gör.</p></li>
<li><p>Bu sayı yeni <span class="math inline">\(k\)</span> değerimiz olacak, SVD'yi tekrar bu <span class="math inline">\(k\)</span> ile işlet.</p></li>
<li><p>Elde edilen <span class="math inline">\(U\)</span> üzerinde kümeleme yap,</p></li>
</ol>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> normalize
<span class="im">import</span> scipy.sparse.linalg <span class="im">as</span> slin
<span class="im">import</span> scipy.linalg <span class="im">as</span> lin
<span class="im">import</span> pandas <span class="im">as</span> pd

df <span class="op">=</span> pd.read_csv(<span class="st">&quot;crx.csv&quot;</span>,sep<span class="op">=</span><span class="st">&#39;,&#39;</span>,na_values<span class="op">=</span>[<span class="st">&#39;?&#39;</span>])
df <span class="op">=</span> df.dropna()

df[<span class="st">&#39;A16&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;A16&#39;</span>].<span class="bu">str</span>.replace(<span class="st">&#39;+&#39;</span>,<span class="st">&#39;1&#39;</span>)
df[<span class="st">&#39;A16&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;A16&#39;</span>].<span class="bu">str</span>.replace(<span class="st">&#39;-&#39;</span>,<span class="st">&#39;0&#39;</span>)
df[<span class="st">&#39;A16&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;A16&#39;</span>].astype(<span class="bu">int</span>)

df2 <span class="op">=</span> one_hot_dataframe(df,[<span class="st">&#39;A1&#39;</span>,<span class="st">&#39;A4&#39;</span>,<span class="st">&#39;A5&#39;</span>,<span class="st">&#39;A6&#39;</span>,<span class="st">&#39;A7&#39;</span>,<span class="st">&#39;A9&#39;</span>,<span class="st">&#39;A10&#39;</span>,<span class="st">&#39;A12&#39;</span>,<span class="st">&#39;A13&#39;</span>])
df2 <span class="op">=</span> df2.drop(<span class="st">&#39;A16&#39;</span>,axis<span class="op">=</span><span class="dv">1</span>)
df2 <span class="op">=</span> np.array(df2)
df3 <span class="op">=</span> df2.copy()
df3 <span class="op">=</span> normalize(df3, norm<span class="op">=</span><span class="st">&#39;l2&#39;</span>, axis<span class="op">=</span><span class="dv">0</span>)
df3 <span class="op">=</span> normalize(df3, norm<span class="op">=</span><span class="st">&#39;l2&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)

u,s,v<span class="op">=</span>slin.svds(df3,k<span class="op">=</span><span class="dv">10</span>)
<span class="bu">print</span> s</code></pre></div>
<pre><code>[  4.45826083   4.49654025   4.68382638   4.93391665   4.98604314
   5.153349     5.63521289   5.70490968   6.68558115  14.81145675]</code></pre>
<p>Bakıyoruz, averajdan yüksek olan en büyük sadece iki kolon var. SVD literatüründe bu kolonların matrisin &quot;enerjisini'' içerdiği söylenir, hakikaten eğer SVD ayrıştırma sonrası bu ilk kolona bu kadar önem verdiyse, onlar önemli, &quot;enerjiyi içeriyor'' olmalıdırlar. Şimdi SVD'yi <span class="math inline">\(k=2\)</span> ile tekrar işletiyoruz,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">u,s,v<span class="op">=</span>slin.svds(df3,k<span class="op">=</span><span class="dv">2</span>)
<span class="bu">print</span> s</code></pre></div>
<pre><code>[  6.68558115  14.81145675]</code></pre>
<p>Şimdi <span class="math inline">\(U\)</span> üzerinde kümeleme yapacağız, ve kontrol için kenara koyduğumuz bilinen etiketler üzerinden kümeleme başarımızı ölçeceğiz. Avustralya Kredi Verisi aslında izlenen (supervised) algoritmalar için kullanılır, ama biz onu izlenmeyen kümeleme problemi için kullandık, bilinen etiketleri veri içinden çıkartıp bir kenara koyuyoruz, ve sonra kümeleme tahmini yaparak bu etiketlerle olan uyumu ölçüyoruz.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">clf <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)
clf.fit(u)
labels_true <span class="op">=</span> np.array(df[<span class="st">&#39;A16&#39;</span>])
labels_pred <span class="op">=</span> clf.labels_
match <span class="op">=</span> np.<span class="bu">sum</span>((labels_true <span class="op">==</span> labels_pred).astype(<span class="bu">int</span>))
<span class="bu">print</span> <span class="bu">float</span>(match)<span class="op">/</span><span class="bu">len</span>(df), <span class="dv">1</span><span class="op">-</span><span class="bu">float</span>(match)<span class="op">/</span><span class="bu">len</span>(df)</code></pre></div>
<pre><code>0.217457886677 0.782542113323</code></pre>
<p>Başarı yüzde %78. Çok iyi. Üstteki örnek küme sayısının (dikkat SVD <span class="math inline">\(k\)</span>'sinden farklı) bilindiğini farz etti. Bazı durumlarda küme sayısını grafiksel olarak görmek mümkündür (ama en iyisi Gaussian Karışım Modeli kullanıp mümkün K'leri AIC ile test etmek, bkz {} yazısı).</p>
<p>Mesela üstteki veri seti için ortalamayı çıkartıp varyansa bölersek ve SVD işletirsek en büyük iki <span class="math inline">\(U\)</span> kolonun grafiği alttaki gibi çıkıyor,</p>
<div class="figure">
<img src="kmeans_4.png" />

</div>
<p>Eğer rasgele yansıtma (random projection) kullansaydık ne olurdu? Bu işlemi birkaç kez yapalım ki rasgele matris <code>Omega</code> değişik şekillerde (ama hala rasgele) üretilince sonuç değişir miydi görelim.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy.random <span class="im">as</span> rand
<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):
     Omega <span class="op">=</span> rand.randn(df3.shape[<span class="dv">1</span>],<span class="dv">30</span>)
     u <span class="op">=</span> np.dot(df3,Omega)
     clf <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)
     clf.fit(u)
     labels_true <span class="op">=</span> np.array(df[<span class="st">&#39;A16&#39;</span>])
     labels_pred <span class="op">=</span> clf.labels_
     match <span class="op">=</span> np.<span class="bu">sum</span>((labels_true <span class="op">==</span> labels_pred).astype(<span class="bu">int</span>))
     <span class="bu">print</span> <span class="bu">float</span>(match)<span class="op">/</span><span class="bu">len</span>(df), <span class="dv">1</span><span class="op">-</span><span class="bu">float</span>(match)<span class="op">/</span><span class="bu">len</span>(df)</code></pre></div>
<pre><code>0.436447166922 0.563552833078
0.258805513017 0.741194486983
0.367534456355 0.632465543645
0.390505359877 0.609494640123
0.456355283308 0.543644716692</code></pre>
<p>Görüldüğü gibi bazen çok iyi sonuçlar alıyor olsak bile bazen çok kötü sonuçlar da alabiliyoruz. Demek ki bu veri setinde SVD tekniği daha başarılı.</p>
<p>Kaynaklar</p>
<p>[1] Corrada, <em>Practicum: Kernelized K-means</em>, <a href="nbviewer.ipython.org/url/cbcb.umd.edu/~hcorrada/PML/src/kmeans.ipynb" class="uri">nbviewer.ipython.org/url/cbcb.umd.edu/~hcorrada/PML/src/kmeans.ipynb</a></p>
<p>[2] UCI Machine Learning Repository, <em>Statlog (Australian Credit Approval) Data Set </em>, <a href="https://archive.ics.uci.edu/ml/datasets/Statlog+%28Australian+Credit+Approval%29">https://archive.ics.uci.edu/ml/datasets/Statlog+%28Australian+Credit+Approval%29</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
