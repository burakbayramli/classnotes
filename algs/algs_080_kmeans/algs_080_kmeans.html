<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>K-Means Kümeleme Metodu</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="k-means-kümeleme-metodu">K-Means Kümeleme Metodu</h1>
<p>Popüler kümeleme algoritmalarından biri k-means algoritması. Bu
metotta kaç tane kümenin olması gerektiği baştan tanımlanır (<span
class="math inline">\(k\)</span> parametresi ile), algoritma bunu
kendisi bulmaz. Metotun geri kalanı basit - bir döngü (iteration) içinde
her basamakta:</p>
<ol type="1">
<li><p>Her nokta için, eldeki küme merkezleri teker teker kontrol edilir
ve o nokta en yakın olan kümeye atanır.</p></li>
<li><p>Atamalar tamamlandıktan sonra her küme içinde hangi noktaların
olduğu bilindiği için her kümedeki noktaların ortalaması alınarak yeni
küme merkezleri hesaplanır. Eski merkez hesapları atılır.</p></li>
<li><p>Başa dönülür. Döngü tekrar ilk adıma döndüğünde, bu sefer yeni
küme merkezleri kullanılarak aynı adımlar tekrarlanacaktır.</p></li>
</ol>
<p>Fakat bir problem yok mu? Daha birinci döngü başlamadan küme
merkezlerinin nerede olduğunu nereden bileceğiz? Burada bir
tavuk-yumurta problemi var, küme merkezleri olmadan noktaları
atayamayız, atama olmadan küme merkezlerini hesaplayamayız.</p>
<p>Bu probleme pratik bir çözüm ilk başta küme merkezlerini (ya da küme
atamalarını) rasgele bir şekilde seçmektir. Pratikte bu yöntem çok iyi
işliyor. Tabii bu rasgelelik yüzünden K-means’in doğru sonuca yaklaşması
(convergence) garanti değildir, ama gerçek dünya uygulamalarında
çoğunlukla kullanışlı kümeler bulunur. Bu potansiyel problemlerden
kaçınmak için k-means pek çok kez işletilebilir (her seferinde yeni
rasgele başlangıçlarla yani) ve aynı sonuca ulaşılıp ulaşılmadığı
kontrol edilebilir.</p>
<p>Pek en iyi k nasıl bulunur? SVD kullanarak grafiğe bakmak (bu yazının
sonunda anlatılıyor) mesela, fakat en iyisi K-Means yerine GMM kulanmak!
Bkz. yazı sonundaki referans.</p>
<p>K-Means EM algoritmasının bir türevi olarak kabul edilebilir, EM
kümeleri bir Gaussian (ya da Gaussian karışımı) gibi görür, ve her
basamakta bu dağılımların merkezini, hem de kovaryansını hesaplar. Yani
kümenin “şekli” de EM tarafından saptanır. Ayrıca EM her noktanın tüm
kümelere olan üyeliklerini “hafif (soft)” olarak hesaplar (bir olasılık
ölçütü üzerinden), fakat K-Means için bu atama nihai (hard membership).
Nokta ya bir kümeye aittir, ya da değildir.</p>
<p>EM’in belli şartlarda yaklaşıksallığı için matematiksel ispat var.
K-Means akıllı tahmin yaparak (heuristic) çalışan bir algoritma olarak
biliniyor. Sonuca yaklaşması bu sebeple garanti değildir, ama daha önce
belirttiğimiz gibi pratikte faydalıdır. Bir sürü alternatif kümeleme
yöntemi olmasına rağmen hala K-Means kullanışlı. Burada bir etken de
K-Means’in çok rahat paralelize edilebilmesi. Bu konu başka bir yazıda
işlenecek.</p>
<p>Örnek test verisi altta</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data <span class="op">=</span> pd.read_csv(<span class="st">&quot;synthetic2.txt&quot;</span>,names<span class="op">=</span>[<span class="st">&#39;a&#39;</span>,<span class="st">&#39;b&#39;</span>],sep<span class="op">=</span><span class="st">&#39;;&#39;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (data.shape)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array(data)</span></code></pre></div>
<pre class="text"><code>(3000, 2)</code></pre>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(data[:,<span class="dv">0</span>],data[:,<span class="dv">1</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;kmeans_1.png&#39;</span>)</span></code></pre></div>
<p><img src="kmeans_1.png" /></p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> euc_to_clusters(x,y):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.sqrt(np.<span class="bu">sum</span>((x<span class="op">-</span>y)<span class="op">**</span><span class="dv">2</span>, axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> KMeans():</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,n_clusters,n_iter<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> n_clusters</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">iter</span> <span class="op">=</span> n_iter</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>,X):</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># her veri noktasi icin rasgele kume merkezi ata</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        labels <span class="op">=</span> [random.randint(<span class="dv">0</span>,<span class="va">self</span>.k<span class="op">-</span><span class="dv">1</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">0</span>])]</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.labels_ <span class="op">=</span> np.array(labels)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.centers_ <span class="op">=</span> np.zeros((<span class="va">self</span>.k,X.shape[<span class="dv">1</span>]))</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.<span class="bu">iter</span>):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># yeni kume merkezleri uret</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.k):</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                <span class="co"># eger kume j icinde hic nokta yoksa, ortalama (mean)</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>                <span class="co"># hesabi yapma, cunku o zaman nan degeri geliyor, ve</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>                <span class="co"># hesabin geri kalani bozuluyor.</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">len</span>(X[<span class="va">self</span>.labels_ <span class="op">==</span> j]) <span class="op">==</span> <span class="dv">0</span>: <span class="cf">continue</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                center <span class="op">=</span> np.mean(X[<span class="va">self</span>.labels_ <span class="op">==</span> j],axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.centers_[j,:] <span class="op">=</span> center</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>            <span class="co"># her nokta icin kume merkezlerine gore kume atamasi yap</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.labels_ <span class="op">=</span> []</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> point <span class="kw">in</span> X:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                c <span class="op">=</span> np.argmin(euc_to_clusters(<span class="va">self</span>.centers_, point))</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.labels_.append(<span class="bu">int</span>(c))</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.labels_ <span class="op">=</span> np.array(<span class="va">self</span>.labels_)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>cf <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">5</span>,n_iter<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>cf.fit(data)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (cf.labels_)</span></code></pre></div>
<pre class="text"><code>[2 2 2 ... 1 1 1]</code></pre>
<p>Üstteki sonucun içinde iki ana vektör var, bu vektörlerden birincisi
içinde 2,1, gibi sayılar görülüyor, bu sayılar her noktaya tekabül eden
küme atamaları. İkinci vektör içinde iki boyutlu <span
class="math inline">\(k\)</span> tane vektör var, bu vektörler de her
kümenin merkez noktası. Merkez noktalarını ham veri üzerinde
grafiklersek (kırmızı noktalar)</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(data[:,<span class="dv">0</span>],data[:,<span class="dv">1</span>])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>plt.ylim([<span class="dv">30000</span>,<span class="dv">70000</span>])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x <span class="kw">in</span> cf.centers_: plt.plot(x[<span class="dv">0</span>],x[<span class="dv">1</span>],<span class="st">&#39;rd&#39;</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;kmeans_2.png&#39;</span>)</span></code></pre></div>
<p><img src="kmeans_2.png" /></p>
<p>Görüldüğü gibi 5 tane küme için üstteki merkezler bulundu. Fena
değil. Eğer 10 dersek</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>cf <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">10</span>,n_iter<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>cf.fit(data)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(data[:,<span class="dv">0</span>],data[:,<span class="dv">1</span>])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.ylim([<span class="dv">30000</span>,<span class="dv">70000</span>])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x <span class="kw">in</span> cf.centers_: plt.plot(x[<span class="dv">0</span>],x[<span class="dv">1</span>],<span class="st">&#39;rd&#39;</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;kmeans_3.png&#39;</span>)</span></code></pre></div>
<p><img src="kmeans_3.png" /></p>
<p>Kategorik ve Sayısal Öğeler İçeren Karışık Veriler</p>
<p>Bazen verimiz hem kategorik hem de sayısal (numeric) değerler
içeriyor olabilir, KMeans yeni küme merkezlerini hesaplarken ortalama
operasyonu kullandığı için sadece sayısal veriler üzerinde çalışabilir
(kategorik verilerin nasıl ortalamasını alalım ki?). Bu durumda ne
yapacağız?</p>
<p>Bir seçenek şu olabilir, kategorik her kolonu her değişik değeri bir
yeni kolona tekabül edecek şekilde sağa doğru açarız, ve o değerin yeni
kolonuna 1 değeri diğerlerine 0 değeri veririz. Bu kodlamaya 1-in-q
kodlaması, 1-in-n kodlaması, ya da İngilizce 1-hot encoding ismi
veriliyor.</p>
<p>Örnek olarak UCI veri bankasından Avustralya Kredi Verisine
bakalım:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;crx.csv&quot;</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df[:<span class="dv">2</span>])</span></code></pre></div>
<pre class="text"><code>  A1     A2    A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 A16
0  b  30.83  0.00  u  g  w  v  1.25  t   t    1   f   g  00202    0   +
1  a  58.67  4.46  u  g  q  h  3.04  t   t    6   f   g  00043  560   +</code></pre>
<p>Bu veride A1, A2, gibi kolon isimleri var, kategorik olanlarda
‘g’,‘w’ gibi değerler görülüyor. Bu kolonları değiştirmek için</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction <span class="im">import</span> DictVectorizer</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_dataframe(data, cols):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    vec <span class="op">=</span> DictVectorizer()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    mkdict <span class="op">=</span> <span class="kw">lambda</span> row: <span class="bu">dict</span>((col, row[col]) <span class="cf">for</span> col <span class="kw">in</span> cols)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    vecData <span class="op">=</span> pd.DataFrame(vec.fit_transform(data[cols].to_dict(orient<span class="op">=</span><span class="st">&#39;records&#39;</span>)).toarray())</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    vecData.columns <span class="op">=</span> vec.get_feature_names_out()</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    vecData.index <span class="op">=</span> data.index</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> data.drop(cols, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> data.join(vecData)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> one_hot_dataframe(df,[<span class="st">&#39;A1&#39;</span>,<span class="st">&#39;A4&#39;</span>,<span class="st">&#39;A5&#39;</span>,<span class="st">&#39;A6&#39;</span>,<span class="st">&#39;A7&#39;</span>,<span class="st">&#39;A9&#39;</span>,<span class="st">&#39;A10&#39;</span>,<span class="st">&#39;A12&#39;</span>,<span class="st">&#39;A13&#39;</span>])</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df2.iloc[<span class="dv">0</span>])</span></code></pre></div>
<pre class="text"><code>A2       30.83
A3         0.0
A8        1.25
A11          1
A14      00202
A15          0
A16          +
A10=f      0.0
A10=t      1.0
A12=f      1.0
A12=t      0.0
A13=g      1.0
A13=p      0.0
A13=s      0.0
A1=?       0.0
A1=a       0.0
A1=b       1.0
A4=?       0.0
A4=l       0.0
A4=u       1.0
A4=y       0.0
A5=?       0.0
A5=g       1.0
A5=gg      0.0
A5=p       0.0
A6=?       0.0
A6=aa      0.0
A6=c       0.0
A6=cc      0.0
A6=d       0.0
A6=e       0.0
A6=ff      0.0
A6=i       0.0
A6=j       0.0
A6=k       0.0
A6=m       0.0
A6=q       0.0
A6=r       0.0
A6=w       1.0
A6=x       0.0
A7=?       0.0
A7=bb      0.0
A7=dd      0.0
A7=ff      0.0
A7=h       0.0
A7=j       0.0
A7=n       0.0
A7=o       0.0
A7=v       1.0
A7=z       0.0
A9=f       0.0
A9=t       1.0
Name: 0, dtype: object</code></pre>
<p>İşlem sonucunda A12=f mesela için 1 verilmiş, ama A12=t (ve diğer her
mümkün değer için yani) 0 değeri verilmiş (sadece bu tek satır için).
Böylece kategorik veriyi sayısal hale çevirmiş olduk.</p>
<p>Fakat işimiz bitti mi? Hayır. Şimdi KMeans bu tür veriyle acaba
düzgün çalışır mıydı onu kendimize soralım. İçinde pek çok 0, bazen 1
içeren veri satırları arasında uzaklık hesabı yapmak ise yarar mı?</p>
<p>Yapay Öğrenim literatüründe bu tür veriler üzerinde kosinüs
benzerliği (cosine similarity) kullanmak daha yaygındır. Bu konuyu {}
yazısında daha iyi görülebilir. Kosinüs benzerliği bize 0 ile 1 arasında
bir değer döndürür. Benzerliği uzaklığa çevirmek için basit bir şekilde
1-benzerlik formülünü kullanabiliriz. O zaman şöyle bir çözüm
kullanabilir: normal sayısal değerler için Öklitsel, kategorik 1-hot
kodlanmış kolonlar için Kosinüs uzaklığı kullanılır, bu uzaklıklar bazı
ağırlıklar üzerinden birleştirilir, ve KMeans bu uzaklık ile iş yapar.
Teknik olarak imkansız değil; KMeans merkez bulmak için ortalama alır ve
Kosinüs uzaklığının verdiği aradaki açı, ortalama alma işlemi ile
uyumludur. Yani içinde hem Öklitsel hem 1-hot kodlanmış verilerin olduğu
vektörlerin ortalamasını alabiliriz, demek ki KMeans işleyebilir.</p>
<p><img src="kmeans_5.jpg" /></p>
<p>Problem şudur, iki uzaklığı birleştiren ağırlıklar ne olmalıdır? Bu
yöntemi denediğimizde bu ağırlıkların ne seçildiğinin çok önemli
olduğunu farkettik, ve kümeleme gibi denetimsiz (unsupervised) bir
yöntemde bu hiperparametreleri deneme / yanılma yöntemi ile bulma
şansımız yoktur.</p>
<p>Bu durumda kullanılabilecek bir yöntem şudur: SVD kullanarak tüm
matrisi azaltmak ve onun üzerinde pür Öklitsel uzaklıklar kullanmak.
Sayısal ve kategorik karışık verileri içeren verileri kümelemek için
tavsiye edilen yöntem şudur:</p>
<ol type="1">
<li><p>Kategorik veriler üzerinde 1-hot kodlama yap.</p></li>
<li><p>Önce kolonları sonra satırları normalize et.</p></li>
<li><p>Tüm matris üzerinde çok küçük olmayan bir <span
class="math inline">\(k\)</span> ile SVD al (mesela alttaki veri seti
için önce 10)</p></li>
<li><p><span class="math inline">\(S\)</span> vektörüne bak, ortalamadan
büyük olan kaç tane hücre olduğunu gör.</p></li>
<li><p>Bu sayı yeni <span class="math inline">\(k\)</span> değerimiz
olacak, SVD’yi tekrar bu <span class="math inline">\(k\)</span> ile
işlet.</p></li>
<li><p>Elde edilen <span class="math inline">\(U\)</span> üzerinde
kümeleme yap,</p></li>
</ol>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> normalize</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse.linalg <span class="im">as</span> slin</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.linalg <span class="im">as</span> lin</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;crx.csv&quot;</span>,sep<span class="op">=</span><span class="st">&#39;,&#39;</span>,na_values<span class="op">=</span>[<span class="st">&#39;?&#39;</span>])</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.dropna()</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;A16&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;A16&#39;</span>].<span class="bu">str</span>.replace(<span class="st">&#39;+&#39;</span>,<span class="st">&#39;1&#39;</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;A16&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;A16&#39;</span>].<span class="bu">str</span>.replace(<span class="st">&#39;-&#39;</span>,<span class="st">&#39;0&#39;</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;A16&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;A16&#39;</span>].astype(<span class="bu">int</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> one_hot_dataframe(df,[<span class="st">&#39;A1&#39;</span>,<span class="st">&#39;A4&#39;</span>,<span class="st">&#39;A5&#39;</span>,<span class="st">&#39;A6&#39;</span>,<span class="st">&#39;A7&#39;</span>,<span class="st">&#39;A9&#39;</span>,<span class="st">&#39;A10&#39;</span>,<span class="st">&#39;A12&#39;</span>,<span class="st">&#39;A13&#39;</span>])</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> df2.drop(<span class="st">&#39;A16&#39;</span>,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> np.array(df2)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>df3 <span class="op">=</span> df2.copy()</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>df3 <span class="op">=</span> normalize(df3, norm<span class="op">=</span><span class="st">&#39;l2&#39;</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>df3 <span class="op">=</span> normalize(df3, norm<span class="op">=</span><span class="st">&#39;l2&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>u,s,v<span class="op">=</span>slin.svds(df3,k<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (s)</span></code></pre></div>
<pre class="text"><code>[ 4.45826083  4.49654025  4.68382638  4.93391665  4.98604314  5.153349
  5.63521289  5.70490968  6.68558115 14.81145675]</code></pre>
<p>Bakıyoruz, averajdan yüksek olan en büyük sadece iki kolon var. SVD
literatüründe bu kolonların matrisin “enerjisini’’ içerdiği söylenir,
hakikaten eğer SVD ayrıştırma sonrası bu ilk kolona bu kadar önem
verdiyse, onlar önemli,”enerjiyi içeriyor’’ olmalıdırlar. Şimdi SVD’yi
<span class="math inline">\(k=2\)</span> ile tekrar işletiyoruz,</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>u,s,v<span class="op">=</span>slin.svds(df3,k<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (s)</span></code></pre></div>
<pre class="text"><code>[ 6.68558115 14.81145675]</code></pre>
<p>Şimdi <span class="math inline">\(U\)</span> üzerinde kümeleme
yapacağız, ve kontrol için kenara koyduğumuz bilinen etiketler üzerinden
kümeleme başarımızı ölçeceğiz. Avustralya Kredi Verisi aslında izlenen
(supervised) algoritmalar için kullanılır, ama biz onu izlenmeyen
kümeleme problemi için kullandık, bilinen etiketleri veri içinden
çıkartıp bir kenara koyuyoruz, ve sonra kümeleme tahmini yaparak bu
etiketlerle olan uyumu ölçüyoruz.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>clf.fit(u)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>labels_true <span class="op">=</span> np.array(df[<span class="st">&#39;A16&#39;</span>])</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>labels_pred <span class="op">=</span> clf.labels_</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>match <span class="op">=</span> np.<span class="bu">sum</span>((labels_true <span class="op">==</span> labels_pred).astype(<span class="bu">int</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="bu">float</span>(match)<span class="op">/</span><span class="bu">len</span>(df), <span class="dv">1</span><span class="op">-</span><span class="bu">float</span>(match)<span class="op">/</span><span class="bu">len</span>(df))</span></code></pre></div>
<pre class="text"><code>0.7856049004594181 0.2143950995405819</code></pre>
<p>Başarı yüzde %78. Çok iyi. Üstteki örnek küme sayısının (dikkat SVD
<span class="math inline">\(k\)</span>’sinden farklı) bilindiğini farz
etti. Bazı durumlarda küme sayısını grafiksel olarak görmek mümkündür
(ama en iyisi Gaussian Karışım Modeli kullanıp mümkün K’leri AIC ile
test etmek, bkz {} yazısı).</p>
<p>Mesela üstteki veri seti için ortalamayı çıkartıp varyansa bölersek
ve SVD işletirsek en büyük iki <span class="math inline">\(U\)</span>
kolonun grafiği alttaki gibi çıkıyor,</p>
<p><img src="kmeans_4.png" /></p>
<p>Eğer rasgele yansıtma (random projection) kullansaydık ne olurdu? Bu
işlemi birkaç kez yapalım ki rasgele matris <code>Omega</code> değişik
şekillerde (ama hala rasgele) üretilince sonuç değişir miydi
görelim.</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.random <span class="im">as</span> rand</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>     Omega <span class="op">=</span> rand.randn(df3.shape[<span class="dv">1</span>],<span class="dv">30</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>     u <span class="op">=</span> np.dot(df3,Omega)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>     clf <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>     clf.fit(u)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>     labels_true <span class="op">=</span> np.array(df[<span class="st">&#39;A16&#39;</span>])</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>     labels_pred <span class="op">=</span> clf.labels_</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>     match <span class="op">=</span> np.<span class="bu">sum</span>((labels_true <span class="op">==</span> labels_pred).astype(<span class="bu">int</span>))</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>     <span class="bu">print</span> (<span class="bu">float</span>(match)<span class="op">/</span><span class="bu">len</span>(df), <span class="dv">1</span><span class="op">-</span><span class="bu">float</span>(match)<span class="op">/</span><span class="bu">len</span>(df))</span></code></pre></div>
<pre class="text"><code>0.7580398162327718 0.24196018376722817
0.5712098009188361 0.4287901990811639
0.3001531393568147 0.6998468606431854
0.44716692189892804 0.552833078101072
0.6125574272588055 0.38744257274119454</code></pre>
<p>Görüldüğü gibi bazen çok iyi sonuçlar alıyor olsak bile bazen çok
kötü sonuçlar da alabiliyoruz. Demek ki bu veri setinde SVD tekniği daha
başarılı.</p>
<p>Kaynaklar</p>
<p>[1] Corrada, <em>Practicum: Kernelized K-means</em>, <a
href="nbviewer.ipython.org/url/cbcb.umd.edu/~hcorrada/PML/src/kmeans.ipynb">nbviewer.ipython.org/url/cbcb.umd.edu/~hcorrada/PML/src/kmeans.ipynb</a></p>
<p>[2] UCI Machine Learning Repository, <em>Statlog (Australian Credit
Approval) Data Set </em>, <a
href="https://archive.ics.uci.edu/ml/datasets/Statlog+%28Australian+Credit+Approval%29">https://archive.ics.uci.edu/ml/datasets/Statlog+%28Australian+Credit+Approval%29</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
