\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
K-Means Kümeleme Metodu

Popüler kümeleme algoritmalarýndan biri k-means algoritmasý. Bu metotta kaç
tane kümenin olmasý gerektiði baþtan tanýmlanýr ($k$ parametresi ile),
algoritma bunu kendisi bulmaz. Metotun geri kalaný basit - bir döngü
(iteration) içinde her basamakta:

1) Her nokta için, eldeki küme merkezleri teker teker kontrol edilir
ve o nokta en yakýn olan kümeye atanýr.

2) Atamalar tamamlandýktan sonra her küme içinde hangi noktalarýn
olduðu bilindiði için her kümedeki noktalarýn ortalamasý alýnarak yeni
küme merkezleri hesaplanýr. Eski merkez hesaplarý atýlýr.

3) Baþa dönülür. Döngü tekrar ilk adýma döndüðünde, bu sefer yeni küme
merkezleri kullanýlarak ayný adýmlar tekrarlanacaktýr.

Fakat bir problem yok mu? Daha birinci döngü baþlamadan küme
merkezlerinin nerede olduðunu nereden bileceðiz? Burada bir
tavuk-yumurta problemi var, küme merkezleri olmadan noktalarý
atayamayýz, atama olmadan küme merkezlerini hesaplayamayýz.

Bu probleme pratik bir çözüm ilk baþta küme merkezlerini (ya da küme
atamalarýný) rasgele bir þekilde seçmektir. Pratikte bu yöntem çok iyi
iþliyor. Tabii bu rasgelelik yüzünden K-means'in doðru sonuca
yaklaþmasý (convergence) garanti deðildir, ama gerçek dünya
uygulamalarýnda çoðunlukla kullanýþlý kümeler bulunur. Bu potansiyel
problemlerden kaçýnmak için k-means pek çok kez iþletilebilir (her
seferinde yeni rasgele baþlangýçlarla yani) ve ayný sonuca ulaþýlýp
ulaþýlmadýðý kontrol edilebilir.

Pek en iyi k nasýl bulunur? SVD kullanarak grafiðe bakmak (bu yazýnýn
sonunda anlatýlýyor) mesela, fakat en iyisi K-Means yerine GMM kulanmak!
Bkz. yazý sonundaki referans.

K-Means EM algoritmasýnýn bir türevi olarak kabul edilebilir, EM
kümeleri bir Gaussian (ya da Gaussian karýþýmý) gibi görür, ve her
basamakta bu daðýlýmlarýn merkezini, hem de kovaryansýný
hesaplar. Yani kümenin "þekli" de EM tarafýndan saptanýr. Ayrýca EM
her noktanýn tüm kümelere olan üyeliklerini "hafif (soft)" olarak
hesaplar (bir olasýlýk ölçütü üzerinden), fakat K-Means için bu atama
nihai (hard membership). Nokta ya bir kümeye aittir, ya da
deðildir.

EM'in belli þartlarda yaklaþýksallýðý için matematiksel ispat
var. K-Means akýllý tahmin yaparak (heuristic) çalýþan bir algoritma
olarak biliniyor. Sonuca yaklaþmasý bu sebeple garanti deðildir, ama
daha önce belirttiðimiz gibi pratikte faydalýdýr. Bir sürü alternatif
kümeleme yöntemi olmasýna raðmen hala K-Means kullanýþlý. 
Burada bir etken de K-Means'in çok rahat paralelize edilebilmesi. Bu
konu baþka bir yazýda iþlenecek.

Örnek test verisi altta

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
data = pd.read_csv("synthetic.txt",names=['a','b'],sep="   ")
print data.shape
data = np.array(data)
\end{minted}

\begin{verbatim}
(3000, 2)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
plt.scatter(data[:,0],data[:,1])
plt.savefig('kmeans_1.png')
\end{minted}

\includegraphics[height=6cm]{kmeans_1.png}
\begin{minted}[fontsize=\footnotesize]{python}
def euc_to_clusters(x,y):
    return np.sqrt(np.sum((x-y)**2, axis=1))

class KMeans():
    def __init__(self,n_clusters,n_iter=10):
        self.k = n_clusters
        self.iter = n_iter
    def fit(self,X):
        # her veri noktasi icin rasgele kume merkezi ata
        labels = [random.randint(0,self.k-1) for i in range(X.shape[0])]
        self.labels_ = np.array(labels)
        self.centers_ = np.zeros((self.k,X.shape[1]))
        for i in range(self.iter):
            # yeni kume merkezleri uret
            for j in range(self.k):
                # eger kume j icinde hic nokta yoksa, ortalama (mean)
                # hesabi yapma, cunku o zaman nan degeri geliyor, ve
                # hesabin geri kalani bozuluyor.
                if len(X[self.labels_ == j]) == 0: continue
                center = np.mean(X[self.labels_ == j],axis=0)
                self.centers_[j,:] = center
            # her nokta icin kume merkezlerine gore kume atamasi yap
            self.labels_ = []
            for point in X:
                c = np.argmin(euc_to_clusters(self.centers_, point))
                self.labels_.append(int(c))

            self.labels_ = np.array(self.labels_)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
cf = KMeans(k=5,iter=20)
cf.fit(data)
print cf.labels_
\end{minted}

\begin{verbatim}
[3 3 3 ..., 2 2 2]
\end{verbatim}

Üstteki sonucun içinde iki ana vektör var, bu vektörlerden birincisi içinde
2,0, gibi sayýlar görülüyor, bu sayýlar her noktaya tekabül eden küme
atamalarý.  Ýkinci vektör içinde iki boyutlu $k$ tane vektör var, bu
vektörler de her kümenin merkez noktasý. Merkez noktalarýný ham veri
üzerinde grafiklersek (kýrmýzý noktalar)

\begin{minted}[fontsize=\footnotesize]{python}
plt.scatter(data[:,0],data[:,1])
plt.hold(True)
plt.ylim([30000,70000])
for x in cf.centers_: plt.plot(x[0],x[1],'rd')
plt.savefig('kmeans_2.png')
\end{minted}

\includegraphics[height=6cm]{kmeans_2.png}

Görüldüðü gibi 5 tane küme için üstteki merkezler bulundu. Fena
deðil. Eðer 10 dersek

\begin{minted}[fontsize=\footnotesize]{python}
cf = KMeans(k=10,iter=30)
cf.fit(data)
plt.scatter(data[:,0],data[:,1])
plt.ylim([30000,70000])
plt.hold(True)
for x in cf.centers_: plt.plot(x[0],x[1],'rd')
plt.savefig('kmeans_3.png')
\end{minted}

\includegraphics[height=6cm]{kmeans_3.png}

Kategorik ve Sayýsal Öðeler Ýçeren Karýþýk Veriler

Bazen verimiz hem kategorik hem de sayýsal (numeric) deðerler içeriyor
olabilir, KMeans yeni küme merkezlerini hesaplarken ortalama operasyonu
kullandýðý için sadece sayýsal veriler üzerinde çalýþabilir (kategorik
verilerin nasýl ortalamasýný alalým ki?). Bu durumda ne yapacaðýz?

Bir seçenek þu olabilir, kategorik her kolonu her deðiþik deðeri bir yeni
kolona tekabül edecek þekilde saða doðru açarýz, ve o deðerin yeni kolonuna
1 deðeri diðerlerine 0 deðeri veririz. Bu kodlamaya 1-in-q kodlamasý,
1-in-n kodlamasý, ya da Ýngilizce 1-hot encoding ismi veriliyor.

Örnek olarak UCI veri bankasýndan Avustralya Kredi Verisine bakalým:

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv("crx.csv")
print df[:2]
\end{minted}

\begin{verbatim}
  A1     A2    A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 A16
0  b  30.83  0.00  u  g  w  v  1.25  t   t    1   f   g  00202    0   +
1  a  58.67  4.46  u  g  q  h  3.04  t   t    6   f   g  00043  560   +
\end{verbatim}

Bu veride A1, A2, gibi kolon isimleri var, kategorik olanlarda 'g','w' gibi
deðerler görülüyor. Bu kolonlarý deðiþtirmek için

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.feature_extraction import DictVectorizer
def one_hot_dataframe(data, cols):
    vec = DictVectorizer()
    mkdict = lambda row: dict((col, row[col]) for col in cols)
    vecData = pd.DataFrame(vec.fit_transform(data[cols].to_dict(outtype='records')).toarray())
    vecData.columns = vec.get_feature_names()
    vecData.index = data.index
    data = data.drop(cols, axis=1)
    data = data.join(vecData)
    return data

df2 = one_hot_dataframe(df,['A1','A4','A5','A6','A7','A9','A10','A12','A13'])
print df2.ix[0]
\end{minted}

\begin{verbatim}
A2       30.83
A3           0
A8        1.25
A11          1
A14      00202
A15          0
A16          +
A10=f        0
A10=t        1
A12=f        1
A12=t        0
A13=g        1
A13=p        0
A13=s        0
A1=?         0
A1=a         0
A1=b         1
A4=?         0
A4=l         0
A4=u         1
A4=y         0
A5=?         0
A5=g         1
A5=gg        0
A5=p         0
A6=?         0
A6=aa        0
A6=c         0
A6=cc        0
A6=d         0
A6=e         0
A6=ff        0
A6=i         0
A6=j         0
A6=k         0
A6=m         0
A6=q         0
A6=r         0
A6=w         1
A6=x         0
A7=?         0
A7=bb        0
A7=dd        0
A7=ff        0
A7=h         0
A7=j         0
A7=n         0
A7=o         0
A7=v         1
A7=z         0
A9=f         0
A9=t         1
Name: 0, Length: 52, dtype: object
\end{verbatim}

Ýþlem sonucunda A12=f mesela için 1 verilmiþ, ama A12=t (ve diðer her
mümkün deðer için yani) 0 deðeri verilmiþ (sadece bu tek satýr
için). Böylece kategorik veriyi sayýsal hale çevirmiþ olduk.

Fakat iþimiz bitti mi? Hayýr. Þimdi KMeans bu tür veriyle acaba düzgün
çalýþýr mýydý onu kendimize soralým. Ýçinde pek çok 0, bazen 1 içeren 
veri satýrlarý arasýnda uzaklýk hesabý yapmak ise yarar mý?

Yapay Öðrenim literatüründe bu tür veriler üzerinde kosinüs benzerliði
(cosine similarity) kullanmak daha yaygýndýr. Bu konuyu {\em SVD, Toplu
  Tavsiye} yazýsýnda daha iyi görülebilir. Kosinüs benzerliði bize 0 ile 1
arasýnda bir deðer döndürür. Benzerliði uzaklýða çevirmek için basit bir
þekilde 1-benzerlik formülünü kullanabiliriz. O zaman þöyle bir çözüm
kullanabilir: normal sayýsal deðerler için Öklitsel, kategorik 1-hot
kodlanmýþ kolonlar için Kosinüs uzaklýðý kullanýlýr, bu uzaklýklar bazý
aðýrlýklar üzerinden birleþtirilir, ve KMeans bu uzaklýk ile iþ
yapar. Teknik olarak imkansýz deðil; KMeans merkez bulmak için ortalama
alýr ve Kosinüs uzaklýðýnýn verdiði aradaki açý, ortalama alma iþlemi ile
uyumludur. Yani içinde hem Öklitsel hem 1-hot kodlanmýþ verilerin olduðu
vektörlerin ortalamasýný alabiliriz, demek ki KMeans iþleyebilir.

\includegraphics[height=4cm]{kmeans_5.jpg}

Problem þudur, iki uzaklýðý birleþtiren aðýrlýklar ne olmalýdýr?  Bu
yöntemi denediðimizde bu aðýrlýklarýn ne seçildiðinin çok önemli olduðunu
farkettik, ve kümeleme gibi denetimsiz (unsupervised) bir yöntemde bu
hiperparametreleri deneme / yanýlma yöntemi ile bulma þansýmýz yoktur.

Bu durumda kullanýlabilecek bir yöntem þudur: SVD kullanarak tüm matrisi
azaltmak ve onun üzerinde pür Öklitsel uzaklýklar kullanmak. Sayýsal ve
kategorik karýþýk verileri içeren verileri kümelemek için tavsiye edilen
yöntem þudur:

1) Kategorik veriler üzerinde 1-hot kodlama yap. 

2) Önce kolonlarý sonra satýrlarý normalize et.

3) Tüm matris üzerinde çok küçük olmayan bir $k$ ile SVD al (mesela alttaki
veri seti için önce 10)

4) $S$ vektörüne bak, ortalamadan büyük olan kaç tane hücre olduðunu gör.

5) Bu sayý yeni $k$ deðerimiz olacak, SVD'yi tekrar bu $k$ ile iþlet. 

6) Elde edilen $U$ üzerinde kümeleme yap,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.preprocessing import normalize
import scipy.sparse.linalg as slin
import scipy.linalg as lin
import pandas as pd

df = pd.read_csv("crx.csv",sep=',',na_values=['?'])
df = df.dropna()

df['A16'] = df['A16'].str.replace('+','1')
df['A16'] = df['A16'].str.replace('-','0')
df['A16'] = df['A16'].astype(int)

df2 = one_hot_dataframe(df,['A1','A4','A5','A6','A7','A9','A10','A12','A13'])
df2 = df2.drop('A16',axis=1)
df2 = np.array(df2)
df3 = df2.copy()
df3 = normalize(df3, norm='l2', axis=0)
df3 = normalize(df3, norm='l2', axis=1)

u,s,v=slin.svds(df3,k=10)
print s
\end{minted}

\begin{verbatim}
[  4.45826083   4.49654025   4.68382638   4.93391665   4.98604314
   5.153349     5.63521289   5.70490968   6.68558115  14.81145675]
\end{verbatim}

Bakýyoruz, averajdan yüksek olan en büyük sadece iki kolon var. SVD
literatüründe bu kolonlarýn matrisin ``enerjisini'' içerdiði söylenir,
hakikaten eðer SVD ayrýþtýrma sonrasý bu ilk kolona bu kadar önem verdiyse,
onlar önemli, ``enerjiyi içeriyor'' olmalýdýrlar. Þimdi SVD'yi $k=2$ ile
tekrar iþletiyoruz,

\begin{minted}[fontsize=\footnotesize]{python}
u,s,v=slin.svds(df3,k=2)
print s
\end{minted}

\begin{verbatim}
[  6.68558115  14.81145675]
\end{verbatim}

Þimdi $U$ üzerinde kümeleme yapacaðýz, ve kontrol için kenara koyduðumuz
bilinen etiketler üzerinden kümeleme baþarýmýzý ölçeceðiz. Avustralya Kredi
Verisi aslýnda izlenen (supervised) algoritmalar için kullanýlýr, ama biz
onu izlenmeyen kümeleme problemi için kullandýk, bilinen etiketleri veri
içinden çýkartýp bir kenara koyuyoruz, ve sonra kümeleme tahmini yaparak bu
etiketlerle olan uyumu ölçüyoruz. 

\begin{minted}[fontsize=\footnotesize]{python}
clf = KMeans(n_clusters=2)
clf.fit(u)
labels_true = np.array(df['A16'])
labels_pred = clf.labels_
match = np.sum((labels_true == labels_pred).astype(int))
print float(match)/len(df), 1-float(match)/len(df)
\end{minted}

\begin{verbatim}
0.217457886677 0.782542113323
\end{verbatim}

Baþarý yüzde \%78. Çok iyi. Üstteki örnek küme sayýsýnýn (dikkat SVD
$k$'sinden farklý) bilindiðini farz etti. Bazý durumlarda küme sayýsýný
grafiksel olarak görmek mümkündür (ama en iyisi Gaussian Karýþým Modeli
kullanýp mümkün K'leri AIC ile test etmek, bkz {\em Ýstatistik, GMM ile
  Kümelemek} yazýsý). 

Mesela üstteki veri seti için ortalamayý çýkartýp varyansa bölersek ve SVD
iþletirsek en büyük iki $U$ kolonun grafiði alttaki gibi çýkýyor,

\includegraphics[height=6cm]{kmeans_4.png}

Eðer rasgele yansýtma (random projection) kullansaydýk ne olurdu? Bu iþlemi
birkaç kez yapalým ki rasgele matris \verb!Omega! deðiþik þekillerde (ama
hala rasgele) üretilince sonuç deðiþir miydi görelim.

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.random as rand
for i in range(5):
     Omega = rand.randn(df3.shape[1],30)
     u = np.dot(df3,Omega)
     clf = KMeans(n_clusters=2)
     clf.fit(u)
     labels_true = np.array(df['A16'])
     labels_pred = clf.labels_
     match = np.sum((labels_true == labels_pred).astype(int))
     print float(match)/len(df), 1-float(match)/len(df)
\end{minted}

\begin{verbatim}
0.436447166922 0.563552833078
0.258805513017 0.741194486983
0.367534456355 0.632465543645
0.390505359877 0.609494640123
0.456355283308 0.543644716692
\end{verbatim}

Görüldüðü gibi bazen çok iyi sonuçlar alýyor olsak bile bazen çok kötü
sonuçlar da alabiliyoruz. Demek ki bu veri setinde SVD tekniði daha
baþarýlý. 

Kaynaklar

[1] Corrada, {\em Practicum: Kernelized K-means}, \url{nbviewer.ipython.org/url/cbcb.umd.edu/~hcorrada/PML/src/kmeans.ipynb}

[2] UCI Machine Learning Repository, {\em Statlog (Australian Credit Approval) Data Set }, \url{https://archive.ics.uci.edu/ml/datasets/Statlog+%28Australian+Credit+Approval%29}


\end{document}
