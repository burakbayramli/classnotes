<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Yapay Sinir Ağları (Neural Networks)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="yapay-sinir-ağları-neural-networks">Yapay Sinir Ağları (Neural
Networks)</h1>
<p>YSA’ları anlamak için onların temel taşı olan tek nöron’u görmemiz
lazım.</p>
<p><img src="mlp_03.png" /></p>
<p>Üstteki resimde üç girdisi ve tek çıktısı olan bir nöron görüyoruz.
Girdi olarak <span class="math inline">\(x_1,x_2,x_3\)</span> ve çıktı
olarak bir fonksiyon hesabı. Literatürde üstteki yapıya perseptron
(perceptron) ismi de veriliyor. Eğer 0/1, evet/hayır türünde çıktıları
modellemek istersek bu fonksiyon çoğunlukla sigmoid fonksiyonu olarak
seçilir. Yani</p>
<p><span class="math display">\[ h (x) = f(w^Tx) = f(\sum_{i=1}^{3}
w_ix_i + b)\]</span></p>
<p>ve</p>
<p><span class="math display">\[ f(z) = \frac{1}{1 +
\exp(-z)}\]</span></p>
<p>ki <span class="math inline">\(w\)</span> ağırlık değerlerinin
taşıyan bir vektördür. Sigmoid seçiminin bir diğer sebebi türevinin
rahat alınabilmesi. Alternatif fonksiyonlar <span
class="math inline">\(\tanh\)</span> olabilirdi, ya da düzeltilmiş
lineer ünite (rectified linear unit -ReLu-) fonksiyonları. ReLu’lar
YSA’ların uzantısı Derin Öğrenim’de oldukca popüler oldu, tanımı</p>
<p><span class="math display">\[ f(z) = \max(0,z)\]</span></p>
<p>Şimdi tek nörondan çok katmanlı bir YSA nasıl oluşturulur
görelim,</p>
<p><img src="mlp_04.png" /></p>
<p>Bu yapıda her <span class="math inline">\(x_i\)</span>’in her nörona
gittiğini görüyoruz, bu katmanlara bu sebeple tamamen bağlanmış katman
(fully-connected layer), ya da yoğun (dense) katman deniyor. Her türlü
girdi / nöron kombinasyonu çarpıldığı için ağırlıklar bir matris içinde
tutulur, ve aktivasyondan önceki vektör basit bir matris çarpımı haline
gelir, mesela 2. katman aktivasyonu <span
class="math inline">\(f_2\)</span> olsun, 1. gizli katman çıktısı</p>
<p><span class="math display">\[ f_2 (W_1x + b) \]</span></p>
<p>Girdi <span class="math inline">\(4 \times 1\)</span>, ağırlık <span
class="math inline">\(W_1\)</span> boyutu <span class="math inline">\(3
\times 4\)</span>, o zaman çarpım <span class="math inline">\(3 \times
1\)</span> olacak, <span class="math inline">\(f_2\)</span> uygulanınca
boyut değişmez (her öge üzerinde aynı fonksiyon işletiliyor) sonuç <span
class="math inline">\(3 \times 1\)</span>, bu da zaten 2. katmana giren
(sonra oradan sonraki katmana çıkan) hesap. Üstteki diyagramda yanlılık
(bias) ayrı bir nöron olarak eklenmiş (bazen bu yapılmayabiliyor).</p>
<p>İki gizli seviyesi (hidden layer) olan bir YSA,</p>
<p><img src="mlp_05.png" /></p>
<p>Görüldüğü gibi çok seviyeli durumda bir seviyenin çıktısı bir diğer
seviyenin girdisi haline geliyor. YSA’lar denetimli (supervised) şekilde
eğitilirler. Elde <span class="math inline">\(x_i,y_i\)</span> veri
noktaları vardır, ve her bir ya da çok boyutlu <span
class="math inline">\(x_i\)</span> yine bir ya da çok boyutlu <span
class="math inline">\(y_i\)</span> ile eşlidir. Aynen regresyonda olduğu
gibi bu veri çiftlerini alırız ve onları en iyi temsil eden YSA’yı
eğitmek için kullanırız. Eğitim tamamlandığında <span
class="math inline">\(i\)</span> katmanındaki <span
class="math inline">\(W_i\)</span> ağırlıkları belli değerlere sahip
olurlar, ve eldeki “model’’ bu ağırlıklardır. Yeni bir veri noktası
verildiğinde o veri noktası ağırlıklar her katmanın <span
class="math inline">\(W_i\)</span>’sı ile çarpılıp, toplanıp, aktivasyon
fonksiyonlarına geçilip, tüm katmanlardan bu şekilde geçirildikten sonra
en sondaki çıktı değerine erişilir.</p>
<p>Yanlılık taşımayan şekilde ağ yapısı görelim ve matris çarpımı ile
tüm girdileri / çıktıları gösterelim,</p>
<p><img src="nn_nobias.PNG" /></p>
<p><span class="math display">\[ Girdi = x_0\]</span></p>
<p><span class="math display">\[ x_1 = f_1(W_1x_0)\]</span></p>
<p><span class="math display">\[ x_2 = f_2(W_2x_1)\]</span></p>
<p><span class="math display">\[ Çıktı = f_3(W_3x_2) \]</span></p>
<p>Aktivasyon için <span class="math inline">\(f_1,f_2,f_3\)</span> ReLu
ya da sigmoid seçilebilir.</p>
<p>İki gizli seviyesi olan (ve yeterince sayıda nöron içeren) YSA’nın
evrensel yaklaşıklayıcı (universal approximator) olduğu iddia edilir,
yani üstteki gibi yeterince çetrefil bir YSA her türlü fonksiyonu
yaklaşık olarak temsil edebilir. İşin tarihine gidersek, ilk başta tek
seviyeli YSA vardı, fakat XOR fonksiyonunu başarılı şekilde temsil
edemedi. Gizli (tek) seviyeli olan bunu başardı.</p>
<p>YSA’lar imaj tanıma alanında bolca kullanılmıştır. Bir imajda, mesela
sayı tanıma uygulamasında, alttaki gibi pek çok resimler olabilir,</p>
<p><img src="mlp_01.png" /> <img src="mlp_02.png" /></p>
<p>Bir imaj nihayetinde o resmi temsil eden gri değerlerin olduğu bir
matristen ibarettir. YSA’ya girdi olması için matris genelde
düzleştirilir, ve tek bir vektör olarak verilir, 8x8 matris 64x1 vektörü
olur, girdi 64 boyutlu bir <span class="math inline">\(x_i\)</span>
çıktı ise tek boyutlu 0,1,2,..,9 gibi etiketler.. Çoğunlukla bu tür
kategorik çıktılar ikisel olarak modellenir, 0,..,9 yerine 10 öğeli bir
ikisel vektör alınır, 9. öğesi 1 diğerleri 0 yapılır.</p>
<p>YSA’lar nasıl eğitilir? Eldeki <span
class="math inline">\(x_i,y_i\)</span> çiftleri YSA’ya verilir, her
<span class="math inline">\(x_i\)</span> önce tahmin için kullanılır, ve
bir YSA’nın tahmin ettiğine, bir de gerçek veriye bakılır. Bu fark,
hata, YSA’nın düzeltilmesi / iyileştirilmesi için kullanılır, burada
geriye yayılım (backpropagation -backprop-) adlı bir teknik var. YSA
içinde, üstte gördüğümüz gibi, sol seviyeden başlayarak sağa gidecek
şekilde, fonksiyonlar, fonksiyonların fonksiyonları şeklinde çetrefil
bir yapı var. Backprop elde hatayı alıp YSA’daki tüm nöronların
ağırlıklarını bu hataya göre düzeltilmesini, düzelmenin “yayılmasını’’
sağlar [1,2]. Bu işlem ardı ardına, farklı her döngüde tekrarlanır, bir
süre sonra YSA eldeki veriyi en iyi temsil eden ideal bir hale
yaklaşacaktır.</p>
<p>Formüller ve Kod</p>
<p><img src="mlp_07.png" /></p>
<p>Backprop’un genel bir resmi üstte: a) Eğitim verisi ileri yönde
(soldan sağa) YSA’ya veriliyor, ve bu veri her nokta için, o anda sahip
olunan ağırlıklara göre bir tahmin üretiyor. b) Eğitim verisindeki
beklenen ve tahmin edilen değerler arasındaki hata hesaplanıyor c) Hata
YSA’ya “geri yönde’’ veriliyor, her ağırlık <span
class="math inline">\(w_i\)</span>’dan <span
class="math inline">\(\frac{\partial E}{\partial w_i}\)</span>
büyüklüğünün bir kısmı çıkartılıyor, bu düzeltmenin ne oranda yapıldığı
dışarıdan, önceden kararlaştırılan bir sabit <span
class="math inline">\(\eta\)</span>’e oranda yapılıyor.</p>
<p>Daha detaylı bir resimde gösterelim,</p>
<p><img src="neural-net.png" /></p>
<p>Tek bir gizli katman var, girdi katmanı <span
class="math inline">\(a_i\)</span>’den gelen sinyaller <span
class="math inline">\(w_{ij}\)</span> ağırlıkları ile çarpılıyor, daha
önce gördüğümüz gibi bu tamamen bağlanmış katman, çünkü tüm girdi
sinyalleri her gizli katman düğümüne hep beraber giriş yapıyorlar (tabii
farklı ağırlıklar üzerinden). Devam edelim, ağırlıklar sonrası bir de
yanlılık (bias) eklenecek (resimde atlandı), ve elde edilen <span
class="math inline">\(z_i = b_j + \sum_i a_i w_{ij}\)</span>. Bu 3.
katmandaki aktivasyon önceki son hal. Gizli katmandan çıkan sinyal <span
class="math inline">\(a_j\)</span>, bu sinyaller son katmanda bu sefer
<span class="math inline">\(w_{jk}\)</span> ağırlıkları ile çarpılıp
toplanacak, <span class="math inline">\(b_k\)</span> yanlılığı
eklenecek, ve sonuç <span class="math inline">\(g_k\)</span> aktivasyon
fonksiyonundan geçirilip çıktı <span class="math inline">\(a_k\)</span>
haline getirilecek.</p>
<p>Bir YSA’yı eğitmek demek öyle bir parametre demeti <span
class="math inline">\(\theta = (W,b)\)</span> bulmaktır ki ağın hatası
en minimal seviyede olsun. Çoğunlukla bu hata hedef (eğitim) çıktısı
<span class="math inline">\(t_k\)</span> ile ağın en son ağırlıklara
göre kendi hesapladığı <span class="math inline">\(a_k\)</span> fark
karelerinin toplamı üzerinden hesaplanır.</p>
<p><span class="math display">\[ E = \frac{1}{2}  \sum_k (a_k -
t_k)^2\]</span></p>
<p>Çıktı katmanının hata fonksiyonu üzerinde direk etkisi olduğu için bu
parametreler için gradyanı hesaplamak oldukca kolay,</p>
<p><span class="math display">\[ \frac{\partial E}{\partial w_{jk}} =
\frac{1}{2} \sum_k (a_t - t_k)^2\]</span></p>
<p><span class="math display">\[ = (a_k-t_k) \frac{\partial }{\partial
w_{jk}} (a_k-t_k)\]</span></p>
<p>Bu türevde Zincirleme Kanununu kullandık. Dikkat edersek toplama
operatörü kayboldu, bunun sebebi <span
class="math inline">\(j\)</span>’inci düğüme göre türev alıyor olmamız o
yüzden <span class="math inline">\(j\)</span>’inci haricinde tüm diğer
parametreler türevde sıfırlanıyor, toplamdan geriye tek bir terim
kalıyor. <span class="math inline">\(t_k\)</span>’in türevi de sıfır
çünkü içinde <span class="math inline">\(w_{jk}\)</span>’ye bağlı hiçbir
değişken yok (<span class="math inline">\(t_k\)</span> dışarıdan verilen
eğitim verisi sonuçta, çoğu durum için sabit sayılabilir). Ayrıca <span
class="math inline">\(a_k = g(z_k)\)</span> olduğunu hatırlayalım. O
zaman</p>
<p><span class="math display">\[  
\frac{\partial E}{\partial w_{jk}}  =
(a_k-t_k)  \frac{\partial}{\partial w_{jk}} a_k
\]</span></p>
<p><span class="math display">\[  
= (a_k-t_k)  \frac{\partial}{\partial w_{jk}} g_k(z_k)
\]</span></p>
<p><span class="math display">\[  
= (a_k-t_k)g_k&#39;(z_k)\frac{\partial}{\partial w_{jk}} z_k
\qquad (1)
\]</span></p>
<p>Yine Zincirleme Kanununu kullandık. Şimdi hatırlayalım ki <span
class="math inline">\(z_k=b_j+\sum_j g_j(z_j)w_{jk}\)</span>, yani <span
class="math inline">\(\frac{\partial z_k}{\partial w_{jk}} = g_j(z_j) =
a_j\)</span>, demek ki</p>
<p><span class="math display">\[  
\frac{\partial E}{\partial w_{jk}} =  (a_k-t_k)g_k&#39;(z_k) a_j
\]</span></p>
<p>Bu üç terimin çarpımı, birincisi ağ çıktısı “tahmini’’ ile hedef
değerinin farkı, diğeri çıktı katmanının aktivasyon fonksiyonunun
türevi. Üçüncüsü gizli katmandaki <span
class="math inline">\(j\)</span>’inci düğümün aktivasyon çıktı değeri.
Bu üçlü çarpımı kısaltılmış şekilde göstermek için <span
class="math inline">\(k\)</span> indisi için <span
class="math inline">\(\delta_k\)</span> değişkeni tanımlayabiliriz,</p>
<p><span class="math display">\[ \delta_k =
(a_k-t_k)g_k&#39;(z_k)\]</span></p>
<p>O zaman</p>
<p><span class="math display">\[ \frac{\partial E}{\partial w_{jk}}
=  \delta_k a_j \]</span></p>
<p><span class="math inline">\(\delta_k\)</span> değişkeni hatanın çıktı
aktivasyonunden geriye doğru filtrelenmiş hali olarak görülebilir, yani
bir tür hata sinyali olarak alınabilir. Kabaca belirtmek gerekirse
üstteki formül her <span class="math inline">\(w_{jk}\)</span>’nin nihai
hataya ne kadar ek yaptığını belirtir. Bu sebeple, eğitim sırasında, bu
hataların tersi yönde gidilmelidir ki hata daha azalsın.</p>
<p><span class="math display">\[ w_{jk} \leftarrow w_{jk} -
\eta  \frac{\partial E}{\partial w_{jk}} \]</span></p>
<p>Çıktı Katman Yanlılığı, <span class="math inline">\(b_k\)</span></p>
<p>Yanlılık için <span class="math inline">\(w_{jk}\)</span>’ye benzer
bir yaklaşımı takip ediyoruz, tek fark (1)’deki 3. terim şöyle,</p>
<p><span class="math display">\[
\frac{\partial }{\partial b_k} z_k =
\frac{\partial }{\partial b_k} \big[ b_k + \sum_j g_j(z_j) \big] = 1
\]</span></p>
<p>ve sonuç olarak alttaki gradyanı elde ediyoruz,</p>
<p><span class="math display">\[  \frac{\partial E}{\partial b_k}
(a_k-t_k) g_k&#39;(z_k)(1)\]</span></p>
<p><span class="math display">\[ = \delta_k\]</span></p>
<p>Yani yanlınlık gradyanı sadece çıktı ünitelerinin geriye yayılmış
hali. Bu mantıklı aslında çünkü yanlılığın aktivasyon üzerindeki
ağırlığı hep 1’e eşit, ileri doğru giden sinyal ne olursa olsun. Bu
sebeple yanlılık gradyanı ileri giden sinyalden hiç etkilenmiyor, sadece
hatanın kendisinden etkileniyor.</p>
<p>Gizli Katman Ağırlıklarının Gradyanı</p>
<p>Bu katmanın nihai hata üzerinde etkisi dolaylı olduğu için gizli
katman ağırlıkları <span class="math inline">\(w_{ij}\)</span> için
gradyanları hesaplamak biraz daha zor. Fakat hesaba yine aynı şekilde
başlıyoruz,</p>
<p><span class="math display">\[ \frac{\partial E}{\partial w_{ij}} =
\frac{1}{2} \sum_k (a_k-t_k)^2
\]</span></p>
<p><span class="math display">\[ = \sum_k (a_k-t_k) \frac{\partial
}{\partial w_{ij}} a_k\]</span></p>
<p>Dikkat edersek toplam operatörü bu sefer kaybolmadı, çünkü
katmanların tam bağlı olması sebebiyle, her gizli katman ünitesinden
çıkan sonuç her çıktı katmanındaki üniteyi etkiliyor. Devam edersek, ve
<span class="math inline">\(a_k = g_k(z_k)\)</span> olmasından
hareketle,</p>
<p><span class="math display">\[
\frac{\partial E}{\partial w_{ij}} =
\sum_k (a_k-t_k) \frac{\partial }{\partial w_{ij}} g_k(z_k)
\]</span></p>
<p><span class="math display">\[
= \sum_k (a_k-t_k) g_k&#39;(z_k)
\frac{ \partial }{ \partial w_{ij} } z_k
\qquad (2)
\]</span></p>
<p>Yine Zincirleme Kanununu kullandık. Evet, şimdi işler biraz daha
karmaşıklaşacak. Üstteki formüldeki üçüncü terimde olan kısmi türeve
bakarsak, bu türev <span class="math inline">\(w_{ij}\)</span>’e göre
ama türevi alınan <span class="math inline">\(z_j\)</span> <span
class="math inline">\(j\)</span> indisine bağlı. Şimdi ne yapacağız?
<span class="math inline">\(z_k\)</span>’yi açınca içinde alt
fonksiyonlar olduğunu görüyoruz,</p>
<p><span class="math display">\[ z_k = b_k + \sum_j a_j
w_{jk}\]</span></p>
<p><span class="math display">\[ = b_k + \sum_j g_j(z_j)
w_{jk}\]</span></p>
<p><span class="math display">\[ = b_k + \sum_j g_j (b_i + \sum_i z_i
w_{ij})w_{jk}  \]</span></p>
<p>Üstteki son denklemdeki son terime göre <span
class="math inline">\(z_k\)</span> sadece dolaylı olarak <span
class="math inline">\(w_{ij}\)</span>’ye bağlı. Bu ayrıca demektir ki
Zincirleme Kanununu kullanarak <span
class="math inline">\(\frac{\partial z_k}{\partial w_{ij}}\)</span>’i
hesaplayabiliriz. Türetimin belki de en püf noktalı yeri burası..</p>
<p><span class="math display">\[
\frac{\partial z_k}{\partial w_{ij}} =
\frac{\partial z_k}{\partial a_j}\frac{\partial a_j}{\partial w_{ij}}
\]</span></p>
<p><span class="math display">\[ = \frac{\partial }{\partial a_j} a_j
w_{jk} \frac{\partial a_j}{\partial w_{ij}}\]</span></p>
<p><span class="math display">\[ = w_{jk} \frac{\partial a_j}{\partial
w_{ij}}\]</span></p>
<p><span class="math display">\[ = w_{jk} \frac{\partial
g_j(z_j)}{\partial w_{ij}}\]</span></p>
<p><span class="math display">\[ = w_{jk}g_k&#39;(z_j)\frac{\partial
z_j}{\partial w_{ij}}\]</span></p>
<p><span class="math display">\[ = w_{jk}g_j&#39;(z_j) \frac{\partial
}{\partial w_{ij}}(b_i + \sum_i a_iw_{ij})\]</span></p>
<p><span class="math display">\[ = w_{jk}g_j&#39;(z_j)a_i\]</span></p>
<p>Eğer üstteki formülü (2)’deki <span
class="math inline">\(z_k\)</span>’ye koyarsak, <span
class="math inline">\(\frac{\partial E}{\partial w_{ij}}\)</span> için
şunu elde ederiz,</p>
<p><span class="math display">\[ \frac{\partial E}{\partial w_{ij}} =
\sum_k (a_k-t_k)g_k&#39;(z_k)w_{jk}g_j&#39;(z_j)a_i
\]</span></p>
<p><span class="math display">\[ g_j&#39;(z_j)a_i \sum_k
(a_k-t_k)g_k&#39;(z_k)w_{jk} \]</span></p>
<p><span class="math display">\[ = a_ig_j&#39;(z_j) \sum_k \delta_k
w_{jk} \]</span></p>
<p>Gizli katman yanlılığının gradyanı için Zincirleme Kanunu ile <span
class="math inline">\(\frac{\partial z_k}{\partial b_i}\)</span>
hesabını yapmak lazım.</p>
<p><span class="math display">\[ \frac{\partial z_k}{\partial b_i} =
w_{jk}g_j&#39;(z_j) \frac{\partial z_j}{\partial b_i}
\]</span></p>
<p><span class="math display">\[ = w_{jk}g_j&#39;(z_j) \frac{\partial
}{\partial b_i} (b_i + \sum_i a_iw_{ij})\]</span></p>
<p><span class="math display">\[ = w_{jk}g_j&#39;(z_j) (1)\]</span></p>
<p>bunun sonucu</p>
<p><span class="math display">\[ \frac{\partial E}{\partial b_i} =
g_j&#39;(z_j) \sum_k \delta_k w_{jk} \]</span></p>
<p><span class="math display">\[ = \delta_j \]</span></p>
<p>Özet olarak YSA eğitimi için yapılan hesaplar şunlar:</p>
<ol type="1">
<li><p>İleri yönde sinyalleri girdiden çıktıya doğru hesapla</p></li>
<li><p>Tahmin <span class="math inline">\(a_k\)</span> ve hedef <span
class="math inline">\(t_k\)</span>’ye göre hata <span
class="math inline">\(E\)</span>’yi hesapla</p></li>
<li><p>Hatayı önceki katmanlardaki ağırlıklar ve aktivasyon
fonksiyonlarının gradyanlarına göre geriye doğru yay.</p></li>
<li><p>Parametreleri her parametrenin gradyanı için <span
class="math inline">\(\theta \leftarrow \theta - \eta \frac{\partial
E}{\partial \theta}\)</span> şeklinde güncelle.</p></li>
</ol>
<p>Kodlar</p>
<p>Alttaki kod [3] baz alınarak yazıldı. Bu arada hem gizli, hem
çıktıdaki aktivasyonlar <span class="math inline">\(g_k,g_j\)</span>, ya
da hata <span class="math inline">\(E\)</span> hesabında farklı seçimler
yapılabilir. Mesela [3] üstte belirttiğimiz gibi hata hesaplıyor ama
[2]’deki hata</p>
<p><span class="math display">\[ E = -\frac{1}{N} \sum_n \sum_i y_{n,i}
\log \hat{y}_{n,i}\]</span></p>
<p>ile hesaplanıyor. Her iki kod çıktı aktivasyonu için “softmax’’ denen
fonksiyon kullanıyor. Softmax 0/1 kararı yapan lojistik fonksiyonun
2’den fazla çıktı boyutu için genellenmiş halidir, yani iki seçimden
biri yerine K seçimden biri için kullanılır, ve</p>
<p><span class="math display">\[ \sigma(z)_j =
\frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}\]</span></p>
<p>ile hesaplanır, <span class="math inline">\(z\)</span> vektöründeki
herhangi bazı reel değerleri “ezerek’’ <span
class="math inline">\(\sigma(z)\)</span> vektörü içinde toplamı 1 olacak
değerlere çevirir.</p>
<p>Alttaki kod gizli katman aktivasyonu için sigmoid kullanmış.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> mlp:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,inputs,targets,nhidden,beta<span class="op">=</span><span class="dv">1</span>,momentum<span class="op">=</span><span class="fl">0.9</span>):</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nin <span class="op">=</span> shape(inputs)[<span class="dv">1</span>]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nout <span class="op">=</span> shape(targets)[<span class="dv">1</span>]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ndata <span class="op">=</span> shape(inputs)[<span class="dv">0</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.nhidden <span class="op">=</span> nhidden</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> beta</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.momentum <span class="op">=</span> momentum</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights1 <span class="op">=</span> (random.rand(<span class="va">self</span>.nin<span class="op">+</span><span class="dv">1</span>,<span class="va">self</span>.nhidden)<span class="op">-</span><span class="fl">0.5</span>)<span class="op">*</span><span class="dv">2</span><span class="op">/</span>sqrt(<span class="va">self</span>.nin)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights2 <span class="op">=</span> (random.rand(<span class="va">self</span>.nhidden<span class="op">+</span><span class="dv">1</span>,<span class="va">self</span>.nout)<span class="op">-</span><span class="fl">0.5</span>)<span class="op">*</span><span class="dv">2</span> <span class="op">/</span> <span class="op">\</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                        sqrt(<span class="va">self</span>.nhidden)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> earlystopping(<span class="va">self</span>,inputs,targets,valid,validtargets,eta,niterations<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        valid <span class="op">=</span> concatenate((valid,<span class="op">-</span>ones((shape(valid)[<span class="dv">0</span>],<span class="dv">1</span>))),axis<span class="op">=</span><span class="dv">1</span>)        </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        old_val_error1 <span class="op">=</span> <span class="dv">100002</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        old_val_error2 <span class="op">=</span> <span class="dv">100001</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        new_val_error <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">while</span> (((old_val_error1 <span class="op">-</span> new_val_error) <span class="op">&gt;</span> <span class="fl">0.001</span>) <span class="kw">or</span> <span class="op">\</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>               ((old_val_error2 <span class="op">-</span> old_val_error1)<span class="op">&gt;</span><span class="fl">0.001</span>)):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            count<span class="op">+=</span><span class="dv">1</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mlptrain(inputs,targets,eta,niterations)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>            old_val_error2 <span class="op">=</span> old_val_error1</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            old_val_error1 <span class="op">=</span> new_val_error</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>            validout <span class="op">=</span> <span class="va">self</span>.mlpfwd(valid)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>            new_val_error <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span><span class="bu">sum</span>((validtargets<span class="op">-</span>validout)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span> <span class="st">&quot;Stopped&quot;</span>, new_val_error,old_val_error1, old_val_error2</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> new_val_error</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> mlptrain(<span class="va">self</span>,inputs,targets,eta,niterations):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> concatenate((inputs,<span class="op">-</span>ones((<span class="va">self</span>.ndata,<span class="dv">1</span>))),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        change <span class="op">=</span> <span class="bu">range</span>(<span class="va">self</span>.ndata)    </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        updatew1 <span class="op">=</span> zeros((shape(<span class="va">self</span>.weights1)))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        updatew2 <span class="op">=</span> zeros((shape(<span class="va">self</span>.weights2)))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(niterations):    </span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.outputs <span class="op">=</span> <span class="va">self</span>.mlpfwd(inputs)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span><span class="bu">sum</span>((targets<span class="op">-</span><span class="va">self</span>.outputs)<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>            <span class="co">#if (n % 100) ==0 : print &quot;Iteration: &quot;,n, &quot; Error: &quot;,error    </span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>            deltao <span class="op">=</span> (targets<span class="op">-</span><span class="va">self</span>.outputs)<span class="op">/</span><span class="va">self</span>.ndata            </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>            deltah <span class="op">=</span> <span class="va">self</span>.hidden<span class="op">*</span>(<span class="fl">1.0</span><span class="op">-</span><span class="va">self</span>.hidden)<span class="op">*\</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>                     (dot(deltao,transpose(<span class="va">self</span>.weights2)))</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>            updatew1 <span class="op">=</span> eta<span class="op">*</span>(dot(transpose(inputs),deltah[:,:<span class="op">-</span><span class="dv">1</span>])) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>                       <span class="va">self</span>.momentum<span class="op">*</span>updatew1</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>            updatew2 <span class="op">=</span> eta<span class="op">*</span>(dot(transpose(<span class="va">self</span>.hidden),deltao)) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>                       <span class="va">self</span>.momentum<span class="op">*</span>updatew2</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.weights1 <span class="op">+=</span> updatew1</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.weights2 <span class="op">+=</span> updatew2                </span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            random.shuffle(change)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> inputs[change,:]</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>            targets <span class="op">=</span> targets[change,:]</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> mlpfwd(<span class="va">self</span>,inputs):</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden <span class="op">=</span> dot(inputs,<span class="va">self</span>.weights1)<span class="op">;</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span><span class="op">+</span>exp(<span class="op">-</span><span class="va">self</span>.beta<span class="op">*</span><span class="va">self</span>.hidden))</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden <span class="op">=</span> concatenate((<span class="va">self</span>.hidden,<span class="op">-</span>ones((shape(inputs)[<span class="dv">0</span>],<span class="dv">1</span>))),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> dot(<span class="va">self</span>.hidden,<span class="va">self</span>.weights2)<span class="op">;</span></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>        normalisers <span class="op">=</span> <span class="bu">sum</span>(exp(outputs),axis<span class="op">=</span><span class="dv">1</span>)<span class="op">*</span>ones((<span class="dv">1</span>,shape(outputs)[<span class="dv">0</span>]))</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> transpose(transpose(exp(outputs))<span class="op">/</span>normalisers)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cross_validation <span class="im">import</span> train_test_split</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlp</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_data():</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> datasets.make_moons(<span class="dv">200</span>, noise<span class="op">=</span><span class="fl">0.20</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>one_hot <span class="op">=</span> <span class="kw">lambda</span> x, K : np.array(x[:,<span class="va">None</span>] <span class="op">==</span> np.arange(K)[<span class="va">None</span>, :], dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> generate_data()</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>y2 <span class="op">=</span> one_hot(y, <span class="dv">2</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> X.shape, y2.shape</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y2, test_size<span class="op">=</span><span class="fl">0.2</span>,random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> X_train.shape, y_train.shape</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> X_test.shape, y_test.shape</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>net <span class="op">=</span> mlp.mlp(X_train,y_train,<span class="dv">2</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> net.earlystopping(X_train,y_train,X_test,y_test,<span class="fl">0.1</span>)</span></code></pre></div>
<pre><code>(200, 2) (200, 2)
(160, 2) (160, 2)
(40, 2) (40, 2)
Stopped 2.86548233378 2.86551829438 2.86620503073
Out[1]: 
2.8654823337797608</code></pre>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(x): </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    inputs2 <span class="op">=</span> np.concatenate((x,<span class="op">-</span>np.ones((np.shape(x)[<span class="dv">0</span>],<span class="dv">1</span>))),axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> net.mlpfwd(inputs2)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.where(outputs<span class="op">&gt;</span><span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">0</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_boundary(XX, yyy):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Set min and max values and give it some padding</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> XX[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">.5</span>, XX[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">.5</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> XX[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">.5</span>, XX[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">.5</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate a grid of points with distance h between them</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict the function value for the whole gid</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.argmax(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the contour and training examples</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    plt.contourf(xx, yy, Z, cmap<span class="op">=</span>plt.cm.Spectral)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    yyy <span class="op">=</span> yyy.argmax(axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    XX1 <span class="op">=</span> XX[yyy<span class="op">==</span><span class="dv">0</span>]<span class="op">;</span> XX2 <span class="op">=</span> XX[yyy<span class="op">==</span><span class="dv">1</span>]</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    plt.scatter(XX1[:, <span class="dv">0</span>], XX1[:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;blue&#39;</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    plt.hold(<span class="va">True</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    plt.scatter(XX2[:, <span class="dv">0</span>], XX2[:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(X, y2)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;mlp_06.png&#39;</span>)</span></code></pre></div>
<p><img src="mlp_06.png" /></p>
<p>Alttaki alternatif bir kod, bu kod [2]’yi baz alıyor, YSA ile 0/1
regresyonu yapacak, gizli katman aktivasyonu için <span
class="math inline">\(\tanh\)</span> kullanılmış. Karar sınırları
grafikleniyor.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets, linear_model</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Config:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    nn_input_dim <span class="op">=</span> <span class="dv">2</span>  <span class="co"># input layer dimensionality</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    nn_output_dim <span class="op">=</span> <span class="dv">2</span>  <span class="co"># output layer dimensionality</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    eta <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># learning rate for gradient descent</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    reg_lambda <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># regularization strength</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_data():</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">0</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> datasets.make_moons(<span class="dv">200</span>, noise<span class="op">=</span><span class="fl">0.20</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> visualize(X, y, model):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    plot_decision_boundary(<span class="kw">lambda</span> x:predict(model,x), X, y)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_boundary(pred_func, X, y):</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">.5</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">.5</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">.5</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">.5</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> pred_func(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    plt.contourf(xx, yy, Z, cmap<span class="op">=</span>plt.cm.Spectral)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>plt.cm.Spectral)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_loss(model, X, y):</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>    num_examples <span class="op">=</span> <span class="bu">len</span>(X)  <span class="co"># training set size</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    W1, b1, W2, b2 <span class="op">=</span> model[<span class="st">&#39;W1&#39;</span>], model[<span class="st">&#39;b1&#39;</span>], model[<span class="st">&#39;W2&#39;</span>], model[<span class="st">&#39;b2&#39;</span>]</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    z1 <span class="op">=</span> X.dot(W1) <span class="op">+</span> b1</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>    a1 <span class="op">=</span> np.tanh(z1)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>    z2 <span class="op">=</span> a1.dot(W2) <span class="op">+</span> b2</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    exp_scores <span class="op">=</span> np.exp(z2)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>    corect_logprobs <span class="op">=</span> <span class="op">-</span>np.log(probs[<span class="bu">range</span>(num_examples), y])</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>    data_loss <span class="op">=</span> np.<span class="bu">sum</span>(corect_logprobs)</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    data_loss <span class="op">+=</span> Config.reg_lambda <span class="op">/</span> <span class="dv">2</span> <span class="op">*</span> (np.<span class="bu">sum</span>(np.square(W1)) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>                 np.<span class="bu">sum</span>(np.square(W2)))</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.</span> <span class="op">/</span> num_examples <span class="op">*</span> data_loss</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(model, x):</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>    W1, b1, W2, b2 <span class="op">=</span> model[<span class="st">&#39;W1&#39;</span>], model[<span class="st">&#39;b1&#39;</span>], model[<span class="st">&#39;W2&#39;</span>], model[<span class="st">&#39;b2&#39;</span>]</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>    z1 <span class="op">=</span> x.dot(W1) <span class="op">+</span> b1</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>    a1 <span class="op">=</span> np.tanh(z1)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>    z2 <span class="op">=</span> a1.dot(W2) <span class="op">+</span> b2</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>    exp_scores <span class="op">=</span> np.exp(z2)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.argmax(probs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_model(X, y, nn_hdim, num_passes<span class="op">=</span><span class="dv">20000</span>, print_loss<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>    num_examples <span class="op">=</span> <span class="bu">len</span>(X)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">0</span>)</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>    W1 <span class="op">=</span> np.random.randn(Config.nn_input_dim, nn_hdim) <span class="op">/</span> np.sqrt(Config.nn_input_dim)</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>    b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, nn_hdim))</span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>    W2 <span class="op">=</span> np.random.randn(nn_hdim, Config.nn_output_dim) <span class="op">/</span> np.sqrt(nn_hdim)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, Config.nn_output_dim))</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> {}</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, num_passes):</span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>        z1 <span class="op">=</span> X.dot(W1) <span class="op">+</span> b1</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>        a1 <span class="op">=</span> np.tanh(z1)</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>        z2 <span class="op">=</span> a1.dot(W2) <span class="op">+</span> b2</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>        exp_scores <span class="op">=</span> np.exp(z2)</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>        delta3 <span class="op">=</span> probs</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        delta3[<span class="bu">range</span>(num_examples), y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>        dW2 <span class="op">=</span> (a1.T).dot(delta3)</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        db2 <span class="op">=</span> np.<span class="bu">sum</span>(delta3, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>        delta2 <span class="op">=</span> delta3.dot(W2.T) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> np.power(a1, <span class="dv">2</span>))</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>        dW1 <span class="op">=</span> np.dot(X.T, delta2)</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>        db1 <span class="op">=</span> np.<span class="bu">sum</span>(delta2, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>        dW2 <span class="op">+=</span> Config.reg_lambda <span class="op">*</span> W2</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>        dW1 <span class="op">+=</span> Config.reg_lambda <span class="op">*</span> W1</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>        W1 <span class="op">+=</span> <span class="op">-</span>Config.eta <span class="op">*</span> dW1</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>        b1 <span class="op">+=</span> <span class="op">-</span>Config.eta <span class="op">*</span> db1</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>        W2 <span class="op">+=</span> <span class="op">-</span>Config.eta <span class="op">*</span> dW2</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>        b2 <span class="op">+=</span> <span class="op">-</span>Config.eta <span class="op">*</span> db2</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> {<span class="st">&#39;W1&#39;</span>: W1, <span class="st">&#39;b1&#39;</span>: b1, <span class="st">&#39;W2&#39;</span>: W2, <span class="st">&#39;b2&#39;</span>: b2}</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> print_loss <span class="kw">and</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">&quot;Loss after iteration </span><span class="sc">%i</span><span class="st">: </span><span class="sc">%f</span><span class="st">&quot;</span> <span class="op">%</span> (i, calculate_loss(model, X, y)))</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mlp2</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> mlp2.generate_data()</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> mlp2.build_model(X, y, <span class="dv">3</span>, print_loss<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>mlp2.plot_decision_boundary(<span class="kw">lambda</span> x:predict(model,x), X, y)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;YSA&#39;</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;mlp_08.png&#39;</span>)</span></code></pre></div>
<pre><code>Loss after iteration 0: 0.432387
Loss after iteration 1000: 0.068947
Loss after iteration 2000: 0.068883
Loss after iteration 3000: 0.070752
Loss after iteration 4000: 0.070748
Loss after iteration 5000: 0.070751
Loss after iteration 6000: 0.070754
Loss after iteration 7000: 0.070756
Loss after iteration 8000: 0.070757
Loss after iteration 9000: 0.070758
Loss after iteration 10000: 0.070758
Loss after iteration 11000: 0.070758
Loss after iteration 12000: 0.070758
Loss after iteration 13000: 0.070758
Loss after iteration 14000: 0.070758
Loss after iteration 15000: 0.070758
Loss after iteration 16000: 0.070758
Loss after iteration 17000: 0.070758
Loss after iteration 18000: 0.070758
Loss after iteration 19000: 0.070758</code></pre>
<p><img src="mlp_08.png" /></p>
<p>Karşılaştırmak için lojistik regresyon kullanalım, ve aynı karar
sınırlarını grafikleyelim,</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> linear_model.LogisticRegressionCV()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>clf.fit(X, y)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>mlp2.plot_decision_boundary(<span class="kw">lambda</span> x: clf.predict(x), X, y)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Lojistik Regresyon&#39;</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;mlp_09.png&#39;</span>)</span></code></pre></div>
<p><img src="mlp_09.png" /></p>
<p>Görüldüğü gibi karar sınırı daha basit, kıyasla YSA çok daha esnek
bir şekilde ayrım yapabiliyor.</p>
<p>Aktivasyon Fonksiyonları</p>
<p>Biraz once ReLu’dan bahsettik, <span
class="math inline">\(tanh\)</span> yerine kullanilabilir. Bir diger
fonksiyon sigmoid. Bu fonksiyonlarin tipik grafikleri alttadir.</p>
<p><img src="mlp_10.png" /></p>
<p>Kaynaklar</p>
<p>[1] Stansbury, <em>Derivation: Error Backpropagation &amp; Gradient
Descent for Neural Networks</em>, <a
href="https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/">https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/</a></p>
<p>[2] Britz, <em>Implementing a Neural Network from Scratch in Python -
An Introduction</em>, <a
href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/</a></p>
<p>[3] Marsland, <em>Machine Learning - An Algorithmic Approach</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
