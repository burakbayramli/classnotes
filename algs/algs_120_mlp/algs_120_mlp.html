<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Yapay Sinir Ağları (Neural Networks)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h1 id="yapay-sinir-ağları-neural-networks">Yapay Sinir Ağları (Neural Networks)</h1>
<p>YSA'ları anlamak için onların temel taşı olan tek nöron'u görmemiz lazım.</p>
<div class="figure">
<img src="mlp_03.png" />

</div>
<p>Üstteki resimde üç girdisi ve tek çıktısı olan bir nöron görüyoruz. Girdi olarak <span class="math inline">\(x_1,x_2,x_3\)</span> ve çıktı olarak bir fonksiyon hesabı. Literatürde üstteki yapıya perseptron (perceptron) ismi de veriliyor. Eğer 0/1, evet/hayır türünde çıktıları modellemek istersek bu fonksiyon çoğunlukla sigmoid fonksiyonu olarak seçilir. Yani</p>
<p><span class="math display">\[ h (x) = f(w^Tx) = f(\sum_{i=1}^{3} w_ix_i + b)\]</span></p>
<p>ve</p>
<p><span class="math display">\[ f(z) = \frac{1}{1 + \exp(-z)}\]</span></p>
<p>ki <span class="math inline">\(w\)</span> ağırlık değerlerinin taşıyan bir vektördür. Sigmoid seçiminin bir diğer sebebi türevinin rahat alınabilmesi. Alternatif fonksiyonlar <span class="math inline">\(\tanh\)</span> olabilirdi, ya da düzeltilmiş lineer ünite (rectified linear unit -ReLu-) fonksiyonları. ReLu'lar YSA'ların uzantısı Derin Öğrenim'de oldukca popüler oldu, tanımı</p>
<p><span class="math display">\[ f(z) = \max(0,z)\]</span></p>
<p>Şimdi tek nörondan çok katmanlı bir YSA nasıl oluşturulur görelim,</p>
<div class="figure">
<img src="mlp_04.png" />

</div>
<p>Bu yapıda her <span class="math inline">\(x_i\)</span>'in her nörona gittiğini görüyoruz, bu katmanlara bu sebeple tamamen bağlanmış katman (fully-connected layer), ya da yoğun (dense) katman deniyor. Her türlü girdi / nöron kombinasyonu çarpıldığı için ağırlıklar bir matris içinde tutulur, ve aktivasyondan önceki vektör basit bir matris çarpımı haline gelir, mesela 2. katman aktivasyonu <span class="math inline">\(f_2\)</span> olsun, 1. gizli katman çıktısı</p>
<p><span class="math display">\[ f_2 (W_1x + b) \]</span></p>
<p>Girdi <span class="math inline">\(4 \times 1\)</span>, ağırlık <span class="math inline">\(W_1\)</span> boyutu <span class="math inline">\(3 \times 4\)</span>, o zaman çarpım <span class="math inline">\(3 \times 1\)</span> olacak, <span class="math inline">\(f_2\)</span> uygulanınca boyut değişmez (her öge üzerinde aynı fonksiyon işletiliyor) sonuç <span class="math inline">\(3 \times 1\)</span>, bu da zaten 2. katmana giren (sonra oradan sonraki katmana çıkan) hesap. Üstteki diyagramda yanlılık (bias) ayrı bir nöron olarak eklenmiş (bazen bu yapılmayabiliyor).</p>
<p>İki gizli seviyesi (hidden layer) olan bir YSA,</p>
<div class="figure">
<img src="mlp_05.png" />

</div>
<p>Görüldüğü gibi çok seviyeli durumda bir seviyenin çıktısı bir diğer seviyenin girdisi haline geliyor. YSA'lar denetimli (supervised) şekilde eğitilirler. Elde <span class="math inline">\(x_i,y_i\)</span> veri noktaları vardır, ve her bir ya da çok boyutlu <span class="math inline">\(x_i\)</span> yine bir ya da çok boyutlu <span class="math inline">\(y_i\)</span> ile eşlidir. Aynen regresyonda olduğu gibi bu veri çiftlerini alırız ve onları en iyi temsil eden YSA'yı eğitmek için kullanırız. Eğitim tamamlandığında <span class="math inline">\(i\)</span> katmanındaki <span class="math inline">\(W_i\)</span> ağırlıkları belli değerlere sahip olurlar, ve eldeki &quot;model'' bu ağırlıklardır. Yeni bir veri noktası verildiğinde o veri noktası ağırlıklar her katmanın <span class="math inline">\(W_i\)</span>'sı ile çarpılıp, toplanıp, aktivasyon fonksiyonlarına geçilip, tüm katmanlardan bu şekilde geçirildikten sonra en sondaki çıktı değerine erişilir.</p>
<p>Yanlılık taşımayan şekilde ağ yapısı görelim ve matris çarpımı ile tüm girdileri / çıktıları gösterelim,</p>
<div class="figure">
<img src="nn_nobias.PNG" />

</div>
<p><span class="math display">\[ Girdi = x_0\]</span></p>
<p><span class="math display">\[ x_1 = f_1(W_1x_0)\]</span></p>
<p><span class="math display">\[ x_2 = f_2(W_2x_1)\]</span></p>
<p><span class="math display">\[ Çıktı = f_3(W_3x_2) \]</span></p>
<p>Aktivasyon için <span class="math inline">\(f_1,f_2,f_3\)</span> ReLu ya da sigmoid seçilebilir.</p>
<p>İki gizli seviyesi olan (ve yeterince sayıda nöron içeren) YSA'nın evrensel yaklaşıklayıcı (universal approximator) olduğu iddia edilir, yani üstteki gibi yeterince çetrefil bir YSA her türlü fonksiyonu yaklaşık olarak temsil edebilir. İşin tarihine gidersek, ilk başta tek seviyeli YSA vardı, fakat XOR fonksiyonunu başarılı şekilde temsil edemedi. Gizli (tek) seviyeli olan bunu başardı.</p>
<p>YSA'lar imaj tanıma alanında bolca kullanılmıştır. Bir imajda, mesela sayı tanıma uygulamasında, alttaki gibi pek çok resimler olabilir,</p>
<p><img src="mlp_01.png" /> <img src="mlp_02.png" /></p>
<p>Bir imaj nihayetinde o resmi temsil eden gri değerlerin olduğu bir matristen ibarettir. YSA'ya girdi olması için matris genelde düzleştirilir, ve tek bir vektör olarak verilir, 8x8 matris 64x1 vektörü olur, girdi 64 boyutlu bir <span class="math inline">\(x_i\)</span> çıktı ise tek boyutlu 0,1,2,..,9 gibi etiketler.. Çoğunlukla bu tür kategorik çıktılar ikisel olarak modellenir, 0,..,9 yerine 10 öğeli bir ikisel vektör alınır, 9. öğesi 1 diğerleri 0 yapılır.</p>
<p>YSA'lar nasıl eğitilir? Eldeki <span class="math inline">\(x_i,y_i\)</span> çiftleri YSA'ya verilir, her <span class="math inline">\(x_i\)</span> önce tahmin için kullanılır, ve bir YSA'nın tahmin ettiğine, bir de gerçek veriye bakılır. Bu fark, hata, YSA'nın düzeltilmesi / iyileştirilmesi için kullanılır, burada geriye yayılım (backpropagation -backprop-) adlı bir teknik var. YSA içinde, üstte gördüğümüz gibi, sol seviyeden başlayarak sağa gidecek şekilde, fonksiyonlar, fonksiyonların fonksiyonları şeklinde çetrefil bir yapı var. Backprop elde hatayı alıp YSA'daki tüm nöronların ağırlıklarını bu hataya göre düzeltilmesini, düzelmenin &quot;yayılmasını'' sağlar [1,2]. Bu işlem ardı ardına, farklı her döngüde tekrarlanır, bir süre sonra YSA eldeki veriyi en iyi temsil eden ideal bir hale yaklaşacaktır.</p>
<p>Formüller ve Kod</p>
<div class="figure">
<img src="mlp_07.png" />

</div>
<p>Backprop'un genel bir resmi üstte: a) Eğitim verisi ileri yönde (soldan sağa) YSA'ya veriliyor, ve bu veri her nokta için, o anda sahip olunan ağırlıklara göre bir tahmin üretiyor. b) Eğitim verisindeki beklenen ve tahmin edilen değerler arasındaki hata hesaplanıyor c) Hata YSA'ya &quot;geri yönde'' veriliyor, her ağırlık <span class="math inline">\(w_i\)</span>'dan <span class="math inline">\(\frac{\partial E}{\partial w_i}\)</span> büyüklüğünün bir kısmı çıkartılıyor, bu düzeltmenin ne oranda yapıldığı dışarıdan, önceden kararlaştırılan bir sabit <span class="math inline">\(\eta\)</span>'e oranda yapılıyor.</p>
<p>Daha detaylı bir resimde gösterelim,</p>
<div class="figure">
<img src="neural-net.png" />

</div>
<p>Tek bir gizli katman var, girdi katmanı <span class="math inline">\(a_i\)</span>'den gelen sinyaller <span class="math inline">\(w_{ij}\)</span> ağırlıkları ile çarpılıyor, daha önce gördüğümüz gibi bu tamamen bağlanmış katman, çünkü tüm girdi sinyalleri her gizli katman düğümüne hep beraber giriş yapıyorlar (tabii farklı ağırlıklar üzerinden). Devam edelim, ağırlıklar sonrası bir de yanlılık (bias) eklenecek (resimde atlandı), ve elde edilen <span class="math inline">\(z_i = b_j + \sum_i a_i w_{ij}\)</span>. Bu 3. katmandaki aktivasyon önceki son hal. Gizli katmandan çıkan sinyal <span class="math inline">\(a_j\)</span>, bu sinyaller son katmanda bu sefer <span class="math inline">\(w_{jk}\)</span> ağırlıkları ile çarpılıp toplanacak, <span class="math inline">\(b_k\)</span> yanlılığı eklenecek, ve sonuç <span class="math inline">\(g_k\)</span> aktivasyon fonksiyonundan geçirilip çıktı <span class="math inline">\(a_k\)</span> haline getirilecek.</p>
<p>Bir YSA'yı eğitmek demek öyle bir parametre demeti <span class="math inline">\(\theta = (W,b)\)</span> bulmaktır ki ağın hatası en minimal seviyede olsun. Çoğunlukla bu hata hedef (eğitim) çıktısı <span class="math inline">\(t_k\)</span> ile ağın en son ağırlıklara göre kendi hesapladığı <span class="math inline">\(a_k\)</span> fark karelerinin toplamı üzerinden hesaplanır.</p>
<p><span class="math display">\[ E = \frac{1}{2}  \sum_k (a_k - t_k)^2\]</span></p>
<p>Çıktı katmanının hata fonksiyonu üzerinde direk etkisi olduğu için bu parametreler için gradyanı hesaplamak oldukca kolay,</p>
<p><span class="math display">\[ \frac{\partial E}{\partial w_{jk}} = \frac{1}{2} \sum_k (a_t - t_k)^2\]</span></p>
<p><span class="math display">\[ = (a_k-t_k) \frac{\partial }{\partial w_{jk}} (a_k-t_k)\]</span></p>
<p>Bu türevde Zincirleme Kanununu kullandık. Dikkat edersek toplama operatörü kayboldu, bunun sebebi <span class="math inline">\(j\)</span>'inci düğüme göre türev alıyor olmamız o yüzden <span class="math inline">\(j\)</span>'inci haricinde tüm diğer parametreler türevde sıfırlanıyor, toplamdan geriye tek bir terim kalıyor. <span class="math inline">\(t_k\)</span>'in türevi de sıfır çünkü içinde <span class="math inline">\(w_{jk}\)</span>'ye bağlı hiçbir değişken yok (<span class="math inline">\(t_k\)</span> dışarıdan verilen eğitim verisi sonuçta, çoğu durum için sabit sayılabilir). Ayrıca <span class="math inline">\(a_k = g(z_k)\)</span> olduğunu hatırlayalım. O zaman</p>
<p><span class="math display">\[  
\frac{\partial E}{\partial w_{jk}}  = 
(a_k-t_k)  \frac{\partial}{\partial w_{jk}} a_k 
\]</span></p>
<p><span class="math display">\[  
= (a_k-t_k)  \frac{\partial}{\partial w_{jk}} g_k(z_k) 
\]</span></p>
<p><span class="math display">\[  
= (a_k-t_k)g_k&#39;(z_k)\frac{\partial}{\partial w_{jk}} z_k 
\qquad (1)
\]</span></p>
<p>Yine Zincirleme Kanununu kullandık. Şimdi hatırlayalım ki <span class="math inline">\(z_k=b_j+\sum_j g_j(z_j)w_{jk}\)</span>, yani <span class="math inline">\(\frac{\partial z_k}{\partial w_{jk}} = g_j(z_j) = a_j\)</span>, demek ki</p>
<p><span class="math display">\[  
\frac{\partial E}{\partial w_{jk}} =  (a_k-t_k)g_k&#39;(z_k) a_j
\]</span></p>
<p>Bu üç terimin çarpımı, birincisi ağ çıktısı &quot;tahmini'' ile hedef değerinin farkı, diğeri çıktı katmanının aktivasyon fonksiyonunun türevi. Üçüncüsü gizli katmandaki <span class="math inline">\(j\)</span>'inci düğümün aktivasyon çıktı değeri. Bu üçlü çarpımı kısaltılmış şekilde göstermek için <span class="math inline">\(k\)</span> indisi için <span class="math inline">\(\delta_k\)</span> değişkeni tanımlayabiliriz,</p>
<p><span class="math display">\[ \delta_k = (a_k-t_k)g_k&#39;(z_k)\]</span></p>
<p>O zaman</p>
<p><span class="math display">\[ \frac{\partial E}{\partial w_{jk}} =  \delta_k a_j \]</span></p>
<p><span class="math inline">\(\delta_k\)</span> değişkeni hatanın çıktı aktivasyonunden geriye doğru filtrelenmiş hali olarak görülebilir, yani bir tür hata sinyali olarak alınabilir. Kabaca belirtmek gerekirse üstteki formül her <span class="math inline">\(w_{jk}\)</span>'nin nihai hataya ne kadar ek yaptığını belirtir. Bu sebeple, eğitim sırasında, bu hataların tersi yönde gidilmelidir ki hata daha azalsın.</p>
<p><span class="math display">\[ w_{jk} \leftarrow w_{jk} - \eta  \frac{\partial E}{\partial w_{jk}} \]</span></p>
<p>Çıktı Katman Yanlılığı, <span class="math inline">\(b_k\)</span></p>
<p>Yanlılık için <span class="math inline">\(w_{jk}\)</span>'ye benzer bir yaklaşımı takip ediyoruz, tek fark (1)'deki 3. terim şöyle,</p>
<p><span class="math display">\[ 
\frac{\partial }{\partial b_k} z_k = 
\frac{\partial }{\partial b_k} \big[ b_k + \sum_j g_j(z_j) \big] = 1 
\]</span></p>
<p>ve sonuç olarak alttaki gradyanı elde ediyoruz,</p>
<p><span class="math display">\[  \frac{\partial E}{\partial b_k} (a_k-t_k) g_k&#39;(z_k)(1)\]</span></p>
<p><span class="math display">\[ = \delta_k\]</span></p>
<p>Yani yanlınlık gradyanı sadece çıktı ünitelerinin geriye yayılmış hali. Bu mantıklı aslında çünkü yanlılığın aktivasyon üzerindeki ağırlığı hep 1'e eşit, ileri doğru giden sinyal ne olursa olsun. Bu sebeple yanlılık gradyanı ileri giden sinyalden hiç etkilenmiyor, sadece hatanın kendisinden etkileniyor.</p>
<p>Gizli Katman Ağırlıklarının Gradyanı</p>
<p>Bu katmanın nihai hata üzerinde etkisi dolaylı olduğu için gizli katman ağırlıkları <span class="math inline">\(w_{ij}\)</span> için gradyanları hesaplamak biraz daha zor. Fakat hesaba yine aynı şekilde başlıyoruz,</p>
<p><span class="math display">\[ \frac{\partial E}{\partial w_{ij}} = 
\frac{1}{2} \sum_k (a_k-t_k)^2
\]</span></p>
<p><span class="math display">\[ = \sum_k (a_k-t_k) \frac{\partial }{\partial w_{ij}} a_k\]</span></p>
<p>Dikkat edersek toplam operatörü bu sefer kaybolmadı, çünkü katmanların tam bağlı olması sebebiyle, her gizli katman ünitesinden çıkan sonuç her çıktı katmanındaki üniteyi etkiliyor. Devam edersek, ve <span class="math inline">\(a_k = g_k(z_k)\)</span> olmasından hareketle,</p>
<p><span class="math display">\[ 
\frac{\partial E}{\partial w_{ij}} = 
\sum_k (a_k-t_k) \frac{\partial }{\partial w_{ij}} g_k(z_k)
\]</span></p>
<p><span class="math display">\[ 
= \sum_k (a_k-t_k) g_k&#39;(z_k)
\frac{ \partial }{ \partial w_{ij} } z_k
\qquad (2)
\]</span></p>
<p>Yine Zincirleme Kanununu kullandık. Evet, şimdi işler biraz daha karmaşıklaşacak. Üstteki formüldeki üçüncü terimde olan kısmi türeve bakarsak, bu türev <span class="math inline">\(w_{ij}\)</span>'e göre ama türevi alınan <span class="math inline">\(z_j\)</span> <span class="math inline">\(j\)</span> indisine bağlı. Şimdi ne yapacağız? <span class="math inline">\(z_k\)</span>'yi açınca içinde alt fonksiyonlar olduğunu görüyoruz,</p>
<p><span class="math display">\[ z_k = b_k + \sum_j a_j w_{jk}\]</span></p>
<p><span class="math display">\[ = b_k + \sum_j g_j(z_j) w_{jk}\]</span></p>
<p><span class="math display">\[ = b_k + \sum_j g_j (b_i + \sum_i z_i w_{ij})w_{jk}  \]</span></p>
<p>Üstteki son denklemdeki son terime göre <span class="math inline">\(z_k\)</span> sadece dolaylı olarak <span class="math inline">\(w_{ij}\)</span>'ye bağlı. Bu ayrıca demektir ki Zincirleme Kanununu kullanarak <span class="math inline">\(\frac{\partial z_k}{\partial w_{ij}}\)</span>'i hesaplayabiliriz. Türetimin belki de en püf noktalı yeri burası..</p>
<p><span class="math display">\[ 
\frac{\partial z_k}{\partial w_{ij}} =
\frac{\partial z_k}{\partial a_j}\frac{\partial a_j}{\partial w_{ij}}
\]</span></p>
<p><span class="math display">\[ = \frac{\partial }{\partial a_j} a_j w_{jk} \frac{\partial a_j}{\partial w_{ij}}\]</span></p>
<p><span class="math display">\[ = w_{jk} \frac{\partial a_j}{\partial w_{ij}}\]</span></p>
<p><span class="math display">\[ = w_{jk} \frac{\partial g_j(z_j)}{\partial w_{ij}}\]</span></p>
<p><span class="math display">\[ = w_{jk}g_k&#39;(z_j)\frac{\partial z_j}{\partial w_{ij}}\]</span></p>
<p><span class="math display">\[ = w_{jk}g_j&#39;(z_j) \frac{\partial }{\partial w_{ij}}(b_i + \sum_i a_iw_{ij})\]</span></p>
<p><span class="math display">\[ = w_{jk}g_j&#39;(z_j)a_i\]</span></p>
<p>Eğer üstteki formülü (2)'deki <span class="math inline">\(z_k\)</span>'ye koyarsak, <span class="math inline">\(\frac{\partial E}{\partial w_{ij}}\)</span> için şunu elde ederiz,</p>
<p><span class="math display">\[ \frac{\partial E}{\partial w_{ij}} = 
\sum_k (a_k-t_k)g_k&#39;(z_k)w_{jk}g_j&#39;(z_j)a_i
\]</span></p>
<p><span class="math display">\[ g_j&#39;(z_j)a_i \sum_k (a_k-t_k)g_k&#39;(z_k)w_{jk} \]</span></p>
<p><span class="math display">\[ = a_ig_j&#39;(z_j) \sum_k \delta_k w_{jk} \]</span></p>
<p>Gizli katman yanlılığının gradyanı için Zincirleme Kanunu ile <span class="math inline">\(\frac{\partial z_k}{\partial b_i}\)</span> hesabını yapmak lazım.</p>
<p><span class="math display">\[ \frac{\partial z_k}{\partial b_i} = 
w_{jk}g_j&#39;(z_j) \frac{\partial z_j}{\partial b_i} 
\]</span></p>
<p><span class="math display">\[ = w_{jk}g_j&#39;(z_j) \frac{\partial }{\partial b_i} (b_i + \sum_i a_iw_{ij})\]</span></p>
<p><span class="math display">\[ = w_{jk}g_j&#39;(z_j) (1)\]</span></p>
<p>bunun sonucu</p>
<p><span class="math display">\[ \frac{\partial E}{\partial b_i} = g_j&#39;(z_j) \sum_k \delta_k w_{jk} \]</span></p>
<p><span class="math display">\[ = \delta_j \]</span></p>
<p>Özet olarak YSA eğitimi için yapılan hesaplar şunlar:</p>
<ol style="list-style-type: decimal">
<li><p>İleri yönde sinyalleri girdiden çıktıya doğru hesapla</p></li>
<li><p>Tahmin <span class="math inline">\(a_k\)</span> ve hedef <span class="math inline">\(t_k\)</span>'ye göre hata <span class="math inline">\(E\)</span>'yi hesapla</p></li>
<li><p>Hatayı önceki katmanlardaki ağırlıklar ve aktivasyon fonksiyonlarının gradyanlarına göre geriye doğru yay.</p></li>
<li><p>Parametreleri her parametrenin gradyanı için <span class="math inline">\(\theta \leftarrow \theta - \eta \frac{\partial E}{\partial \theta}\)</span> şeklinde güncelle.</p></li>
</ol>
<p>Kodlar</p>
<p>Alttaki kod [3] baz alınarak yazıldı. Bu arada hem gizli, hem çıktıdaki aktivasyonlar <span class="math inline">\(g_k,g_j\)</span>, ya da hata <span class="math inline">\(E\)</span> hesabında farklı seçimler yapılabilir. Mesela [3] üstte belirttiğimiz gibi hata hesaplıyor ama [2]'deki hata</p>
<p><span class="math display">\[ E = -\frac{1}{N} \sum_n \sum_i y_{n,i} \log \hat{y}_{n,i}\]</span></p>
<p>ile hesaplanıyor. Her iki kod çıktı aktivasyonu için &quot;softmax'' denen fonksiyon kullanıyor. Softmax 0/1 kararı yapan lojistik fonksiyonun 2'den fazla çıktı boyutu için genellenmiş halidir, yani iki seçimden biri yerine K seçimden biri için kullanılır, ve</p>
<p><span class="math display">\[ \sigma(z)_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}\]</span></p>
<p>ile hesaplanır, <span class="math inline">\(z\)</span> vektöründeki herhangi bazı reel değerleri &quot;ezerek'' <span class="math inline">\(\sigma(z)\)</span> vektörü içinde toplamı 1 olacak değerlere çevirir.</p>
<p>Alttaki kod gizli katman aktivasyonu için sigmoid kullanmış.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> numpy <span class="im">import</span> <span class="op">*</span>

<span class="kw">class</span> mlp:
    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,inputs,targets,nhidden,beta<span class="op">=</span><span class="dv">1</span>,momentum<span class="op">=</span><span class="fl">0.9</span>):
        <span class="va">self</span>.nin <span class="op">=</span> shape(inputs)[<span class="dv">1</span>]
        <span class="va">self</span>.nout <span class="op">=</span> shape(targets)[<span class="dv">1</span>]
        <span class="va">self</span>.ndata <span class="op">=</span> shape(inputs)[<span class="dv">0</span>]
        <span class="va">self</span>.nhidden <span class="op">=</span> nhidden
        <span class="va">self</span>.beta <span class="op">=</span> beta
        <span class="va">self</span>.momentum <span class="op">=</span> momentum
        <span class="va">self</span>.weights1 <span class="op">=</span> (random.rand(<span class="va">self</span>.nin<span class="op">+</span><span class="dv">1</span>,<span class="va">self</span>.nhidden)<span class="op">-</span><span class="fl">0.5</span>)<span class="op">*</span><span class="dv">2</span><span class="op">/</span>sqrt(<span class="va">self</span>.nin)
        <span class="va">self</span>.weights2 <span class="op">=</span> (random.rand(<span class="va">self</span>.nhidden<span class="op">+</span><span class="dv">1</span>,<span class="va">self</span>.nout)<span class="op">-</span><span class="fl">0.5</span>)<span class="op">*</span><span class="dv">2</span> <span class="op">/</span> <span class="op">\</span>
                        sqrt(<span class="va">self</span>.nhidden)

    <span class="kw">def</span> earlystopping(<span class="va">self</span>,inputs,targets,valid,validtargets,eta,niterations<span class="op">=</span><span class="dv">100</span>):
    
        valid <span class="op">=</span> concatenate((valid,<span class="op">-</span>ones((shape(valid)[<span class="dv">0</span>],<span class="dv">1</span>))),axis<span class="op">=</span><span class="dv">1</span>)        
        old_val_error1 <span class="op">=</span> <span class="dv">100002</span>
        old_val_error2 <span class="op">=</span> <span class="dv">100001</span>
        new_val_error <span class="op">=</span> <span class="dv">100000</span>
        
        count <span class="op">=</span> <span class="dv">0</span>
        <span class="cf">while</span> (((old_val_error1 <span class="op">-</span> new_val_error) <span class="op">&gt;</span> <span class="fl">0.001</span>) <span class="kw">or</span> <span class="op">\</span>
               ((old_val_error2 <span class="op">-</span> old_val_error1)<span class="op">&gt;</span><span class="fl">0.001</span>)):
            count<span class="op">+=</span><span class="dv">1</span>
            <span class="va">self</span>.mlptrain(inputs,targets,eta,niterations)
            old_val_error2 <span class="op">=</span> old_val_error1
            old_val_error1 <span class="op">=</span> new_val_error
            validout <span class="op">=</span> <span class="va">self</span>.mlpfwd(valid)
            new_val_error <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span><span class="bu">sum</span>((validtargets<span class="op">-</span>validout)<span class="op">**</span><span class="dv">2</span>)
            
        <span class="bu">print</span> <span class="st">&quot;Stopped&quot;</span>, new_val_error,old_val_error1, old_val_error2
        <span class="cf">return</span> new_val_error
        
    <span class="kw">def</span> mlptrain(<span class="va">self</span>,inputs,targets,eta,niterations):
        inputs <span class="op">=</span> concatenate((inputs,<span class="op">-</span>ones((<span class="va">self</span>.ndata,<span class="dv">1</span>))),axis<span class="op">=</span><span class="dv">1</span>)
        change <span class="op">=</span> <span class="bu">range</span>(<span class="va">self</span>.ndata)    
        updatew1 <span class="op">=</span> zeros((shape(<span class="va">self</span>.weights1)))
        updatew2 <span class="op">=</span> zeros((shape(<span class="va">self</span>.weights2)))
        <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(niterations):    
            <span class="va">self</span>.outputs <span class="op">=</span> <span class="va">self</span>.mlpfwd(inputs)
            error <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span><span class="bu">sum</span>((targets<span class="op">-</span><span class="va">self</span>.outputs)<span class="op">**</span><span class="dv">2</span>)
            <span class="co">#if (n % 100) ==0 : print &quot;Iteration: &quot;,n, &quot; Error: &quot;,error    </span>
            deltao <span class="op">=</span> (targets<span class="op">-</span><span class="va">self</span>.outputs)<span class="op">/</span><span class="va">self</span>.ndata            
            deltah <span class="op">=</span> <span class="va">self</span>.hidden<span class="op">*</span>(<span class="fl">1.0</span><span class="op">-</span><span class="va">self</span>.hidden)<span class="op">*\</span>
                     (dot(deltao,transpose(<span class="va">self</span>.weights2)))
            updatew1 <span class="op">=</span> eta<span class="op">*</span>(dot(transpose(inputs),deltah[:,:<span class="op">-</span><span class="dv">1</span>])) <span class="op">+</span> <span class="op">\</span>
                       <span class="va">self</span>.momentum<span class="op">*</span>updatew1
            updatew2 <span class="op">=</span> eta<span class="op">*</span>(dot(transpose(<span class="va">self</span>.hidden),deltao)) <span class="op">+</span> <span class="op">\</span>
                       <span class="va">self</span>.momentum<span class="op">*</span>updatew2
            <span class="va">self</span>.weights1 <span class="op">+=</span> updatew1
            <span class="va">self</span>.weights2 <span class="op">+=</span> updatew2                

            random.shuffle(change)
            inputs <span class="op">=</span> inputs[change,:]
            targets <span class="op">=</span> targets[change,:]
            
    <span class="kw">def</span> mlpfwd(<span class="va">self</span>,inputs):
        <span class="va">self</span>.hidden <span class="op">=</span> dot(inputs,<span class="va">self</span>.weights1)<span class="op">;</span>
        <span class="va">self</span>.hidden <span class="op">=</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="fl">1.0</span><span class="op">+</span>exp(<span class="op">-</span><span class="va">self</span>.beta<span class="op">*</span><span class="va">self</span>.hidden))
        <span class="va">self</span>.hidden <span class="op">=</span> concatenate((<span class="va">self</span>.hidden,<span class="op">-</span>ones((shape(inputs)[<span class="dv">0</span>],<span class="dv">1</span>))),axis<span class="op">=</span><span class="dv">1</span>)
        outputs <span class="op">=</span> dot(<span class="va">self</span>.hidden,<span class="va">self</span>.weights2)<span class="op">;</span>
        normalisers <span class="op">=</span> <span class="bu">sum</span>(exp(outputs),axis<span class="op">=</span><span class="dv">1</span>)<span class="op">*</span>ones((<span class="dv">1</span>,shape(outputs)[<span class="dv">0</span>]))
        <span class="cf">return</span> transpose(transpose(exp(outputs))<span class="op">/</span>normalisers)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.cross_validation <span class="im">import</span> train_test_split
<span class="im">from</span> sklearn <span class="im">import</span> datasets
<span class="im">import</span> mlp

<span class="kw">def</span> generate_data():
    np.random.seed(<span class="dv">0</span>)
    X, y <span class="op">=</span> datasets.make_moons(<span class="dv">200</span>, noise<span class="op">=</span><span class="fl">0.20</span>)
    <span class="cf">return</span> X, y

one_hot <span class="op">=</span> <span class="kw">lambda</span> x, K : np.array(x[:,<span class="va">None</span>] <span class="op">==</span> np.arange(K)[<span class="va">None</span>, :], dtype<span class="op">=</span><span class="bu">int</span>)

X, y <span class="op">=</span> generate_data()
y2 <span class="op">=</span> one_hot(y, <span class="dv">2</span>)
<span class="bu">print</span> X.shape, y2.shape
X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y2, test_size<span class="op">=</span><span class="fl">0.2</span>,random_state<span class="op">=</span><span class="dv">0</span>)
<span class="bu">print</span> X_train.shape, y_train.shape
<span class="bu">print</span> X_test.shape, y_test.shape
net <span class="op">=</span> mlp.mlp(X_train,y_train,<span class="dv">2</span>)
<span class="bu">print</span> net.earlystopping(X_train,y_train,X_test,y_test,<span class="fl">0.1</span>)</code></pre></div>
<pre><code>(200, 2) (200, 2)
(160, 2) (160, 2)
(40, 2) (40, 2)
Stopped 2.86548233378 2.86551829438 2.86620503073
Out[1]: 
2.8654823337797608</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> predict(x): 
    inputs2 <span class="op">=</span> np.concatenate((x,<span class="op">-</span>np.ones((np.shape(x)[<span class="dv">0</span>],<span class="dv">1</span>))),axis<span class="op">=</span><span class="dv">1</span>)
    outputs <span class="op">=</span> net.mlpfwd(inputs2)
    <span class="cf">return</span> np.where(outputs<span class="op">&gt;</span><span class="fl">0.5</span>,<span class="dv">1</span>,<span class="dv">0</span>)

<span class="kw">def</span> plot_decision_boundary(XX, yyy):
    <span class="co"># Set min and max values and give it some padding</span>
    x_min, x_max <span class="op">=</span> XX[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">.5</span>, XX[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">.5</span>
    y_min, y_max <span class="op">=</span> XX[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">.5</span>, XX[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">.5</span>
    h <span class="op">=</span> <span class="fl">0.01</span>
    <span class="co"># Generate a grid of points with distance h between them</span>
    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    <span class="co"># Predict the function value for the whole gid</span>
    Z <span class="op">=</span> predict(np.c_[xx.ravel(), yy.ravel()])
    Z <span class="op">=</span> Z.argmax(axis<span class="op">=</span><span class="dv">1</span>)
    Z <span class="op">=</span> Z.reshape(xx.shape)
    <span class="co"># Plot the contour and training examples</span>
    plt.contourf(xx, yy, Z, cmap<span class="op">=</span>plt.cm.Spectral)
    yyy <span class="op">=</span> yyy.argmax(axis<span class="op">=</span><span class="dv">1</span>)
    XX1 <span class="op">=</span> XX[yyy<span class="op">==</span><span class="dv">0</span>]<span class="op">;</span> XX2 <span class="op">=</span> XX[yyy<span class="op">==</span><span class="dv">1</span>]
    plt.scatter(XX1[:, <span class="dv">0</span>], XX1[:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;blue&#39;</span>)
    plt.hold(<span class="va">True</span>)
    plt.scatter(XX2[:, <span class="dv">0</span>], XX2[:, <span class="dv">1</span>], color<span class="op">=</span><span class="st">&#39;red&#39;</span>)

plot_decision_boundary(X, y2)
plt.savefig(<span class="st">&#39;mlp_06.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="mlp_06.png" />

</div>
<p>Alttaki alternatif bir kod, bu kod [2]'yi baz alıyor, YSA ile 0/1 regresyonu yapacak, gizli katman aktivasyonu için <span class="math inline">\(\tanh\)</span> kullanılmış. Karar sınırları grafikleniyor.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">from</span> sklearn <span class="im">import</span> datasets, linear_model

<span class="kw">class</span> Config:
    nn_input_dim <span class="op">=</span> <span class="dv">2</span>  <span class="co"># input layer dimensionality</span>
    nn_output_dim <span class="op">=</span> <span class="dv">2</span>  <span class="co"># output layer dimensionality</span>
    eta <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># learning rate for gradient descent</span>
    reg_lambda <span class="op">=</span> <span class="fl">0.01</span>  <span class="co"># regularization strength</span>

<span class="kw">def</span> generate_data():
    np.random.seed(<span class="dv">0</span>)
    X, y <span class="op">=</span> datasets.make_moons(<span class="dv">200</span>, noise<span class="op">=</span><span class="fl">0.20</span>)
    <span class="cf">return</span> X, y

<span class="kw">def</span> visualize(X, y, model):
    plot_decision_boundary(<span class="kw">lambda</span> x:predict(model,x), X, y)

<span class="kw">def</span> plot_decision_boundary(pred_func, X, y):
    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">.5</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">.5</span>
    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="fl">.5</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="fl">.5</span>
    h <span class="op">=</span> <span class="fl">0.01</span>
    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    Z <span class="op">=</span> pred_func(np.c_[xx.ravel(), yy.ravel()])
    Z <span class="op">=</span> Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap<span class="op">=</span>plt.cm.Spectral)
    plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>plt.cm.Spectral)

<span class="kw">def</span> calculate_loss(model, X, y):
    num_examples <span class="op">=</span> <span class="bu">len</span>(X)  <span class="co"># training set size</span>
    W1, b1, W2, b2 <span class="op">=</span> model[<span class="st">&#39;W1&#39;</span>], model[<span class="st">&#39;b1&#39;</span>], model[<span class="st">&#39;W2&#39;</span>], model[<span class="st">&#39;b2&#39;</span>]
    z1 <span class="op">=</span> X.dot(W1) <span class="op">+</span> b1
    a1 <span class="op">=</span> np.tanh(z1)
    z2 <span class="op">=</span> a1.dot(W2) <span class="op">+</span> b2
    exp_scores <span class="op">=</span> np.exp(z2)
    probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)
    corect_logprobs <span class="op">=</span> <span class="op">-</span>np.log(probs[<span class="bu">range</span>(num_examples), y])
    data_loss <span class="op">=</span> np.<span class="bu">sum</span>(corect_logprobs)
    data_loss <span class="op">+=</span> Config.reg_lambda <span class="op">/</span> <span class="dv">2</span> <span class="op">*</span> (np.<span class="bu">sum</span>(np.square(W1)) <span class="op">+</span> <span class="op">\</span>
                 np.<span class="bu">sum</span>(np.square(W2)))
    <span class="cf">return</span> <span class="fl">1.</span> <span class="op">/</span> num_examples <span class="op">*</span> data_loss


<span class="kw">def</span> predict(model, x):
    W1, b1, W2, b2 <span class="op">=</span> model[<span class="st">&#39;W1&#39;</span>], model[<span class="st">&#39;b1&#39;</span>], model[<span class="st">&#39;W2&#39;</span>], model[<span class="st">&#39;b2&#39;</span>]
    z1 <span class="op">=</span> x.dot(W1) <span class="op">+</span> b1
    a1 <span class="op">=</span> np.tanh(z1)
    z2 <span class="op">=</span> a1.dot(W2) <span class="op">+</span> b2
    exp_scores <span class="op">=</span> np.exp(z2)
    probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)
    <span class="cf">return</span> np.argmax(probs, axis<span class="op">=</span><span class="dv">1</span>)

<span class="kw">def</span> build_model(X, y, nn_hdim, num_passes<span class="op">=</span><span class="dv">20000</span>, print_loss<span class="op">=</span><span class="va">False</span>):
    num_examples <span class="op">=</span> <span class="bu">len</span>(X)
    np.random.seed(<span class="dv">0</span>)
    W1 <span class="op">=</span> np.random.randn(Config.nn_input_dim, nn_hdim) <span class="op">/</span> np.sqrt(Config.nn_input_dim)
    b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, nn_hdim))
    W2 <span class="op">=</span> np.random.randn(nn_hdim, Config.nn_output_dim) <span class="op">/</span> np.sqrt(nn_hdim)
    b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, Config.nn_output_dim))
    model <span class="op">=</span> {}

    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, num_passes):
        z1 <span class="op">=</span> X.dot(W1) <span class="op">+</span> b1
        a1 <span class="op">=</span> np.tanh(z1)
        z2 <span class="op">=</span> a1.dot(W2) <span class="op">+</span> b2
        exp_scores <span class="op">=</span> np.exp(z2)
        probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)

        delta3 <span class="op">=</span> probs
        delta3[<span class="bu">range</span>(num_examples), y] <span class="op">-=</span> <span class="dv">1</span>
        dW2 <span class="op">=</span> (a1.T).dot(delta3)
        db2 <span class="op">=</span> np.<span class="bu">sum</span>(delta3, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)
        delta2 <span class="op">=</span> delta3.dot(W2.T) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> np.power(a1, <span class="dv">2</span>))
        dW1 <span class="op">=</span> np.dot(X.T, delta2)
        db1 <span class="op">=</span> np.<span class="bu">sum</span>(delta2, axis<span class="op">=</span><span class="dv">0</span>)

        dW2 <span class="op">+=</span> Config.reg_lambda <span class="op">*</span> W2
        dW1 <span class="op">+=</span> Config.reg_lambda <span class="op">*</span> W1

        W1 <span class="op">+=</span> <span class="op">-</span>Config.eta <span class="op">*</span> dW1
        b1 <span class="op">+=</span> <span class="op">-</span>Config.eta <span class="op">*</span> db1
        W2 <span class="op">+=</span> <span class="op">-</span>Config.eta <span class="op">*</span> dW2
        b2 <span class="op">+=</span> <span class="op">-</span>Config.eta <span class="op">*</span> db2

        model <span class="op">=</span> {<span class="st">&#39;W1&#39;</span>: W1, <span class="st">&#39;b1&#39;</span>: b1, <span class="st">&#39;W2&#39;</span>: W2, <span class="st">&#39;b2&#39;</span>: b2}
        <span class="cf">if</span> print_loss <span class="kw">and</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:
            <span class="bu">print</span>(<span class="st">&quot;Loss after iteration </span><span class="sc">%i</span><span class="st">: </span><span class="sc">%f</span><span class="st">&quot;</span> <span class="op">%</span> (i, calculate_loss(model, X, y)))

    <span class="cf">return</span> model</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> mlp2
X, y <span class="op">=</span> mlp2.generate_data()

model <span class="op">=</span> mlp2.build_model(X, y, <span class="dv">3</span>, print_loss<span class="op">=</span><span class="va">True</span>)
mlp2.plot_decision_boundary(<span class="kw">lambda</span> x:predict(model,x), X, y)
plt.title(<span class="st">&#39;YSA&#39;</span>)
plt.savefig(<span class="st">&#39;mlp_08.png&#39;</span>)</code></pre></div>
<pre><code>Loss after iteration 0: 0.432387
Loss after iteration 1000: 0.068947
Loss after iteration 2000: 0.068883
Loss after iteration 3000: 0.070752
Loss after iteration 4000: 0.070748
Loss after iteration 5000: 0.070751
Loss after iteration 6000: 0.070754
Loss after iteration 7000: 0.070756
Loss after iteration 8000: 0.070757
Loss after iteration 9000: 0.070758
Loss after iteration 10000: 0.070758
Loss after iteration 11000: 0.070758
Loss after iteration 12000: 0.070758
Loss after iteration 13000: 0.070758
Loss after iteration 14000: 0.070758
Loss after iteration 15000: 0.070758
Loss after iteration 16000: 0.070758
Loss after iteration 17000: 0.070758
Loss after iteration 18000: 0.070758
Loss after iteration 19000: 0.070758</code></pre>
<div class="figure">
<img src="mlp_08.png" />

</div>
<p>Karşılaştırmak için lojistik regresyon kullanalım, ve aynı karar sınırlarını grafikleyelim,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">clf <span class="op">=</span> linear_model.LogisticRegressionCV()
clf.fit(X, y)
mlp2.plot_decision_boundary(<span class="kw">lambda</span> x: clf.predict(x), X, y)
plt.title(<span class="st">&#39;Lojistik Regresyon&#39;</span>)
plt.savefig(<span class="st">&#39;mlp_09.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="mlp_09.png" />

</div>
<p>Görüldüğü gibi karar sınırı daha basit, kıyasla YSA çok daha esnek bir şekilde ayrım yapabiliyor.</p>
<p>Aktivasyon Fonksiyonları</p>
<p>Biraz once ReLu'dan bahsettik, <span class="math inline">\(tanh\)</span> yerine kullanilabilir. Bir diger fonksiyon sigmoid. Bu fonksiyonlarin tipik grafikleri alttadir.</p>
<div class="figure">
<img src="mlp_10.png" />

</div>
<p>Kaynaklar</p>
<p>[1] Stansbury, <em>Derivation: Error Backpropagation &amp; Gradient Descent for Neural Networks</em>, <a href="https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/" class="uri">https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/</a></p>
<p>[2] Britz, <em>Implementing a Neural Network from Scratch in Python - An Introduction</em>, <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" class="uri">http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/</a></p>
<p>[3] Marsland, <em>Machine Learning - An Algorithmic Approach</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
