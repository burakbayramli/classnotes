<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="otomatik-türev-almak-automatic-differentiation--ad-">Otomatik Türev Almak (Automatic Differentiation -AD-)</h1>
<p>Matematikte türev hesaplamanın birkaç yöntemi var; bunlardan birincisi Calculus'ta öğretilen sembolik türevdir, diğeri sayısal türevdir. AD üçüncü bir yöntem sayılıyor, özellikle programlama bağlamında çok faydalı bir özelliği var, <em>herhangi</em> bir yazılım fonksiyonunu alıp, bir veri noktası bağlamında, o fonksiyonun içinde ne türlü temel diğer fonksiyonlar olursa olsun onu kendi türevini hesaplayacak hale çevirebiliyor. Burada kod değişimi söz konusu, değişim işlem anında dinamik, ya da kaynak kod seviyesinde derleme öncesi yapılabiliyor. Fonksiyon <code>if</code>, <code>goto</code> gibi dallanma, koşulsal ifadeler içeriyor olabilir (ama bu ifadeler üzerinden türev alınan değişkene bağlı olmamalıdır), <span class="math inline">\(\cos,\sin\)</span> gibi trigonometrik ifadeler, polinomlar, <span class="math inline">\(\max,\min\)</span> gibi gayrı-lineer ifadeler, ya da başka herhangi temel hesapları kullanıyor olabilir, son derece çetrefil bir takım hesaplar zincirleme yapılıyor olabilir. Eğer hesap deterministik bir şekilde yapılabiliyorsa (aynı girdiler için hep aynı değer hesaplanıyor) AD onu alıp kendi türevini hesaplayabilen hale çevirebiliyor.</p>
<p>Bu son derece kuvvetli bir özellik. Pek çok optimizasyon yaklaşımında, mesela gradyan inişi (gradient descent) minimum noktası bulmak için bir fonksiyonun türevine ihtiyaç duyar. Eğer <span class="math inline">\(f(x)\)</span> basit, analitik olarak türevi kolay alınabilen bir fonksiyon ise problem yok. Olmadığı zaman AD iyi bir çözümdür.</p>
<p>İkiz Sayılar (Dual Numbers)</p>
<p>Lineer Cebir'de ikiz sayılar reel sayıları genişleterek yeni bir öğe eklerler, bu öğe üzerinde tanımlanan cebire göre <span class="math inline">\(\epsilon^2 = 0\)</span> olmalıdır, ve <span class="math inline">\(\epsilon \ne 0\)</span> [1]. Bu yeni öğe üzerinden her sayı artık ikiz şekilde belirtilir, <span class="math inline">\(z = a + b\epsilon\)</span>, ki <span class="math inline">\(a,b\)</span> birer reel sayıdır. Herhangi bir matematikçi kendisine göre bir cebir tanımlayabilir, bunu biliyoruz, operasyonlar tablolar ile tanımlanır, vs. <span class="math inline">\(\epsilon^2=0\)</span> kavramı hayali sayılardaki <span class="math inline">\(i^2 = -1\)</span>'e benzetilebilir. İkiz sayıları yazılımda depolamak için <span class="math inline">\((a,b)\)</span> gibi bir çift yeterlidir.</p>
<p>AD amacı için <span class="math inline">\(x \mapsto x + \dot{x} \epsilon\)</span> olarak tanımlarız, ki <span class="math inline">\(\epsilon^2 = 0, d \ne 0\)</span> kullanıyoruz, yani <span class="math inline">\(x\)</span>'in kırpılmış Taylor açılımını yapıyoruz, ayrıca bu tanım bir ikiz sayı. Taylor açılımını hatırlarsak bir fonksiyon için herhangi bir <span class="math inline">\(a\)</span> noktasında <span class="math inline">\(f(t) = f(a) + f&#39;(a)(t-a)\)</span> idi, bu durumda fonksiyon <span class="math inline">\(x\)</span>'in kendisi, <span class="math inline">\(f(x)=x\)</span>, açılım <span class="math inline">\(x\)</span> noktasında yani <span class="math inline">\(a=x\)</span>, ve <span class="math inline">\(f(x)=x=f(x)+f&#39;(x)(x-a)\)</span>, ve <span class="math inline">\(=x+f&#39;(x)(x-a)\)</span>. Açılımdaki <span class="math inline">\(x-a\)</span> bir <span class="math inline">\(\epsilon\)</span> olarak görülebilir, zaten normal Taylor açılımı için de çok ufak bir adım olarak hesaplanmalıdır, ve bu ufak adımın karesi de normal olarak sıfıra yaklaşır. Gerçi <span class="math inline">\(a=x\)</span> olunca <span class="math inline">\(x-a=0\)</span> olur ama yeni bir cebir yaratarak bu problemden kurtulmak istemişler herhalde.</p>
<p>Bu şekilde oluşan aritmetiğe bakarsak,</p>
<p><span class="math display">\[ 
x + y \mapsto 
(x+\dot{x}\epsilon)  + (y+\dot{y}\epsilon) = 
xy + x\dot{y}\epsilon + \dot{x}y\epsilon +
\underbrace{\dot{x}\dot{y}\epsilon^2}_{=0} 
\]</span> <span class="math display">\[  = xy + (x\dot{y} + \dot{x}y)\epsilon  \]</span></p>
<p>Başka bir işlem</p>
<p><span class="math display">\[ xy \mapsto (x+\dot{x}\epsilon)(y+\dot{y}\epsilon)  =
(xy) + (x\dot(y) + \dot{x}y)\epsilon
\]</span></p>
<p>Bir diğeri</p>
<p><span class="math display">\[ -(x+\dot{x}\epsilon) = -x - \dot{x}\epsilon\]</span></p>
<p>Ya da</p>
<p><span class="math display">\[ 
\frac{1}{x+\dot{x}\epsilon} = 
\frac{1}{x} - \frac{\dot{x}}{x^2}\epsilon, \quad (x \ne 0)
\]</span></p>
<p>Dikkat edilirse <span class="math inline">\(\epsilon\)</span>'nin katsayıları sembolik türev sonuçlarını birebir takip ediyorlar. Bu sonuçtan istifade edebiliriz, fonksiyonları şu şekilde tanımlarız,</p>
<p><span class="math display">\[ 
g(x + \dot{x}d) = g(x) + g&#39;(\dot{x}d) 
\qquad (1)
\]</span></p>
<p>O zaman mesela <span class="math inline">\(\sin,\cos\)</span> ya da pek çok diğer fonksiyonu <span class="math inline">\(g\)</span> olarak alırsak onları şu şekilde açmak mümkün</p>
<p><span class="math display">\[ sin(x + \dot{x}d) = sin(x) + cos(x)\dot{x}d \]</span></p>
<p><span class="math display">\[ \cos(x+\dot{x}d) = \cos(x) - \sin(x)\dot{x}d\]</span></p>
<p><span class="math display">\[ e^{x+\dot{x}d} = e^x + e^x \dot{x}d\]</span></p>
<p><span class="math display">\[ \log(x + \dot{x}d) = \log(x) + \frac{\dot{x}}{x}d , \quad x \ne 0\]</span></p>
<p>Zincirleme Kanunu, yani <span class="math inline">\(f(g(..))\)</span>, üstteki açılımı da kullanarak beklenen şekilde işleyecek,</p>
<p><span class="math display">\[ f(g(x + \dot{x}\epsilon)) =  f(g(x) + g&#39;(x)\dot{x}\epsilon)  \]</span></p>
<p><span class="math display">\[ = f(g(x)) + f&#39;(g(x))g&#39;(x)\dot{x} \epsilon\]</span></p>
<p>Dikkat edersek <span class="math inline">\(\epsilon\)</span>'un katsayısı aynen önce olduğu gibi <span class="math inline">\(f(g(..))\)</span>'nin türevini taşıyor.</p>
<p>Demek ki ikiz sayıları türevi alınmamış fonksiyon sonucu ve türevi alınmış değeri program içinde taşıyan veri yapıları olarak kullanabiliriz. O zaman temel bazı operasyonları (fonksiyonları) (1) formülasyonuna uyacak şekilde kodlarsak, bu temel fonksiyonları içeren her türlü diğer kompozisyon Zincir Kuralı üzerinden aynı şekilde türev alınmamış ve alınmış değerler taşınıyor olacaktır.</p>
<p>Örnek</p>
<p>Elimizde</p>
<p><span class="math display">\[ f(x_1,x_2) = x_1x_2 + \sin(x_1)\]</span></p>
<p>var. İkiz sayılar ile açalım,</p>
<p><span class="math display">\[ f(x_1 + \dot{x_1}\epsilon_1, x_1 + \dot{x_2}\epsilon_2) = 
(x_1 + \dot{x_1}\epsilon_1)(x_2 + \dot{x_2}\epsilon_2) \sin(x_1+x_1\dot{x_1}\epsilon_1)
\]</span></p>
<p><span class="math display">\[ 
= x_1x_2 + (x_2 + \cos(x_1))\dot{x_1}\epsilon_1 
+ x_1\dot{x_2}\epsilon_2
+ x_2\dot{x_1}\epsilon_1
\]</span></p>
<p>ki <span class="math inline">\(\epsilon_1\epsilon_2 = 0\)</span>.</p>
<p>O zaman bir fonksiyonun türevini hesaplamak için türevi bu standart olmayan şekilde hesaplayıp, ilgilendiğimiz türevin değişkenini 1 olarak atarsak, istediğimiz türev değerini <span class="math inline">\(x=a\)</span> noktasında elde ederiz.</p>
<p>Eğer kod için düşünürsek, değişim şu şekilde olacak (soldaki orijinal program, sağdaki ikiz program)</p>
<div class="figure">
<img src="autodiff_02.png" />

</div>
<p>Yazılmış kodu görelim,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> f(x1, x2):
    w3 <span class="op">=</span> x1 <span class="op">*</span> x2
    w4 <span class="op">=</span> np.sin(x1)
    w5 <span class="op">=</span> w3 <span class="op">+</span> w4
    <span class="cf">return</span> w5

<span class="bu">print</span> <span class="st">&#39;f&#39;</span>, f(<span class="dv">10</span>, <span class="dv">20</span>)
h <span class="op">=</span> <span class="fl">0.01</span>
<span class="bu">print</span> <span class="st">u&#39;sayısal türev&#39;, (f(10+h, 20)-f(10, 20)) / h</span></code></pre></div>
<pre><code>f 199.455978889
sayısal türev 19.1636625383</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> f(x1, x2, dx1, dx2):
    df <span class="op">=</span> [<span class="fl">0.</span>,<span class="fl">0.</span>]
    w3 <span class="op">=</span> x1 <span class="op">*</span> x2
    dw3 <span class="op">=</span> dx1<span class="op">*</span>x2 <span class="op">+</span> x1<span class="op">*</span>dx2    
    w4 <span class="op">=</span> np.sin(x1)
    dw4 <span class="op">=</span> np.cos(x1) <span class="op">*</span> dx1    
    w5 <span class="op">=</span> w3 <span class="op">+</span> w4
    dw5 <span class="op">=</span> dw3 <span class="op">+</span> dw4
    df[<span class="dv">0</span>] <span class="op">=</span> w5
    df[<span class="dv">1</span>] <span class="op">=</span> dw5
    <span class="cf">return</span> df
<span class="bu">print</span> <span class="st">&#39;AD&#39;</span>, f(<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">1</span>,<span class="dv">0</span>)</code></pre></div>
<pre><code>AD [199.45597888911064, 19.160928470923547]</code></pre>
<p>Sembolik olarak türevin <span class="math inline">\(\frac{\partial f}{\partial x_2} = x_1\)</span> olduğunu biliyoruz Üstteki program aynı sonuca erişti.</p>
<p>AD için Python'da <code>autograd</code> paketi otomatik türev alınmasını sağlar. Önceki örnek için</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> autograd.numpy <span class="im">as</span> np
<span class="im">from</span> autograd <span class="im">import</span> elementwise_grad
<span class="im">from</span> autograd <span class="im">import</span> grad

<span class="kw">def</span> f(x1, x2):
    w3 <span class="op">=</span> x1 <span class="op">*</span> x2
    w4 <span class="op">=</span> np.sin(x1)
    w5 <span class="op">=</span> w3 <span class="op">+</span> w4
    <span class="cf">return</span> w5

fg <span class="op">=</span> grad(f)
<span class="bu">print</span> fg(<span class="dv">10</span>,<span class="dv">20</span>)</code></pre></div>
<pre><code>19.1609284709</code></pre>
<p>Dikkat: üstteki kod daha önce gösterilen ile aynı tek bir fark ile, <code>numpy</code> kütüphanesini <code>autograd</code>'den alıyoruz, çünkü o üzerinde AD değişimi yapılmış olan <code>numpy</code>.</p>
<p>Gradyanı, yani <span class="math inline">\(x\)</span>'in her boyutu için kısmi türevi içeren vektörel olarak türevleri görmek istiyorsak,</p>
<p><span class="math display">\[ \nabla f = \left[\begin{array}{r}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2}
\end{array}\right]\]</span></p>
<p>Yani bir değişkeni sabit tutup diğerini değiştirince elde edilen türev bu. Şimdi <span class="math inline">\(x_0 = \left[\begin{array}{cc}10&amp;20\end{array}\right]\)</span> noktasında gradyan değerini hesaplayalım,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> f(xvec):
    w3 <span class="op">=</span> xvec[<span class="dv">0</span>] <span class="op">*</span> xvec[<span class="dv">1</span>]
    w4 <span class="op">=</span> np.sin(xvec[<span class="dv">0</span>])
    w5 <span class="op">=</span> w3 <span class="op">+</span> w4
    <span class="cf">return</span> w5

fg <span class="op">=</span> grad(f)
x0 <span class="op">=</span> np.array([<span class="fl">10.</span>,<span class="fl">20.</span>])
<span class="bu">print</span> fg(x0)</code></pre></div>
<pre><code>[ 19.16092847  10.        ]</code></pre>
<p>Daha bitmedi: Altta <span class="math inline">\(\tanh\)</span>'nin türevini alıyoruz, hatta türevin türevi, onun türevi derken arka arkaya 6 defa zincirleme türev alıyoruz, AD bana mısın demiyor (!).</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> autograd.numpy <span class="im">as</span> np
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">from</span> autograd <span class="im">import</span> elementwise_grad

<span class="kw">def</span> tanh(x):
    <span class="cf">return</span> (<span class="fl">1.0</span> <span class="op">-</span> np.exp(<span class="op">-</span>x))  <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))

d_fun      <span class="op">=</span> elementwise_grad(tanh)       <span class="co"># 1. Türev</span>
dd_fun     <span class="op">=</span> elementwise_grad(d_fun)      <span class="co"># 2. Türev</span>
ddd_fun    <span class="op">=</span> elementwise_grad(dd_fun)     <span class="co"># 3. Türev</span>
dddd_fun   <span class="op">=</span> elementwise_grad(ddd_fun)    <span class="co"># 4. Türev</span>
ddddd_fun  <span class="op">=</span> elementwise_grad(dddd_fun)   <span class="co"># 5. Türev</span>
dddddd_fun <span class="op">=</span> elementwise_grad(ddddd_fun)  <span class="co"># 6. Türev</span>

x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">7</span>, <span class="dv">7</span>, <span class="dv">200</span>)
plt.plot(x, tanh(x),
         x, d_fun(x),
         x, dd_fun(x),
         x, ddd_fun(x),
         x, dddd_fun(x),
         x, ddddd_fun(x),
         x, dddddd_fun(x))

plt.axis(<span class="st">&#39;off&#39;</span>)
plt.savefig(<span class="st">&quot;autodiff_03.png&quot;</span>)</code></pre></div>
<div class="figure">
<img src="autodiff_03.png" />

</div>
<p>İleri, Geri</p>
<p>Üstte gösterilen teknik aslında ileri mod (forward mode) AD olarak biliniyor. Hesap ağaçı üzerinde göstermek gerekirse,</p>
<div class="figure">
<img src="autodiff_04.png" />

</div>
<p>Geriye gitmek te mümkün, buna geri mod'u (reverse mode) ismi veriliyor.</p>
<div class="figure">
<img src="autodiff_05.png" />

</div>
<p>Yapay Sinir Ağları ve AD</p>
<p>Derin Öğrenim için oluşturulan YSA'lar oldukca çetrefil olabilir (bkz {}), <span class="math inline">\(\max\)</span>, evrişim (convolution) gibi operasyonlar içeriyor olabilirler. Bu ağları eğitmek için türevi elle hesaplamak çok zordur. Fakat AD tüm gereken gradyanları hesaplar, ve hataları geriye yayarak (backpropagation) ağırlıkları optimal değerlerine getirir. Şimdi basit YSA'nın AD ile kodlamasını görelim [4],</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> autograd.numpy <span class="im">as</span> np  <span class="co"># Thinly wrapped version of numpy</span>
<span class="im">from</span> autograd <span class="im">import</span> grad
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">from</span> sklearn <span class="im">import</span> datasets, linear_model

np.random.seed(<span class="dv">0</span>)
X, y <span class="op">=</span> datasets.make_moons(<span class="dv">200</span>, noise<span class="op">=</span><span class="fl">0.20</span>)
n <span class="op">=</span> <span class="dv">2</span> <span class="co"># dimensionality</span>
points_per_class <span class="op">=</span><span class="dv">100</span> 
num_classes <span class="op">=</span> <span class="dv">2</span> 
m <span class="op">=</span> points_per_class<span class="op">*</span>num_classes   
    
fig <span class="op">=</span> plt.figure()
plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">40</span>, cmap<span class="op">=</span>plt.cm.Spectral)
plt.xlim([<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>])
plt.ylim([<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>])

h <span class="op">=</span> <span class="fl">0.05</span>
x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span>
y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span>
xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

X_test <span class="op">=</span> np.c_[xx.ravel(), yy.ravel()]

<span class="kw">def</span> plot_model(scores):
    Z <span class="op">=</span> scores.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap<span class="op">=</span>plt.cm.Paired, alpha<span class="op">=</span><span class="fl">0.8</span>)
    plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>plt.cm.Paired)
    plt.xlabel(<span class="st">&#39;x1&#39;</span>)
    plt.ylabel(<span class="st">&#39;x2&#39;</span>)
    plt.xlim(xx.<span class="bu">min</span>(), xx.<span class="bu">max</span>())
    plt.ylim(yy.<span class="bu">min</span>(), yy.<span class="bu">max</span>())
    plt.xticks(())
    plt.yticks(())

<span class="co"># ReLU: &quot;rectified linear unit&quot; nonlinearity</span>
<span class="kw">def</span> relu(z):
    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, z)

<span class="co"># Initialize parameters randomly</span>
h  <span class="op">=</span> <span class="dv">10</span> <span class="co"># size of hidden layer</span>
W1 <span class="op">=</span> <span class="fl">0.01</span> <span class="op">*</span> np.random.randn(n,h)
b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>,h))
W2 <span class="op">=</span> <span class="fl">0.01</span> <span class="op">*</span> np.random.randn(h,num_classes)
b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>,num_classes))

<span class="co"># Select hyperparameters</span>
iters      <span class="op">=</span> <span class="dv">1000</span>
eta  <span class="op">=</span> <span class="fl">1e-0</span>
lambda_val <span class="op">=</span> <span class="fl">1e-3</span> <span class="co"># regularization strength</span>

<span class="kw">def</span> compute_loss(params):
    W1, b1, W2, b2 <span class="op">=</span> params
    
    hidden <span class="op">=</span> relu(np.dot(X, W1) <span class="op">+</span> b1)
    scores <span class="op">=</span> np.dot(hidden, W2) <span class="op">+</span> b2
    exp_scores <span class="op">=</span> np.exp(scores)
    probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)
    logprob_correct_class <span class="op">=</span> <span class="op">-</span>np.log(probs[<span class="bu">range</span>(m),y])
    data_loss <span class="op">=</span> np.<span class="bu">sum</span>(logprob_correct_class)<span class="op">/</span>m    <span class="co"># cross-entropy</span>
    reg_loss <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> lambda_val <span class="op">*</span> (np.<span class="bu">sum</span>(W1<span class="op">*</span>W1) <span class="op">+</span> np.<span class="bu">sum</span>(W2<span class="op">*</span>W2))    
    <span class="cf">return</span> data_loss <span class="op">+</span> reg_loss

<span class="co"># This is the gradient of the entire feedforward training</span>
gradient <span class="op">=</span> grad(compute_loss)

<span class="co"># Gradient descent loop</span>
<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iters):  
    <span class="co"># Print diagnostic</span>
    loss <span class="op">=</span> compute_loss((W1, b1, W2, b2))      
    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">200</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span> <span class="st">&quot;iteration </span><span class="sc">%d</span><span class="st">: loss </span><span class="sc">%f</span><span class="st">&quot;</span> <span class="op">%</span> (i, loss)        
    dW1, db1, dW2, db2 <span class="op">=</span> gradient((W1, b1, W2, b2))    
    <span class="co"># perform a parameter update</span>
    W1 <span class="op">+=</span> <span class="op">-</span>eta <span class="op">*</span> dW1
    b1 <span class="op">+=</span> <span class="op">-</span>eta <span class="op">*</span> db1
    W2 <span class="op">+=</span> <span class="op">-</span>eta <span class="op">*</span> dW2
    b2 <span class="op">+=</span> <span class="op">-</span>eta <span class="op">*</span> db2
    
<span class="kw">def</span> predict(X):
    hidden <span class="op">=</span> relu(np.dot(X, W1) <span class="op">+</span> b1)
    scores <span class="op">=</span> np.dot(hidden, W2) <span class="op">+</span> b2
    pred <span class="op">=</span> np.argmax(scores, axis<span class="op">=</span><span class="dv">1</span>)
    <span class="cf">return</span> pred

plot_model(predict(X_test))
plt.savefig(<span class="st">&#39;autodiff_01.png&#39;</span>)</code></pre></div>
<pre><code>iteration 0: loss 0.693097
iteration 200: loss 0.291406
iteration 400: loss 0.277980
iteration 600: loss 0.276930
iteration 800: loss 0.276666</code></pre>
<div class="figure">
<img src="autodiff_01.png" />

</div>
<p>Daha basit bir örnek görelim, mesela Lojistik Regresyon. Elle türev almaya gerek kalmadan çok basit bir şekilde tahmin, kayıp fonksiyonları üzerinden direk rasgele gradyan inişi ile kodlamayı yapabiliyoruz.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> autograd.numpy <span class="im">as</span> np
<span class="im">from</span> autograd <span class="im">import</span> grad
<span class="im">from</span> autograd.util <span class="im">import</span> quick_grad_check
<span class="im">from</span> builtins <span class="im">import</span> <span class="bu">range</span>

<span class="kw">def</span> sigmoid(x):
    <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>(np.tanh(x) <span class="op">+</span> <span class="dv">1</span>)

<span class="kw">def</span> logistic_predictions(weights, inputs):
    <span class="cf">return</span> sigmoid(np.dot(inputs, weights))

<span class="kw">def</span> training_loss(weights):
    preds <span class="op">=</span> logistic_predictions(weights, inputs)
    label_probabilities <span class="op">=</span> preds <span class="op">*</span> targets <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> preds) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> targets)
    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(np.log(label_probabilities))

inputs <span class="op">=</span> np.array([[<span class="fl">0.52</span>, <span class="fl">1.12</span>,  <span class="fl">0.77</span>],
                   [<span class="fl">0.88</span>, <span class="fl">-1.08</span>, <span class="fl">0.15</span>],
                   [<span class="fl">0.52</span>, <span class="fl">0.06</span>, <span class="fl">-1.30</span>],
                   [<span class="fl">0.74</span>, <span class="fl">-2.49</span>, <span class="fl">1.39</span>]])
targets <span class="op">=</span> np.array([<span class="va">True</span>, <span class="va">True</span>, <span class="va">False</span>, <span class="va">True</span>])


training_gradient_fun <span class="op">=</span> grad(training_loss)
weights <span class="op">=</span> np.array([<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])
<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):
    weights <span class="op">-=</span> training_gradient_fun(weights) <span class="op">*</span> <span class="fl">0.1</span>
<span class="bu">print</span>(<span class="st">&quot;Trained loss:&quot;</span>, training_loss(weights))
<span class="bu">print</span> weights</code></pre></div>
<pre><code>(&#39;Trained loss:&#39;, 0.042172397668071952)
[ 1.40509236 -0.37749486  2.34249055]</code></pre>
<p>Kaynaklar</p>
<p>[1] Wikipedia, <em>Dual number</em>, <a href="https://en.wikipedia.org/wiki/Dual_number" class="uri">https://en.wikipedia.org/wiki/Dual_number</a></p>
<p>[2] Berland, <em>Automatic Differentiation</em>, <a href="http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf" class="uri">http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf</a></p>
<p>[3] Griewank, <em>Evaluating Derivatives</em></p>
<p>[4] Sheldon, <em>Neural Net Example</em>, <a href="https://people.cs.umass.edu/~sheldon/teaching/cs335/lec/neural-net-case-studies.html" class="uri">https://people.cs.umass.edu/~sheldon/teaching/cs335/lec/neural-net-case-studies.html</a></p>
<p>[5] Ghaffari, <em>Automatic Differentiation</em>, <a href="http://www.cas.mcmaster.ca/~cs777/presentations/AD.pdf" class="uri">http://www.cas.mcmaster.ca/~cs777/presentations/AD.pdf</a></p>
<p>[6] Autograd, <a href="https://github.com/HIPS/autograd" class="uri">https://github.com/HIPS/autograd</a></p>
</body>
</html>
