<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Otomatik Türev Almak (Automatic Differentiation -AD-)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="otomatik-türev-almak-automatic-differentiation--ad-">Otomatik
Türev Almak (Automatic Differentiation -AD-)</h1>
<p>Matematikte türev hesaplamanın birkaç yöntemi var; bunlardan
birincisi Calculus’ta öğretilen sembolik türevdir, diğeri sayısal
türevdir. AD üçüncü bir yöntem sayılıyor, özellikle programlama
bağlamında çok faydalı bir özelliği var, <em>herhangi</em> bir yazılım
fonksiyonunu alıp, bir veri noktası bağlamında, o fonksiyonun içinde ne
türlü temel diğer fonksiyonlar olursa olsun onu kendi türevini
hesaplayacak hale çevirebiliyor. Burada kod değişimi söz konusu, değişim
işlem anında dinamik, ya da kaynak kod seviyesinde derleme öncesi
yapılabiliyor. Fonksiyon <code>if</code>, <code>goto</code> gibi
dallanma, koşulsal ifadeler içeriyor olabilir (ama bu ifadeler üzerinden
türev alınan değişkene bağlı olmamalıdır), <span
class="math inline">\(\cos,\sin\)</span> gibi trigonometrik ifadeler,
polinomlar, <span class="math inline">\(\max,\min\)</span> gibi
gayrı-lineer ifadeler, ya da başka herhangi temel hesapları kullanıyor
olabilir, son derece çetrefil bir takım hesaplar zincirleme yapılıyor
olabilir. Eğer hesap deterministik bir şekilde yapılabiliyorsa (aynı
girdiler için hep aynı değer hesaplanıyor) AD onu alıp kendi türevini
hesaplayabilen hale çevirebiliyor.</p>
<p>Bu son derece kuvvetli bir özellik. Pek çok optimizasyon
yaklaşımında, mesela gradyan inişi (gradient descent) minimum noktası
bulmak için bir fonksiyonun türevine ihtiyaç duyar. Eğer <span
class="math inline">\(f(x)\)</span> basit, analitik olarak türevi kolay
alınabilen bir fonksiyon ise problem yok. Olmadığı zaman AD iyi bir
çözümdür.</p>
<p>İkiz Sayılar (Dual Numbers)</p>
<p>Lineer Cebir’de ikiz sayılar reel sayıları genişleterek yeni bir öğe
eklerler, bu öğe üzerinde tanımlanan cebire göre <span
class="math inline">\(\epsilon^2 = 0\)</span> olmalıdır, ve <span
class="math inline">\(\epsilon \ne 0\)</span> [1]. Bu yeni öğe üzerinden
her sayı artık ikiz şekilde belirtilir, <span class="math inline">\(z =
a + b\epsilon\)</span>, ki <span class="math inline">\(a,b\)</span>
birer reel sayıdır. Herhangi bir matematikçi kendisine göre bir cebir
tanımlayabilir, bunu biliyoruz, operasyonlar tablolar ile tanımlanır,
vs. <span class="math inline">\(\epsilon^2=0\)</span> kavramı hayali
sayılardaki <span class="math inline">\(i^2 = -1\)</span>’e
benzetilebilir. İkiz sayıları yazılımda depolamak için <span
class="math inline">\((a,b)\)</span> gibi bir çift yeterlidir.</p>
<p>AD amacı için <span class="math inline">\(x \mapsto x + \dot{x}
\epsilon\)</span> olarak tanımlarız, ki <span
class="math inline">\(\epsilon^2 = 0, d \ne 0\)</span> kullanıyoruz,
yani <span class="math inline">\(x\)</span>’in kırpılmış Taylor
açılımını yapıyoruz, ayrıca bu tanım bir ikiz sayı. Taylor açılımını
hatırlarsak bir fonksiyon için herhangi bir <span
class="math inline">\(a\)</span> noktasında <span
class="math inline">\(f(t) = f(a) + f&#39;(a)(t-a)\)</span> idi, bu
durumda fonksiyon <span class="math inline">\(x\)</span>’in kendisi,
<span class="math inline">\(f(x)=x\)</span>, açılım <span
class="math inline">\(x\)</span> noktasında yani <span
class="math inline">\(a=x\)</span>, ve <span
class="math inline">\(f(x)=x=f(x)+f&#39;(x)(x-a)\)</span>, ve <span
class="math inline">\(=x+f&#39;(x)(x-a)\)</span>. Açılımdaki <span
class="math inline">\(x-a\)</span> bir <span
class="math inline">\(\epsilon\)</span> olarak görülebilir, zaten normal
Taylor açılımı için de çok ufak bir adım olarak hesaplanmalıdır, ve bu
ufak adımın karesi de normal olarak sıfıra yaklaşır. Gerçi <span
class="math inline">\(a=x\)</span> olunca <span
class="math inline">\(x-a=0\)</span> olur ama yeni bir cebir yaratarak
bu problemden kurtulmak istemişler herhalde.</p>
<p>Bu şekilde oluşan aritmetiğe bakarsak,</p>
<p><span class="math display">\[
x + y \mapsto
(x+\dot{x}\epsilon)  + (y+\dot{y}\epsilon) =
xy + x\dot{y}\epsilon + \dot{x}y\epsilon +
\underbrace{\dot{x}\dot{y}\epsilon^2}_{=0}
\]</span> <span class="math display">\[  = xy + (x\dot{y} +
\dot{x}y)\epsilon  \]</span></p>
<p>Başka bir işlem</p>
<p><span class="math display">\[ xy \mapsto
(x+\dot{x}\epsilon)(y+\dot{y}\epsilon)  =
(xy) + (x\dot(y) + \dot{x}y)\epsilon
\]</span></p>
<p>Bir diğeri</p>
<p><span class="math display">\[ -(x+\dot{x}\epsilon) = -x -
\dot{x}\epsilon\]</span></p>
<p>Ya da</p>
<p><span class="math display">\[
\frac{1}{x+\dot{x}\epsilon} =
\frac{1}{x} - \frac{\dot{x}}{x^2}\epsilon, \quad (x \ne 0)
\]</span></p>
<p>Dikkat edilirse <span class="math inline">\(\epsilon\)</span>’nin
katsayıları sembolik türev sonuçlarını birebir takip ediyorlar. Bu
sonuçtan istifade edebiliriz, fonksiyonları şu şekilde tanımlarız,</p>
<p><span class="math display">\[
g(x + \dot{x}d) = g(x) + g&#39;(\dot{x}d)
\qquad (1)
\]</span></p>
<p>O zaman mesela <span class="math inline">\(\sin,\cos\)</span> ya da
pek çok diğer fonksiyonu <span class="math inline">\(g\)</span> olarak
alırsak onları şu şekilde açmak mümkün</p>
<p><span class="math display">\[ sin(x + \dot{x}d) = sin(x) +
cos(x)\dot{x}d \]</span></p>
<p><span class="math display">\[ \cos(x+\dot{x}d) = \cos(x) -
\sin(x)\dot{x}d\]</span></p>
<p><span class="math display">\[ e^{x+\dot{x}d} = e^x + e^x
\dot{x}d\]</span></p>
<p><span class="math display">\[ \log(x + \dot{x}d) = \log(x) +
\frac{\dot{x}}{x}d , \quad x \ne 0\]</span></p>
<p>Zincirleme Kanunu, yani <span
class="math inline">\(f(g(..))\)</span>, üstteki açılımı da kullanarak
beklenen şekilde işleyecek,</p>
<p><span class="math display">\[ f(g(x + \dot{x}\epsilon)) =  f(g(x) +
g&#39;(x)\dot{x}\epsilon)  \]</span></p>
<p><span class="math display">\[ = f(g(x)) +
f&#39;(g(x))g&#39;(x)\dot{x} \epsilon\]</span></p>
<p>Dikkat edersek <span class="math inline">\(\epsilon\)</span>’un
katsayısı aynen önce olduğu gibi <span
class="math inline">\(f(g(..))\)</span>’nin türevini taşıyor.</p>
<p>Demek ki ikiz sayıları türevi alınmamış fonksiyon sonucu ve türevi
alınmış değeri program içinde taşıyan veri yapıları olarak
kullanabiliriz. O zaman temel bazı operasyonları (fonksiyonları) (1)
formülasyonuna uyacak şekilde kodlarsak, bu temel fonksiyonları içeren
her türlü diğer kompozisyon Zincir Kuralı üzerinden aynı şekilde türev
alınmamış ve alınmış değerler taşınıyor olacaktır.</p>
<p>Örnek</p>
<p>Elimizde</p>
<p><span class="math display">\[ f(x_1,x_2) = x_1x_2 +
\sin(x_1)\]</span></p>
<p>var. İkiz sayılar ile açalım,</p>
<p><span class="math display">\[ f(x_1 + \dot{x_1}\epsilon_1, x_1 +
\dot{x_2}\epsilon_2) =
(x_1 + \dot{x_1}\epsilon_1)(x_2 + \dot{x_2}\epsilon_2)
\sin(x_1+x_1\dot{x_1}\epsilon_1)
\]</span></p>
<p><span class="math display">\[
= x_1x_2 + (x_2 + \cos(x_1))\dot{x_1}\epsilon_1
+ x_1\dot{x_2}\epsilon_2
+ x_2\dot{x_1}\epsilon_1
\]</span></p>
<p>ki <span class="math inline">\(\epsilon_1\epsilon_2 = 0\)</span>.</p>
<p>O zaman bir fonksiyonun türevini hesaplamak için türevi bu standart
olmayan şekilde hesaplayıp, ilgilendiğimiz türevin değişkenini 1 olarak
atarsak, istediğimiz türev değerini <span
class="math inline">\(x=a\)</span> noktasında elde ederiz.</p>
<p>Eğer kod için düşünürsek, değişim şu şekilde olacak (soldaki orijinal
program, sağdaki ikiz program)</p>
<p><img src="autodiff_02.png" /></p>
<p>Yazılmış kodu görelim,</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x1, x2):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    w3 <span class="op">=</span> x1 <span class="op">*</span> x2</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    w4 <span class="op">=</span> np.sin(x1)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    w5 <span class="op">=</span> w3 <span class="op">+</span> w4</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w5</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;f&#39;</span>, f(<span class="dv">10</span>, <span class="dv">20</span>))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">u&#39;sayısal türev&#39;</span>, (f(<span class="dv">10</span><span class="op">+</span>h, <span class="dv">20</span>)<span class="op">-</span>f(<span class="dv">10</span>, <span class="dv">20</span>)) <span class="op">/</span> h)</span></code></pre></div>
<pre class="text"><code>f 199.45597888911064
sayısal türev 19.163662538264248</code></pre>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x1, x2, dx1, dx2):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> [<span class="fl">0.</span>,<span class="fl">0.</span>]</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    w3 <span class="op">=</span> x1 <span class="op">*</span> x2</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    dw3 <span class="op">=</span> dx1<span class="op">*</span>x2 <span class="op">+</span> x1<span class="op">*</span>dx2    </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    w4 <span class="op">=</span> np.sin(x1)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    dw4 <span class="op">=</span> np.cos(x1) <span class="op">*</span> dx1    </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    w5 <span class="op">=</span> w3 <span class="op">+</span> w4</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    dw5 <span class="op">=</span> dw3 <span class="op">+</span> dw4</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    df[<span class="dv">0</span>] <span class="op">=</span> w5</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    df[<span class="dv">1</span>] <span class="op">=</span> dw5</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;AD&#39;</span>, f(<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">1</span>,<span class="dv">0</span>))</span></code></pre></div>
<pre class="text"><code>AD [np.float64(199.45597888911064), np.float64(19.160928470923547)]</code></pre>
<p>Sembolik olarak türevin <span class="math inline">\(\frac{\partial
f}{\partial x_2} = x_1\)</span> olduğunu biliyoruz Üstteki program aynı
sonuca erişti.</p>
<p>AD için Python’da <code>autograd</code> paketi otomatik türev
alınmasını sağlar. Önceki örnek için</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> elementwise_grad</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> grad</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x1, x2):</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    w3 <span class="op">=</span> x1 <span class="op">*</span> x2</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    w4 <span class="op">=</span> np.sin(x1)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    w5 <span class="op">=</span> w3 <span class="op">+</span> w4</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w5</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>fg <span class="op">=</span> grad(f)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (fg(<span class="fl">10.0</span>,<span class="fl">20.0</span>))</span></code></pre></div>
<pre class="text"><code>19.160928470923547</code></pre>
<p>Dikkat: üstteki kod daha önce gösterilen ile aynı tek bir fark ile,
<code>numpy</code> kütüphanesini <code>autograd</code>’den alıyoruz,
çünkü o üzerinde AD değişimi yapılmış olan <code>numpy</code>.</p>
<p>Gradyanı, yani <span class="math inline">\(x\)</span>’in her boyutu
için kısmi türevi içeren vektörel olarak türevleri görmek
istiyorsak,</p>
<p><span class="math display">\[ \nabla f = \left[\begin{array}{r}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2}
\end{array}\right]\]</span></p>
<p>Yani bir değişkeni sabit tutup diğerini değiştirince elde edilen
türev bu. Şimdi <span class="math inline">\(x_0 =
\left[\begin{array}{cc}10&amp;20\end{array}\right]\)</span> noktasında
gradyan değerini hesaplayalım,</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(xvec):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    w3 <span class="op">=</span> xvec[<span class="dv">0</span>] <span class="op">*</span> xvec[<span class="dv">1</span>]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    w4 <span class="op">=</span> np.sin(xvec[<span class="dv">0</span>])</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    w5 <span class="op">=</span> w3 <span class="op">+</span> w4</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w5</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>fg <span class="op">=</span> grad(f)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> np.array([<span class="fl">10.</span>,<span class="fl">20.</span>])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (fg(x0))</span></code></pre></div>
<pre class="text"><code>[19.16092847 10.        ]</code></pre>
<p>Daha bitmedi: Altta <span class="math inline">\(\tanh\)</span>’nin
türevini alıyoruz, hatta türevin türevi, onun türevi derken arka arkaya
6 defa zincirleme türev alıyoruz, AD bana mısın demiyor (!).</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> elementwise_grad</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tanh(x):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (<span class="fl">1.0</span> <span class="op">-</span> np.exp(<span class="op">-</span>x))  <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>d_fun      <span class="op">=</span> elementwise_grad(tanh)       <span class="co"># 1. Türev</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>dd_fun     <span class="op">=</span> elementwise_grad(d_fun)      <span class="co"># 2. Türev</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>ddd_fun    <span class="op">=</span> elementwise_grad(dd_fun)     <span class="co"># 3. Türev</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>dddd_fun   <span class="op">=</span> elementwise_grad(ddd_fun)    <span class="co"># 4. Türev</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>ddddd_fun  <span class="op">=</span> elementwise_grad(dddd_fun)   <span class="co"># 5. Türev</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>dddddd_fun <span class="op">=</span> elementwise_grad(ddddd_fun)  <span class="co"># 6. Türev</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">7</span>, <span class="dv">7</span>, <span class="dv">200</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>plt.plot(x, tanh(x),</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>         x, d_fun(x),</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>         x, dd_fun(x),</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>         x, ddd_fun(x),</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>         x, dddd_fun(x),</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>         x, ddddd_fun(x),</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>         x, dddddd_fun(x))</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">&#39;off&#39;</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&quot;autodiff_03.png&quot;</span>)</span></code></pre></div>
<p><img src="autodiff_03.png" /></p>
<p>İleri, Geri</p>
<p>Üstte gösterilen teknik aslında ileri mod (forward mode) AD olarak
biliniyor. Hesap ağaçı üzerinde göstermek gerekirse,</p>
<p><img src="autodiff_04.png" /></p>
<p>Geriye gitmek te mümkün, buna geri mod’u (reverse mode) ismi
veriliyor.</p>
<p><img src="autodiff_05.png" /></p>
<p>Yapay Sinir Ağları ve AD</p>
<p>Derin Öğrenim için oluşturulan YSA’lar oldukca çetrefil olabilir (bkz
{}), <span class="math inline">\(\max\)</span>, evrişim (convolution)
gibi operasyonlar içeriyor olabilirler. Bu ağları eğitmek için türevi
elle hesaplamak çok zordur. Fakat AD tüm gereken gradyanları hesaplar,
ve hataları geriye yayarak (backpropagation) ağırlıkları optimal
değerlerine getirir. Şimdi basit YSA’nın AD ile kodlamasını görelim
[4],</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np  <span class="co"># Thinly wrapped version of numpy</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> grad</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets, linear_model</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> datasets.make_moons(<span class="dv">200</span>, noise<span class="op">=</span><span class="fl">0.20</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">2</span> <span class="co"># dimensionality</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>points_per_class <span class="op">=</span><span class="dv">100</span> </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>num_classes <span class="op">=</span> <span class="dv">2</span> </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> points_per_class<span class="op">*</span>num_classes   </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, s<span class="op">=</span><span class="dv">40</span>, cmap<span class="op">=</span>plt.cm.Spectral)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>plt.xlim([<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>plt.ylim([<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>])</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="fl">0.05</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h),</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>                     np.arange(y_min, y_max, h))</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.c_[xx.ravel(), yy.ravel()]</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_model(scores):</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> scores.reshape(xx.shape)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    plt.contourf(xx, yy, Z, cmap<span class="op">=</span>plt.cm.Paired, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>plt.cm.Paired)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&#39;x1&#39;</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&#39;x2&#39;</span>)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    plt.xlim(xx.<span class="bu">min</span>(), xx.<span class="bu">max</span>())</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    plt.ylim(yy.<span class="bu">min</span>(), yy.<span class="bu">max</span>())</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    plt.xticks(())</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>    plt.yticks(())</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="co"># ReLU: &quot;rectified linear unit&quot; nonlinearity</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(z):</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, z)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize parameters randomly</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>h  <span class="op">=</span> <span class="dv">10</span> <span class="co"># size of hidden layer</span></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> <span class="fl">0.01</span> <span class="op">*</span> np.random.randn(n,h)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>,h))</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> <span class="fl">0.01</span> <span class="op">*</span> np.random.randn(h,num_classes)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>,num_classes))</span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Select hyperparameters</span></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>iters      <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>eta  <span class="op">=</span> <span class="fl">1e-0</span></span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>lambda_val <span class="op">=</span> <span class="fl">1e-3</span> <span class="co"># regularization strength</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_loss(params):</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>    W1, b1, W2, b2 <span class="op">=</span> params</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    hidden <span class="op">=</span> relu(np.dot(X, W1) <span class="op">+</span> b1)</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.dot(hidden, W2) <span class="op">+</span> b2</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    exp_scores <span class="op">=</span> np.exp(scores)</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>    probs <span class="op">=</span> exp_scores <span class="op">/</span> np.<span class="bu">sum</span>(exp_scores, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>    logprob_correct_class <span class="op">=</span> <span class="op">-</span>np.log(probs[<span class="bu">range</span>(m),y])</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    data_loss <span class="op">=</span> np.<span class="bu">sum</span>(logprob_correct_class)<span class="op">/</span>m    <span class="co"># cross-entropy</span></span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>    reg_loss <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> lambda_val <span class="op">*</span> (np.<span class="bu">sum</span>(W1<span class="op">*</span>W1) <span class="op">+</span> np.<span class="bu">sum</span>(W2<span class="op">*</span>W2))    </span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data_loss <span class="op">+</span> reg_loss</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a><span class="co"># This is the gradient of the entire feedforward training</span></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>gradient <span class="op">=</span> grad(compute_loss)</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient descent loop</span></span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iters):  </span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print diagnostic</span></span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> compute_loss((W1, b1, W2, b2))      </span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">%</span> <span class="dv">200</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span> (<span class="st">&quot;iteration </span><span class="sc">%d</span><span class="st">: loss </span><span class="sc">%f</span><span class="st">&quot;</span> <span class="op">%</span> (i, loss))</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>    dW1, db1, dW2, db2 <span class="op">=</span> gradient((W1, b1, W2, b2))    </span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a>    <span class="co"># perform a parameter update</span></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>    W1 <span class="op">+=</span> <span class="op">-</span>eta <span class="op">*</span> dW1</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a>    b1 <span class="op">+=</span> <span class="op">-</span>eta <span class="op">*</span> db1</span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>    W2 <span class="op">+=</span> <span class="op">-</span>eta <span class="op">*</span> dW2</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">+=</span> <span class="op">-</span>eta <span class="op">*</span> db2</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(X):</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>    hidden <span class="op">=</span> relu(np.dot(X, W1) <span class="op">+</span> b1)</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> np.dot(hidden, W2) <span class="op">+</span> b2</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> np.argmax(scores, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pred</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>plot_model(predict(X_test))</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;autodiff_01.png&#39;</span>)</span></code></pre></div>
<pre class="text"><code>iteration 0: loss 0.693097
iteration 200: loss 0.291406
iteration 400: loss 0.277980
iteration 600: loss 0.276930
iteration 800: loss 0.276666</code></pre>
<p><img src="autodiff_01.png" /></p>
<p>Daha basit bir örnek görelim, mesela Lojistik Regresyon. Elle türev
almaya gerek kalmadan çok basit bir şekilde tahmin, kayıp fonksiyonları
üzerinden direk rasgele gradyan inişi ile kodlamayı yapabiliyoruz.</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> grad</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd.util <span class="im">import</span> quick_grad_check</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> builtins <span class="im">import</span> <span class="bu">range</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>(np.tanh(x) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic_predictions(weights, inputs):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sigmoid(np.dot(inputs, weights))</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> training_loss(weights):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> logistic_predictions(weights, inputs)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    label_probabilities <span class="op">=</span> preds <span class="op">*</span> targets <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> preds) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> targets)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.<span class="bu">sum</span>(np.log(label_probabilities))</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.array([[<span class="fl">0.52</span>, <span class="fl">1.12</span>,  <span class="fl">0.77</span>],</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>                   [<span class="fl">0.88</span>, <span class="op">-</span><span class="fl">1.08</span>, <span class="fl">0.15</span>],</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>                   [<span class="fl">0.52</span>, <span class="fl">0.06</span>, <span class="op">-</span><span class="fl">1.30</span>],</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>                   [<span class="fl">0.74</span>, <span class="op">-</span><span class="fl">2.49</span>, <span class="fl">1.39</span>]])</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>targets <span class="op">=</span> np.array([<span class="va">True</span>, <span class="va">True</span>, <span class="va">False</span>, <span class="va">True</span>])</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>training_gradient_fun <span class="op">=</span> grad(training_loss)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.array([<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>])</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">-=</span> training_gradient_fun(weights) <span class="op">*</span> <span class="fl">0.1</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Trained loss:&quot;</span>, training_loss(weights))</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (weights)</span></code></pre></div>
<pre class="text"><code>Trained loss: 0.04217239766807184
[ 1.40509236 -0.37749486  2.34249055]</code></pre>
<p>Kaynaklar</p>
<p>[1] Wikipedia, <em>Dual number</em>, <a
href="https://en.wikipedia.org/wiki/Dual_number">https://en.wikipedia.org/wiki/Dual_number</a></p>
<p>[2] Berland, <em>Automatic Differentiation</em>, <a
href="http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf">http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf</a></p>
<p>[3] Griewank, <em>Evaluating Derivatives</em></p>
<p>[4] Sheldon, <em>Neural Net Example</em>, <a
href="https://people.cs.umass.edu/~sheldon/teaching/cs335/lec/neural-net-case-studies.html">https://people.cs.umass.edu/~sheldon/teaching/cs335/lec/neural-net-case-studies.html</a></p>
<p>[5] Ghaffari, <em>Automatic Differentiation</em>, <a
href="http://www.cas.mcmaster.ca/~cs777/presentations/AD.pdf">http://www.cas.mcmaster.ca/~cs777/presentations/AD.pdf</a></p>
<p>[6] Autograd, <a
href="https://github.com/HIPS/autograd">https://github.com/HIPS/autograd</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
