\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Otomatik Türev Almak (Automatic Differentiation -AD-)

Matematikte türev hesaplamanýn birkaç yöntemi var; bunlardan birincisi
Calculus'ta öðretilen sembolik türevdir, diðeri sayýsal türevdir. AD üçüncü
bir yöntem sayýlýyor, özellikle programlama baðlamýnda çok faydalý bir
özelliði var, {\em herhangi} bir yazýlým fonksiyonunu alýp, bir veri
noktasý baðlamýnda, o fonksiyonun içinde ne türlü temel diðer fonksiyonlar
olursa olsun onu kendi türevini hesaplayacak hale çevirebiliyor. Burada kod
deðiþimi söz konusu, deðiþim iþlem anýnda dinamik, ya da kaynak kod
seviyesinde derleme öncesi yapýlabiliyor. Fonksiyon \verb!if!, \verb!goto!
gibi dallanma, koþulsal ifadeler içeriyor olabilir (ama bu ifadeler
üzerinden türev alýnan deðiþkene baðlý olmamalýdýr), $\cos,\sin$ gibi
trigonometrik ifadeler, polinomlar, $\max,\min$ gibi gayrý-lineer ifadeler,
ya da baþka herhangi temel hesaplarý kullanýyor olabilir, son derece
çetrefil bir takým hesaplar zincirleme yapýlýyor olabilir. Eðer hesap
deterministik bir þekilde yapýlabiliyorsa (ayný girdiler için hep ayný
deðer hesaplanýyor) AD onu alýp kendi türevini hesaplayabilen hale
çevirebiliyor.

Bu son derece kuvvetli bir özellik. Pek çok optimizasyon yaklaþýmýnda,
mesela gradyan iniþi (gradient descent) minimum noktasý bulmak için bir
fonksiyonun türevine ihtiyaç duyar. Eðer $f(x)$ basit, analitik olarak
türevi kolay alýnabilen bir fonksiyon ise problem yok. Olmadýðý zaman AD
iyi bir çözümdür.

Ýkiz Sayýlar (Dual Numbers)

Lineer Cebir'de ikiz sayýlar reel sayýlarý geniþleterek yeni bir öðe
eklerler, bu öðe üzerinde tanýmlanan cebire göre $\epsilon^2 = 0$
olmalýdýr, ve $\epsilon \ne 0$ [1]. Bu yeni öðe üzerinden her sayý artýk
ikiz þekilde belirtilir, $z = a + b\epsilon$, ki $a,b$ birer reel
sayýdýr. Herhangi bir matematikçi kendisine göre bir cebir tanýmlayabilir,
bunu biliyoruz, operasyonlar tablolar ile tanýmlanýr, vs.  $\epsilon^2=0$
kavramý hayali sayýlardaki $i^2 = -1$'e benzetilebilir. Ýkiz sayýlarý
yazýlýmda depolamak için $(a,b)$ gibi bir çift yeterlidir.

AD amacý için $x \mapsto x + \dot{x} \epsilon$ olarak tanýmlarýz, ki
$\epsilon^2 = 0, d \ne 0$ kullanýyoruz, yani $x$'in kýrpýlmýþ Taylor
açýlýmýný yapýyoruz, ayrýca bu taným bir ikiz sayý. Taylor açýlýmýný
hatýrlarsak bir fonksiyon için herhangi bir $a$ noktasýnda
$f(t) = f(a) + f'(a)(t-a)$ idi, bu durumda fonksiyon $x$'in kendisi,
$f(x)=x$, açýlým $x$ noktasýnda yani $a=x$, ve $f(x)=x=f(x)+f'(x)(x-a)$, ve
$=x+f'(x)(x-a)$. Açýlýmdaki $x-a$ bir $\epsilon$ olarak görülebilir, zaten
normal Taylor açýlýmý için de çok ufak bir adým olarak hesaplanmalýdýr, ve
bu ufak adýmýn karesi de normal olarak sýfýra yaklaþýr. Gerçi $a=x$ olunca
$x-a=0$ olur ama yeni bir cebir yaratarak bu problemden kurtulmak
istemiþler herhalde.

Bu þekilde oluþan aritmetiðe bakarsak,

$$ 
x + y \mapsto 
(x+\dot{x}\epsilon)  + (y+\dot{y}\epsilon) = 
xy + x\dot{y}\epsilon + \dot{x}y\epsilon +
\underbrace{\dot{x}\dot{y}\epsilon^2}_{=0} 
$$
$$  = xy + (x\dot{y} + \dot{x}y)\epsilon  $$

Baþka bir iþlem

$$ xy \mapsto (x+\dot{x}\epsilon)(y+\dot{y}\epsilon)  =
(xy) + (x\dot(y) + \dot{x}y)\epsilon
$$

Bir diðeri

$$ -(x+\dot{x}\epsilon) = -x - \dot{x}\epsilon$$ 

Ya da

$$ 
\frac{1}{x+\dot{x}\epsilon} = 
\frac{1}{x} - \frac{\dot{x}}{x^2}\epsilon, \quad (x \ne 0)
$$

Dikkat edilirse $\epsilon$'nin katsayýlarý sembolik türev sonuçlarýný
birebir takip ediyorlar. Bu sonuçtan istifade edebiliriz, fonksiyonlarý þu
þekilde tanýmlarýz,

$$ 
g(x + \dot{x}d) = g(x) + g'(\dot{x}d) 
\mlabel{1}
$$

O zaman mesela $\sin,\cos$ ya da pek çok diðer fonksiyonu $g$ olarak
alýrsak onlarý þu þekilde açmak mümkün

$$ sin(x + \dot{x}d) = sin(x) + cos(x)\dot{x}d $$

$$ \cos(x+\dot{x}d) = \cos(x) - \sin(x)\dot{x}d$$

$$ e^{x+\dot{x}d} = e^x + e^x \dot{x}d$$

$$ \log(x + \dot{x}d) = \log(x) + \frac{\dot{x}}{x}d , \quad x \ne 0$$

Zincirleme Kanunu, yani $f(g(..))$, üstteki açýlýmý da kullanarak beklenen
þekilde iþleyecek,

$$ f(g(x + \dot{x}\epsilon)) =  f(g(x) + g'(x)\dot{x}\epsilon)  $$

$$ = f(g(x)) + f'(g(x))g'(x)\dot{x} \epsilon$$

Dikkat edersek $\epsilon$'un katsayýsý aynen önce olduðu gibi
$f(g(..))$'nin türevini taþýyor. 

Demek ki ikiz sayýlarý türevi alýnmamýþ fonksiyon sonucu ve türevi alýnmýþ
deðeri program içinde taþýyan veri yapýlarý olarak kullanabiliriz. O
zaman temel bazý operasyonlarý (fonksiyonlarý) (1) formülasyonuna uyacak
þekilde kodlarsak, bu temel fonksiyonlarý içeren her türlü diðer
kompozisyon Zincir Kuralý üzerinden ayný þekilde türev alýnmamýþ ve alýnmýþ
deðerler taþýnýyor olacaktýr.

Örnek

Elimizde 

$$ f(x_1,x_2) = x_1x_2 + \sin(x_1)$$

var. Ýkiz sayýlar ile açalým, 

$$ f(x_1 + \dot{x_1}\epsilon_1, x_1 + \dot{x_2}\epsilon_2) = 
(x_1 + \dot{x_1}\epsilon_1)(x_2 + \dot{x_2}\epsilon_2) \sin(x_1+x_1\dot{x_1}\epsilon_1)
$$

$$ 
= x_1x_2 + (x_2 + \cos(x_1))\dot{x_1}\epsilon_1 
+ x_1\dot{x_2}\epsilon_2
+ x_2\dot{x_1}\epsilon_1
$$

ki $\epsilon_1\epsilon_2 = 0$. 

O zaman bir fonksiyonun türevini hesaplamak için türevi bu standart olmayan
þekilde hesaplayýp, ilgilendiðimiz türevin deðiþkenini 1 olarak atarsak,
istediðimiz türev deðerini $x=a$ noktasýnda elde ederiz.

Eðer kod için düþünürsek, deðiþim þu þekilde olacak (soldaki orijinal
program, saðdaki ikiz program) 

\includegraphics[width=30em]{autodiff_02.png}

Yazýlmýþ kodu görelim,

\begin{minted}[fontsize=\footnotesize]{python}
def f(x1, x2):
    w3 = x1 * x2
    w4 = np.sin(x1)
    w5 = w3 + w4
    return w5

print 'f', f(10, 20)
h = 0.01
print u'sayýsal türev', (f(10+h, 20)-f(10, 20)) / h
\end{minted}

\begin{verbatim}
f 199.455978889
sayýsal türev 19.1636625383
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
def f(x1, x2, dx1, dx2):
    df = [0.,0.]
    w3 = x1 * x2
    dw3 = dx1*x2 + x1*dx2    
    w4 = np.sin(x1)
    dw4 = np.cos(x1) * dx1    
    w5 = w3 + w4
    dw5 = dw3 + dw4
    df[0] = w5
    df[1] = dw5
    return df
print 'AD', f(10,20,1,0)
\end{minted}

\begin{verbatim}
AD [199.45597888911064, 19.160928470923547]
\end{verbatim}

Sembolik olarak türevin $\frac{\partial f}{\partial x_2} = x_1$ olduðunu biliyoruz Üstteki program ayný 
sonuca eriþti. 

AD için Python'da \verb!autograd! paketi otomatik türev alýnmasýný
saðlar. Önceki örnek için

\begin{minted}[fontsize=\footnotesize]{python}
import autograd.numpy as np
from autograd import elementwise_grad
from autograd import grad

def f(x1, x2):
    w3 = x1 * x2
    w4 = np.sin(x1)
    w5 = w3 + w4
    return w5

fg = grad(f)
print fg(10,20)
\end{minted}

\begin{verbatim}
19.1609284709
\end{verbatim}

Dikkat: üstteki kod daha önce gösterilen ile ayný tek bir fark ile,
\verb!numpy! kütüphanesini \verb!autograd!'den alýyoruz, çünkü o üzerinde
AD deðiþimi yapýlmýþ olan \verb!numpy!.

Gradyaný, yani $x$'in her boyutu için kýsmi türevi içeren vektörel olarak
türevleri görmek istiyorsak, 

$$ \nabla f = \left[\begin{array}{r}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2}
\end{array}\right]$$

Yani bir deðiþkeni sabit tutup diðerini deðiþtirince elde edilen türev
bu. Þimdi $x_0 = \left[\begin{array}{cc}10&20\end{array}\right]$ noktasýnda
gradyan deðerini hesaplayalým,

\begin{minted}[fontsize=\footnotesize]{python}
def f(xvec):
    w3 = xvec[0] * xvec[1]
    w4 = np.sin(xvec[0])
    w5 = w3 + w4
    return w5

fg = grad(f)
x0 = np.array([10.,20.])
print fg(x0)
\end{minted}

\begin{verbatim}
[ 19.16092847  10.        ]
\end{verbatim}

Daha bitmedi: Altta $\tanh$'nin türevini alýyoruz, hatta türevin türevi,
onun türevi derken arka arkaya 6 defa zincirleme türev alýyoruz, AD bana
mýsýn demiyor (!).

\begin{minted}[fontsize=\footnotesize]{python}
import autograd.numpy as np
import matplotlib.pyplot as plt
from autograd import elementwise_grad

def tanh(x):
    return (1.0 - np.exp(-x))  / (1.0 + np.exp(-x))

d_fun      = elementwise_grad(tanh)       # 1. Türev
dd_fun     = elementwise_grad(d_fun)      # 2. Türev
ddd_fun    = elementwise_grad(dd_fun)     # 3. Türev
dddd_fun   = elementwise_grad(ddd_fun)    # 4. Türev
ddddd_fun  = elementwise_grad(dddd_fun)   # 5. Türev
dddddd_fun = elementwise_grad(ddddd_fun)  # 6. Türev

x = np.linspace(-7, 7, 200)
plt.plot(x, tanh(x),
         x, d_fun(x),
         x, dd_fun(x),
         x, ddd_fun(x),
         x, dddd_fun(x),
         x, ddddd_fun(x),
         x, dddddd_fun(x))

plt.axis('off')
plt.savefig("autodiff_03.png")
\end{minted}

\includegraphics[width=20em]{autodiff_03.png}

Ýleri, Geri

Üstte gösterilen teknik aslýnda ileri mod (forward mode) AD olarak
biliniyor. Hesap aðaçý üzerinde göstermek gerekirse,

\includegraphics[width=20em]{autodiff_04.png}

Geriye gitmek te mümkün, buna geri mod'u (reverse mode) ismi veriliyor. 

\includegraphics[width=20em]{autodiff_05.png}

Yapay Sinir Aðlarý ve AD

Derin Öðrenim için oluþturulan YSA'lar oldukca çetrefil olabilir (bkz {\em
  Yapay Sinir Aðlarý (Neural Networks)}), $\max$, evriþim (convolution)
gibi operasyonlar içeriyor olabilirler. Bu aðlarý eðitmek için türevi elle
hesaplamak çok zordur. Fakat AD tüm gereken gradyanlarý hesaplar, ve
hatalarý geriye yayarak (backpropagation) aðýrlýklarý optimal deðerlerine
getirir. Þimdi basit YSA'nýn AD ile kodlamasýný görelim [4],

\begin{minted}[fontsize=\footnotesize]{python}
import autograd.numpy as np  # Thinly wrapped version of numpy
from autograd import grad
import matplotlib.pyplot as plt
from sklearn import datasets, linear_model

np.random.seed(0)
X, y = datasets.make_moons(200, noise=0.20)
n = 2 # dimensionality
points_per_class =100 
num_classes = 2 
m = points_per_class*num_classes   
    
fig = plt.figure()
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)
plt.xlim([-1,1])
plt.ylim([-1,1])

h = 0.05
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

X_test = np.c_[xx.ravel(), yy.ravel()]

def plot_model(scores):
    Z = scores.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.xticks(())
    plt.yticks(())

# ReLU: "rectified linear unit" nonlinearity
def relu(z):
    return np.maximum(0, z)

# Initialize parameters randomly
h  = 10 # size of hidden layer
W1 = 0.01 * np.random.randn(n,h)
b1 = np.zeros((1,h))
W2 = 0.01 * np.random.randn(h,num_classes)
b2 = np.zeros((1,num_classes))

# Select hyperparameters
iters      = 1000
eta  = 1e-0
lambda_val = 1e-3 # regularization strength

def compute_loss(params):
    W1, b1, W2, b2 = params
    
    hidden = relu(np.dot(X, W1) + b1)
    scores = np.dot(hidden, W2) + b2
    exp_scores = np.exp(scores)
    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    logprob_correct_class = -np.log(probs[range(m),y])
    data_loss = np.sum(logprob_correct_class)/m    # cross-entropy
    reg_loss = 0.5 * lambda_val * (np.sum(W1*W1) + np.sum(W2*W2))    
    return data_loss + reg_loss

# This is the gradient of the entire feedforward training
gradient = grad(compute_loss)

# Gradient descent loop
for i in range(iters):  
    # Print diagnostic
    loss = compute_loss((W1, b1, W2, b2))      
    if i % 200 == 0: print "iteration %d: loss %f" % (i, loss)        
    dW1, db1, dW2, db2 = gradient((W1, b1, W2, b2))    
    # perform a parameter update
    W1 += -eta * dW1
    b1 += -eta * db1
    W2 += -eta * dW2
    b2 += -eta * db2
    
def predict(X):
    hidden = relu(np.dot(X, W1) + b1)
    scores = np.dot(hidden, W2) + b2
    pred = np.argmax(scores, axis=1)
    return pred

plot_model(predict(X_test))
plt.savefig('autodiff_01.png')
\end{minted}

\begin{verbatim}
iteration 0: loss 0.693097
iteration 200: loss 0.291406
iteration 400: loss 0.277980
iteration 600: loss 0.276930
iteration 800: loss 0.276666
\end{verbatim}

\includegraphics[height=6cm]{autodiff_01.png}

Daha basit bir örnek görelim, mesela Lojistik Regresyon. Elle türev almaya
gerek kalmadan çok basit bir þekilde tahmin, kayýp fonksiyonlarý üzerinden
direk rasgele gradyan iniþi ile kodlamayý yapabiliyoruz.

\begin{minted}[fontsize=\footnotesize]{python}
import autograd.numpy as np
from autograd import grad
from autograd.util import quick_grad_check
from builtins import range

def sigmoid(x):
    return 0.5*(np.tanh(x) + 1)

def logistic_predictions(weights, inputs):
    return sigmoid(np.dot(inputs, weights))

def training_loss(weights):
    preds = logistic_predictions(weights, inputs)
    label_probabilities = preds * targets + (1 - preds) * (1 - targets)
    return -np.sum(np.log(label_probabilities))

inputs = np.array([[0.52, 1.12,  0.77],
                   [0.88, -1.08, 0.15],
                   [0.52, 0.06, -1.30],
                   [0.74, -2.49, 1.39]])
targets = np.array([True, True, False, True])


training_gradient_fun = grad(training_loss)
weights = np.array([0.0, 0.0, 0.0])
for i in range(100):
    weights -= training_gradient_fun(weights) * 0.1
print("Trained loss:", training_loss(weights))
print weights
\end{minted}

\begin{verbatim}
('Trained loss:', 0.042172397668071952)
[ 1.40509236 -0.37749486  2.34249055]
\end{verbatim}

Kaynaklar

[1] Wikipedia, {\em Dual number}, 
    \url{https://en.wikipedia.org/wiki/Dual_number}

[2] Berland, {\em Automatic Differentiation}, 
    \url{http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf}

[3] Griewank, {\em Evaluating Derivatives}

[4] Sheldon, {\em Neural Net Example}, 
    \url{https://people.cs.umass.edu/~sheldon/teaching/cs335/lec/neural-net-case-studies.html}

[5] Ghaffari, {\em Automatic Differentiation}, 
    \url{http://www.cas.mcmaster.ca/~cs777/presentations/AD.pdf}

[6] Autograd, \url{https://github.com/HIPS/autograd}

\end{document}
