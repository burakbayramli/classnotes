<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Uzun Kısa-Vade Hafıza Ağları (Long Short-Term Memory Networks, LSTM)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1
id="uzun-kısa-vade-hafıza-ağları-long-short-term-memory-networks-lstm">Uzun
Kısa-Vade Hafıza Ağları (Long Short-Term Memory Networks, LSTM)</h1>
<p>Kendini tekrarlayan YSA (RNN) yapılarının içindeki gizli konum <span
class="math inline">\(h_t\)</span> (önceki yazıda <span
class="math inline">\(s_t\)</span>) olarak bir zaman diliminden bir
diğerine aktarılabiliyordu, ve bu sırada bir matris çarpımı üzerinden
değişime uğrayabiliyordu. Böylece her zaman diliminde yeni görülen
verinin “hafıza’’ olarak ta tanımlanabilen <span
class="math inline">\(h_t\)</span>’ye etkisi olabiliyordu. RNN dış dünya
hakkındaki iç modelini böyle güncelliyordu.</p>
<p>Fakat RNN ile tarif edilen bu güncellemeye hiç bir sınır getirmedik.
Biraz düşünürsek bu güncellemenin biraz kaotik bir hal alabileceğini
görebiliriz [1]. Mesela bir filmi kare kare izleyerek filmde neler
olduğunu tarif etmeye uğraşan bir RNN düşünelim. Bir karede bir
karakterin ABD’de olduğunu düşünebilir, ama sonraki karede karakterin
suşi yediğini görüyor ve Japonya’da olduğuna karar verebilir, sonra
Panda ayısı görüyor ve karakteri kuzey kutbunda zannediyor.</p>
<p>Bu tarif edilen kaos enformasyonun çok hızlı etki ettiğini ve aynı
hızda yokolduğuna işaret. Bu tür bir yapıda modelin uzun vadeli hafıza
tutması oldukça zor. Bize gereken modelin sadece güncelleme yapması
değil, güncelleme yapmayı da öğrenmesi. Ali adlı bir karakter film
karesinde yoksa o kareler Ali hakkındaki bilgiyi güncellemek için
kullanılmamalı, aynı şekilde Ayşe’nin içinde olmadığı kareler onun
hakkındaki bilgiyi güncellemek için kullanılmamalı.</p>
<p>Çözüm için şöyle bir yaklaşım kullanabiliriz.</p>
<ol type="1">
<li><p>Bir “unutma’’ mekanizması ekle. Film seyrediyoruz, bir sahne
bitiyor, o sahnenin hangi gün, saat kaçta, nerede olduğunu unutuyoruz.
Fakat bir karakter o sahnede ölmüşse, bunu hatırlıyoruz. Modelin ne
zaman hatırlayacağını, ne zaman unutacağını öğrenmesini istiyoruz
(dikkat sadece belli bir şekilde unutması, hatırlaması değil, tüm
bunları nasıl, ne zaman yapacağını öğrenmesi).</p></li>
<li><p>Bir belleğe yazma (zulaya atma?) mekanizması. Modelin yeni bir
kare gördüğünde o karedeki bilginin kaydetmeye değer olup olmadığına
karar vermesi lazım, ve bu öğrenilse iyi olur.</p></li>
<li><p>.. ki yeni bir girdi gelince model ihtiyacı olmadığı bilgiyi
unutacak. Sonra girdinin hangi kısmının faydalı olduğuna karar verecek
ve o kısmı uzun-vadeli hafızasına kaydedecek.</p></li>
<li><p>Bir odaklanma mekanizması. Uzun-vadeli hafızanın hangi kısmı sık
kullanım gerekiriyor, işlem hafızası (working memory) hangisi, buna
karar vermek.</p></li>
</ol>
<p>Bize gereken bir uzun kısa-vade hafıza ağıdır, teknik ismiyle LSTM.
RNN her zaman adımında hafızasını kontrolsüz bir şekilde
güncelleyebiliyorken, bir LSTM hafızasını çok daha seçici, kararlı bir
şekilde günceller, bunu yaparken spesifik öğrenme mekanizmaları kullanır
ki bu mekanizma ona görülen bilginin hangi kısmının hatırlanmaya değer,
hangisinin güncellenmesinin gerekli olduğunu, ve hangisinin daha fazla
odaklanılmaya ihtiyaç duyduğunu belirler.</p>
<p>Matematiksel olarak <span class="math inline">\(t\)</span> anında bir
<span class="math inline">\(x_t\)</span> girdisi alıyoruz, uzun-vadeli
ve işlem hafızası <span class="math inline">\(C_{t-1}\)</span> ve <span
class="math inline">\(h_{t-1}\)</span> bir önceki zaman diliminden bir
önceki bu zamana aktarılıyor ve onları bir şekilde güncellemek
istiyoruz. Bize gereken bir tür hatırlama geçidi (remember gate), bu
elektronik devrelerdeki gibi bir geçit, 0 ile 1 arasında olacak <span
class="math inline">\(n\)</span> tane sayı, bu sayı <span
class="math inline">\(n\)</span> hafıza ögesinin ne kadar
hatırlanacağını, yani ne kadar uzun-vadeli olup olmayacağını
belirleyecek. 1 tut, 0 unut demek olacak.</p>
<p>Ufak bir YSA kullanarak bu geçidi öğrenebiliriz,</p>
<p><span class="math display">\[ f_t = \sigma (W_r x_t + U_r h_{t-1})
\]</span></p>
<p>Bu basit, sığ (derin olmayan) bir YSA, <span
class="math inline">\(\sigma\)</span> sigmoid aktivasyonu. Sigmoid
kullandık çünkü 0 ile 1 arasında çıktıya ihtiyacımız var. Şimdi girdiden
öğreneceğimiz bilgiyi hesaplamamız lazım, bu bilgi uzun-vadeli hafızamız
için bir aday olacak.</p>
<p><span class="math display">\[ C_t&#39; = \phi(W_l x_t + U_l
h_{t-1})\]</span></p>
<p><span class="math inline">\(\phi\)</span> bir aktivasyon fonksiyonu,
çoğunlukla <span class="math inline">\(\tanh\)</span> olarak
seçilir.</p>
<p>Fakat bir adayı hafızamıza eklemeden önce hangi bölümlerinin
kullanıma, kaydetmeye değer olduğunu öğrenmemiz gerekir. Web’de bir şey
okurken kendi zihnimizde neler olduğunu düşünelim. Bir haber makalesi
okuyoruz mesela, Trabzonspor’un hep kötüye gittiğini, hep yanlış
tranferler yaptığını anlatan bir haber okuyoruz, ama bu haberi
fenerbahce.org sitesinde okuyorsak o habere daha az önem
verebiliriz.</p>
<p><span class="math display">\[ i_t = \sigma (W_s x_t + U_s h_{t-1})
\]</span></p>
<p>Şimdi tüm bu basamakları birleştirelim. İhtiyacımız olmayan
hafızaları unuttuktan ve bilgilerin faydalı olabilecek kısımlarını
sakladıktan sonra, elimize bir güncellenmiş uzun-vadeli bellek
geçer,</p>
<p><span class="math display">\[ C_t = f_t \circ c_{t-1} \circ i_t \circ
\tilde{C}_t \]</span></p>
<p>ki <span class="math inline">\(\circ\)</span> operasyonu her iki
taraftaki değişkenin içindeki her ögenin birer birer çarpılması
(element-wise multiplication) demek.</p>
<p>Sonra işlem hafızasını güncellememiz lazım, uzun-vade belleğimizi
anlık işlem için faydalı olabilecek şekilde nasıl odaklarız, onu
öğrenmek istiyoruz. O zaman bir odaklanma vektörü öğreniriz,</p>
<p><span class="math display">\[ o_t = \sigma (W_f x_t + U_f h_{t-1})
\]</span></p>
<p>O zaman işlem hafızamız</p>
<p><span class="math display">\[ h_t = o_t \circ \phi (C_t)\]</span></p>
<p>Yani odak değeri 1 olan öğelere tam dikkatimizi veriyoruz, 0 olanlara
hiç dikkat etmiyoruz.</p>
<p>Kuşbakışı ile tüm resmi görelim, kendini tekrar eden LSTM yapısı
alttaki gibi,</p>
<p><img src="lstm_02.png" /></p>
<p>Sol ve sağdaki hücreler ortadakinin kopyası.</p>
<p>Kıyasla bir RNN’nin iç yapısı çok daha basittir,</p>
<p><img src="lstm_03.png" /></p>
<p>LSTM resmindeki her birimin formülleriyle beraber teker teker tekrar
üzerinden geçmek gerekirse [2] - ilk adım hücre bilgisinden neleri
atacağımıza karar vermek. Bu karar unutma karar tabakası adı verilen bir
sigmoid tabakasında veriliyor, tabaka <span
class="math inline">\(h_{t-1}\)</span> ve <span
class="math inline">\(x_t\)</span>’ye bakıyor ve <span
class="math inline">\(C_{t-1}\)</span> hücre konumundaki her değer için
0 ile 1 arasında bir sayı üretiyor. 1 değeri tamamen tut, 0 değeri
tamamen unut anlamına geliyor (tüm <span
class="math inline">\(b\)</span> değerleri yanlılık -bias- için).</p>
<p><img src="lstm_04.png" /></p>
<p>Bir sonraki adım hücrede hangi yeni bilgiyi depolayacağımıza karar
vermek. Bu kararın iki parçası var, önce girdi geçit tabakası (input
gate layer) hangi değerleri güncelleyeceğimize karar veriyor, ardından
bir <span class="math inline">\(\tanh\)</span> tabakası bir “aday
vektörü’’ <span class="math inline">\(\tilde{C}_t\)</span> üretiyor, bu
vektör, adı üzerinde, hücre konumuna eklenmeye aday bilgiler. Ardından
bu iki vektör birleştirilip konumu güncellemek için yeni bir vektör
yaratılıyor.</p>
<p><img src="lstm_05.png" /></p>
<p>Sıra güncelleme iş bantına (update conveyor) geldi. Herhalde bu isim
verilmiş çünkü hücrenin en üstünde direk soldan sağa giden bu ok bir tür
fabrika iş bantı gibi, bir ana akım hattı. Bir kaç bilgi akımı bu ana
kola giriyor. Bu iş bantının görevi hücrenin eski konum bilgisini
güncellemek, <span class="math inline">\(C_{t-1}\)</span>’i <span
class="math inline">\(C_t\)</span> haline getirmek. Önceki adımlar ne
yapılması gerektiğine karar vermişti, şimdi bu kararları eyleme geçirme
zamanı. Eski konum bilgisini <span class="math inline">\(f_t\)</span>
ile çarpıyoruz, böylece unutmak istediklerimizi unutuyoruz. Sonra <span
class="math inline">\(i_t * C_t&#39;\)</span>’yi ekliyoruz, bunlar yeni
konumu ne kadar güncellemek istediğimizi belirleyen ağırlıklarla
ölçeklenmiş yeni aday değerler.</p>
<p><img src="lstm_06.png" /></p>
<p>En son adım tahmin adımı, bu adımda artık çıktının ne olacağına karar
veriyoruz. Çıktı mevcut hücre konumunu baz alacak, ama onun filtrelenmiş
bir hali olacak. İlk önce bir sigmoid işleterek konumun hangi kısmının
çıktıya verileceğini kararlaştırıyoruz. Ardından mevcut konum bilgisini
bir <span class="math inline">\(\tanh\)</span>’e veriyoruz, ki bu
değerlerin <span class="math inline">\(-1,+1\)</span> arasında gelmesini
sağlıyoruz, ve bu sonucu sigmoid’den gelen değer ile çarpıyoruz, ki
sadece istediğimiz kısmın dışarı verilmesini sağlayalım.</p>
<p><img src="lstm_07.png" /></p>
<p>Şimdi üstteki formülleri kullanarak TensorFlow ile sıfırdan bir LSTM
kodlayalım. Amacımız daha önce RNN için gördüğümüz zaman serisini
öğrenmek, bu seriyi 5’er 5’er okuyacağız, yani yanyana 5 seri öğesini
okuyacağız, ve 6. seri değeri hedef değeri olacak, seri içinden bu
şekilde örneklem yaparak bir ufak toptan eğitim seti yaratacağız. Sonra
serinin hiç görmediğimiz “geleceğini’’ tahmin etmeye uğraşacağız.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pprint <span class="im">import</span> pprint</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>tf.reset_default_graph()</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>tf.set_random_seed(<span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>LSTM_SIZE <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>t_min, t_max <span class="op">=</span> <span class="dv">0</span>, <span class="dv">30</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>resolution <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>instruction_count <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>epoch <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(t):</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t <span class="op">*</span> np.sin(t) <span class="op">/</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.sin(t<span class="op">*</span><span class="dv">5</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> next_batch(batch_size, n_steps):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    t0 <span class="op">=</span> np.random.rand(batch_size, <span class="dv">1</span>) <span class="op">*</span> (t_max <span class="op">-</span> t_min <span class="op">-</span> n_steps <span class="op">*</span> resolution)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    Ts <span class="op">=</span> t0 <span class="op">+</span> np.arange(<span class="fl">0.</span>, n_steps <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> resolution</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    ys <span class="op">=</span> f(Ts)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> ys[:, :<span class="op">-</span><span class="dv">1</span>].reshape(<span class="op">-</span><span class="dv">1</span>, n_steps, <span class="dv">1</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> ys[:, <span class="dv">1</span>:].reshape(<span class="op">-</span><span class="dv">1</span>, n_steps, <span class="dv">1</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y[:,<span class="op">-</span><span class="dv">1</span>,:]</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> y.flatten()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> y.shape    </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">list</span>(X),y</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>reference_input_data,reference_output_data <span class="op">=</span> next_batch(<span class="dv">400</span>, sequence_length)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># verinin 1/4&#39;u egitim gerisi test</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>NUM_EXAMPLES <span class="op">=</span> <span class="bu">len</span>(reference_input_data) <span class="op">/</span> <span class="dv">4</span> </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>test_input <span class="op">=</span> reference_input_data[NUM_EXAMPLES:]</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>test_output <span class="op">=</span> reference_output_data[NUM_EXAMPLES:] </span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>train_input <span class="op">=</span> reference_input_data[:NUM_EXAMPLES]</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>train_output <span class="op">=</span> reference_output_data[:NUM_EXAMPLES]</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> train_input[<span class="dv">1</span>]</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> train_output[<span class="dv">1</span>]</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> tf.placeholder(tf.float32, </span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>                      [<span class="va">None</span>, sequence_length, instruction_count], </span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>                      name<span class="op">=</span><span class="st">&#39;data&#39;</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> tf.transpose(tf.placeholder(tf.float32, [<span class="va">None</span>], name<span class="op">=</span><span class="st">&#39;target&#39;</span>))</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>FEATURE_SIZE <span class="op">=</span> <span class="dv">1</span> </span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> default_weights_and_bias():</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    Weights <span class="op">=</span> tf.Variable(tf.truncated_normal([LSTM_SIZE, </span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>                                               LSTM_SIZE <span class="op">+</span> FEATURE_SIZE], </span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>                                               <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.1</span>))</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> tf.Variable(tf.constant(<span class="fl">0.0</span>, shape <span class="op">=</span> [LSTM_SIZE, <span class="dv">1</span>]))</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Weights, bias</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>W_f, _ <span class="op">=</span> default_weights_and_bias()</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>b_f <span class="op">=</span> tf.Variable(tf.constant(<span class="fl">1.0</span>, shape <span class="op">=</span> [LSTM_SIZE, <span class="dv">1</span>]))</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Unutma tabakasi</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f_t(ht_minus_1_and_xt):</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.sigmoid(tf.matmul(W_f, ht_minus_1_and_xt) <span class="op">+</span> b_f)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>W_i, b_i <span class="op">=</span> default_weights_and_bias()</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="co"># Girdi gecidi tabakasi</span></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> i_t(ht_minus_1_and_xt):</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.sigmoid(tf.matmul(W_i, ht_minus_1_and_xt) <span class="op">+</span> b_i)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>W_C, b_c <span class="op">=</span> default_weights_and_bias()</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="co"># is banti icin adaylar</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> candidate_C_t(ht_minus_1_and_xt):</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.tanh(tf.matmul(W_C, ht_minus_1_and_xt) <span class="op">+</span> b_c)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> C_t(ht_minus_1_and_xt, Conveyor, CandConv):</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> f_t(ht_minus_1_and_xt) <span class="op">*</span> Conveyor <span class="op">+</span> i_t(ht_minus_1_and_xt) <span class="op">*</span> CandConv</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>W_o, b_o <span class="op">=</span> default_weights_and_bias()</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="co"># guncellenmis is banti</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> h_t(ht_minus_1_and_xt, FinalConveyor):</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>    o_t <span class="op">=</span> tf.sigmoid(tf.matmul(W_o, ht_minus_1_and_xt) <span class="op">+</span> b_o)    </span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> o_t <span class="op">*</span> tf.tanh(FinalConveyor)</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lstm_cell(ht_minus_1_and_Conveyor, xt):</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>    ht_minus_1, Conveyor <span class="op">=</span> ht_minus_1_and_Conveyor</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>    ht_minus_1_and_xt <span class="op">=</span> tf.transpose(tf.concat([ht_minus_1, xt], <span class="dv">1</span>))</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>    CandidateConveyor <span class="op">=</span> candidate_C_t(ht_minus_1_and_xt)</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>    FinalConveyor <span class="op">=</span> C_t(ht_minus_1_and_xt, Conveyor, CandidateConveyor)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>    lstm_prediction <span class="op">=</span> tf.transpose(h_t(ht_minus_1_and_xt, FinalConveyor))</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(lstm_prediction, FinalConveyor)</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>data_length <span class="op">=</span> tf.shape(data)[<span class="dv">0</span>]</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lstm_loop(last_lstm_prediction, last_state, step):</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>    lstm_prediction, state <span class="op">=</span> lstm_cell((last_lstm_prediction, last_state),</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>                                       data[:, step, :])</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lstm_prediction, state, tf.add(step, <span class="dv">1</span>)</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>initial_Conveyor <span class="op">=</span> tf.zeros([LSTM_SIZE, data_length])</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>initial_prediction <span class="op">=</span> tf.zeros([data_length, LSTM_SIZE])</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>timesteps <span class="op">=</span> sequence_length</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>for_each_time_step <span class="op">=</span> <span class="kw">lambda</span> a, b, step: tf.less(step, timesteps)</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>arg <span class="op">=</span> (initial_prediction, initial_Conveyor, <span class="dv">0</span>)</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>lstm_prediction, lstm_state, _ <span class="op">=</span> tf.while_loop(for_each_time_step,</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>                                               lstm_loop, arg,</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>                                               parallel_iterations<span class="op">=</span><span class="dv">32</span>)</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>weight <span class="op">=</span> tf.Variable(tf.truncated_normal([LSTM_SIZE, <span class="dv">1</span>]))</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> tf.Variable(tf.constant(<span class="fl">0.0</span>, shape<span class="op">=</span>[<span class="dv">1</span>]))</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> tf.matmul(lstm_prediction, weight) <span class="op">+</span> bias</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.name_scope(<span class="st">&#39;mean_square_error&#39;</span>):</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>    mean_square_error <span class="op">=</span> tf.reduce_sum(tf.square(tf.subtract(target, tf.unstack(prediction, axis <span class="op">=</span> <span class="dv">1</span>))))</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>tf.summary.scalar(<span class="st">&#39;mean_square_error&#39;</span>, mean_square_error)</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.AdamOptimizer()</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>minimize <span class="op">=</span> optimizer.minimize(mean_square_error)</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>mistakes <span class="op">=</span> tf.not_equal(target, tf.<span class="bu">round</span>(tf.unstack(prediction, axis <span class="op">=</span> <span class="dv">1</span>)))</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.InteractiveSession()</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>date <span class="op">=</span> <span class="bu">str</span>(datetime.datetime.now())</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>init_op <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>saver <span class="op">=</span> tf.train.Saver() </span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>sess.run(init_op)</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(epoch):</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">20</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>        mean_squ_err <span class="op">=</span> sess.run(mean_square_error, {data: test_input, target: test_output})</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Epoch </span><span class="sc">{:4d}</span><span class="st"> | mean squ error </span><span class="sc">{: 3.1f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(i <span class="op">+</span> <span class="dv">1</span>, mean_squ_err))</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>    sess.run(minimize,{data: train_input, target: train_output})</span></code></pre></div>
<pre><code>(400,)
[[ 4.00159218]
 [ 4.4298434 ]
 [ 4.67217731]
 [ 4.52697138]]
3.87853505042
Epoch   20 | mean squ error  3181.8
Epoch   40 | mean squ error  1860.6
Epoch   60 | mean squ error  1307.7
Epoch   80 | mean squ error  903.1
Epoch  100 | mean squ error  628.8
Epoch  120 | mean squ error  477.8
Epoch  140 | mean squ error  390.5
Epoch  160 | mean squ error  323.5
Epoch  180 | mean squ error  271.9
Epoch  200 | mean squ error  233.8
Epoch  220 | mean squ error  204.4
Epoch  240 | mean squ error  180.8
Epoch  260 | mean squ error  161.2
Epoch  280 | mean squ error  144.9
Epoch  300 | mean squ error  131.7
Epoch  320 | mean squ error  121.4
Epoch  340 | mean squ error  113.0
Epoch  360 | mean squ error  105.7
Epoch  380 | mean squ error  99.0
Epoch  400 | mean squ error  92.6
Epoch  420 | mean squ error  86.6
Epoch  440 | mean squ error  80.9
Epoch  460 | mean squ error  75.4
Epoch  480 | mean squ error  70.1
Epoch  500 | mean squ error  64.9</code></pre>
<p>Tahminleri üretirken eğitim verisinin en sonundaki
<code>sequence_length</code> kadar öğeyi alıp sonraki 1 değeri
üretiyoruz, bu değeri alıp tahmin için kullanılan biraz önce
kullandığımız <code>sequence_length</code> kadar değerin sonuna ekleyip
son <code>sequence_length</code> değer üzerinden tekrar bir tahmin
üretiyoruz, böyle gidiyor, ve bu şekilde serinin hiç görmediğimiz
kısmını tahmin ediyoruz.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(t):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t <span class="op">*</span> np.sin(t) <span class="op">/</span> <span class="dv">3</span> <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> np.sin(t<span class="op">*</span><span class="dv">5</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.linspace(t_min, t_max, <span class="bu">int</span>((t_max <span class="op">-</span> t_min) <span class="op">/</span> resolution))</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> f(t)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>newx <span class="op">=</span> <span class="bu">list</span>(t[<span class="op">-</span>sequence_length:])</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>newy <span class="op">=</span> <span class="bu">list</span>(y[<span class="op">-</span>sequence_length:])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">40</span>): <span class="co"># bu kadar daha uret</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>   tst_input <span class="op">=</span> np.array(newy[<span class="op">-</span>sequence_length:]).reshape(sequence_length,<span class="dv">1</span>)   </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>   res <span class="op">=</span> sess.run(prediction, { data: [ tst_input ]  } )</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>   newy.append(res)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>   newx.append(t_max <span class="op">+</span> (i<span class="op">*</span>resolution))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>plt.plot(t,y)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>plt.plot(newx,newy,<span class="st">&#39;g&#39;</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;lstm_01.png&#39;</span>)</span></code></pre></div>
<p><img src="lstm_01.png" /></p>
<p>Yeşil renkli kısım tahmin. Fena değil.</p>
<p>Zaman Serisi Sınıflandırmak</p>
<p>Şimdi TF’in kendi LSTM çağrısını kullanarak zaman serisi
sınıflandırması yapalım. İki farklı sınıfa ait olan zaman serisi verimiz
var [3], mikroelektronik yarı-iletken üretiminden gelen bir veri,
fabrikasyon sırasında algılayıcılar bu iki türlü seriyi kaydediyor,
onları ayırtetmek bizim görevimiz. Okuma yöntemi kodlayalım,</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd, zipfile</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.contrib <span class="im">import</span> rnn</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>training_iters <span class="op">=</span> <span class="dv">100000</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">25</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>display_step <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>n_input <span class="op">=</span> <span class="dv">1</span> </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>n_steps <span class="op">=</span> <span class="dv">152</span> </span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">128</span> </span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>n_classes <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> zipfile.ZipFile(<span class="st">&#39;wafer.zip&#39;</span>, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> z:</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>      df_train <span class="op">=</span>  pd.read_csv(z.<span class="bu">open</span>(<span class="st">&#39;Wafer/wafer_TRAIN.txt&#39;</span>),header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>      df_test <span class="op">=</span>  pd.read_csv(z.<span class="bu">open</span>(<span class="st">&#39;Wafer/wafer_TEST.txt&#39;</span>),header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> minibatches(batch_size,<span class="bu">input</span><span class="op">=</span><span class="st">&quot;train&quot;</span>):</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>      df <span class="op">=</span> <span class="va">None</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="bu">input</span><span class="op">==</span><span class="st">&quot;train&quot;</span>: df<span class="op">=</span>df_train</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> <span class="bu">input</span><span class="op">==</span><span class="st">&quot;test&quot;</span>: df<span class="op">=</span>df_test</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>      df <span class="op">=</span> np.array(df)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>      <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df)):</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            batch_x <span class="op">=</span> []<span class="op">;</span> batch_y <span class="op">=</span> []</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>                  batch_x.append(<span class="bu">list</span>(df[i,<span class="dv">1</span>:]))</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>                  batch_y.append([<span class="bu">int</span>(df[i,<span class="dv">0</span>]<span class="op">==-</span><span class="dv">1</span>), <span class="bu">int</span>(df[i,<span class="dv">0</span>]<span class="op">==</span><span class="dv">1</span>) ])</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            batch_x <span class="op">=</span> np.array(batch_x).reshape(batch_size,n_steps,<span class="dv">1</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>            batch_y <span class="op">=</span> np.array(batch_y).reshape(batch_size,<span class="dv">2</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">yield</span> batch_x, batch_y                  </span></code></pre></div>
<p>TF hesap çizitini kodlayalım ve eğitelim,</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reset_graph(seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    tf.reset_default_graph()</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    tf.set_random_seed(seed)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>reset_graph()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.placeholder(<span class="st">&quot;float&quot;</span>, [<span class="va">None</span>, n_steps, n_input])</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.placeholder(<span class="st">&quot;float&quot;</span>, [<span class="va">None</span>, n_classes])</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> {</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;out&#39;</span>: tf.Variable(tf.random_normal([n_hidden, n_classes]))</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>biases <span class="op">=</span> {</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;out&#39;</span>: tf.Variable(tf.random_normal([n_classes]))</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LSTM(x, weights, biases):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> tf.unstack(x, n_steps, <span class="dv">1</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    lstm_cell <span class="op">=</span> rnn.BasicLSTMCell(n_hidden)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    outputs, states <span class="op">=</span> rnn.static_rnn(lstm_cell, x, dtype<span class="op">=</span>tf.float32)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.matmul(outputs[<span class="op">-</span><span class="dv">1</span>], weights[<span class="st">&#39;out&#39;</span>]) <span class="op">+</span> biases[<span class="st">&#39;out&#39;</span>]</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> LSTM(x, weights, biases)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>correct_pred <span class="op">=</span> tf.equal(tf.argmax(pred,<span class="dv">1</span>), tf.argmax(y,<span class="dv">1</span>))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>new_pred <span class="op">=</span> tf.argmax(y,<span class="dv">1</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;cost&#39;</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>scf <span class="op">=</span> tf.nn.softmax_cross_entropy_with_logits(logits<span class="op">=</span>pred, labels<span class="op">=</span>y)</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>cost <span class="op">=</span> tf.reduce_mean(scf)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;optimizer&#39;</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.AdamOptimizer(learning_rate<span class="op">=</span>learning_rate).minimize(cost)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    sess.run(init)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>    step <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Keep training until reach max iterations</span></span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    b_it <span class="op">=</span> minibatches(batch_size)</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> step <span class="op">&lt;</span> <span class="bu">int</span>(<span class="dv">1000</span> <span class="op">/</span> batch_size):</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>          batch_x, batch_y <span class="op">=</span> <span class="bu">next</span>(b_it)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>          sess.run(optimizer, feed_dict<span class="op">=</span>{x: batch_x, y: batch_y})</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>          <span class="cf">if</span> step <span class="op">%</span> display_step <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Calculate batch accuracy</span></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>                acc <span class="op">=</span> sess.run(accuracy, feed_dict<span class="op">=</span>{x: batch_x, y: batch_y})</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Calculate batch loss</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> sess.run(cost, feed_dict<span class="op">=</span>{x: batch_x, y: batch_y})</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">&quot;Iter &quot;</span> <span class="op">+</span> <span class="bu">str</span>(step) <span class="op">+</span> <span class="st">&quot;, Minibatch Loss= &quot;</span> <span class="op">+</span> <span class="op">\</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>                      <span class="st">&quot;</span><span class="sc">{:.6f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(loss) <span class="op">+</span> <span class="st">&quot;, Training Accuracy= &quot;</span> <span class="op">+</span> <span class="op">\</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>                      <span class="st">&quot;</span><span class="sc">{:.5f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(acc))</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>          step <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Optimization Finished!&quot;</span>)</span></code></pre></div>
<pre><code>cost
optimizer
Iter 10, Minibatch Loss= 1.847300, Training Accuracy= 0.00000
Iter 20, Minibatch Loss= 0.049264, Training Accuracy= 1.00000
Iter 30, Minibatch Loss= 0.176535, Training Accuracy= 1.00000
Optimization Finished!</code></pre>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>saver <span class="op">=</span> tf.train.Saver()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>real <span class="op">=</span> []</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> []</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_x, batch_y <span class="kw">in</span> minibatches(<span class="dv">1</span>,<span class="bu">input</span><span class="op">=</span><span class="st">&quot;test&quot;</span>):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>      res <span class="op">=</span> sess.run(new_pred, feed_dict<span class="op">=</span>{x: batch_x, y: batch_y})</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>      pred.append(res[<span class="dv">0</span>])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>      real.append(np.argmax(batch_y[<span class="dv">0</span>]))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    fpr, tpr, thresholds <span class="op">=</span> metrics.roc_curve(np.array(real), np.array(pred))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> <span class="st">&#39;AUC&#39;</span>, metrics.auc(fpr, tpr)      </span></code></pre></div>
<pre><code>AUC 1.0</code></pre>
<p>Sonuç yüzde 100.</p>
<p>Kaynaklar</p>
<p>[1] Chen, <em>Exploring LSTMs: Understanding Basics (Part One)</em>,
<a
href="https://www.topbots.com/exploring-lstm-tutorial-part-1-recurrent-neural-network-deep-learning/">https://www.topbots.com/exploring-lstm-tutorial-part-1-recurrent-neural-network-deep-learning/</a></p>
<p>[2] Shell, <em>Do It Yourself LSTM with TensorFlow</em>, <a
href="https://chrisschell.de/2017/07/10/do_it_yourself_lstm_with_tensorflow.html">https://chrisschell.de/2017/07/10/do_it_yourself_lstm_with_tensorflow.html</a></p>
<p>[3] Olszewski, <em>Wafer Dataset</em>, <a
href="http://timeseriesclassification.com/description.php?Dataset=Wafer">http://timeseriesclassification.com/description.php?Dataset=Wafer</a></p>
<p>[4] Olah, <em>Understanding LSTM Networks</em>, <a
href="https://colah.github.io/posts/2015-08-Understanding-LSTMs">https://colah.github.io/posts/2015-08-Understanding-LSTMs</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
