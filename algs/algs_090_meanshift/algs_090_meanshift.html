<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  
  
  
  
</head>
<body>
<h1 id="ortalama-kaydırma-ile-kümeleme-mean-shift-clustering">Ortalama Kaydırma ile Kümeleme (Mean Shift Clustering)</h1>
<p>Kümeleme yapmak için bir metot daha: Ortalama Kaydırma metotu. Bu metodun mesela GMM gibi bir metottan farkı, küme sayısının önceden belirtilmeye ihtiyacı olmamasıdır, küme sayısı otomatik olarak metot tarafından saptanır.</p>
<p>&quot;Küme&quot; olarak saptanan aslında veri içindeki tüm yoğunluk bölgelerinin merkezleridir, yani alttaki resmin sağ kısmındaki bölgeler.</p>
<div class="figure">
<img src="dist.png" />

</div>
<p>Başlangıç neresidir? Başlangıç tüm noktalardır, yani her noktadan başlanarak</p>
<ol style="list-style-type: decimal">
<li><p>O nokta etrafında (yeterince büyük) bir pencere tanımla</p></li>
<li><p>Bu pencere içine düşen tüm noktaları hesaba katarak bir ortalama yer hesapla</p></li>
<li><p>Pencereyi yeni ortalama noktayı merkezine alacak şekilde kaydır</p></li>
</ol>
<p>Metotun ismi buradan geliyor, çünkü pencere yeni ortalamaya doğru &quot;kaydırılıyor&quot;. Altta bir noktadan başlanarak yapılan hareketi görüyoruz. Kaymanın sağa doğru olması mantıklı çünkü tek pencere içinden bakınca bile yoğunluğun &quot;sağ tarafa doğru&quot; olduğu görülmekte. Yöntemin püf noktası burada.</p>
<p><img src="mean_2.png" /> <img src="mean_3.png" /></p>
<p><img src="mean_4.png" /> <img src="mean_5.png" /></p>
<p><img src="mean_6.png" /> <img src="mean_7.png" /></p>
<p>Eğer yoğunluk merkezine çok yakın bir noktadan / noktalardan başlamışsak ne olur?</p>
<p>O zaman ilerleme o başlangıç noktası için anında bitecek, çünkü hemen yoğunluk merkezine gelmiş olacağız. Diğer yönlerden gelen pencereler de aynı yere gelecekler tabii, o zaman aynı / yakın yoğunluk merkezlerini aynı küme olarak kabul etmemiz gerekir. Bu &quot;aynı küme irdelemesi&quot; sayısal hesaplama açısından ufak farklar gösterebilir tabii, ve bu ufak farkı gözönüne alarak &quot;küme birleştirme&quot; mantığını da eklemek gerekiyor.</p>
<p>Ortalama Kaydırma sisteminde pencere büyüklüğü kullanıcı tarafından tanımlanır. Optimal pencere büyüklüğünü nasıl buluruz? Deneme yanılma yöntemi, verinin tarifsel istatistiklerine kestirme bir hesap (estimate) etmek, ya da kullanıcının aynı istatistiklere bakarak tahminde bulunması. Birkaç farklı pencere büyüklüğü de denenebilir. Bu konu literatürde (İng. bandwidth selection) adı altında uzun uzadıya tartışılmaktadır.</p>
<p>Eğer yoğunluk merkezine çok yakın bir noktadan / noktalardan başlamışsak ne olur?</p>
<p>O zaman ilerleme o başlangıç noktası için anında bitecek, çünkü hemen yoğunluk merkezine gelmiş olacağız. Diğer yönlerden gelen pencereler de aynı yere gelecekler tabii, o zaman aynı / yakın yoğunluk merkezlerini aynı küme olarak kabul etmemiz gerekir. Bu &quot;aynı küme irdelemesi&quot; sayısal hesaplama açısından ufak farklar gösterebilir tabii, ve bu ufak farkı gözönüne alarak &quot;küme birleştirme&quot; mantığını da eklemek gerekiyor.</p>
<div class="figure">
<img src="start.png" />

</div>
<p>Altta örnek veri ve kodu bulunabilir. Metot küme sayısı 17'yi otomatik olarak buluyor (gerçek küme sayısı 20, bkz [7] yazısı).</p>
<p>Alternatif bir kod <code>meanshift_alternatıve.py</code> dosyasında bulunabilir, bu kod pencereler kaydırırken onların üzerinden geçtiği noktaları &quot;sahiplenen&quot; türden bir kod. Yani [encere hareketini durdurduğunda hem küme merkezini hem de o kümenin altındaki noktaları bulmuş oluyoruz. Tabii sonraki pencereler bazı noktaları önceki kümelerden çalabilirler. Neyse, işlemin normal işleyişine göre bir sonraki pencere seçilecektir ve bu pencere &quot;geriye kalan noktalar&quot; üzerinden işlem yapacaktır. Beklenir ki, işlem ilerledikçe işlenmesi gereken noktalar azalacaktır ve yöntemin bu sebeple klasik yönteme göre daha hızlı işleyeceği tahmin edilebilir. Hakikaten de böyledir.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> pandas <span class="im">import</span> <span class="op">*</span>
data <span class="op">=</span> read_csv(<span class="st">&quot;../kmeans/synthetic.txt&quot;</span>,comment<span class="op">=</span><span class="st">&#39;#&#39;</span>,header<span class="op">=</span><span class="va">None</span>,sep<span class="op">=</span><span class="st">&quot;   &quot;</span>)
<span class="bu">print</span> data.shape
data <span class="op">=</span> np.array(data)</code></pre></div>
<pre><code>(3000, 2)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.scatter(data[:,<span class="dv">0</span>],data[:,<span class="dv">1</span>])
plt.savefig(<span class="st">&#39;meanshift_1.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="meanshift_1.png" />

</div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.neighbors <span class="im">import</span> NearestNeighbors
<span class="im">from</span> sklearn.utils <span class="im">import</span> extmath

<span class="kw">def</span> mean_shift(X, bandwidth<span class="op">=</span><span class="va">None</span>, max_iterations<span class="op">=</span><span class="dv">300</span>):
    
    seeds <span class="op">=</span> X
    n_samples, n_features <span class="op">=</span> X.shape
    stop_thresh <span class="op">=</span> <span class="fl">1e-3</span> <span class="op">*</span> bandwidth  <span class="co"># when mean has converged</span>
    center_intensity_dict <span class="op">=</span> {}
    nbrs <span class="op">=</span> NearestNeighbors(radius<span class="op">=</span>bandwidth).fit(X)

    <span class="co"># For each seed, climb gradient until convergence or max_iterations</span>
    <span class="cf">for</span> my_mean <span class="kw">in</span> seeds:
        completed_iterations <span class="op">=</span> <span class="dv">0</span>
        <span class="cf">while</span> <span class="va">True</span>:
            <span class="co"># Find mean of points within bandwidth</span>
            i_nbrs <span class="op">=</span> nbrs.radius_neighbors([my_mean], bandwidth,
                                           return_distance<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]
            points_within <span class="op">=</span> X[i_nbrs]
            <span class="cf">if</span> <span class="bu">len</span>(points_within) <span class="op">==</span> <span class="dv">0</span>:
                <span class="cf">break</span>  <span class="co"># Depending on seeding strategy this condition may occur</span>
            my_old_mean <span class="op">=</span> my_mean  <span class="co"># save the old mean</span>
            my_mean <span class="op">=</span> np.mean(points_within, axis<span class="op">=</span><span class="dv">0</span>)
            <span class="co"># If converged or at max_iterations, addS the cluster</span>
            <span class="cf">if</span> (extmath.norm(my_mean <span class="op">-</span> my_old_mean) <span class="op">&lt;</span> stop_thresh <span class="kw">or</span>
                    completed_iterations <span class="op">==</span> max_iterations):
                center_intensity_dict[<span class="bu">tuple</span>(my_mean)] <span class="op">=</span> <span class="bu">len</span>(points_within)
                <span class="cf">break</span>
            completed_iterations <span class="op">+=</span> <span class="dv">1</span>

    <span class="co"># POST PROCESSING: remove near duplicate points</span>
    <span class="co"># If the distance between two kernels is less than the bandwidth,</span>
    <span class="co"># then we have to remove one because it is a duplicate. Remove the</span>
    <span class="co"># one with fewer points.</span>
    sorted_by_intensity <span class="op">=</span> <span class="bu">sorted</span>(center_intensity_dict.items(),
                                 key<span class="op">=</span><span class="kw">lambda</span> tup: tup[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)
    sorted_centers <span class="op">=</span> np.array([tup[<span class="dv">0</span>] <span class="cf">for</span> tup <span class="kw">in</span> sorted_by_intensity])
    unique <span class="op">=</span> np.ones(<span class="bu">len</span>(sorted_centers), dtype<span class="op">=</span>np.<span class="bu">bool</span>)
    nbrs <span class="op">=</span> NearestNeighbors(radius<span class="op">=</span>bandwidth).fit(sorted_centers)
    <span class="cf">for</span> i, center <span class="kw">in</span> <span class="bu">enumerate</span>(sorted_centers):
        <span class="cf">if</span> unique[i]:
            neighbor_idxs <span class="op">=</span> nbrs.radius_neighbors([center],
                                                  return_distance<span class="op">=</span><span class="va">False</span>)[<span class="dv">0</span>]
            unique[neighbor_idxs] <span class="op">=</span> <span class="dv">0</span>
            unique[i] <span class="op">=</span> <span class="dv">1</span>  <span class="co"># leave the current point as unique</span>
    cluster_centers <span class="op">=</span> sorted_centers[unique]

    <span class="co"># ASSIGN LABELS: a point belongs to the cluster that it is closest to</span>
    nbrs <span class="op">=</span> NearestNeighbors(n_neighbors<span class="op">=</span><span class="dv">1</span>).fit(cluster_centers)
    labels <span class="op">=</span> np.zeros(n_samples, dtype<span class="op">=</span>np.<span class="bu">int</span>)
    distances, idxs <span class="op">=</span> nbrs.kneighbors(X)
    labels <span class="op">=</span> idxs.flatten()
    
    <span class="cf">return</span> cluster_centers, labels

cluster_centers, labels <span class="op">=</span> mean_shift(np.array(data), <span class="dv">4000</span>)

<span class="bu">print</span> <span class="bu">len</span>(cluster_centers)</code></pre></div>
<pre><code>17</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.scatter(data[:,<span class="dv">0</span>],data[:,<span class="dv">1</span>])
plt.hold(<span class="va">True</span>)
<span class="cf">for</span> x <span class="kw">in</span> asarray(cluster_centers): plt.plot(x[<span class="dv">0</span>],x[<span class="dv">1</span>],<span class="st">&#39;rd&#39;</span>)
plt.savefig(<span class="st">&#39;meanshift_2.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="meanshift_2.png" />

</div>
<p>Teorik Konular</p>
<p>Bu metotu teorik bir yapıya oturtmak için onu yazının ilk başındaki resimde olduğu gibi görmek gerekiyor, yani mesela o ilk resmin sağındaki 2 boyuttaki veri dağılımı (ki ayrıksal, sayısal), 3 boyuttaki sürekli (continuous) bir başka dağılımın yansıması sanki, ki o zaman 2 boyuttaki yoğunluk bölgeleri sürekli dağılımdaki tepe noktalarını temsil ediyorlar, ve biz o sürekli versiyondaki tepe noktalarını bulmalıyız. Fakat kümeleme işleminin elinde sadece 2 boyuttaki veriler var, o zaman sürekli dağılımı bir şekilde yaratmak lazım.</p>
<p>Bunu yapmak için problem / veri önce bir Çekirdek Yoğunluk Kestirimi (Kernel Density Estimation -KDE-) problemi gibi görülüyor, ki her nokta üzerine bir çekirdek fonksiyonu koyularak ve onların toplamı alınarak sayısal dağılım pürüzsüz bir hale getiriliyor. Ortalama Kaydırma için gerekli kayma &quot;yönü&quot; ise işte bu yeni sürekli fonksiyonun gradyanıdır deniyor (elimizde bir sürekli fonksiyon olduğu için türev rahatlıkla alabiliyoruz), ve gradyan yerel tepe noktasını gösterdiği için o yöne yapılan hareket bizi yavaş yavaş tepeye götürecektir. Bu hareketin yerel tepeleri bulacağı, ve tüm yöntemin nihai olarak sonuca yaklaşacağı (convergence) matematiksel olarak ispat edilebilir.</p>
<p>KDE ile elde edilen teorik dağılım fonksiyonunun içbükey olup olmadığı önemli değil (ki mesela lojistik regresyonda bu önemliydi), çünkü nihai tepe noktasını değil, birkaç yerel tepe noktasından birini (hatta hepsini) bulmakla ilgileniyoruz. Gradyan bizi bu noktaya taşıyacaktır.</p>
<p>Kaynaklar</p>
<p>[1] Babu, <em>Mean-Shift : Theory</em>, <a href="http://www.serc.iisc.ernet.in/~venky/SE263/slides/Mean-Shift-Theory.pdf" class="uri">http://www.serc.iisc.ernet.in/~venky/SE263/slides/Mean-Shift-Theory.pdf</a></p>
<p>[2] Thirumuruganathan, <em>Introduction To Mean Shift Algorithm</em>, <a href="http://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/" class="uri">http://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/</a></p>
<p>[3] Derpanis, <em>Mean Shift Clustering</em>, <a href="http://www.cse.yorku.ca/~kosta/CompVis_Notes/mean_shift.pdf" class="uri">http://www.cse.yorku.ca/~kosta/CompVis_Notes/mean_shift.pdf</a></p>
<p>[4] Fisher, <em>Mean Shift Clustering</em>, <a href="http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/TUZEL1/MeanShift.pdf" class="uri">http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/TUZEL1/MeanShift.pdf</a></p>
<p>[5] Scikit Learn, <em>Documentation</em>, <a href="http://scikit-learn.org" class="uri">http://scikit-learn.org</a></p>
<p>[6] Gingold, <a href="http://yotamgingold.com/code/MeanShiftCluster.py" class="uri">http://yotamgingold.com/code/MeanShiftCluster.py</a></p>
<p>[7] Bayramlı, Istatistik, <em>GMM ile Kümelemek</em></p>
</body>
</html>
