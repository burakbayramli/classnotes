\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Ortalama Kaydýrma ile Kümeleme (Mean Shift Clustering)

Kümeleme yapmak için bir metot daha: Ortalama Kaydýrma metotu. Bu metodun
mesela GMM gibi bir metottan farký, küme sayýsýnýn önceden belirtilmeye
ihtiyacý olmamasýdýr, küme sayýsý otomatik olarak metot tarafýndan
saptanýr.

"Küme" olarak saptanan aslýnda veri içindeki tüm yoðunluk bölgelerinin
merkezleridir, yani alttaki resmin sað kýsmýndaki bölgeler.

\includegraphics[height=5cm]{dist.png}

Baþlangýç neresidir? Baþlangýç tüm noktalardýr, yani her noktadan
baþlanarak

1. O nokta etrafýnda (yeterince büyük) bir pencere tanýmla

2. Bu pencere içine düþen tüm noktalarý hesaba katarak bir ortalama yer hesapla

3. Pencereyi yeni ortalama noktayý merkezine alacak þekilde kaydýr

Metotun ismi buradan geliyor, çünkü pencere yeni ortalamaya doðru
"kaydýrýlýyor". Altta bir noktadan baþlanarak yapýlan hareketi
görüyoruz.  Kaymanýn saða doðru olmasý mantýklý çünkü tek pencere
içinden bakýnca bile yoðunluðun "sað tarafa doðru" olduðu
görülmekte. Yöntemin püf noktasý burada.

\includegraphics[height=4cm]{mean_2.png}
\includegraphics[height=4cm]{mean_3.png}

\includegraphics[height=4cm]{mean_4.png}
\includegraphics[height=4cm]{mean_5.png}

\includegraphics[height=4cm]{mean_6.png}
\includegraphics[height=4cm]{mean_7.png}

Eðer yoðunluk merkezine çok yakýn bir noktadan / noktalardan
baþlamýþsak ne olur?

O zaman ilerleme o baþlangýç noktasý için anýnda bitecek, çünkü hemen
yoðunluk merkezine gelmiþ olacaðýz. Diðer yönlerden gelen pencereler
de ayný yere gelecekler tabii, o zaman ayný / yakýn yoðunluk
merkezlerini ayný küme olarak kabul etmemiz gerekir. Bu "ayný küme
irdelemesi" sayýsal hesaplama açýsýndan ufak farklar gösterebilir
tabii, ve bu ufak farký gözönüne alarak "küme birleþtirme" mantýðýný da
eklemek gerekiyor.

Ortalama Kaydýrma sisteminde pencere büyüklüðü kullanýcý tarafýndan
tanýmlanýr.  Optimal pencere büyüklüðünü nasýl buluruz? Deneme yanýlma
yöntemi, verinin tarifsel istatistiklerine kestirme bir hesap (estimate)
etmek, ya da kullanýcýnýn ayný istatistiklere bakarak tahminde
bulunmasý. Birkaç farklý pencere büyüklüðü de denenebilir. Bu konu
literatürde (Ýng. bandwidth selection) adý altýnda uzun uzadýya
tartýþýlmaktadýr.

Eðer yoðunluk merkezine çok yakýn bir noktadan / noktalardan
baþlamýþsak ne olur?

O zaman ilerleme o baþlangýç noktasý için anýnda bitecek, çünkü hemen
yoðunluk merkezine gelmiþ olacaðýz. Diðer yönlerden gelen pencereler
de ayný yere gelecekler tabii, o zaman ayný / yakýn yoðunluk
merkezlerini ayný küme olarak kabul etmemiz gerekir. Bu "ayný küme
irdelemesi" sayýsal hesaplama açýsýndan ufak farklar gösterebilir
tabii, ve bu ufak farký gözönüne alarak "küme birleþtirme" mantýðýný da
eklemek gerekiyor.

\includegraphics[height=6cm]{start.png}

Altta örnek veri ve kodu bulunabilir. Metot küme sayýsý 17'yi otomatik
olarak buluyor (gerçek küme sayýsý 20, bkz [7]  yazýsý).

Alternatif bir kod \verb!meanshift_alternatýve.py! dosyasýnda
bulunabilir, bu kod pencereler kaydýrýrken onlarýn üzerinden geçtiði
noktalarý "sahiplenen" türden bir kod. Yani [encere hareketini
durdurduðunda hem küme merkezini hem de o kümenin altýndaki noktalarý
bulmuþ oluyoruz.  Tabii sonraki pencereler bazý noktalarý önceki
kümelerden çalabilirler. Neyse, iþlemin normal iþleyiþine göre bir
sonraki pencere seçilecektir ve bu pencere "geriye kalan noktalar"
üzerinden iþlem yapacaktýr.  Beklenir ki, iþlem ilerledikçe iþlenmesi
gereken noktalar azalacaktýr ve yöntemin bu sebeple klasik yönteme
göre daha hýzlý iþleyeceði tahmin edilebilir. Hakikaten de böyledir.

\begin{minted}[fontsize=\footnotesize]{python}
from pandas import *
data = read_csv("../kmeans/synthetic.txt",comment='#',header=None,sep="   ")
print data.shape
data = np.array(data)
\end{minted}

\begin{verbatim}
(3000, 2)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
plt.scatter(data[:,0],data[:,1])
plt.savefig('meanshift_1.png')
\end{minted}

\includegraphics[height=6cm]{meanshift_1.png}
\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.neighbors import NearestNeighbors
from sklearn.utils import extmath

def mean_shift(X, bandwidth=None, max_iterations=300):
    
    seeds = X
    n_samples, n_features = X.shape
    stop_thresh = 1e-3 * bandwidth  # when mean has converged
    center_intensity_dict = {}
    nbrs = NearestNeighbors(radius=bandwidth).fit(X)

    # For each seed, climb gradient until convergence or max_iterations
    for my_mean in seeds:
        completed_iterations = 0
        while True:
            # Find mean of points within bandwidth
            i_nbrs = nbrs.radius_neighbors([my_mean], bandwidth,
                                           return_distance=False)[0]
            points_within = X[i_nbrs]
            if len(points_within) == 0:
                break  # Depending on seeding strategy this condition may occur
            my_old_mean = my_mean  # save the old mean
            my_mean = np.mean(points_within, axis=0)
            # If converged or at max_iterations, addS the cluster
            if (extmath.norm(my_mean - my_old_mean) < stop_thresh or
                    completed_iterations == max_iterations):
                center_intensity_dict[tuple(my_mean)] = len(points_within)
                break
            completed_iterations += 1

    # POST PROCESSING: remove near duplicate points
    # If the distance between two kernels is less than the bandwidth,
    # then we have to remove one because it is a duplicate. Remove the
    # one with fewer points.
    sorted_by_intensity = sorted(center_intensity_dict.items(),
                                 key=lambda tup: tup[1], reverse=True)
    sorted_centers = np.array([tup[0] for tup in sorted_by_intensity])
    unique = np.ones(len(sorted_centers), dtype=np.bool)
    nbrs = NearestNeighbors(radius=bandwidth).fit(sorted_centers)
    for i, center in enumerate(sorted_centers):
        if unique[i]:
            neighbor_idxs = nbrs.radius_neighbors([center],
                                                  return_distance=False)[0]
            unique[neighbor_idxs] = 0
            unique[i] = 1  # leave the current point as unique
    cluster_centers = sorted_centers[unique]

    # ASSIGN LABELS: a point belongs to the cluster that it is closest to
    nbrs = NearestNeighbors(n_neighbors=1).fit(cluster_centers)
    labels = np.zeros(n_samples, dtype=np.int)
    distances, idxs = nbrs.kneighbors(X)
    labels = idxs.flatten()
    
    return cluster_centers, labels

cluster_centers, labels = mean_shift(np.array(data), 4000)

print len(cluster_centers)
\end{minted}

\begin{verbatim}
17
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
plt.scatter(data[:,0],data[:,1])
plt.hold(True)
for x in asarray(cluster_centers): plt.plot(x[0],x[1],'rd')
plt.savefig('meanshift_2.png')
\end{minted}

\includegraphics[height=6cm]{meanshift_2.png}

Teorik Konular

Bu metotu teorik bir yapýya oturtmak için onu yazýnýn ilk baþýndaki
resimde olduðu gibi görmek gerekiyor, yani mesela o ilk resmin
saðýndaki 2 boyuttaki veri daðýlýmý (ki ayrýksal, sayýsal), 3
boyuttaki sürekli (continuous) bir baþka daðýlýmýn yansýmasý sanki, ki
o zaman 2 boyuttaki yoðunluk bölgeleri sürekli daðýlýmdaki tepe
noktalarýný temsil ediyorlar, ve biz o sürekli versiyondaki tepe
noktalarýný bulmalýyýz. Fakat kümeleme iþleminin elinde sadece 2
boyuttaki veriler var, o zaman sürekli daðýlýmý bir þekilde yaratmak
lazým.

Bunu yapmak için problem / veri önce bir Çekirdek Yoðunluk Kestirimi
(Kernel Density Estimation -KDE-) problemi gibi görülüyor, ki her
nokta üzerine bir çekirdek fonksiyonu koyularak ve onlarýn toplamý
alýnarak sayýsal daðýlým pürüzsüz bir hale getiriliyor. Ortalama
Kaydýrma için gerekli kayma "yönü" ise iþte bu yeni sürekli
fonksiyonun gradyanýdýr deniyor (elimizde bir sürekli fonksiyon olduðu
için türev rahatlýkla alabiliyoruz), ve gradyan yerel tepe noktasýný
gösterdiði için o yöne yapýlan hareket bizi yavaþ yavaþ tepeye
götürecektir. Bu hareketin yerel tepeleri bulacaðý, ve tüm yöntemin
nihai olarak sonuca yaklaþacaðý (convergence) matematiksel olarak
ispat edilebilir.

KDE ile elde edilen teorik daðýlým fonksiyonunun içbükey olup olmadýðý
önemli deðil (ki mesela lojistik regresyonda bu önemliydi), çünkü
nihai tepe noktasýný deðil, birkaç yerel tepe noktasýndan birini
(hatta hepsini) bulmakla ilgileniyoruz. Gradyan bizi bu noktaya
taþýyacaktýr.

Kaynaklar

[1] Babu, {\em Mean-Shift : Theory}, \url{http://www.serc.iisc.ernet.in/~venky/SE263/slides/Mean-Shift-Theory.pdf}

[2] Thirumuruganathan, {\em Introduction To Mean Shift Algorithm}, \url{http://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/}

[3] Derpanis, {\em Mean Shift Clustering}, \url{http://www.cse.yorku.ca/~kosta/CompVis_Notes/mean_shift.pdf}

[4] Fisher, {\em Mean Shift Clustering}, \url{http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/TUZEL1/MeanShift.pdf}

[5] Scikit Learn, {\em Documentation}, \url{http://scikit-learn.org}

[6] Gingold, \url{http://yotamgingold.com/code/MeanShiftCluster.py}

[7] Bayramli, Istatistik, {\em GMM ile Kümelemek}

\end{document}
