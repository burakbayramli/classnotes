<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Derin Öğrenme ile Doğal Dil İşlemek (Natural Language Processing -NLP-)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1
id="derin-öğrenme-ile-doğal-dil-işlemek-natural-language-processing--nlp-">Derin
Öğrenme ile Doğal Dil İşlemek (Natural Language Processing -NLP-)</h1>
<p>Doküman sınıflamak, bir film için yazılmış yorumu beğendi / beğenmedi
şeklinde irdelemek; tüm bu işlemler doğal dil işlemek kategorisine
girer, ve derin yapay sinir ağları (DYSA) bu alanda kullanılabilir.</p>
<p>Doküman nasıl temsil edilir?</p>
<p>Doküman kelimelerden oluşur, fakat kelimeler sayısal değil kategorik
şeyler, DYSA kullanmak için kelimelerin sayısallaştırılması lazım. Bir
çözüm 1-hot kodlaması, tüm dokümanlardaki tüm kelimeler on bin kelimelik
bir “sözlükten’’ geliyorsa, her kelime için on bin boyutunda bir vektör
yaratırız, bu vektörde kelimelerin yerleri önceden bellidir,”cat
(kedi)’’ kelimesi mesela 300. indis, o zaman “cat’’ kelimesini temsil
için 10,000 büyüklüğündeki bir vektörün 300. öğesi 1 diğer 9999 ögesi 0
olur.</p>
<p>Bu temsil şekli biraz israflı degil mi? Ayrıca kelimeler arasında
benzerlik için bize hiçbir fayda getirmiyor.</p>
<p>Daha önce [4] yazısında boyut azaltma işleminden bahsettik. Bir
kelimeyi, ya da dökümanı her ikisinin ilişkisini içeren bir matris
üzerinde SVD işlettikten sonra daha ufak bir boyutta temsil
edebiliyorduk. Bu azaltılmış boyutta, ki boyutu binlere varan ham veri
için azaltılmış boyut <span class="math inline">\(k=10,20,100\)</span>
gibi olabiliyordu, kelimeler pür sayısal hale geliyordu ve kelimelere
tekabül eden <span class="math inline">\(k\)</span> büyüklüğündeki
vektörlerin, anlamsal bağlamda birbirine yakınlık ya da uzaklıkları bu
sayılar üzerinden ölçülebiliyordu. Bu tür bir temsilde bazı kelimeler
sayısal olarak şu halde olabiliyordu,</p>
<pre><code>cat:  (0.01359, ..., -0.2524, 1.0048, 0.06259)
mat:  (0.01396, ..., 0.033483, -0.10007, 0.1158)
chills: (-0.24776, ..., 0.079717, 0.23865, -0.014213)
sat:  (-0.35609, ..., -0.35413, 0.38511, -0.070976)</code></pre>
<p>Derin YSA ile aynı sonuç kelime gömme (word embedding) mekanizması
ile elde ediliyor. Fikir aslında gayet basit ve dahiyane. Kelime yerine
onları temsil eden sayısal vektörler YSA’nın bir tabakasına “gömülür’’
ve aynen YSA’nın diğer katmanları gibi ağırlık olarak addedilip
eğitilirler. Başlangıçta tüm kelimelerin gömme vektörleri rasgele
sayılardır, eğitim ilerledikçe bu değerler anlamlı hale gelirler.</p>
<p>Kodlama kabaca şöyle: tüm dokümanlar üzerinden sözlüğü oluştururuz.
Kelimeler 1-hot vektörü değil, tek bir indis haline getirilir, üstteki
“cat’’ sadece 300 sayısına dönüşür yani. Gömme tabakası için sadece
sınırlı sayıda kelimeyi alırız, mesela ilk 5’i, yani her dokümanın ilk 5
kelimesi tutulur, gerisi atılır, eğer eksik varsa dolgulama (padding)
ile sıfırlar eklenip 5’e getirilir. Tabii bu indis değerleri YSA için
direk kullanılamaz, bir sonraki aşama, YSA’ya bir gömme tabakası
eklemek, YSA’nın eğitimde kullanacağı esas değerler bunlar. Her (tek)
kelimenin gömme boyutu da önceden kararlaştırılır (gömme vektörünün
sayısal boyutu), <span class="math inline">\(n\)</span> diyelim, mesela
<span class="math inline">\(n=4\)</span>, eğer sözlük büyüklüğü <span
class="math inline">\(|V|\)</span> ise, <span class="math inline">\(n
\times |V|\)</span> boyutunda bir büyük gömme referans matrisi elde
edilir.</p>
<p><img src="nlp_02.png" /></p>
<p>Bu referans matrisin başlangıç değerleri rasgeledir. Bu örnekte YSA
içinde bulunan gömme girdi katmanının tamamı 5 x 4 = 20 olacaktır. Altta
“cat chills on a mat (kedi paspas üzerinde takılıyor)’’ cümlesini
görüyoruz,</p>
<p><img src="nlp_03.png" /></p>
<p>Üstteki girdiyi olduğu gibi alabilirdik, yani girdi katmanı 5 x 4
boyutundaki bir “tensor’’ da olabilirdi (modern YSA araçları çok boyutlu
tensorlar ile rahatça çalışırlar), biz basitleştirme amacıyla vektörün
düzleştirildiğini düşünelim,</p>
<p><img src="nlp_04.png" /></p>
<p>Burada ilginç bir durum var, alışılagelen YSA kodlamasından farklı
olarak <span class="math inline">\(x\)</span> vektörüne “girdi’’ dedik,
fakat <span class="math inline">\(x\)</span>’i bir tamamen bağlanmış
ağırlık tabakası olarak görmek daha doğru. Fakat bu ağırlık tabakası
diğer ağırlık tabakaları gibi de değil; Her eğitim veri noktasının
içindeki kelimelerin indisleri üzerinden <em>referans gömme
matrisindeki</em> uygun satırlar çekilip o anda bir <span
class="math inline">\(x\)</span> haline getiriliyor. Ardından geriye
yayılma ile YSA hata düzeltme yapacağı zaman gömme referans matrisindeki
uygun vektörler güncelleniyor.</p>
<p>Bu kadar. Şimdi eğitim hedef değerlerine bakalım. Burada farklı
yaklaşımlar var, üstteki <em>Lineer Cebir</em> yazısında bahsedilen YSA
komşu kelimeleri tahmin etmeye uğraşıyordu. Bir başka numara bir cümleyi
alıp içindeki tek bir kelimeyi “bozmak’’, oraya anlamsız bir kelime
getirmek, ve bu yeni cümleyi 0, bozulmamış olanını 1 etiketiyle eğitmek.
Cümleler nasıl olsa hazır var, onları bozmak kolay, bu şekilde iki
kategorili bir sınıflama problemi elde ediyoruz. Örnek boyutlarla
[3],</p>
<p><span class="math inline">\(x \in \mathbb{R}^{20 \times 1}, W \in
\mathbb{R}^{8 \times 20}, U \in \mathbb{R}^{8 \times 1}\)</span></p>
<p>olsa, örnek girdi,</p>
<p><span class="math display">\[ x = \left[\begin{array}{rrrrr}
x_{cat} &amp; x_{chills} &amp; x_{on} &amp; x_{a} &amp; x_{mat}
\end{array}\right]\]</span></p>
<p>olacak şekilde,</p>
<p><span class="math display">\[ s = U^T a\]</span></p>
<p><span class="math display">\[ a = f(z)\]</span></p>
<p><span class="math display">\[ z = Wx + b\]</span></p>
<p>katmanları tasarlanabilir.</p>
<p><img src="nlp_05.png" /></p>
<p>TensorFlow</p>
<p>TF bağlamında gömme tabakası ile referans matrisi arasındaki ilişki
<code>embedding_lookup</code> çağrısı ile yapılıyor. Bu çağrı bir
matriste indis erişimi sadece,</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>tf.reset_default_graph()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> tf.constant([<span class="dv">10</span>,<span class="dv">20</span>,<span class="dv">30</span>,<span class="dv">40</span>])</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>ids <span class="op">=</span> tf.constant([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>     <span class="bu">print</span> tf.nn.embedding_lookup(params,ids).<span class="bu">eval</span>()</span></code></pre></div>
<pre><code>[10 20 30 40]</code></pre>
<p>Fakat YSA içine konduğu zaman bu çağrı ve onun oluşturduğu katman
geriye yayılma sırasında nasıl davranacağını biliyor.</p>
<p>Şimdi daha geniş bir örnek olarak [2] verisi üzerinde üstte tarif
edilen türden basit ağ yapısını oluşturalım. Veri <em>Rotten
Tomatoes</em> adlı film yorum sitesinden alınan kullanıcı yorumları
(yazdığı kelimeler yani) ve kullanıcının filmi beğendi / beğenmedi
şeklindeki hissiyatı 0/1 olarak içeriyor (bozma işlemine gerek yok, her
iki etiket için bol veri mevcut). Bu bir doğal dil işleme ikisel
sınıflama problemi. Veriye bakalım,</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> data_helpers</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.contrib <span class="im">import</span> learn</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>dev_sample_percentage <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>positive_data_file <span class="op">=</span> <span class="st">&quot;./data/rt-polarity.pos&quot;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>negative_data_file <span class="op">=</span> <span class="st">&quot;./data/rt-polarity.neg&quot;</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">120</span> <span class="co"># bir kelime icin gomme boyutu</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>x_text, y <span class="op">=</span> data_helpers.load_data_and_labels(positive_data_file, negative_data_file)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> y[<span class="dv">3</span>], x_text[<span class="dv">3</span>]</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> y[<span class="dv">4</span>], x_text[<span class="dv">4</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> y[<span class="dv">10000</span>], x_text[<span class="dv">10000</span>]</span></code></pre></div>
<pre><code>[0 1] if you sometimes like to go to the movies to have fun , wasabi is a good place to start
[0 1] emerges as something rare , an issue movie that &#39;s so honest and keenly observed that it does n&#39;t feel like one
[1 0] like mike is a slight and uninventive movie like the exalted michael jordan referred to in the title , many can aspire but none can equal</code></pre>
<p>Örnek gösterdiğimiz üç yoruma bakıyoruz, birincisi, ikincisi pozitif,
üçüncüsü negatif. Yorumlarda kullanılan kelimelerde mesela ilkinde “a
good place to start (iyi bir başlangıç noktası)’’ yorumu pozitifsel bir
hava taşıyor, üçüncüde”uninventive (bir yenilik yok)’’ kelimesi
kullanılmış, negatif. YSA eğitildikten sonra bu kelimelerin doğru
temsilsel (gömme) ağırlıklarını ve onların 0/1 hedefine olan
bağlantısını öğrenmeyi umuyoruz.</p>
<p>İndis matrisini yaratalım,</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>max_document_length <span class="op">=</span> <span class="bu">max</span>([<span class="bu">len</span>(x.split(<span class="st">&quot; &quot;</span>)) <span class="cf">for</span> x <span class="kw">in</span> x_text])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;dokuman buyuklugu&#39;</span>, max_document_length</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>vocab_processor <span class="op">=</span> learn.preprocessing.VocabularyProcessor(max_document_length)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array(<span class="bu">list</span>(vocab_processor.fit_transform(x_text)))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> x</span></code></pre></div>
<pre><code>dokuman buyuklugu 56
[[    1     2     3 ...,     0     0     0]
 [    1    31    32 ...,     0     0     0]
 [   57    58    59 ...,     0     0     0]
 ..., 
 [   75    84  1949 ...,     0     0     0]
 [    1  2191  2690 ...,     0     0     0]
 [11512     3   147 ...,     0     0     0]]</code></pre>
<p>İndis için <code>x</code> kullanımı kafa karıştırmasın, [1] kodlaması
o şekilde seçmiş.</p>
<p>Ardından gömme yapılır, ve bu ağırlıklar <code>fully_connected</code>
ile tam bağlanmış YSA tabakasına verilir, oradan çıkan sonuç ise iki
kategorili softmax’e verilir. Tüm kod,</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># nlp1.py</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> data_helpers</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.contrib <span class="im">import</span> learn</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>dev_sample_percentage <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>positive_data_file <span class="op">=</span> <span class="st">&quot;./data/rt-polarity.pos&quot;</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>negative_data_file <span class="op">=</span> <span class="st">&quot;./data/rt-polarity.neg&quot;</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">120</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>x_text, y <span class="op">=</span> data_helpers.load_data_and_labels(positive_data_file, negative_data_file)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>max_document_length <span class="op">=</span> <span class="bu">max</span>([<span class="bu">len</span>(x.split(<span class="st">&quot; &quot;</span>)) <span class="cf">for</span> x <span class="kw">in</span> x_text])</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>vocab_processor <span class="op">=</span> learn.preprocessing.VocabularyProcessor(max_document_length)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array(<span class="bu">list</span>(vocab_processor.fit_transform(x_text)))</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>shuffle_indices <span class="op">=</span> np.random.permutation(np.arange(<span class="bu">len</span>(y)))</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>x_shuffled <span class="op">=</span> x[shuffle_indices]</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>y_shuffled <span class="op">=</span> y[shuffle_indices]</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>dev_sample_index <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="bu">int</span>(dev_sample_percentage <span class="op">*</span> <span class="bu">float</span>(<span class="bu">len</span>(y)))</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>x_train, x_dev <span class="op">=</span> x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>y_train, y_dev <span class="op">=</span> y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Vocabulary Size: </span><span class="sc">{:d}</span><span class="st">&quot;</span>.<span class="bu">format</span>(<span class="bu">len</span>(vocab_processor.vocabulary_)))</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Train/Dev split: </span><span class="sc">{:d}</span><span class="st">/</span><span class="sc">{:d}</span><span class="st">&quot;</span>.<span class="bu">format</span>(<span class="bu">len</span>(y_train), <span class="bu">len</span>(y_dev)))</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>tf.reset_default_graph()</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>num_classes<span class="op">=</span>y_train.shape[<span class="dv">1</span>]</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>sequence_length<span class="op">=</span>x_train.shape[<span class="dv">1</span>]</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>input_x <span class="op">=</span> tf.placeholder(tf.int32, [<span class="va">None</span>, sequence_length])</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>input_y <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, num_classes])</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co"># rasgele agirliklar</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> tf.Variable(tf.random_uniform([<span class="bu">len</span>(vocab_processor.vocabulary_),</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>                                   embedding_dim], <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>))</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>ec <span class="op">=</span> tf.nn.embedding_lookup(W, input_x)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="co"># duzlestir</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>embed <span class="op">=</span> tf.contrib.layers.flatten(ec)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> tf.contrib.layers.fully_connected(inputs<span class="op">=</span>embed, num_outputs<span class="op">=</span><span class="dv">2</span>, </span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>                                           activation_fn<span class="op">=</span>tf.nn.softmax)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> tf.argmax(scores, <span class="dv">1</span>)</span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> tf.nn.softmax_cross_entropy_with_logits(logits<span class="op">=</span>scores, labels<span class="op">=</span>input_y)</span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.reduce_mean(losses) </span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>correct_predictions <span class="op">=</span> tf.equal(predictions, tf.argmax(input_y, <span class="dv">1</span>))</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> tf.reduce_mean(tf.cast(correct_predictions, <span class="st">&quot;float&quot;</span>))</span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>global_step <span class="op">=</span> tf.Variable(<span class="dv">0</span>, trainable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.AdamOptimizer(<span class="fl">1e-3</span>)</span>
<span id="cb9-64"><a href="#cb9-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-65"><a href="#cb9-65" aria-hidden="true" tabindex="-1"></a>grads_and_vars <span class="op">=</span> optimizer.compute_gradients(loss)</span>
<span id="cb9-66"><a href="#cb9-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-67"><a href="#cb9-67" aria-hidden="true" tabindex="-1"></a>train_op <span class="op">=</span> optimizer.apply_gradients(grads_and_vars, global_step<span class="op">=</span>global_step)</span>
<span id="cb9-68"><a href="#cb9-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-69"><a href="#cb9-69" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.Session()</span>
<span id="cb9-70"><a href="#cb9-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-71"><a href="#cb9-71" aria-hidden="true" tabindex="-1"></a>sess.run(tf.global_variables_initializer())</span>
<span id="cb9-72"><a href="#cb9-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-73"><a href="#cb9-73" aria-hidden="true" tabindex="-1"></a>batches <span class="op">=</span> data_helpers.batch_iter(<span class="bu">list</span>(<span class="bu">zip</span>(x_train, y_train)),batch_size,num_epochs)</span>
<span id="cb9-74"><a href="#cb9-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-75"><a href="#cb9-75" aria-hidden="true" tabindex="-1"></a>saver <span class="op">=</span> tf.train.Saver(tf.global_variables())</span>
<span id="cb9-76"><a href="#cb9-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-77"><a href="#cb9-77" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,batch <span class="kw">in</span> <span class="bu">enumerate</span>(batches):</span>
<span id="cb9-78"><a href="#cb9-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-79"><a href="#cb9-79" aria-hidden="true" tabindex="-1"></a>    x_batch, y_batch <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>batch)    </span>
<span id="cb9-80"><a href="#cb9-80" aria-hidden="true" tabindex="-1"></a>    feed_dict <span class="op">=</span> { input_x: x_batch, input_y: y_batch }    </span>
<span id="cb9-81"><a href="#cb9-81" aria-hidden="true" tabindex="-1"></a>    sess.run(train_op, feed_dict)</span>
<span id="cb9-82"><a href="#cb9-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-83"><a href="#cb9-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">30</span>) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb9-84"><a href="#cb9-84" aria-hidden="true" tabindex="-1"></a>        feed_dict2 <span class="op">=</span> { input_x: x_dev, input_y: y_dev }</span>
<span id="cb9-85"><a href="#cb9-85" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">=</span> sess.run(accuracy, feed_dict)</span>
<span id="cb9-86"><a href="#cb9-86" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">=</span> sess.run(accuracy, feed_dict2)</span>
<span id="cb9-87"><a href="#cb9-87" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span> train_acc, test_acc</span>
<span id="cb9-88"><a href="#cb9-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">200</span>) <span class="op">==</span> <span class="dv">0</span>: <span class="co"># arada sirada modeli kaydet</span></span>
<span id="cb9-89"><a href="#cb9-89" aria-hidden="true" tabindex="-1"></a>        path <span class="op">=</span> saver.save(sess, <span class="st">&quot;/tmp/nlpembed1&quot;</span>)</span></code></pre></div>
<p>Bu kodun başarısı yüzde 67 civarı.</p>
<p>Evrişim Tabakası</p>
<p>Fakat daha yapılacaklar var. NLP alanında DYSA moda olmadan önce
mesela lojistik regresyon ile eğitim yapıldığında bir n-gram yaklaşımı
vardı. Bir doküman sayısal hale getirilirken kelimeleri tek tek
alabiliriz, ya da, yanyana gelen her iki (2-gram), üç (3-gram) kelime
demetlerini sanki başlı başına kelimelermiş gibi sayısala çevirebiliriz.
Eğer mesela “küçük ev’’ ve”sarı kedi’’ 2-gramları sürekli dökümanlarda
beraber görülüyorsa, bu kelime çiftinin bir sınıflayıcı kuvveti
olabilir, ve n-gram yaklaşımı bunu kullanmaya uğraşır. n-gram işlemi bu
yaklaşımlarda çoğunlukla bir önişlem (preprocessing) aşamasında veriden
yeni veri çıkartarak yapılırdı (dokümanın kelimelerini sırayla ikişer
ikişer, üçer üçer okuyarak), DYSA ile elimizde daha kullanışlı bir silah
var. Evrişim.</p>
<p><img src="nlp_01.png" /></p>
<p>Üstte görülen örnekte gömme tabakası <span class="math inline">\(9
\times 6\)</span> boyutunda, bu tabaka üzerinde farklı boyutlarda
evrişim operasyonları uygulanıyor, mesela 2 x 6 boyutlu bir evrişim var
(kırmızı renkli), üstten başlanıp birer birer aşağı kaydırılarak ikinci
tabakadaki sonuç elde ediliyor, aynı şekilde 3 x 6 bir diğeri (sarı
renkli), vs. Böylece ikinci seviyedeki vektörler elde ediliyor, bu
vektörler üzerine max-pool işlemi uygulanıyor üçüncü seviye elde
ediliyor, ve oradan gelen sonuçlar softmax’e veriliyor.</p>
<p>Evrişim bir blok olarak girdi üzerinde işletildiği için, yanyana olan
kelimeler arasında alaka bulabilmesi gayet normal. Ayrıca değişik
boyutlarda, pek çok farklı filtre tanımladık, yani pek çok farklı
ilişkiyi bu filtreler üzerinden yakalamaya uğraştık.</p>
<p>Evrişim içeren genişletilmiş kod altta.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># nlp2.py</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> data_helpers</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.contrib <span class="im">import</span> learn</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>dev_sample_percentage <span class="op">=</span> <span class="fl">.1</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>positive_data_file <span class="op">=</span> <span class="st">&quot;./data/rt-polarity.pos&quot;</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>negative_data_file <span class="op">=</span> <span class="st">&quot;./data/rt-polarity.neg&quot;</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>filter_sizes <span class="op">=</span> <span class="st">&quot;3,4,5&quot;</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>num_filters <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>dropout_keep_prob <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>l2_reg_lambda <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">70</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>num_epochs <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>x_text, y <span class="op">=</span> data_helpers.load_data_and_labels(positive_data_file, negative_data_file)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>max_document_length <span class="op">=</span> <span class="bu">max</span>([<span class="bu">len</span>(x.split(<span class="st">&quot; &quot;</span>)) <span class="cf">for</span> x <span class="kw">in</span> x_text])</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>vocab_processor <span class="op">=</span> learn.preprocessing.VocabularyProcessor(max_document_length)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array(<span class="bu">list</span>(vocab_processor.fit_transform(x_text)))</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>shuffle_indices <span class="op">=</span> np.random.permutation(np.arange(<span class="bu">len</span>(y)))</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>x_shuffled <span class="op">=</span> x[shuffle_indices]</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>y_shuffled <span class="op">=</span> y[shuffle_indices]</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>dev_sample_index <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> <span class="bu">int</span>(dev_sample_percentage <span class="op">*</span> <span class="bu">float</span>(<span class="bu">len</span>(y)))</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>x_train, x_dev <span class="op">=</span> x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>y_train, y_dev <span class="op">=</span> y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Vocabulary Size: </span><span class="sc">{:d}</span><span class="st">&quot;</span>.<span class="bu">format</span>(<span class="bu">len</span>(vocab_processor.vocabulary_)))</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Train/Dev split: </span><span class="sc">{:d}</span><span class="st">/</span><span class="sc">{:d}</span><span class="st">&quot;</span>.<span class="bu">format</span>(<span class="bu">len</span>(y_train), <span class="bu">len</span>(y_dev)))</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>tf.reset_default_graph()</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>num_classes<span class="op">=</span>y_train.shape[<span class="dv">1</span>]</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>sequence_length<span class="op">=</span>x_train.shape[<span class="dv">1</span>]</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>filter_sizes<span class="op">=</span><span class="bu">list</span>(<span class="bu">map</span>(<span class="bu">int</span>, filter_sizes.split(<span class="st">&quot;,&quot;</span>)))</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>input_x <span class="op">=</span> tf.placeholder(tf.int32, [<span class="va">None</span>, sequence_length])</span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>input_y <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, num_classes])</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>dropout_keep_prob <span class="op">=</span> tf.placeholder(tf.float32)</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>l2_loss <span class="op">=</span> tf.constant(<span class="fl">0.0</span>)</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> tf.Variable(tf.random_uniform([<span class="bu">len</span>(vocab_processor.vocabulary_),</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>                                   embedding_dim], <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>))</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>embedded_chars <span class="op">=</span> tf.nn.embedding_lookup(W, input_x)</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>embedded_chars_expanded <span class="op">=</span> tf.expand_dims(embedded_chars, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>pooled_outputs <span class="op">=</span> []</span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, filter_size <span class="kw">in</span> <span class="bu">enumerate</span>(filter_sizes):</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>    filter_shape <span class="op">=</span> [filter_size, embedding_dim, <span class="dv">1</span>, num_filters]</span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> tf.Variable(tf.truncated_normal(filter_shape, stddev<span class="op">=</span><span class="fl">0.1</span>))</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> tf.Variable(tf.constant(<span class="fl">0.1</span>, shape<span class="op">=</span>[num_filters]))</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    conv <span class="op">=</span> tf.nn.conv2d(</span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>        embedded_chars_expanded,</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>        W,</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>        strides<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">&quot;VALID&quot;</span>)</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> tf.nn.relu(tf.nn.bias_add(conv, b))</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>    pooled <span class="op">=</span> tf.nn.max_pool(</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>        h, ksize<span class="op">=</span>[<span class="dv">1</span>, sequence_length <span class="op">-</span> filter_size <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>        strides<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">&#39;VALID&#39;</span>,</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>        name<span class="op">=</span><span class="st">&quot;pool&quot;</span>)</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>    pooled_outputs.append(pooled)</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>num_filters_total <span class="op">=</span> num_filters <span class="op">*</span> <span class="bu">len</span>(filter_sizes)</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a>h_pool <span class="op">=</span> tf.concat(pooled_outputs, <span class="dv">3</span>)</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>h_pool_flat <span class="op">=</span> tf.reshape(h_pool, [<span class="op">-</span><span class="dv">1</span>, num_filters_total])</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>h_drop <span class="op">=</span> tf.nn.dropout(h_pool_flat, dropout_keep_prob)</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>l2_loss <span class="op">=</span> tf.constant(<span class="fl">0.0</span>)</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> tf.Variable(tf.random_normal(shape<span class="op">=</span>[num_filters_total, num_classes]))</span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> tf.Variable(tf.constant(<span class="fl">0.1</span>, shape<span class="op">=</span>[num_classes]))</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>l2_loss <span class="op">+=</span> tf.nn.l2_loss(W)</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>l2_loss <span class="op">+=</span> tf.nn.l2_loss(b)</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> tf.nn.xw_plus_b(h_drop, W, b)</span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> tf.argmax(scores, <span class="dv">1</span>)</span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> tf.nn.softmax_cross_entropy_with_logits(logits<span class="op">=</span>scores, labels<span class="op">=</span>input_y)</span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.reduce_mean(losses) <span class="op">+</span> l2_reg_lambda <span class="op">*</span> l2_loss</span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>correct_predictions <span class="op">=</span> tf.equal(predictions, tf.argmax(input_y, <span class="dv">1</span>))</span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> tf.reduce_mean(tf.cast(correct_predictions, <span class="st">&quot;float&quot;</span>))</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>global_step <span class="op">=</span> tf.Variable(<span class="dv">0</span>, trainable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.AdamOptimizer(<span class="fl">1e-3</span>)</span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>grads_and_vars <span class="op">=</span> optimizer.compute_gradients(loss)</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>train_op <span class="op">=</span> optimizer.apply_gradients(grads_and_vars, global_step<span class="op">=</span>global_step)</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.Session()</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>sess.run(tf.global_variables_initializer())</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>batches <span class="op">=</span> data_helpers.batch_iter(</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>    <span class="bu">list</span>(<span class="bu">zip</span>(x_train, y_train)), batch_size, num_epochs)</span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a>saver <span class="op">=</span> tf.train.Saver(tf.global_variables())</span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,batch <span class="kw">in</span> <span class="bu">enumerate</span>(batches):</span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a>    x_batch, y_batch <span class="op">=</span> <span class="bu">zip</span>(<span class="op">*</span>batch)</span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a>    feed_dict <span class="op">=</span> {</span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>        input_x: x_batch,input_y: y_batch, dropout_keep_prob: dropout_kp</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a>    sess.run(train_op, feed_dict)</span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">30</span>) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>        feed_dict2 <span class="op">=</span> {</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a>            input_x: x_dev,</span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>            input_y: y_dev,</span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>            dropout_keep_prob: dropout_kp</span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a>        train_acc <span class="op">=</span> sess.run(accuracy, feed_dict)</span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a>        test_acc <span class="op">=</span> sess.run(accuracy, feed_dict2)</span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span> train_acc, test_acc</span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="op">%</span> <span class="dv">100</span>) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>        path <span class="op">=</span> saver.save(sess, <span class="st">&quot;/tmp/nlpembed2&quot;</span>)</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
<p>Bu kod eğitim sonrası yüzde 74 başarıya erişti, kodun orijinal yazarı
[1] yüzde 76’yi görmüş, zaten bu veri seti üzerinde bilinen en iyi
başarı bu civarda.</p>
<p>Gömme Temsilinin Transferi</p>
<p>Ya kelimelerin komşuyla olan ilişkisini, ya da bozma tekniği, ya da
başka şekilde etiket elde edip eğittiğimiz gömülü referans matrisindeki
ağırlıkların anlamsal bir önemi olduğunu söylemiştik. O zaman, eğer
yeterince büyük bir sözlük, etiket ile eğitince elde ettiğimiz gömme
ağırlıklarının bir uygulamadan alınıp bir diğerinde de kullanılabilmesi
gerekir, değil mi? Bu sorunun cevabı evet. Gömme ağırlıklarını transfer
etmek hakikaten mümkün, ve bunu yapan pek çok kişi var.</p>
<p>Ödev</p>
<p>1’inci kodu işletin, ve gömme referans matris ağırlıklarına bakın.
Örnek bir kelime seçip o kelimenin gömme matrisindeki diğerlerine olan
mesafesini Öklitsel uzaklık hesabıyla bulun, en yakın on kelimeyi
gösterin.</p>
<p>Kaynaklar</p>
<p>[1] Britz, <em>Implementing a CNN for Text Classification in
TensorFlow</em>, <a
href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/">http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/</a></p>
<p>[2] Cornell U., <em>Movie Review Data</em>, <a
href="http://www.cs.cornell.edu/people/pabo/movie-review-data">http://www.cs.cornell.edu/people/pabo/movie-review-data</a></p>
<p>[3] Socher, <em>Deep Learning for NLP, without Magic),
\url{https://nlp.stanford.edu/courses/NAACL2013/NAACL2013-Socher-Manning-DeepLearning.pdf</em></p>
<p>[4] Bayramlı, Lineer Cebir, <em>SVD ile Kümeleme, Benzerlik</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
