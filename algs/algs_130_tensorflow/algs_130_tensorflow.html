<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Tensorflow ile Regresyon, YSA</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="tensorflow-ile-regresyon-ysa">Tensorflow ile Regresyon, YSA</h1>
<p>Toptan basit lineer regresyon hesabını Tensorflow [5] ile nasıl
yaparız? Regresyonu California emlak veri seti üzerinde işleteceğiz, bu
veride bölge bazlı olarak ev sahiplerinin ortalama yaşı, geliri, gibi
değişkenler ile hedef değişkeni olan ev fiyatı kayıtlı. Hedef ve kaynak
değişkenler arasındaki lineer ilişki lineer regresyon ile
hesaplanabilir, tanıdık formül,</p>
<p><span class="math display">\[
\hat{\theta} = (X^TX )^{-1} X^T y
\]</span></p>
<p>Veriye bir yanlılık (sadece 1 değeri içeren yeni bir kolon) ekleyip
üstteki hesabı yapalım.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reset_graph(seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    tf.reset_default_graph()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    tf.set_random_seed(seed)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>housing <span class="op">=</span> fetch_california_housing(data_home<span class="op">=</span><span class="st">&quot;/home/burak/Downloads/scikit-data&quot;</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> housing[<span class="st">&#39;data&#39;</span>].shape</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> housing[<span class="st">&#39;target&#39;</span>][:<span class="dv">5</span>]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>m, n <span class="op">=</span> housing.data.shape</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>housing_data_plus_bias <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), housing.data]</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.constant(housing_data_plus_bias, dtype<span class="op">=</span>tf.float32, name<span class="op">=</span><span class="st">&quot;X&quot;</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.constant(housing.target.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>tf.float32, name<span class="op">=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>XT <span class="op">=</span> tf.transpose(X)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    theta_value <span class="op">=</span> theta.<span class="bu">eval</span>()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;theta&#39;</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> theta_value</span></code></pre></div>
<pre><code>(20640, 8)
[ 4.526  3.585  3.521  3.413  3.422]
theta
[[ -3.68059006e+01]
 [  4.36796039e-01]
 [  9.45724174e-03]
 [ -1.07348330e-01]
 [  6.44418657e-01]
 [ -3.95741154e-06]
 [ -3.78908939e-03]
 [ -4.20193195e-01]
 [ -4.33070064e-01]]</code></pre>
<p>TF’in matris çarpımı için <code>matmul</code>, tersini alma için
<code>matrix_inverse</code> çağrılarını görüyoruz. Üstteki kodu olduğu
gibi alıp pek çok işlemci üzerinde direk paralel şekilde işletebiliriz,
aynı işi pür numpy bazlı kodla yapmak daha külfetli olurdu.</p>
<p>Gradyan İnişi</p>
<p>Klasik toptan usül ile hesabı gördük. Peki gradyan inişi ile lineer
regresyon nasıl yaparız? Burada iki yaklaşımı göstereceğiz, biri daha
zor, diğeri daha kolay. Zor olan matematiksel olarak elle kendimizin
gradyan türevini alması, daha kolay olanı türevi TF içindeki otomatik
türev alma mekanizmasını kullanarak o işi de TF’e yaptırmak.</p>
<p>Veriyi hazırlayalım ve çiziti oluşturalım; başlangıç <span
class="math inline">\(\theta\)</span> değeri rasgele atansın, <span
class="math inline">\(X,y\)</span> değerleri verinin kendisi olacak,
tahmin ve hata için fonksiyonlar olsun.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>scaled_housing_data <span class="op">=</span> scaler.fit_transform(housing.data)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>scaled_housing_data_plus_bias <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), scaled_housing_data]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>reset_graph()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.constant(scaled_housing_data_plus_bias, dtype<span class="op">=</span>tf.float32, name<span class="op">=</span><span class="st">&quot;X&quot;</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.constant(housing.target.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>tf.float32, name<span class="op">=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> tf.Variable(tf.random_uniform([n <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>], <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>, seed<span class="op">=</span><span class="dv">42</span>), name<span class="op">=</span><span class="st">&quot;theta&quot;</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> tf.matmul(X, theta, name<span class="op">=</span><span class="st">&quot;predictions&quot;</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>error <span class="op">=</span> y_pred <span class="op">-</span> y</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> tf.reduce_mean(tf.square(error), name<span class="op">=</span><span class="st">&quot;mse&quot;</span>)</span></code></pre></div>
<p>Elle türevi alınmış gradyan nedir? [1, sf. 261]’e göre formül</p>
<p><span class="math display">\[
\nabla_\theta MSE(\theta) = \frac{2}{m} X^T (X \cdot \theta - y)
\]</span></p>
<p>ve gradyan güncellemesi</p>
<p><span class="math display">\[
\theta^{t+1} = \theta - \eta \nabla_\theta MSE(\theta)
\]</span></p>
<p>Alttaki TF kodu bir döngü içinde gradyan güncellemesi yapacak ve hata
karelerinin ortalaması (mean square error -MSE-) hesaplayıp ekrana
basacak. MSE’in gittikçe aşağı inmesi lazım, çünkü gradyanın tersi
yönünde hatayı azaltacak şekilde hareket ediyoruz.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>gradients <span class="op">=</span> <span class="dv">2</span><span class="op">/</span>np.<span class="bu">float</span>(m) <span class="op">*</span> tf.matmul(tf.transpose(X), error)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>training_op <span class="op">=</span> tf.assign(theta, theta<span class="op">-</span>(learning_rate<span class="op">*</span>gradients))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    sess.run(init)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(<span class="st">&quot;Epoch&quot;</span>, epoch, <span class="st">&quot;MSE =&quot;</span>, mse.<span class="bu">eval</span>())</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    sess.run(training_op)    </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    best_theta <span class="op">=</span> theta.<span class="bu">eval</span>()</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;theta&#39;</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> best_theta</span></code></pre></div>
<pre><code>(&#39;Epoch&#39;, 0, &#39;MSE =&#39;, 9.1615429)
(&#39;Epoch&#39;, 100, &#39;MSE =&#39;, 0.71450073)
(&#39;Epoch&#39;, 200, &#39;MSE =&#39;, 0.56670469)
(&#39;Epoch&#39;, 300, &#39;MSE =&#39;, 0.55557162)
(&#39;Epoch&#39;, 400, &#39;MSE =&#39;, 0.54881161)
(&#39;Epoch&#39;, 500, &#39;MSE =&#39;, 0.54363626)
(&#39;Epoch&#39;, 600, &#39;MSE =&#39;, 0.53962916)
(&#39;Epoch&#39;, 700, &#39;MSE =&#39;, 0.53650916)
(&#39;Epoch&#39;, 800, &#39;MSE =&#39;, 0.53406781)
(&#39;Epoch&#39;, 900, &#39;MSE =&#39;, 0.53214705)
theta
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393812]
 [-0.04269557]
 [-0.66145277]
 [-0.63752776]]</code></pre>
<p>Otomatik Türev ile Gradyan</p>
<p>Sembolik türev yerine TF içindeki <code>autodiff</code> paketine
türevi aldıralım ve gradyan inişini böyle yapalım,</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>gradients <span class="op">=</span> tf.gradients(mse, [theta])[<span class="dv">0</span>] <span class="co"># otomatik turev</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>training_op <span class="op">=</span> tf.assign(theta, theta<span class="op">-</span>(learning_rate<span class="op">*</span>gradients))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    sess.run(init)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(<span class="st">&quot;Epoch&quot;</span>, epoch, <span class="st">&quot;MSE =&quot;</span>, mse.<span class="bu">eval</span>())</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    sess.run(training_op)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    best_theta <span class="op">=</span> theta.<span class="bu">eval</span>()</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> best_theta</span></code></pre></div>
<pre><code>(&#39;Epoch&#39;, 0, &#39;MSE =&#39;, 9.1615429)
(&#39;Epoch&#39;, 100, &#39;MSE =&#39;, 0.71450061)
(&#39;Epoch&#39;, 200, &#39;MSE =&#39;, 0.56670463)
(&#39;Epoch&#39;, 300, &#39;MSE =&#39;, 0.55557162)
(&#39;Epoch&#39;, 400, &#39;MSE =&#39;, 0.54881167)
(&#39;Epoch&#39;, 500, &#39;MSE =&#39;, 0.5436362)
(&#39;Epoch&#39;, 600, &#39;MSE =&#39;, 0.53962916)
(&#39;Epoch&#39;, 700, &#39;MSE =&#39;, 0.53650916)
(&#39;Epoch&#39;, 800, &#39;MSE =&#39;, 0.53406781)
(&#39;Epoch&#39;, 900, &#39;MSE =&#39;, 0.53214717)
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393811]
 [-0.04269556]
 [-0.66145277]
 [-0.6375277 ]]</code></pre>
<p>Aynı sonuca eriştik.</p>
<p>Daha da basitleştirebiliriz, üstteki kodda <code>assign</code> ile
gradyan inişi için gereken çıkartma işlemi elle yapıldı. TF paketi
içinde bu çıkartmayı yapacak optimizasyon rutinleri de var, mesela
<code>GradientDescentOptimizer</code>.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.GradientDescentOptimizer(learning_rate<span class="op">=</span>learning_rate)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>training_op <span class="op">=</span> optimizer.minimize(mse)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    sess.run(init)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span>(<span class="st">&quot;Epoch&quot;</span>, epoch, <span class="st">&quot;MSE =&quot;</span>, mse.<span class="bu">eval</span>())</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        sess.run(training_op)    </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    best_theta <span class="op">=</span> theta.<span class="bu">eval</span>()</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;theta&#39;</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(best_theta)</span></code></pre></div>
<pre><code>(&#39;Epoch&#39;, 0, &#39;MSE =&#39;, 9.1615429)
(&#39;Epoch&#39;, 100, &#39;MSE =&#39;, 0.71450061)
(&#39;Epoch&#39;, 200, &#39;MSE =&#39;, 0.56670463)
(&#39;Epoch&#39;, 300, &#39;MSE =&#39;, 0.55557162)
(&#39;Epoch&#39;, 400, &#39;MSE =&#39;, 0.54881167)
(&#39;Epoch&#39;, 500, &#39;MSE =&#39;, 0.5436362)
(&#39;Epoch&#39;, 600, &#39;MSE =&#39;, 0.53962916)
(&#39;Epoch&#39;, 700, &#39;MSE =&#39;, 0.53650916)
(&#39;Epoch&#39;, 800, &#39;MSE =&#39;, 0.53406781)
(&#39;Epoch&#39;, 900, &#39;MSE =&#39;, 0.53214717)
theta
[[ 2.06855249]
 [ 0.88740271]
 [ 0.14401658]
 [-0.34770882]
 [ 0.36178368]
 [ 0.00393811]
 [-0.04269556]
 [-0.66145277]
 [-0.6375277 ]]</code></pre>
<p>Görüldüğü gibi matris işlemi içeren her türlü hesap TF ile
kodlanabilir, bu yapıldığında kodlar rahat bir şekilde paralelize
edilebilir. Yapay öğrenimde ne kadar çok lineer cebir kullanımı olduğunu
biliyoruz, ayrıca türev almak otomatikleştirildiği için akla gelebilecek
her türlü lineer cebir, optimizasyon işlemi TF üzerinden
kodlanabilir.</p>
<p>TensorFlow ile Çok Katmanlı Yapay Sinir Ağları (Neural Network
-NN-)</p>
<p>Şimdi çok katmanlı NN ile regresyon yapma örneği görelim. Bir NN
bildiğimiz gibi evrensel yaklaşıklayıcı, yeterli veri var ise her türlü
fonksiyonu yaklaşık olarak temsil edebilir, öğrenebilir. Düz lineer
regresyon ile yaptığımız da bir bakıma budur, bilinmeyen bir fonksiyonu
veriden öğrenmeye uğraşırız, fakat nihai fonksiyon lineer olmalı. NN ile
lineer, gayrı-lineer her türlü fonksiyon temsil edilebilir.</p>
<p>Her katmanda girdi ağırlıklar ile çarpılacak, bir yanlılık (bias)
eklenecek, ve sonuç bir aktivasyon fonksiyonuna verilecek (altta
kullanılan ReLu), buradan çıkan sonuç bir sonraki katmana aktarılacak.
Aktivasyon lazım çünkü aktivasyon olmasa, her katman sadece ağırlıklarla
çarpılan sonucu bir sonraki katmana verse, tüm NN’in işlemi ardı ardına
matrislerin çarpımı olarak ta görülebilirdi, ama bu bir lineer işlem
olurdu, o zaman NN gayri-lineerligi modelleyemezdi. Diğer yandan lineer
regresyon bir bakıma tek katmanlı ve aktivasyonu olmayan bir NN gibi
görülebilir.</p>
<p>Örnek için kullanılan veri seti Low Birth Weight [2] verisi, bu
veride yeni doğan çocukların annenin tıbbi verileri ile çocuğun
doğduğundaki ağırlığı kaydedilmiş, ve bu hedef değişkeni ile diğerleri
arasındaki ilişkiyi öğrenmek istiyoruz. Veriyi okuyup normalize
edelim,</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.python.framework <span class="im">import</span> ops</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>cols_of_interest <span class="op">=</span> [<span class="st">&#39;AGE&#39;</span>, <span class="st">&#39;LWT&#39;</span>, <span class="st">&#39;RACE&#39;</span>, <span class="st">&#39;SMOKE&#39;</span>, <span class="st">&#39;PTL&#39;</span>, <span class="st">&#39;HT&#39;</span>, <span class="st">&#39;UI&#39;</span>, <span class="st">&#39;FTV&#39;</span>]</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;lowbwt.dat&#39;</span>,sep<span class="op">=</span><span class="st">&#39;\s*&#39;</span>,engine<span class="op">=</span><span class="st">&#39;python&#39;</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.array(df[cols_of_interest])</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>y_vals <span class="op">=</span> np.array(df[<span class="st">&#39;BWT&#39;</span>])</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>ops.reset_default_graph()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>tf.set_random_seed(<span class="dv">3</span>)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">3</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.Session()</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>tmp <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(x_vals)), size<span class="op">=</span><span class="bu">len</span>(x_vals), replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>first <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(x_vals)<span class="op">*</span><span class="fl">0.80</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> tmp[:first]</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>test_indices <span class="op">=</span> tmp[first:]</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>x_vals_train <span class="op">=</span> x_vals[train_indices]</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>x_vals_test <span class="op">=</span> x_vals[test_indices]</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>y_vals_train <span class="op">=</span> y_vals[train_indices]</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>y_vals_test <span class="op">=</span> y_vals[test_indices]</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_cols(m):</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    col_max <span class="op">=</span> m.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    col_min <span class="op">=</span> m.<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (m<span class="op">-</span>col_min) <span class="op">/</span> (col_max <span class="op">-</span> col_min)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>x_vals_train <span class="op">=</span> np.nan_to_num(normalize_cols(x_vals_train))</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>x_vals_test <span class="op">=</span> np.nan_to_num(normalize_cols(x_vals_test))</span></code></pre></div>
<p>NN katmanlarını hazırlayalım [3], üç katman olacak, katmanlarda
sırasıyla 25, 10 ve 3 tane nöron olacak. Kayıp (loss) fonksiyonu L1
(mutlak değer) üzerinden hesaplanıyor. Optimize edici olarak TF’in
<code>AdamOptimizer</code>’ini kullanacağız.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_weight(shape, st_dev):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    weight <span class="op">=</span> tf.Variable(tf.random_normal(shape, stddev<span class="op">=</span>st_dev))</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(weight)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_bias(shape, st_dev):</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    bias <span class="op">=</span> tf.Variable(tf.random_normal(shape, stddev<span class="op">=</span>st_dev))</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(bias)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>x_data <span class="op">=</span> tf.placeholder(shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">8</span>], dtype<span class="op">=</span>tf.float32)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>y_target <span class="op">=</span> tf.placeholder(shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">1</span>], dtype<span class="op">=</span>tf.float32)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fully_connected(input_layer, weights, biases):</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    layer <span class="op">=</span> tf.add(tf.matmul(input_layer, weights), biases)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(tf.nn.relu(layer))</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>weight_1 <span class="op">=</span> init_weight(shape<span class="op">=</span>[<span class="dv">8</span>, <span class="dv">25</span>], st_dev<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>bias_1 <span class="op">=</span> init_bias(shape<span class="op">=</span>[<span class="dv">25</span>], st_dev<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>layer_1 <span class="op">=</span> fully_connected(x_data, weight_1, bias_1)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>weight_2 <span class="op">=</span> init_weight(shape<span class="op">=</span>[<span class="dv">25</span>, <span class="dv">10</span>], st_dev<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>bias_2 <span class="op">=</span> init_bias(shape<span class="op">=</span>[<span class="dv">10</span>], st_dev<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>layer_2 <span class="op">=</span> fully_connected(layer_1, weight_2, bias_2)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>weight_3 <span class="op">=</span> init_weight(shape<span class="op">=</span>[<span class="dv">10</span>, <span class="dv">3</span>], st_dev<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>bias_3 <span class="op">=</span> init_bias(shape<span class="op">=</span>[<span class="dv">3</span>], st_dev<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>layer_3 <span class="op">=</span> fully_connected(layer_2, weight_3, bias_3)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>weight_4 <span class="op">=</span> init_weight(shape<span class="op">=</span>[<span class="dv">3</span>, <span class="dv">1</span>], st_dev<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>bias_4 <span class="op">=</span> init_bias(shape<span class="op">=</span>[<span class="dv">1</span>], st_dev<span class="op">=</span><span class="fl">10.0</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>final_output <span class="op">=</span> fully_connected(layer_3, weight_4, bias_4)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.reduce_mean(tf.<span class="bu">abs</span>(y_target <span class="op">-</span> final_output))</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>my_opt <span class="op">=</span> tf.train.AdamOptimizer(<span class="fl">0.02</span>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>train_step <span class="op">=</span> my_opt.minimize(loss)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.initialize_all_variables()</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>sess.run(init)</span></code></pre></div>
<p>NN ağ çizit yapısı kuruldu, hatta ağırlıkların değerleri bile var.
Tabii başta bu değerler rasgele değerler, yani istediğimiz nihai
değerler değiller. Fakat üsttekinin geçerli bir NN olduğunu ispatlamak
için dışarıdan bir test verisi versek bize bir hesap yapacağını
göreceğiz. Bunun için yer tutuculara tek bir veri noktası verelim, ve
sonuç düğümünün hesaplanmasını isteyelim,</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="fl">20.</span>,<span class="fl">181.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">0.</span>,<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">0.</span>]) <span class="co"># uyduruk veri</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.reshape(x, (<span class="dv">1</span>,<span class="dv">8</span>))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> sess.run(final_output,feed_dict<span class="op">=</span>{x_data: x})</span></code></pre></div>
<pre><code>[[ 8723006.]]</code></pre>
<p>Bu sayı çok büyük, problem açısından tabii ki anlamsız, NN’i eğitim
verisiyle eğittikçe NN ağırlıkları gerçek hallerine yaklaşacaklar, o
zaman üstteki gibi bir veri noktası için daha iyi bir sonuç
alabileceğiz, fakat bir hesabın yapılabildiğini görüyoruz.</p>
<p>Şimdi veriyi 80/20 oranında eğitim / doğrulama olarak ayıralım, ve
eğitim veri seti üzerinde eğitim yapalım. Eğitim verisinden
<code>batch_size</code> büyüklüğünde mini toptan parçalar (minibatch)
örnekleyelim, ve her eğitim döngüsünde optimize ediciye bu parçaları
verelim.</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>loss_vec <span class="op">=</span> []<span class="op">;</span> test_loss <span class="op">=</span> []</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    rand_index <span class="op">=</span> np.random.choice(<span class="bu">len</span>(x_vals_train), size<span class="op">=</span>batch_size)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    rand_x <span class="op">=</span> x_vals_train[rand_index]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    rand_y <span class="op">=</span> np.transpose([y_vals_train[rand_index]])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    sess.run(train_step, feed_dict<span class="op">=</span>{x_data: rand_x, y_target: rand_y})</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    temp_loss <span class="op">=</span> sess.run(loss, feed_dict<span class="op">=</span>{x_data: rand_x, y_target: rand_y})</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    loss_vec.append(temp_loss)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> {x_data: x_vals_test, y_target: np.transpose([y_vals_test])}</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    test_temp_loss <span class="op">=</span> sess.run(loss, feed_dict<span class="op">=</span>d)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    test_loss.append(test_temp_loss)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span><span class="dv">25</span><span class="op">==</span><span class="dv">0</span>:</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span> <span class="op">+</span> <span class="bu">str</span>(i<span class="op">+</span><span class="dv">1</span>) <span class="op">+</span> <span class="st">&#39;. Kayip = &#39;</span> <span class="op">+</span> <span class="bu">str</span>(temp_loss))</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot loss over time</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_vec, <span class="st">&#39;k-&#39;</span>, label<span class="op">=</span><span class="st">u&#39;Eğitim Kaybı&#39;</span>)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>plt.plot(test_loss, <span class="st">&#39;r--&#39;</span>, label<span class="op">=</span><span class="st">u&#39;Test Kaybı&#39;</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">u&#39;Her Epoch İçinde Kayıp&#39;</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epoch&#39;</span>)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">u&#39;Kayıp&#39;</span>)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">&quot;upper right&quot;</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;tf_03.png&#39;</span>)</span></code></pre></div>
<pre><code>Epoch: 25. Kayip = 16744.3
Epoch: 50. Kayip = 8367.32
Epoch: 75. Kayip = 4687.56
Epoch: 100. Kayip = 1767.07
Epoch: 125. Kayip = 1297.27
Epoch: 150. Kayip = 852.689
Epoch: 175. Kayip = 705.81
Epoch: 200. Kayip = 728.779</code></pre>
<p><img src="tf_03.png" /></p>
<p>Görüldüğü gibi eğitim ve test kaybı birbirine yakın ki bu iyiye
işaret, model iyi öğreniyor, eksik uyum, aşırı uyum durumları yok
demektir, ve nihai kayıp her iki tarafta da 0.7 kg civarı.</p>
<p>Not: Girdi verisinde yaklaşık 200 satır veri olduğuna dikkat, şu
sorulabilir, eğitim sırasında 200 kere dönüyoruz, her döngüde 200 civarı
toptan parça kullanıyoruz, bu nasıl oluyor? Ufak parçaların içeriğini
{}, o zaman döngü sayısı, parça sayısına bakarsak bir anlamda veriyi
örnekleyerek çoğaltmış (upsample) oluyoruz. Bu NN eğitiminde sürekli
ortaya çıkan bir kavram, aklımızda olsun.</p>
<p>İki Kategori Sınıflaması</p>
<p>Peki istatistikte lojistik regresyon ile yaptığımız iki kategori
arasında sınıflamayı çok katmanlı NN ile nasıl yaparız? Mesela bebeğin
2.5 kg altında doğup doğmaması diyelim, bu bir riskli durum, ve iki
farklı sınıf olarak görülebilecek bu hedef değişkeni için 0/1 tarzında
bir eğitim yapmak istiyoruz. İlk akla gelebilecek çözüm bir önceki NN’i
alıp oradan gelecek tahminleri eşik değeri 2.5 üstünde mi altında mı
diye ek bir filtrelemeden geçirmek, yani regresyon sonucunu 0/1
tahminine çevirmek. Bu yaklaşım yüzde 55 civarı başarı veriyor.</p>
<p>Daha iyisi, daha farklı bir NN’i özellikle 0/1 hedefi için
eğitmek.</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.python.framework <span class="im">import</span> ops</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>cols_of_interest <span class="op">=</span> [<span class="st">&#39;AGE&#39;</span>, <span class="st">&#39;LWT&#39;</span>, <span class="st">&#39;RACE&#39;</span>, <span class="st">&#39;SMOKE&#39;</span>, <span class="st">&#39;PTL&#39;</span>, <span class="st">&#39;HT&#39;</span>, <span class="st">&#39;UI&#39;</span>, <span class="st">&#39;FTV&#39;</span>]</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;lowbwt.dat&#39;</span>,sep<span class="op">=</span><span class="st">&#39;\s*&#39;</span>,engine<span class="op">=</span><span class="st">&#39;python&#39;</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.array(df[cols_of_interest])</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>y_vals <span class="op">=</span> np.array(df[<span class="st">&#39;LOW&#39;</span>])</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>ops.reset_default_graph()</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>tf.set_random_seed(<span class="dv">1</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.Session()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>tmp <span class="op">=</span> np.random.choice(<span class="bu">range</span>(<span class="bu">len</span>(x_vals)), size<span class="op">=</span><span class="bu">len</span>(x_vals), replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>first <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(x_vals)<span class="op">*</span><span class="fl">0.80</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>train_indices <span class="op">=</span> tmp[:first]</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>test_indices <span class="op">=</span> tmp[first:]</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>x_vals_train <span class="op">=</span> x_vals[train_indices]</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>x_vals_test <span class="op">=</span> x_vals[test_indices]</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>y_vals_train <span class="op">=</span> y_vals[train_indices]</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>y_vals_test <span class="op">=</span> y_vals[test_indices]</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize_cols(m):</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    col_max <span class="op">=</span> m.<span class="bu">max</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    col_min <span class="op">=</span> m.<span class="bu">min</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (m<span class="op">-</span>col_min) <span class="op">/</span> (col_max <span class="op">-</span> col_min)</span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>x_vals_train <span class="op">=</span> np.nan_to_num(normalize_cols(x_vals_train))</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>x_vals_test <span class="op">=</span> np.nan_to_num(normalize_cols(x_vals_test))</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>x_data <span class="op">=</span> tf.placeholder(shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">8</span>], dtype<span class="op">=</span>tf.float32)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>y_target <span class="op">=</span> tf.placeholder(shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">1</span>], dtype<span class="op">=</span>tf.float32)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_variable(shape):</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(tf.Variable(tf.random_normal(shape<span class="op">=</span>shape)))</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logistic(input_layer, multiplication_weight, bias_weight, activation <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    linear_layer <span class="op">=</span> tf.add(tf.matmul(input_layer, multiplication_weight), bias_weight)</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> activation:</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(tf.nn.sigmoid(linear_layer))</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span>(linear_layer)</span></code></pre></div>
<p>Üstteki <code>logistic</code> çağrısı ağırlıklarla çarpım sonucunu
bir sigmoid fonksiyonundan geçiriyor. Yeni NN’imiz 20, 20, 10 nöron
içeren bu yeni stili kullanacak.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>A1 <span class="op">=</span> init_variable(shape<span class="op">=</span>[<span class="dv">8</span>,<span class="dv">20</span>])</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_variable(shape<span class="op">=</span>[<span class="dv">20</span>])</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>logistic_layer1 <span class="op">=</span> logistic(x_data, A1, b1)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>A2 <span class="op">=</span> init_variable(shape<span class="op">=</span>[<span class="dv">20</span>,<span class="dv">10</span>])</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_variable(shape<span class="op">=</span>[<span class="dv">10</span>])</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>logistic_layer2 <span class="op">=</span> logistic(logistic_layer1, A2, b2)</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>A3 <span class="op">=</span> init_variable(shape<span class="op">=</span>[<span class="dv">10</span>,<span class="dv">1</span>])</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>b3 <span class="op">=</span> init_variable(shape<span class="op">=</span>[<span class="dv">1</span>])</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>final_output <span class="op">=</span> logistic(logistic_layer2, A3, b3, activation<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>tmp <span class="op">=</span> tf.nn.sigmoid_cross_entropy_with_logits(logits<span class="op">=</span>final_output, labels<span class="op">=</span>y_target)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.reduce_mean(tmp)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>my_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate <span class="op">=</span> <span class="fl">0.002</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>train_step <span class="op">=</span> my_opt.minimize(loss)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>sess.run(init)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> tf.<span class="bu">round</span>(tf.nn.sigmoid(final_output))</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>predictions_correct <span class="op">=</span> tf.cast(tf.equal(prediction, y_target), tf.float32)</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> tf.reduce_mean(predictions_correct)</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>loss_vec <span class="op">=</span> []<span class="op">;</span> train_acc <span class="op">=</span> []<span class="op">;</span> test_acc <span class="op">=</span> []</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1500</span>):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    rand_index <span class="op">=</span> np.random.choice(<span class="bu">len</span>(x_vals_train), size<span class="op">=</span>batch_size)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    rand_x <span class="op">=</span> x_vals_train[rand_index]</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    rand_y <span class="op">=</span> np.transpose([y_vals_train[rand_index]])</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    sess.run(train_step, feed_dict<span class="op">=</span>{x_data: rand_x, y_target: rand_y})</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>    temp_loss <span class="op">=</span> sess.run(loss, feed_dict<span class="op">=</span>{x_data: rand_x, y_target: rand_y})</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    loss_vec.append(temp_loss)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    d1 <span class="op">=</span> {x_data: x_vals_train, y_target: np.transpose([y_vals_train])}</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    temp_acc_train <span class="op">=</span> sess.run(accuracy, feed_dict<span class="op">=</span>d1)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    train_acc.append(temp_acc_train)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    d2 <span class="op">=</span> {x_data: x_vals_test, y_target: np.transpose([y_vals_test])}</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    temp_acc_test <span class="op">=</span> sess.run(accuracy, feed_dict<span class="op">=</span>d2)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    test_acc.append(temp_acc_test)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>)<span class="op">%</span><span class="dv">150</span><span class="op">==</span><span class="dv">0</span>: <span class="bu">print</span>(<span class="st">&#39;Loss = &#39;</span> <span class="op">+</span> <span class="bu">str</span>(temp_loss))</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>plt.plot(loss_vec, <span class="st">&#39;k-&#39;</span>)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">u&quot;Her Epoch İçin Çapraz Entropi Kaybı&quot;</span>)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">u&quot;Epoch&quot;</span>)</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">u&quot;Çapraz Entropi Kaybı&quot;</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="fl">0.0</span>,<span class="fl">2.0</span>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;tf_04.png&#39;</span>)        </span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>plt.plot(train_acc, <span class="st">&#39;k-&#39;</span>, label<span class="op">=</span><span class="st">u&quot;Eğitim Seti Doğrulugu&quot;</span>)</span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>plt.plot(test_acc, <span class="st">&#39;r--&#39;</span>, label<span class="op">=</span><span class="st">u&quot;Test Set Doğrulugu&quot;</span>)</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">u&quot;Eğitim ve Test Doğruluğu&quot;</span>)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Epoch&quot;</span>)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">u&quot;Doğruluk&quot;</span>)</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="fl">0.0</span>,<span class="fl">0.8</span>)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">&#39;lower right&#39;</span>)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;tf_05.png&#39;</span>)        </span></code></pre></div>
<pre><code>Loss = 0.615425
Loss = 0.593725
Loss = 0.575125
Loss = 0.664008
Loss = 0.667132
Loss = 0.617393
Loss = 0.504238
Loss = 0.447125
Loss = 0.530287
Loss = 0.528585</code></pre>
<p><img src="tf_04.png" /></p>
<p><img src="tf_05.png" /></p>
<p>Çoklu Kategori Hedefi</p>
<p>Çiçeklerin ölçümleri ve çiçek çeşitlerini içeren ünlü IRIS veri
setinde 4 boyutlu sayısal veri ile 3 çiçek çeşidi arasında softmax ile
seçim yapacak bir kodu görelim. Bu kodda tamamen tamamen bağlanmış
katman (fully-connected layer) hesabını elle değil, TF çağrısı
<code>tf.contrib.layers.fully_connected</code> ile yapıyoruz.</p>
<p>İlk önce sadece tek tamamen bağlanmış katman içeren bir kod
görelim,</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> preprocessing</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>tf.set_random_seed(<span class="dv">0</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> [<span class="st">&quot;sepal_length&quot;</span>, <span class="st">&quot;sepal_width&quot;</span>, <span class="st">&quot;petal_length&quot;</span>, <span class="st">&quot;petal_width&quot;</span>, <span class="st">&quot;iris_class&quot;</span>]</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(<span class="st">&quot;iris.data&quot;</span>, sep<span class="op">=</span><span class="st">&quot;,&quot;</span>,names<span class="op">=</span>cols)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> data.sample(frac<span class="op">=</span><span class="dv">1</span>,random_state<span class="op">=</span><span class="dv">0</span>).reset_index(drop<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>all_x <span class="op">=</span> data[[<span class="st">&quot;sepal_length&quot;</span>, <span class="st">&quot;sepal_width&quot;</span>, <span class="st">&quot;petal_length&quot;</span>, <span class="st">&quot;petal_width&quot;</span>]]</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>min_max_scaler <span class="op">=</span> preprocessing.MinMaxScaler()</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>all_x <span class="op">=</span> min_max_scaler.fit_transform(all_x)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>all_y <span class="op">=</span> pd.get_dummies(data.iris_class)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>train_x, test_x, train_y, test_y <span class="op">=</span> train_test_split(all_x, all_y, test_size<span class="op">=</span><span class="fl">0.20</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_x.shape)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(train_y.shape)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_x.shape)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(test_y.shape)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>tf.reset_default_graph()</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, np.shape(train_x)[<span class="dv">1</span>]], name<span class="op">=</span><span class="st">&quot;x&quot;</span>)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, np.shape(train_y)[<span class="dv">1</span>]], name<span class="op">=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> tf.contrib.layers.fully_connected(inputs<span class="op">=</span>x,</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>                                               num_outputs<span class="op">=</span>np.shape(train_y)[<span class="dv">1</span>], </span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>                                               activation_fn<span class="op">=</span>tf.nn.softmax)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>cost <span class="op">=</span> tf.losses.softmax_cross_entropy(onehot_labels<span class="op">=</span>y,logits<span class="op">=</span>prediction)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>correct_prediction <span class="op">=</span> tf.equal(tf.argmax(prediction,<span class="dv">1</span>),tf.argmax(y, <span class="dv">1</span>))</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.AdagradOptimizer(learning_rate).minimize(cost)</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.InteractiveSession()</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>sess.run(init)</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>training_epochs <span class="op">=</span> <span class="dv">3000</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(training_epochs):</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>    sess.run([optimizer, cost], feed_dict<span class="op">=</span>{x: train_x, y: train_y})</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Dogruluk:&quot;</span>, accuracy.<span class="bu">eval</span>({x: test_x, y: test_y}))    </span></code></pre></div>
<pre><code>(120, 4)
(120, 3)
(30, 4)
(30, 3)
(&#39;Dogruluk:&#39;, 0.76666665)</code></pre>
<p>Şimdi sırasıyla 10,20,10 olacak şekilde üç katmanlı bir YSA ile
sınıflama yapalım,</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>tf.reset_default_graph()</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, np.shape(train_x)[<span class="dv">1</span>]], name<span class="op">=</span><span class="st">&quot;x&quot;</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, np.shape(train_y)[<span class="dv">1</span>]], name<span class="op">=</span><span class="st">&quot;y&quot;</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>h1 <span class="op">=</span> <span class="dv">10</span> <span class="co"># 1. gizli (hidden) katman</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>h2 <span class="op">=</span> <span class="dv">20</span> <span class="co"># 2. gizli katman</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>h3 <span class="op">=</span> <span class="dv">10</span> <span class="co"># 3. gizli katman</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>k1 <span class="op">=</span> tf.contrib.layers.fully_connected(inputs<span class="op">=</span>x,</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>                                       num_outputs<span class="op">=</span>h1, </span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>                                       activation_fn<span class="op">=</span>tf.nn.relu)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>k2 <span class="op">=</span> tf.contrib.layers.fully_connected(inputs<span class="op">=</span>k1,</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>                                       num_outputs<span class="op">=</span>h2, </span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>                                       activation_fn<span class="op">=</span>tf.nn.relu)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>k3 <span class="op">=</span> tf.contrib.layers.fully_connected(inputs<span class="op">=</span>k2,</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>                                       num_outputs<span class="op">=</span>h3, </span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>                                       activation_fn<span class="op">=</span>tf.nn.relu)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> tf.contrib.layers.fully_connected(inputs<span class="op">=</span>k3,</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>                                               num_outputs<span class="op">=</span>np.shape(train_y)[<span class="dv">1</span>], </span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>                                               activation_fn<span class="op">=</span>tf.nn.softmax)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>cost <span class="op">=</span> tf.losses.softmax_cross_entropy(onehot_labels<span class="op">=</span>y,logits<span class="op">=</span>prediction)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>correct_prediction <span class="op">=</span> tf.equal(tf.argmax(prediction,<span class="dv">1</span>),tf.argmax(y, <span class="dv">1</span>))</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> tf.reduce_mean(tf.cast(correct_prediction,tf.float32))</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.AdagradOptimizer(learning_rate).minimize(cost)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.InteractiveSession()</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>sess.run(init)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>training_epochs <span class="op">=</span> <span class="dv">3000</span></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(training_epochs):</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>    sess.run([optimizer, cost], feed_dict<span class="op">=</span>{x: train_x, y: train_y})</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Dogruluk:&quot;</span>, accuracy.<span class="bu">eval</span>({x: test_x, y: test_y}))    </span></code></pre></div>
<pre><code>(&#39;Dogruluk:&#39;, 0.96666664)</code></pre>
<p>Notlar</p>
<p>Vektorler, gradyanlar hakkinda birkac cabuk not,</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="co"># y + 2x = 2x + x</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.constant(<span class="fl">0.</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> x</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>g1 <span class="op">=</span> tf.gradients(x <span class="op">+</span> y, [x, y])</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>xvec <span class="op">=</span> tf.Variable([[<span class="fl">1.</span>], [<span class="fl">2.</span>],[<span class="fl">3.</span>]]) </span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>yvec <span class="op">=</span> tf.Variable([[<span class="fl">3.</span>], [<span class="fl">4.</span>],[<span class="fl">5.</span>]]) </span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>xvec <span class="op">=</span> xvec <span class="op">+</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]]).T</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>zvec <span class="op">=</span> tf.matmul(tf.transpose(xvec),yvec)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>g2 <span class="op">=</span> tf.gradients(zvec, xvec)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    sess.run(tf.global_variables_initializer())</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (sess.run(g1))</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (sess.run(xvec))</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (sess.run(yvec))</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (sess.run(zvec))</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (sess.run(g2))</span></code></pre></div>
<pre><code>[3.0, 1.0]
[[2.]
 [3.]
 [4.]]
[[3.]
 [4.]
 [5.]]
[[38.]]
[array([[3.],
       [4.],
       [5.]], dtype=float32)]</code></pre>
<p>Kaynaklar</p>
<p>[1] Géron, <em>Hands-On Machine Learning with Scikit-Learn and
TensorFlow</em></p>
<p>[2] Heidelberg U., <em>Risk Factors Associated with Low Infant Birth
Weight</em>, <a
href="http://www.statlab.uni-heidelberg.de/data/linmod/birthweight.html">http://www.statlab.uni-heidelberg.de/data/linmod/birthweight.html</a></p>
<p>[3] McClure, <em>TensorFlow Machine Learning Cookbook</em></p>
<p>[4] Bayramlı, Bilgisayar Bilim, <em>Otomatik Türev Almak</em></p>
<p>[5] Bayramlı, Tensorflow, <a
href="https://burakbayramli.github.io/dersblog/sk/2022/10/tensorflow.html">https://burakbayramli.github.io/dersblog/sk/2022/10/tensorflow.html</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
