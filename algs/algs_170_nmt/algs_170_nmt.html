<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Otomatik Tercüme, Makine Tercümesi (Machine Translation)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="otomatik-tercüme-makine-tercümesi-machine-translation">Otomatik
Tercüme, Makine Tercümesi (Machine Translation)</h1>
<p>Dizin-Dizin İlişkisini Öğrenmek (Sequence to Sequence Learning)</p>
<p>Dillerarası otomatik tercüme yazılımı Google Translate ile popüler
hale geldi. Google bu servisi ilk kodladığında parça parça, istatistiki
bir yaklaşım kullanarak kodlamış, fakat 2017 yılında bu servis tamamen
derin öğrenme üzerinden işleyecek şekilde değiştirildi, kod satır sayısı
500,000’den 500’e indi!</p>
<p>DO bazlı tercüme sistemleri nasıl işler?</p>
<p><img src="seq_02.png" /></p>
<p>Not: kaynak cümlesi ters şekilde girilmiş, bu mimarı ilk teklif
edildiğinde bu şekilde yapılıyordu, fakat [3]’e göre her eğitim verisi
için dinamik şekilde yaratılabilen RNN hücreleri durumunda buna gerek
yok.</p>
<p>Servisin temelinde, üstte görüldüğü gibi bir RNN tabakası var. Fakat
bu RNN yapısında ilk (soldaki) bölüm kodlayıcı, ikinci (sağdaki) bölüm
kod çözücü olarak planlanmış. Eğitim verisinde kaynak ve hedef cümle
beraber, yanyana olarak hem girdi olarak veriliyor, ayrıca tercüme sonuç
cümlesi alınıp bir de etiket verisi olarak kullanılıyor, bir farkla,
etiketteki cümle zaman indisinde eğitimdeki sonuç cümlenin bir geri
kaydırılmış hali.</p>
<p>Kaynak, sonuç tercüme cümleleri farklı boylarda olabilir, hem aynı
eğitim noktası içinde birbirlerinden, hem de değişik eğitim noktalarında
kendilerinden bile farklı boyutlarda olabilirler, bu sebeple RNN öğe
sayıları dinamik şekilde, her eğitim verisine göre farklı olacak. Fakat
bu farklı boyutlar karışıklık yaratmıyor, çünkü tercüme için önemli olan
şey kodlayıcı bloktan kod çözücü bloğa geçen gizli konum.</p>
<p>Bu konuma daha önceki yazılarda <span
class="math inline">\(h\)</span> adı vermiştik. Eğitim süreci şöyle, tüm
cümleler + tüm kelimeler üzerinden bir sözlük oluştururuz, bu sözlüğe
göre her kelimeye bir tam sayı indis değeri atarız, sonra kelimeleri tam
sayılara çevirip gömme tabakasına veririz, bu tabaka reel sayı içeren
vektörlere dönüşür, ve eğitim ilerledikçe referans gömme matrisinde
kelimelerin temsil değerleri iyileşir. Bunlar otomatik oluyor tabii, biz
soldan sağa YSA’ya her eğitim veri noktasındaki kelimeleri teker teker
geçiyoruz, bir eğitim noktası için önce birinci kelimeyi ilk RNN
öğesine, oradan çıkan <span class="math inline">\(h\)</span>’yi ve
ikinci kelimeyi ikinci RNN öğesine, böyle devam ediyor.</p>
<p>Kodlayıcıdan çıkış olduğu anda (dikkat hala tek bir eğitim noktasını
işliyoruz) elimizde olan <span class="math inline">\(h\)</span>’nin özel
bir anlamı var. Üstteki mimariye göre bu gizli katman tüm kaynak cümleyi
temsil eden bir <span class="math inline">\(h\)</span>’dir. Başta pek
değildir ama zaman geçtikçe öyle olacaktır. <span
class="math inline">\(h\)</span> boyutu önceden planlanan şekilde, yani
cümleye göre küçülüp büyüyen bir şey değil. Neyse tabii ileri besleme
orada durmuyor, kod çözücüye devam ediliyor, burada kelimeler sonuç
tercümeden geliyor, şimdi onun kelimelerini almaya başlıyoruz ve
etikette bahsettiğimiz şekilde kaydırılan kelimelere tekabül edecek
şekilde eğitime devam ediyoruz, ve sağa en sona gelince bir eğitim
noktası ile işimiz bitiyor.</p>
<p>Hedef kelimeleri softmax olarak planlanmış, yani kod çözücüdeki RNN
öğeleri mümkün tüm kelimeler üzerinden bir olasılık vektörü üretiyor.
Gerçek dünya uygulamalarında bu yaklaşım külfetli olabilir, çünkü sözlük
çok büyük ise softmax boyutu tek bir kelime çıktısı için olasılıkları
temsil etmek için çok fazla boyutlu olmalıdır, burada performansı
iyileştirebilecek başka bazı yaklaşımlar var, ama kavramsal olarak
çıktının sanki her mümkün kelime üzerinden bir softmax olduğunu
düşünebiliriz.</p>
<p>Eğitim bittikten sonra hiç görülmemiş yeni test verisi için tercüme
nasıl yaparız? Biraz önce gördüğümüz gibi kaynak cümlenin kelimeleri
soldan sağa YSA’ya verilir, kod çözücüye geldiğimizde <span
class="math inline">\(h\)</span> ile beraber Go sembolü verilecektir, ve
bu sembol sonuç tercümede ilk kelimeyi üretir. Tercümenin ilk kelimesini
bu şekilde elde etmiş oluruz. Eğer eğitim iyi yapılmışsa derin YSA ilk
kelimeyi güzel bir şekilde üretecektir (daha doğrusu softmax tüm
kelimelerin olasılıklarını hesaplar, biz bu olasılıklara göre en olası
kelimeyi örnekleme yaparak alırız). Sonra bu üretilen kelimeyi alıp
alttan YSA’ya (artık kod çözücüde tabii) beslemeye devam ederiz, mesela
Go sonucu “içeri’’ kelimesi verilmiş, biz”içeri’’ kelimesini alttan
ikinci RNN öğesine veririz, bu bize üstten “girmesine’’ kelimesini
üretebilir, böyle devam ederiz.</p>
<p>Dikkat Etme Vektörü (Attention Vector)</p>
<p>Bazı yaklaşımlara göre kodlayıcı bloktan çıkan <span
class="math inline">\(h\)</span> bir cümleyi temsil etmek için yeterli
görülmüyor, kod çözücü bloğundaki RNN öğelerinden kaynak cümledeki tüm
kelimelere giden bir dikkat etme vektörü üzerinden bağlantı koyuluyor.
Detaylar [2]‘de bulunabilir. Alttaki örnekte İngilizce “I am a
student’‘, yani ben bir öğrenciyim cümlesinin Fransızca karşılığı “Je
suis etuidant’’ gösterilmiş.</p>
<p><img src="attention.jpg" /></p>
<p>Ayrıca RNN katmanı tek bir zincir olmayabilir, üst üste konulmuş
birkaç katmandan da oluşuyor olabilir. Resimde istiflenmiş iki RNN
seviyesi görüyoruz mesela.</p>
<p>Not: Bilgisayar ile söyleşi yapılmasını sağlayan chatbot teknolojisi
aslında üstteki tercüme teknolojisinin değişik bir kullanımı sadece.
Eğer kaynak ve sonuç cümleler aynı cümlenin iki farklı dildeki karşılığı
yerine iki kişi arasındaki konuşmalar olsaydı, YSA yapısı gömme
tabakası, cümleler alakası üzerinden “bir konuşmayı’’ öğrenmeye
başlardı.”Nasılsın’’ cümlesine “çok iyiyim’’ karşılığı veriliyor, bu iki
cümle ve onun gibi cümleleri üstteki teknikle eğitince yavaş yavaş YSA
nasıl karşılık vereceğini öğrenebilmeye başlıyor.</p>
<p>Örnek kod [1] alttadır, veri [4]’ten.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># translate.py</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, os</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>checkpoint_path <span class="op">=</span> <span class="st">&quot;/home/burak/Downloads/model.ckpt&quot;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> data_utils <span class="im">import</span> (</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    process_data,split_data,generate_epoch,generate_batch,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_cell(FLAGS, dropout, scope):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(scope):</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        rnn_cell_type <span class="op">=</span> tf.nn.rnn_cell.BasicLSTMCell</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        single_cell <span class="op">=</span> rnn_cell_type(FLAGS.num_hidden_units)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        single_cell <span class="op">=</span> tf.nn.rnn_cell.DropoutWrapper(single_cell,</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>            output_keep_prob<span class="op">=</span><span class="dv">1</span><span class="op">-</span>dropout)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        stacked_cell <span class="op">=</span> tf.nn.rnn_cell.MultiRNNCell(</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>            [single_cell] <span class="op">*</span> FLAGS.num_layers)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stacked_cell</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_inputs(FLAGS, input_data, vocab_size, scope):</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(scope, reuse<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        W_input <span class="op">=</span> tf.get_variable(<span class="st">&quot;W_input&quot;</span>,</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>            [vocab_size, FLAGS.num_hidden_units])</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># embeddings will be shape [input_data dimensions, num_hidden units]</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    embeddings <span class="op">=</span> tf.nn.embedding_lookup(W_input, input_data)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> embeddings</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rnn_softmax(FLAGS, outputs, scope):</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.variable_scope(scope, reuse<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        W_softmax <span class="op">=</span> tf.get_variable(<span class="st">&quot;W_softmax&quot;</span>,</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>            [FLAGS.num_hidden_units, FLAGS.sp_vocab_size])</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        b_softmax <span class="op">=</span> tf.get_variable(<span class="st">&quot;b_softmax&quot;</span>, [FLAGS.sp_vocab_size])</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> tf.matmul(outputs, W_softmax) <span class="op">+</span> b_softmax</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logits</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> model(<span class="bu">object</span>):</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, FLAGS):</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_inputs <span class="op">=</span> tf.placeholder(tf.int32, shape<span class="op">=</span>[<span class="va">None</span>, <span class="va">None</span>],</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">&#39;encoder_inputs&#39;</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_inputs <span class="op">=</span> tf.placeholder(tf.int32, shape<span class="op">=</span>[<span class="va">None</span>, <span class="va">None</span>],</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">&#39;decoder_inputs&#39;</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> tf.placeholder(tf.int32, shape<span class="op">=</span>[<span class="va">None</span>, <span class="va">None</span>],</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">&#39;targets&#39;</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.en_seq_lens <span class="op">=</span> tf.placeholder(tf.int32, shape<span class="op">=</span>[<span class="va">None</span>, ],</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">&quot;en_seq_lens&quot;</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sp_seq_lens <span class="op">=</span> tf.placeholder(tf.int32, shape<span class="op">=</span>[<span class="va">None</span>, ],</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>            name<span class="op">=</span><span class="st">&quot;sp_seq_lens&quot;</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> tf.placeholder(tf.float32)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.variable_scope(<span class="st">&#39;encoder&#39;</span>) <span class="im">as</span> scope:</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Encoder RNN cell</span></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder_stacked_cell <span class="op">=</span> rnn_cell(FLAGS, <span class="va">self</span>.dropout,</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>                scope<span class="op">=</span>scope)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Embed encoder inputs</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>            W_input <span class="op">=</span> tf.get_variable(<span class="st">&quot;W_input&quot;</span>,</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>                [FLAGS.en_vocab_size, FLAGS.num_hidden_units])</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.embedded_encoder_inputs <span class="op">=</span> rnn_inputs(FLAGS,</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.encoder_inputs, FLAGS.en_vocab_size, scope<span class="op">=</span>scope)</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Outputs from encoder RNN</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.all_encoder_outputs, <span class="va">self</span>.encoder_state <span class="op">=</span> tf.nn.dynamic_rnn(</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>                cell<span class="op">=</span><span class="va">self</span>.encoder_stacked_cell,</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>                inputs<span class="op">=</span><span class="va">self</span>.embedded_encoder_inputs,</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>                sequence_length<span class="op">=</span><span class="va">self</span>.en_seq_lens, time_major<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>                dtype<span class="op">=</span>tf.float32)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.variable_scope(<span class="st">&#39;decoder&#39;</span>) <span class="im">as</span> scope:</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Initial state is last relevant state from encoder</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.decoder_initial_state <span class="op">=</span> <span class="va">self</span>.encoder_state</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Decoder RNN cell</span></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.decoder_stacked_cell <span class="op">=</span> rnn_cell(FLAGS, <span class="va">self</span>.dropout,</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>                scope<span class="op">=</span>scope)</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Embed decoder RNN inputs</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>            W_input <span class="op">=</span> tf.get_variable(<span class="st">&quot;W_input&quot;</span>,</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>                [FLAGS.sp_vocab_size, FLAGS.num_hidden_units])</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.embedded_decoder_inputs <span class="op">=</span> rnn_inputs(FLAGS, <span class="va">self</span>.decoder_inputs,</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>                FLAGS.sp_vocab_size, scope<span class="op">=</span>scope)</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Outputs from encoder RNN</span></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.all_decoder_outputs, <span class="va">self</span>.decoder_state <span class="op">=</span> tf.nn.dynamic_rnn(</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>                cell<span class="op">=</span><span class="va">self</span>.decoder_stacked_cell,</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>                inputs<span class="op">=</span><span class="va">self</span>.embedded_decoder_inputs,</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>                sequence_length<span class="op">=</span><span class="va">self</span>.sp_seq_lens, time_major<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>                initial_state<span class="op">=</span><span class="va">self</span>.decoder_initial_state)</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Softmax on decoder RNN outputs</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>            W_softmax <span class="op">=</span> tf.get_variable(<span class="st">&quot;W_softmax&quot;</span>,</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>                [FLAGS.num_hidden_units, FLAGS.sp_vocab_size])</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>            b_softmax <span class="op">=</span> tf.get_variable(<span class="st">&quot;b_softmax&quot;</span>, [FLAGS.sp_vocab_size])</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Logits</span></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.decoder_outputs_flat <span class="op">=</span> tf.reshape(<span class="va">self</span>.all_decoder_outputs,</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>                [<span class="op">-</span><span class="dv">1</span>, FLAGS.num_hidden_units])</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.logits_flat <span class="op">=</span> rnn_softmax(FLAGS, <span class="va">self</span>.decoder_outputs_flat,</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>                scope<span class="op">=</span>scope)</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Loss with masking</span></span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>            targets_flat <span class="op">=</span> tf.reshape(<span class="va">self</span>.targets, [<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>            losses_flat <span class="op">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>                logits<span class="op">=</span><span class="va">self</span>.logits_flat, labels<span class="op">=</span>targets_flat</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> tf.sign(tf.to_float(targets_flat))</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>            masked_losses <span class="op">=</span> mask <span class="op">*</span> losses_flat</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>            masked_losses <span class="op">=</span> tf.reshape(masked_losses,  tf.shape(<span class="va">self</span>.targets))</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.loss <span class="op">=</span> tf.reduce_mean(</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>                tf.reduce_sum(masked_losses, reduction_indices<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Optimization</span></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> tf.Variable(<span class="fl">0.0</span>, trainable<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>        trainable_vars <span class="op">=</span> tf.trainable_variables()</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># clip the gradient to avoid vanishing or blowing up gradients</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>        grads, _ <span class="op">=</span> tf.clip_by_global_norm(</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>            tf.gradients(<span class="va">self</span>.loss, trainable_vars), FLAGS.max_gradient_norm)</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> tf.train.AdamOptimizer(<span class="va">self</span>.lr)</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.train_optimizer <span class="op">=</span> optimizer.apply_gradients(</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>            <span class="bu">zip</span>(grads, trainable_vars))</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>        <span class="co">#self.saver = tf.train.Saver(tf.all_variables())</span></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, sess, FLAGS, batch_encoder_inputs, batch_decoder_inputs,</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>        batch_targets, batch_en_seq_lens, batch_sp_seq_lens, dropout):</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>        input_feed <span class="op">=</span> {<span class="va">self</span>.encoder_inputs: batch_encoder_inputs,</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.decoder_inputs: batch_decoder_inputs,</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.targets: batch_targets,</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.en_seq_lens: batch_en_seq_lens,</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sp_seq_lens: batch_sp_seq_lens,</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dropout: dropout}</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>        output_feed <span class="op">=</span> [<span class="va">self</span>.loss, <span class="va">self</span>.train_optimizer]</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> sess.run(output_feed, input_feed)</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs[<span class="dv">0</span>], outputs[<span class="dv">1</span>]</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> parameters(<span class="bu">object</span>):</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_en_vocab_size <span class="op">=</span> <span class="dv">30000</span></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_sp_vocab_size <span class="op">=</span> <span class="dv">30000</span></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_epochs <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.batch_size <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_hidden_units <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate_decay_factor <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.max_gradient_norm <span class="op">=</span> <span class="fl">5.0</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(sess, FLAGS):</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>    tf_model <span class="op">=</span> model(FLAGS)</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> <span class="st">&quot;Created a new model&quot;</span></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>    sess.run(tf.initialize_all_variables())</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf_model</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> restore_model(sess, FLAGS):</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>    tf_model <span class="op">=</span> model(FLAGS)</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>    tf_model.saver.restore(sess, checkpoint_path) </span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf_model</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(FLAGS):</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load the data</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>    en_token_ids, en_seq_lens, en_vocab_dict, en_rev_vocab_dict <span class="op">=</span> <span class="op">\</span></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>        process_data(<span class="st">&#39;data/tst2013.en&#39;</span>, max_vocab_size<span class="op">=</span><span class="dv">30000</span>, target_lang<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>    sp_token_ids, sp_seq_lens, sp_vocab_dict, sp_rev_vocab_dict <span class="op">=</span> <span class="op">\</span></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>        process_data(<span class="st">&#39;data/tst2013.tr&#39;</span>, max_vocab_size<span class="op">=</span><span class="dv">30000</span>, target_lang<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Split into train and validation sets</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>    train_encoder_inputs, train_decoder_inputs, train_targets, <span class="op">\</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>        train_en_seq_lens, train_sp_seq_len, <span class="op">\</span></span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>        valid_encoder_inputs, valid_decoder_inputs, valid_targets, <span class="op">\</span></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>        valid_en_seq_lens, valid_sp_seq_len <span class="op">=</span> <span class="op">\</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>        split_data(en_token_ids, sp_token_ids, en_seq_lens, sp_seq_lens,</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>            train_ratio<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="bu">open</span>(<span class="st">&#39;data/vocab_en.pkl&#39;</span>, <span class="st">&#39;wb&#39;</span>)</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>    pickle.dump(en_vocab_dict, output)</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>    output.close()</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> <span class="bu">open</span>(<span class="st">&#39;data/vocab_sp.pkl&#39;</span>, <span class="st">&#39;wb&#39;</span>)</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>    pickle.dump(sp_vocab_dict, output)</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>    output.close()</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update parameters</span></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>    FLAGS.en_vocab_size <span class="op">=</span> <span class="bu">len</span>(en_vocab_dict)</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>    FLAGS.sp_vocab_size <span class="op">=</span> <span class="bu">len</span>(sp_vocab_dict)</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> <span class="st">&#39;len(en_vocab_dict)&#39;</span>, <span class="bu">len</span>(en_vocab_dict)</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> <span class="st">&#39;len(sp_vocab_dict)&#39;</span>, <span class="bu">len</span>(sp_vocab_dict)</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start session</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create new model or load old one</span></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>        f <span class="op">=</span> checkpoint_path <span class="op">+</span> <span class="st">&quot;.index&quot;</span></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span> f</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>        exit()</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> os.path.isfile(f):</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> restore_model(sess)</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> create_model(sess, FLAGS)</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training begins</span></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">=</span> []</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch_num, epoch <span class="kw">in</span> <span class="bu">enumerate</span>(generate_epoch(train_encoder_inputs,</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>            train_decoder_inputs, train_targets,</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>            train_en_seq_lens, train_sp_seq_len,</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a>            FLAGS.num_epochs, FLAGS.batch_size)):</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span> <span class="st">&quot;EPOCH: </span><span class="sc">%i</span><span class="st">&quot;</span> <span class="op">%</span> (epoch_num)</span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Decay learning rate</span></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>            sess.run(tf.assign(model.lr, FLAGS.learning_rate <span class="op">*</span> <span class="op">\</span></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>                (FLAGS.learning_rate_decay_factor <span class="op">**</span> epoch_num)))</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>            batch_loss <span class="op">=</span> []</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> batch_num, (batch_encoder_inputs, batch_decoder_inputs,</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a>                batch_targets, batch_en_seq_lens,</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a>                batch_sp_seq_lens) <span class="kw">in</span> <span class="bu">enumerate</span>(epoch):</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>                loss, _ <span class="op">=</span> model.step(sess, FLAGS,</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>                    batch_encoder_inputs, batch_decoder_inputs, batch_targets,</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>                    batch_en_seq_lens, batch_sp_seq_lens,</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>                    FLAGS.dropout)</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span> loss</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>                batch_loss.append(loss)</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span> <span class="st">&#39;mean: &#39;</span>, np.mean(batch_loss)</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span> <span class="st">&quot;Saving the model.&quot;</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>            model.saver.save(sess, checkpoint_path)</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>    FLAGS <span class="op">=</span> parameters()</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>    train(FLAGS)</span></code></pre></div>
<p>Kaynaklar</p>
<p>[1] Mohandas, <em>The Neural Perspective, RNN - Part 3 - Encoder -
Decoder</em>,<a
href="https://theneuralperspective.com/2016/11/20/recurrent-neural-networks-rnn-part-3-encoder-decoder/">https://theneuralperspective.com/2016/11/20/recurrent-neural-networks-rnn-part-3-encoder-decoder/</a></p>
<p>[2] TensorFlow, <em>TensorFlow Neural Machine Translation
Tutorial</em>, <a
href="https://github.com/tensorflow/nmt">https://github.com/tensorflow/nmt</a></p>
<p>[3] Géron, <em>Hands-On Machine Learning with Scikit-Learn and
TensorFlow</em></p>
<p>[4] ManyThings Verisi, İngilizce-Türkçe, <em>Tab-delimited Bilingual
Sentence Pairs</em>, <a
href="https://drive.google.com/uc?export=view&amp;id=16fsAVPaPgp9gW9mdLmKz0OOR9lQ1M9WU">https://drive.google.com/uc?export=view&amp;id=16fsAVPaPgp9gW9mdLmKz0OOR9lQ1M9WU</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
