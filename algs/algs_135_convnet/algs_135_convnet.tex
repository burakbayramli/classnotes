\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Evriþimsel Aðlar, Derin Öðrenim (Convolutional Nets -Convnet-, Deep Learning)

Convnet'ler YSA'lara yeni bazý özellikler ekledi. Öncelikle gizli katman
artýk ikiden daha fazla derinliðe gidebiliyor. Diðer bir ek, mesela veriye
ilk dokunan katmaný sadece evriþim operasyonu için kullanmak. 

Evriþim boyutu önceden belli bir matrisi tüm veri üzerinde kaydýrarak sonuç
deðerleri kaydetmekten ibaret, görüntü iþlemede yapýlan çoðu filtreleme
iþlemi bir evriþim operasyonu. Mesela 2 x 2 boyutlu bir filtre matrisini
tüm veri üzerinde kaydýrýrýz, her kaydýrma sýrasýnda o bölgede filtre
matrisini deðerler ile çarparýz, sonucu hatýrlarýz, çarpýlan bölgeye
tekabül eden sonuç matrisinde sonucu yazarýz. Evriþim matrisi 

$$ A = \left[\begin{array}{rrr}
1 &  0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 1
\end{array}\right]$$

olsun, görüntü (image) üzerinde çarpým iþlemini adým adým gösterelim,
sonuç evriþtirilmiþ özellik (convolved feature) içinde,

\includegraphics[width=9em]{conv-0.png}
\hspace{1cm}
\includegraphics[width=9em]{conv-1.png}
\hspace{1cm}
\includegraphics[width=9em]{conv-2.png}

\includegraphics[width=8em]{conv-3.png}
\hspace{1cm}
\includegraphics[width=8em]{conv-4.png}
\hspace{1cm}
\includegraphics[width=8em]{conv-5.png}

\includegraphics[width=8em]{conv-6.png}
\hspace{1cm}
\includegraphics[width=8em]{conv-7.png}
\hspace{1cm}
\includegraphics[width=8em]{conv-8.png}

Üstteki örnekte kaydýrma (stride) sayýsý 1. Kaydýrma sayýsý evriþim
matrisini uyguladýktan sonra bir sonraki iþlem için kaç hücre yana
kayacaðýmýzý kontrol eder. Evriþim matrisinin boyutunun, kaydýrma sayýsýnýn
sonuç matrisin boyutu üzerinde etkileri olacaktýr. 5 x 5 matrisi üzerinden
1 kaydýrma ile 3 x 3 evriþim uygulayýnca 3 x 3 boyutunda bir sonuç elde
ettik. 

Görüntü iþlemede yatay, dikey çizgileri daha belirgin hale getiren, ortaya
çýkartan türden, bilinen filtreler vardýr, diðer türler de mevcuttur.  Fakat
derin öðrenim bu evriþim matrisinin içeriðini, ayrýca onu diðer katmanlara
baðlayan aðýrlýklarý da otomatik olarak öðrenir!  Çünkü eðer çizgileri
ortaya çýkartmak öðrenme iþleminin bütününe fayda getiriyorsa öðrenme
süreci sýrasýnda evriþim matrisinin deðerleri o deðerlere
evrilir. Evriþimleri bu þekilde aðda kullanmanýn convnet'lerin alt
katmanlarýndaki hata düzeltme iþlemini daha rahatlaþtýrdýðý keþfedildi.

Bir yenilik aktivasyon için ReLu (doðrultan lineer ünite -rectified linear
unit-) kullanmak, bir diðer katman ``aþaðý örnekleme (downsampling)'' yapan
katman, mesela $2 \times 2$ içindeki bir pencere içine düþen öðelerin
maksimumunu almak. Bir baþkasý ``veri atma (dropout)'' katmaný, veri içinde
bir katmandan gelen baðlantýlarýn bir kýsmý (dýþarýdan ayarlanabilir)
rasgele þekilde yoksayýlýyor, bu durumda model elde kalanlar ile uydurma
iþlemini yapmaya uðraþacak, böylece modelde aþýrý uygunluk (överfitting)
problemlerinden kaçýnýlmýþ olunuyor, potansiyel olarak daha saðlam (robust)
bir modelin ortaya çýkmasý saðlanýyor.

\includegraphics[height=6cm]{convnet_06.png}

Böyle gide gide bir derin að yapýsý ortaya çýkartmýþ oluyoruz. 

Not: Çoðunlukla evriþim tabakasýndan sonra ReLu aktviyasyonuna baðlantý
yapýlýr, çünkü eðer aktivasyon olmasa, evriþim sonuçta aðýrlýklarla çarpým
ve toplam iþlemidir, ve bu bir lineer iþlemdir, o zaman tüm NN'in iþlemi
ardý ardýna matrislerin çarpýmý olarak ta görülebilirdi, ve bu bir lineer
iþlem olurdu, o zaman NN gayri-lineerligi modelleyemezdi.

TensorFlow ile Evriþim

Þimdi TF kullanarak evriþim yapalým, iki resim üzerinde örneði görelim,

\begin{minted}[fontsize=\footnotesize]{python}
import numpy as np
import tensorflow as tf
from sklearn.datasets import load_sample_images

# Load sample images
dataset = np.array(load_sample_images().images, dtype=np.float32)
batch_size, height, width, channels = dataset.shape
print 'veri tensor boyutu', dataset.shape
plt.imshow(dataset[0,:,:,:] / 255.) 
plt.savefig('conv-9.png')
plt.imshow(dataset[1,:,:,:] / 255.) 
plt.savefig('conv-10.png')
\end{minted}

\begin{verbatim}
veri tensor boyutu (2, 427, 640, 3)
\end{verbatim}

\includegraphics[width=20em]{conv-9.png}
\includegraphics[width=20em]{conv-10.png}

Bu iki resmi bir ``tensor'' yani çok boyutlu matris içine koyduk, resmin
boyutlarý 640 x 427, eðer tek bir resme bakýyor olsaydýk (640,427) boyutlu
tek bir matris ile iþ yapabilirdik, fakat TF ardý ardýna gelen verileri de
bir tensorun ayrý boyutlarý olarak kabul edebiliyor. Ayrýca tensorun son
boyutu renk kanallarý, üç tane var, RGB renk sistemý için kýrmýzý (red),
yeþil (green) ve mavi (blue) için. Böylece üstteki tensor boyutlarýný elde
ettik.

Evriþim için 4 tane filtre tanýmlayacaðýz. Bu filtrelerin içeriðinin elle
burada tanýmlayacaðýz, yapay öðrenme baðlamýnda bu içerik eðitim sýrasýnda
otomatik olarak öðrenilir, þimdi örnek amaçlý olarak filtre deðerlerini biz
atýyoruz. Ayrýca istediðimiz kadar filtre tanýmlayabilirdik, yapay
öðrenmede istenen o'dur ki bu filtreler doðru deðerlere doðru evrilir, her
filtre görüntünün deðiþik yerlerine, biçimlerine odaklanmaya
baþlayabilirler. 

\begin{minted}[fontsize=\footnotesize]{python}
in_channels = channels
out_channels = 4
W = np.zeros(shape=(7, 7, in_channels, out_channels), dtype=np.float32)
W[:, 3, :, 0] = 1  # yatay cizgi
W[3, :, :, 1] = 1  # dikey cizgi
W[5, :, :, 2] = 1  
W[2, :, :, 3] = 1  

X = tf.placeholder(tf.float32, shape=(None, height, width, channels))
convolution = tf.nn.conv2d(X, W, strides=[1,2,2,1], padding="SAME")
with tf.Session() as sess:
    output = sess.run(convolution, feed_dict={X: dataset})
    print u'evriþimden gelen tensor', output.shape
\end{minted}

\begin{verbatim}
evriþimden gelen tensor (2, 214, 320, 4)
\end{verbatim}

Filtrelerin ilkinde mesela sadece yatay bir çizgi var, bu filtre yatay
piksellerin bir ortalamasýný alýyor olacak, vs. Parametrelerden kaydýrma
yatay 2 dikey olarak tanýmlý, o zaman her imaj kabaca yarýsýna inecek,
dolgu (padding) için \verb!SAME! diyerek dolgu yap demiþiz, yani kaydýrma
olmasaydý giren çýkan tensor ayný olurdu. Peki 3 tane ayrý kanal bilgisine
ne oldu? Görüntü boyutlar azalarak (320,214) oldu, 4 tane filtre çýkýþý
var, 2 tane imaj. Kanallar üzerinden bir toplam alýndý, yani ayrý kanal
bilgisi artýk yok. Bu aslýnda istenen bir þey çünkü bir görüntünün R,G,B
kanallarý üzerinde ayrý ayrý irdeleme yapmak istemeyiz, sýnýflama, karar
mekanizmasýný tüm imaj üzerinden iþletmek isteriz.

Sonuçlar altta, iki tane filtre sonucu örnek olarak seçildi.

\begin{minted}[fontsize=\footnotesize]{python}
def plot_image(image):
    plt.imshow(image, cmap="gray", interpolation="nearest")
    plt.axis("off")
    
image_index = 0; feature_map_index = 2
plot_image(output[image_index, :, :, feature_map_index])
plt.savefig('conv-out-%d-%d.png' % (image_index,feature_map_index))

image_index = 1; feature_map_index = 1
plot_image(output[image_index, :, :, feature_map_index])
plt.savefig('conv-out-%d-%d.png' % (image_index,feature_map_index))
\end{minted}

\includegraphics[width=20em]{conv-out-0-2.png}
\includegraphics[width=20em]{conv-out-1-1.png}

Cok kanallý durum için açýk olmadýysa bir daha vurgulayalým, evriþim
katmaný evriþim operatörünü imaj ya da baþka bir tensor üzerinde
gezdirirken üzerinde olduðu tüm girdi tabakalarýný, tüm kanallarý
kapsayacak þekilde, çarpýp sonucu tek bir öðeye götürür. Bu çarpým için
herhalde kanallar bir þekilde düzleþtirilip aðýrlýklara eþleniyor. Neyse,
filtreyi kaydýrdýkça çarpým sonucu çýktýda farklý öðeler elde edilir, ve bu
bize tek boyutlu bir sonuç kesiti verir. Eðer birden fazla evriþim filtresi
uygularsak onlarýn sonuçlarý farklý sonuç tabakalarý olarak diðer
tabakalarýn üzerinde istiflenir.

\includegraphics[width=20em]{conv-11.png}

Mesela altta 20x30 boyutlu ve 3 kanallý bir tensor girdimiz olsun, bu
tensorlardan kaç tane olduðunu þimdilik tanýmlamadýk (mini girdi yýðýnýnda
yani). Þimdi girdi üzerinde 16 tane filtre gezdiriyoruz, ve 20x30x16
boyutlu çýktý elde ediyoruz.

\begin{minted}[fontsize=\footnotesize]{python}
import tensorflow as tf
tf.reset_default_graph()
input = tf.placeholder(tf.float32, [None, 20, 30, 3])
conv_layer = tf.layers.conv2d(inputs=input, filters=16, 
                              kernel_size=(2,2), padding='same')
print conv_layer
\end{minted}

\begin{verbatim}
Tensor("conv2d/BiasAdd:0", shape=(?, 20, 30, 16), dtype=float32)
\end{verbatim}

MNIST

Altta MNIST verisini iþleyebilen mimarýnýn TF olmadan pür Python kullanan
kodunu görüyoruz. Bu yapý ünlü YSA araþtýrmacýsý Yann LeCun'un LeNet adý
verilen mimarisi. Veri [5]'ten indirilebilir.

\inputminted[fontsize=\footnotesize]{python}{convnet2.py}

\begin{verbatim}
Loading training data...
KNN
0.82
    Epoch      |    Train err  |   Test error  
              0|           0.92|           0.93
              1|           0.89|            0.9
              2|          0.798|           0.82
              3|          0.767|           0.74
              4|          0.727|           0.67
              5|          0.678|           0.71
              6|          0.627|           0.65
              7|          0.563|           0.56
              8|          0.503|           0.53
              9|          0.445|           0.49
             10|          0.418|           0.45
             11|          0.357|           0.41
             12|           0.34|           0.39
             13|          0.322|           0.36
             14|          0.287|           0.32
             15|          0.258|           0.29
             16|          0.261|           0.33
             17|          0.232|           0.27
             18|          0.216|           0.25
             19|          0.206|           0.25
             20|          0.188|           0.22
             21|          0.171|           0.23
             22|          0.164|           0.23
             23|          0.175|           0.25
             24|          0.121|           0.21
             25|          0.152|           0.19
             26|          0.169|           0.21
             27|          0.145|           0.18
             28|          0.119|           0.22
             29|          0.099|           0.18
             30|          0.073|           0.16
             31|          0.083|            0.2
             32|          0.066|           0.17
             33|           0.06|           0.18
             34|          0.052|           0.18
             35|          0.042|           0.18
             36|          0.034|           0.16
             37|          0.034|           0.18
             38|          0.027|           0.17
             39|          0.023|           0.15
             40|          0.026|           0.18
             41|          0.017|           0.16
             42|          0.012|           0.16
             43|          0.011|           0.14
             44|          0.011|           0.12
             45|          0.008|           0.14
             46|          0.009|           0.13
             47|          0.007|           0.15
             48|          0.006|           0.12
             49|          0.005|           0.15
\end{verbatim}

Tarih

Convnet'lerin son zamanlarda baþarýlý olmaya baþlamasýnda bazý etkenler
þunlar: en önemlisi araþtýrmacýlarýn dýþbükey olmayan kayýp
fonksiyonlarýndan (non-convex loss functions) korkmadan (!) onlarýn
minimumunu rasgele gradyan iniþi üzerinden bulabilmeye
baþlamalarý. Convnet, HMM, Gaussian karýþýmlar gibi pek çok alanda dýþbükey
olmayan bir durum vardýr, hatta bir bakýma ciddi her yapay öðrenim alanýnda
bu durum ortaya çýkar. Bu optimizasyon problemi rasgele gradyan iniþi ile
çözülmeye baþlandý. Gerçi rasgele gradyan iniþi çok basit bir yöntemdir,
fakat araþtýrmacýlar yýllarca bu yöntemi dýþbükey olmayan yerlerde
kullanmadýlar çünkü yakýnsama (convergence) garantileri teorik olarak
mümkün deðildi. Kýyasla bir dýþbükey fonksiyonun minimumuna varmak doðru
yöntemler ile garantidir. Fakat son araþtýrmalara göre rasgele gradyan
iniþi ile optimizasyon eðitim verisi üzerinden yakýnsama garantisi olmasa
bile test verisi üzerinde bazý garantilerin olduðu ortaya çýktý, ki test
skoru eðitim skorundan daha önemli.

Bir diðer geliþme að yapýsýnda gizli katmandaki nöron sayýsýný
geniþletince, minimum noktasýnýn pek çok yerde ortaya çýkabilmesi, ve bu
noktalarýn birbiriyle aþaðý yukarý ayný olmasý. Yani gradyan iniþi bu
noktalardan birini bulduðunda iþ bitmiþ sayýlabiliyor. Ayrýca çok boyut
olunca yerel minima'da takýlýp kalmadan bir yan boyuta atlayýp önümüzdeki
tepenin etrafýndan dolaþabilme þansý ortaya çýkýyor [4].

Ek bir geliþme herhangi bir yazýlým fonksiyonun türevinin alýnabilmesini
saðlayan otomatik türev alma (automatic differentiation) tekniðinin
yaygýnlaþmasý, ki bu teknik çetrefil nöron fonksiyonlarý içerebilen
convnet'ler için faydalý oldu. Not: dikkat otomatik türev ile {\em her
  türlü} fonksiyon hatta \verb!if, while! komutlarý içeren bir kod
parçasýnýn bile türevi alýnabiliyor (bkz [2] yazýsý). Türevler önemli çünkü
gradyan iniþi minimuma gidebilmek için türev kullanmalý. Üstteki kodda
dikkat çekmiþ olabilir, sinir aðýnda aslýnda yaptýðýmýz tek hesap ileri
doðru beslemeli (feed-forward) olan hesap; her veri noktasý için aðýn
baþýndan baþlayarak girdi / iþlem / sonuçlarý ardý ardýna bir sonraki
katmana aktarýyoruz, otomatik türev pek çok katmanlý bu iþlemlerin türevini
alarak bize o gidiþin yaptýðý tahminlerin hatasýný düzeltmek için gidilmesi
gereken gradyan yönünü veriyor.

Bazý negatifler: Convnet'lerin eðitimi hýzlý deðildir, ciddi kullanýmlar
için GPU, ya da paralel CPU kullanmak þart. Ayrýca convnet'lerin bir fark
yaratmasý için oldukça çok veriye ihtiyaçlarý var, normal ölçek veri
ortamýnda diðer yaklaþýmlar da convnet'ten çok daha kötü deðiller. Bir
diðeri bir YSA'nýn, yapý olarak, istatistiki bir anlamýnýn olmamasý: Bir að
yapýsý var, fakat bu aðýn tamamýnýn olasýlýksal olarak irdelenmesi mümkün
deðil. Kýyasla bir GMM bir olasýlýðý temsil eder, bu sebeple yaptýðý
hesaplarýn mesela güven aralýðýný hesaplamak mümkündür. Diðer yanda mesela
týbbi bir uygulama baðlamýnda YSA bir tahmin yapýnca bundan ``ne kadar emin
olduðu'' sorusunun direk cevabý alýnamýyor. Bu alanda araþtýrmalar var
tabii, ilginç bazý buluþlar bunu mümkün kýlabilir.

Kaynaklar

[2] Bayramli, Bilgisayar Bilim, {\em Otomatik Türev}

[3] {\em Machine learning algorithms}, https://github.com/rushter/MLAlgorithms

[4] LeCun, {\em Who is Afraid of Non-Convex Loss Functions?}, \url{https://youtu.be/8zdo6cnCW2w}

[5] LeCun, {\em MNIST Verisi}, \url{http://yann.lecun.com/exdb/mnist}


\end{document}


