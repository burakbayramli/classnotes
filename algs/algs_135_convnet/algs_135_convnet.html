<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Evrişimsel Ağlar, Derin Öğrenim (Convolutional Nets -Convnet-, Deep Learning)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1
id="evrişimsel-ağlar-derin-öğrenim-convolutional-nets--convnet--deep-learning">Evrişimsel
Ağlar, Derin Öğrenim (Convolutional Nets -Convnet-, Deep Learning)</h1>
<p>Convnet’ler YSA’lara yeni bazı özellikler ekledi. Öncelikle gizli
katman artık ikiden daha fazla derinliğe gidebiliyor. Diğer bir ek,
mesela veriye ilk dokunan katmanı sadece evrişim operasyonu için
kullanmak.</p>
<p>Evrişim boyutu önceden belli bir matrisi tüm veri üzerinde kaydırarak
sonuç değerleri kaydetmekten ibaret, görüntü işlemede yapılan çoğu
filtreleme işlemi bir evrişim operasyonu. Mesela 2 x 2 boyutlu bir
filtre matrisini tüm veri üzerinde kaydırırız, her kaydırma sırasında o
bölgede filtre matrisini değerler ile çarparız, sonucu hatırlarız,
çarpılan bölgeye tekabül eden sonuç matrisinde sonucu yazarız. Evrişim
matrisi</p>
<p><span class="math display">\[ A = \left[\begin{array}{rrr}
1 &amp;  0 &amp; 1 \\ 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 1
\end{array}\right]\]</span></p>
<p>olsun, görüntü (image) üzerinde çarpım işlemini adım adım gösterelim,
sonuç evriştirilmiş özellik (convolved feature) içinde,</p>
<p><img src="conv-0.png" /></p>
<p><img src="conv-1.png" /></p>
<p><img src="conv-2.png" /></p>
<p><img src="conv-3.png" /></p>
<p><img src="conv-4.png" /></p>
<p><img src="conv-5.png" /></p>
<p><img src="conv-6.png" /></p>
<p><img src="conv-7.png" /></p>
<p><img src="conv-8.png" /></p>
<p>Üstteki örnekte kaydırma (stride) sayısı 1. Kaydırma sayısı evrişim
matrisini uyguladıktan sonra bir sonraki işlem için kaç hücre yana
kayacağımızı kontrol eder. Evrişim matrisinin boyutunun, kaydırma
sayısının sonuç matrisin boyutu üzerinde etkileri olacaktır. 5 x 5
matrisi üzerinden 1 kaydırma ile 3 x 3 evrişim uygulayınca 3 x 3
boyutunda bir sonuç elde ettik.</p>
<p>Görüntü işlemede yatay, dikey çizgileri daha belirgin hale getiren,
ortaya çıkartan türden, bilinen filtreler vardır, diğer türler de
mevcuttur. Fakat derin öğrenim bu evrişim matrisinin içeriğini, ayrıca
onu diğer katmanlara bağlayan ağırlıkları da otomatik olarak öğrenir!
Çünkü eğer çizgileri ortaya çıkartmak öğrenme işleminin bütününe fayda
getiriyorsa öğrenme süreci sırasında evrişim matrisinin değerleri o
değerlere evrilir. Evrişimleri bu şekilde ağda kullanmanın convnet’lerin
alt katmanlarındaki hata düzeltme işlemini daha rahatlaştırdığı
keşfedildi.</p>
<p>Bir yenilik aktivasyon için ReLu (doğrultan lineer ünite -rectified
linear unit-) kullanmak, bir diğer katman “aşağı örnekleme
(downsampling)’’ yapan katman, mesela <span class="math inline">\(2
\times 2\)</span> içindeki bir pencere içine düşen öğelerin maksimumunu
almak. Bir başkası”veri atma (dropout)’’ katmanı, veri içinde bir
katmandan gelen bağlantıların bir kısmı (dışarıdan ayarlanabilir)
rasgele şekilde yoksayılıyor, bu durumda model elde kalanlar ile uydurma
işlemini yapmaya uğraşacak, böylece modelde aşırı uygunluk (överfitting)
problemlerinden kaçınılmış olunuyor, potansiyel olarak daha sağlam
(robust) bir modelin ortaya çıkması sağlanıyor.</p>
<p><img src="convnet_06.png" /></p>
<p>Böyle gide gide bir derin ağ yapısı ortaya çıkartmış oluyoruz.</p>
<p>Not: Çoğunlukla evrişim tabakasından sonra ReLu aktviyasyonuna
bağlantı yapılır, çünkü eğer aktivasyon olmasa, evrişim sonuçta
ağırlıklarla çarpım ve toplam işlemidir, ve bu bir lineer işlemdir, o
zaman tüm NN’in işlemi ardı ardına matrislerin çarpımı olarak ta
görülebilirdi, ve bu bir lineer işlem olurdu, o zaman NN
gayri-lineerligi modelleyemezdi.</p>
<p>TensorFlow ile Evrişim</p>
<p>Şimdi TF kullanarak evrişim yapalım, iki resim üzerinde örneği
görelim,</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tf.__version__)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_sample_images</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Disable TensorFlow 2.x eager execution</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>tf.compat.v1.disable_eager_execution()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load sample images</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> np.array(load_sample_images().images, dtype<span class="op">=</span>np.float32)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>batch_size, height, width, channels <span class="op">=</span> dataset.shape</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;veri tensor boyutu&#39;</span>, dataset.shape)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.imshow(dataset[<span class="dv">0</span>, :, :, :] <span class="op">/</span> <span class="fl">255.</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;conv-9.png&#39;</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.imshow(dataset[<span class="dv">1</span>, :, :, :] <span class="op">/</span> <span class="fl">255.</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;conv-10.png&#39;</span>)</span></code></pre></div>
<pre class="text"><code>2.19.0
veri tensor boyutu (2, 427, 640, 3)</code></pre>
<p><img src="conv-9.png" /></p>
<p><img src="conv-10.png" /></p>
<p>Bu iki resmi bir “tensor’’ yani çok boyutlu matris içine koyduk,
resmin boyutları 640 x 427, eğer tek bir resme bakıyor olsaydık
(640,427) boyutlu tek bir matris ile iş yapabilirdik, fakat TF ardı
ardına gelen verileri de bir tensorun ayrı boyutları olarak kabul
edebiliyor. Ayrıca tensorun son boyutu renk kanalları, üç tane var, RGB
renk sistemı için kırmızı (red), yeşil (green) ve mavi (blue) için.
Böylece üstteki tensor boyutlarını elde ettik.</p>
<p>Evrişim için 4 tane filtre tanımlayacağız. Bu filtrelerin içeriğinin
elle burada tanımlayacağız, yapay öğrenme bağlamında bu içerik eğitim
sırasında otomatik olarak öğrenilir, şimdi örnek amaçlı olarak filtre
değerlerini biz atıyoruz. Ayrıca istediğimiz kadar filtre
tanımlayabilirdik, yapay öğrenmede istenen o’dur ki bu filtreler doğru
değerlere doğru evrilir, her filtre görüntünün değişik yerlerine,
biçimlerine odaklanmaya başlayabilirler.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>in_channels <span class="op">=</span> channels</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>out_channels <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>W_np <span class="op">=</span> np.zeros(shape<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>, in_channels, out_channels), dtype<span class="op">=</span>np.float32)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>W_np[:, <span class="dv">3</span>, :, <span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span>  <span class="co"># yatay cizgi</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>W_np[<span class="dv">3</span>, :, :, <span class="dv">1</span>] <span class="op">=</span> <span class="dv">1</span>  <span class="co"># dikey cizgi</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>W_np[<span class="dv">5</span>, :, :, <span class="dv">2</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>W_np[<span class="dv">2</span>, :, :, <span class="dv">3</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a new graph</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> tf.Graph()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> graph.as_default():</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use tf.compat.v1.placeholder for the input X within the graph context</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> tf.compat.v1.placeholder(tf.float32, shape<span class="op">=</span>(<span class="va">None</span>, height, width, channels))</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define W as a tf.constant or tf.Variable within the graph context</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If W is meant to be a constant filter, tf.constant is appropriate.</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> tf.constant(W_np, dtype<span class="op">=</span>tf.float32) <span class="co"># Using the numpy array W_np</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    convolution <span class="op">=</span> tf.nn.conv2d(X, W, strides<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>], padding<span class="op">=</span><span class="st">&quot;SAME&quot;</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a session to run the operations in the defined graph</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.compat.v1.Session(graph<span class="op">=</span>graph) <span class="im">as</span> sess:</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize variables if you had any tf.Variable in the graph (not strictly needed for tf.constant)</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># sess.run(tf.compat.v1.global_variables_initializer())</span></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> sess.run(convolution, feed_dict<span class="op">=</span>{X: dataset})</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">u&#39;evrişimden gelen tensor&#39;</span>, output.shape)</span></code></pre></div>
<pre class="text"><code>evrişimden gelen tensor (2, 214, 320, 4)</code></pre>
<p>Filtrelerin ilkinde mesela sadece yatay bir çizgi var, bu filtre
yatay piksellerin bir ortalamasını alıyor olacak, vs. Parametrelerden
kaydırma yatay 2 dikey olarak tanımlı, o zaman her imaj kabaca yarısına
inecek, dolgu (padding) için <code>SAME</code> diyerek dolgu yap
demişiz, yani kaydırma olmasaydı giren çıkan tensor aynı olurdu. Peki 3
tane ayrı kanal bilgisine ne oldu? Görüntü boyutlar azalarak (320,214)
oldu, 4 tane filtre çıkışı var, 2 tane imaj. Kanallar üzerinden bir
toplam alındı, yani ayrı kanal bilgisi artık yok. Bu aslında istenen bir
şey çünkü bir görüntünün R,G,B kanalları üzerinde ayrı ayrı irdeleme
yapmak istemeyiz, sınıflama, karar mekanizmasını tüm imaj üzerinden
işletmek isteriz.</p>
<p>Sonuçlar altta, iki tane filtre sonucu örnek olarak seçildi.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_image(image):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    plt.imshow(image, cmap<span class="op">=</span><span class="st">&quot;gray&quot;</span>, interpolation<span class="op">=</span><span class="st">&quot;nearest&quot;</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    plt.axis(<span class="st">&quot;off&quot;</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>image_index <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> feature_map_index <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>plot_image(output[image_index, :, :, feature_map_index])</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;conv-out-</span><span class="sc">%d</span><span class="st">-</span><span class="sc">%d</span><span class="st">.png&#39;</span> <span class="op">%</span> (image_index,feature_map_index))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>image_index <span class="op">=</span> <span class="dv">1</span><span class="op">;</span> feature_map_index <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>plot_image(output[image_index, :, :, feature_map_index])</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;conv-out-</span><span class="sc">%d</span><span class="st">-</span><span class="sc">%d</span><span class="st">.png&#39;</span> <span class="op">%</span> (image_index,feature_map_index))</span></code></pre></div>
<p><img src="conv-out-0-2.png" /></p>
<p><img src="conv-out-1-1.png" /></p>
<p>Cok kanallı durum için açık olmadıysa bir daha vurgulayalım, evrişim
katmanı evrişim operatörünü imaj ya da başka bir tensor üzerinde
gezdirirken üzerinde olduğu tüm girdi tabakalarını, tüm kanalları
kapsayacak şekilde, çarpıp sonucu tek bir öğeye götürür. Bu çarpım için
herhalde kanallar bir şekilde düzleştirilip ağırlıklara eşleniyor.
Neyse, filtreyi kaydırdıkça çarpım sonucu çıktıda farklı öğeler elde
edilir, ve bu bize tek boyutlu bir sonuç kesiti verir. Eğer birden fazla
evrişim filtresi uygularsak onların sonuçları farklı sonuç tabakaları
olarak diğer tabakaların üzerinde istiflenir.</p>
<p><img src="conv-11.png" /></p>
<p>Mesela altta 20x30 boyutlu ve 3 kanallı bir tensor girdimiz olsun, bu
tensorlardan kaç tane olduğunu şimdilik tanımlamadık (mini girdi
yığınında yani). Şimdi girdi üzerinde 16 tane filtre gezdiriyoruz, ve
20x30x16 boyutlu çıktı elde ediyoruz.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>graph <span class="op">=</span> tf.Graph()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> graph.as_default():</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">input</span> <span class="op">=</span> tf.compat.v1.placeholder(tf.float32, shape<span class="op">=</span>([<span class="va">None</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">3</span>]))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    conv_layer_instance <span class="op">=</span> tf.keras.layers.Conv2D(</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        filters<span class="op">=</span><span class="dv">16</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        kernel_size<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">&#39;same&#39;</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (conv_layer_instance)</span></code></pre></div>
<pre class="text"><code>&lt;Conv2D name=conv2d, built=False&gt;</code></pre>
<p>MNIST</p>
<p>Altta MNIST verisini işleyebilen mimarının TF olmadan pür Python
kullanan kodunu görüyoruz. Bu yapı ünlü YSA araştırmacısı Yann LeCun’un
LeNet adı verilen mimarisi. Veri [5]’ten indirilebilir.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;Convolutional neural net on MNIST, modeled on &#39;LeNet-5&#39;,</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf&quot;&quot;&quot;</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy.random <span class="im">as</span> npr</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.scipy.signal</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> grad</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> builtins <span class="im">import</span> <span class="bu">range</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gzip</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> struct</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> array</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>convolve <span class="op">=</span> autograd.scipy.signal.convolve</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> WeightsParser(<span class="bu">object</span>):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;A helper class to index into a parameter vector.&quot;&quot;&quot;</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.idxs_and_shapes <span class="op">=</span> {}</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.N <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_weights(<span class="va">self</span>, name, shape):</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> <span class="va">self</span>.N</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.N <span class="op">+=</span> np.prod(shape)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.idxs_and_shapes[name] <span class="op">=</span> (<span class="bu">slice</span>(start, <span class="va">self</span>.N), shape)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get(<span class="va">self</span>, vect, name):</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        idxs, shape <span class="op">=</span> <span class="va">self</span>.idxs_and_shapes[name]</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.reshape(vect[idxs], shape)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_batches(N_total, N_batch):</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    start <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    batches <span class="op">=</span> []</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> start <span class="op">&lt;</span> N_total:</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        batches.append(<span class="bu">slice</span>(start, start <span class="op">+</span> N_batch))</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        start <span class="op">+=</span> N_batch</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> batches</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logsumexp(X, axis, keepdims<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    max_X <span class="op">=</span> np.<span class="bu">max</span>(X)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> max_X <span class="op">+</span> np.log(np.<span class="bu">sum</span>(np.exp(X <span class="op">-</span> max_X), axis<span class="op">=</span>axis, keepdims<span class="op">=</span>keepdims))</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_nn_funs(input_shape, layer_specs, L2_reg):</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    parser <span class="op">=</span> WeightsParser()</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    cur_shape <span class="op">=</span> input_shape</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> layer_specs:</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>        N_weights, cur_shape <span class="op">=</span> layer.build_weights_dict(cur_shape)</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>        parser.add_weights(layer, (N_weights,))</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predictions(W_vect, inputs):</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Outputs normalized log-probabilities.</span></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span class="co">        shape of inputs : [data, color, y, x]&quot;&quot;&quot;</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>        cur_units <span class="op">=</span> inputs</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> layer_specs:</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>            cur_weights <span class="op">=</span> parser.get(W_vect, layer)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>            cur_units <span class="op">=</span> layer.forward_pass(cur_units, cur_weights)</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> cur_units</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> loss(W_vect, X, T):</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>        log_prior <span class="op">=</span> <span class="op">-</span>L2_reg <span class="op">*</span> np.dot(W_vect, W_vect)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>        log_lik <span class="op">=</span> np.<span class="bu">sum</span>(predictions(W_vect, X) <span class="op">*</span> T)</span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span> log_prior <span class="op">-</span> log_lik</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> frac_err(W_vect, X, T):</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.mean(np.argmax(T, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">!=</span> <span class="op">\</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>                       np.argmax(predictions(W_vect, X), axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> parser.N, predictions, loss, frac_err</span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> conv_layer(<span class="bu">object</span>):</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, kernel_shape, num_filters):</span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kernel_shape <span class="op">=</span> kernel_shape</span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_filters <span class="op">=</span> num_filters</span>
<span id="cb8-73"><a href="#cb8-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-74"><a href="#cb8-74" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_pass(<span class="va">self</span>, inputs, param_vector):</span>
<span id="cb8-75"><a href="#cb8-75" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input dimensions:  [data, color_in, y, x]</span></span>
<span id="cb8-76"><a href="#cb8-76" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Params dimensions: [color_in, color_out, y, x]</span></span>
<span id="cb8-77"><a href="#cb8-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output dimensions: [data, color_out, y, x]</span></span>
<span id="cb8-78"><a href="#cb8-78" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> <span class="va">self</span>.parser.get(param_vector, <span class="st">&#39;params&#39;</span>)</span>
<span id="cb8-79"><a href="#cb8-79" aria-hidden="true" tabindex="-1"></a>        biases <span class="op">=</span> <span class="va">self</span>.parser.get(param_vector, <span class="st">&#39;biases&#39;</span>)</span>
<span id="cb8-80"><a href="#cb8-80" aria-hidden="true" tabindex="-1"></a>        conv <span class="op">=</span> convolve(inputs, params,</span>
<span id="cb8-81"><a href="#cb8-81" aria-hidden="true" tabindex="-1"></a>                        axes<span class="op">=</span>([<span class="dv">2</span>, <span class="dv">3</span>], [<span class="dv">2</span>, <span class="dv">3</span>]),</span>
<span id="cb8-82"><a href="#cb8-82" aria-hidden="true" tabindex="-1"></a>                        dot_axes <span class="op">=</span> ([<span class="dv">1</span>], [<span class="dv">0</span>]),</span>
<span id="cb8-83"><a href="#cb8-83" aria-hidden="true" tabindex="-1"></a>                        mode<span class="op">=</span><span class="st">&#39;valid&#39;</span>)</span>
<span id="cb8-84"><a href="#cb8-84" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> conv <span class="op">+</span> biases</span>
<span id="cb8-85"><a href="#cb8-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-86"><a href="#cb8-86" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_weights_dict(<span class="va">self</span>, input_shape):</span>
<span id="cb8-87"><a href="#cb8-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input shape : [color, y, x] (don&#39;t need to know number of data yet)</span></span>
<span id="cb8-88"><a href="#cb8-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parser <span class="op">=</span> WeightsParser()</span>
<span id="cb8-89"><a href="#cb8-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parser.add_weights(<span class="st">&#39;params&#39;</span>, (input_shape[<span class="dv">0</span>], <span class="va">self</span>.num_filters)</span>
<span id="cb8-90"><a href="#cb8-90" aria-hidden="true" tabindex="-1"></a>                                          <span class="op">+</span> <span class="va">self</span>.kernel_shape)</span>
<span id="cb8-91"><a href="#cb8-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parser.add_weights(<span class="st">&#39;biases&#39;</span>, (<span class="dv">1</span>, <span class="va">self</span>.num_filters, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb8-92"><a href="#cb8-92" aria-hidden="true" tabindex="-1"></a>        output_shape <span class="op">=</span> (<span class="va">self</span>.num_filters,) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb8-93"><a href="#cb8-93" aria-hidden="true" tabindex="-1"></a>                       <span class="va">self</span>.conv_output_shape(input_shape[<span class="dv">1</span>:], <span class="va">self</span>.kernel_shape)</span>
<span id="cb8-94"><a href="#cb8-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.parser.N, output_shape</span>
<span id="cb8-95"><a href="#cb8-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-96"><a href="#cb8-96" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> conv_output_shape(<span class="va">self</span>, A, B):</span>
<span id="cb8-97"><a href="#cb8-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (A[<span class="dv">0</span>] <span class="op">-</span> B[<span class="dv">0</span>] <span class="op">+</span> <span class="dv">1</span>, A[<span class="dv">1</span>] <span class="op">-</span> B[<span class="dv">1</span>] <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb8-98"><a href="#cb8-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-99"><a href="#cb8-99" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> maxpool_layer(<span class="bu">object</span>):</span>
<span id="cb8-100"><a href="#cb8-100" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, pool_shape):</span>
<span id="cb8-101"><a href="#cb8-101" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pool_shape <span class="op">=</span> pool_shape</span>
<span id="cb8-102"><a href="#cb8-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-103"><a href="#cb8-103" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_weights_dict(<span class="va">self</span>, input_shape):</span>
<span id="cb8-104"><a href="#cb8-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># input_shape dimensions: [color, y, x]</span></span>
<span id="cb8-105"><a href="#cb8-105" aria-hidden="true" tabindex="-1"></a>        output_shape <span class="op">=</span> <span class="bu">list</span>(input_shape)</span>
<span id="cb8-106"><a href="#cb8-106" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>]:</span>
<span id="cb8-107"><a href="#cb8-107" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> input_shape[i <span class="op">+</span> <span class="dv">1</span>] <span class="op">%</span> <span class="va">self</span>.pool_shape[i] <span class="op">==</span> <span class="dv">0</span>, <span class="op">\</span></span>
<span id="cb8-108"><a href="#cb8-108" aria-hidden="true" tabindex="-1"></a>                <span class="st">&quot;maxpool shape should tile input exactly&quot;</span></span>
<span id="cb8-109"><a href="#cb8-109" aria-hidden="true" tabindex="-1"></a>            output_shape[i <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> input_shape[i <span class="op">+</span> <span class="dv">1</span>] <span class="op">/</span> <span class="va">self</span>.pool_shape[i]</span>
<span id="cb8-110"><a href="#cb8-110" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">0</span>, output_shape</span>
<span id="cb8-111"><a href="#cb8-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-112"><a href="#cb8-112" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_pass(<span class="va">self</span>, inputs, param_vector):</span>
<span id="cb8-113"><a href="#cb8-113" aria-hidden="true" tabindex="-1"></a>        new_shape <span class="op">=</span> inputs.shape[:<span class="dv">2</span>]</span>
<span id="cb8-114"><a href="#cb8-114" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> [<span class="dv">0</span>, <span class="dv">1</span>]:</span>
<span id="cb8-115"><a href="#cb8-115" aria-hidden="true" tabindex="-1"></a>            pool_width <span class="op">=</span> <span class="va">self</span>.pool_shape[i]</span>
<span id="cb8-116"><a href="#cb8-116" aria-hidden="true" tabindex="-1"></a>            img_width <span class="op">=</span> inputs.shape[i <span class="op">+</span> <span class="dv">2</span>]</span>
<span id="cb8-117"><a href="#cb8-117" aria-hidden="true" tabindex="-1"></a>            new_shape <span class="op">+=</span> (pool_width, img_width <span class="op">//</span> pool_width)</span>
<span id="cb8-118"><a href="#cb8-118" aria-hidden="true" tabindex="-1"></a>        result <span class="op">=</span> inputs.reshape(new_shape)</span>
<span id="cb8-119"><a href="#cb8-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.<span class="bu">max</span>(np.<span class="bu">max</span>(result, axis<span class="op">=</span><span class="dv">2</span>), axis<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb8-120"><a href="#cb8-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-121"><a href="#cb8-121" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> full_layer(<span class="bu">object</span>):</span>
<span id="cb8-122"><a href="#cb8-122" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size):</span>
<span id="cb8-123"><a href="#cb8-123" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.size <span class="op">=</span> size</span>
<span id="cb8-124"><a href="#cb8-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-125"><a href="#cb8-125" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> build_weights_dict(<span class="va">self</span>, input_shape):</span>
<span id="cb8-126"><a href="#cb8-126" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input shape is anything (all flattened)</span></span>
<span id="cb8-127"><a href="#cb8-127" aria-hidden="true" tabindex="-1"></a>        input_size <span class="op">=</span> np.prod(input_shape, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb8-128"><a href="#cb8-128" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parser <span class="op">=</span> WeightsParser()</span>
<span id="cb8-129"><a href="#cb8-129" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parser.add_weights(<span class="st">&#39;params&#39;</span>, (input_size, <span class="va">self</span>.size))</span>
<span id="cb8-130"><a href="#cb8-130" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.parser.add_weights(<span class="st">&#39;biases&#39;</span>, (<span class="va">self</span>.size,))</span>
<span id="cb8-131"><a href="#cb8-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.parser.N, (<span class="va">self</span>.size,)</span>
<span id="cb8-132"><a href="#cb8-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-133"><a href="#cb8-133" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_pass(<span class="va">self</span>, inputs, param_vector):</span>
<span id="cb8-134"><a href="#cb8-134" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> <span class="va">self</span>.parser.get(param_vector, <span class="st">&#39;params&#39;</span>)</span>
<span id="cb8-135"><a href="#cb8-135" aria-hidden="true" tabindex="-1"></a>        biases <span class="op">=</span> <span class="va">self</span>.parser.get(param_vector, <span class="st">&#39;biases&#39;</span>)</span>
<span id="cb8-136"><a href="#cb8-136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> inputs.ndim <span class="op">&gt;</span> <span class="dv">2</span>:</span>
<span id="cb8-137"><a href="#cb8-137" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">=</span> inputs.reshape((inputs.shape[<span class="dv">0</span>], np.prod(inputs.shape[<span class="dv">1</span>:])))</span>
<span id="cb8-138"><a href="#cb8-138" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.nonlinearity(np.dot(inputs[:, :], params) <span class="op">+</span> biases)</span>
<span id="cb8-139"><a href="#cb8-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-140"><a href="#cb8-140" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> tanh_layer(full_layer):</span>
<span id="cb8-141"><a href="#cb8-141" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nonlinearity(<span class="va">self</span>, x):</span>
<span id="cb8-142"><a href="#cb8-142" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.tanh(x)</span>
<span id="cb8-143"><a href="#cb8-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-144"><a href="#cb8-144" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> softmax_layer(full_layer):</span>
<span id="cb8-145"><a href="#cb8-145" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> nonlinearity(<span class="va">self</span>, x):</span>
<span id="cb8-146"><a href="#cb8-146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">-</span> logsumexp(x, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-147"><a href="#cb8-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-148"><a href="#cb8-148" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mnist():</span>
<span id="cb8-149"><a href="#cb8-149" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parse_labels(filename):</span>
<span id="cb8-150"><a href="#cb8-150" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> gzip.<span class="bu">open</span>(filename, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> fh:</span>
<span id="cb8-151"><a href="#cb8-151" aria-hidden="true" tabindex="-1"></a>            magic, num_data <span class="op">=</span> struct.unpack(<span class="st">&quot;&gt;II&quot;</span>, fh.read(<span class="dv">8</span>))</span>
<span id="cb8-152"><a href="#cb8-152" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.array(array.array(<span class="st">&quot;B&quot;</span>, fh.read()), dtype<span class="op">=</span>np.uint8)</span>
<span id="cb8-153"><a href="#cb8-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-154"><a href="#cb8-154" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> parse_images(filename):</span>
<span id="cb8-155"><a href="#cb8-155" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> gzip.<span class="bu">open</span>(filename, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> fh:</span>
<span id="cb8-156"><a href="#cb8-156" aria-hidden="true" tabindex="-1"></a>            magic, num_data, rows, cols <span class="op">=</span> struct.unpack(<span class="st">&quot;&gt;IIII&quot;</span>, fh.read(<span class="dv">16</span>))</span>
<span id="cb8-157"><a href="#cb8-157" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.array(array.array(<span class="st">&quot;B&quot;</span>, fh.read()),</span>
<span id="cb8-158"><a href="#cb8-158" aria-hidden="true" tabindex="-1"></a>                            dtype<span class="op">=</span>np.uint8).reshape(num_data, rows, cols)</span>
<span id="cb8-159"><a href="#cb8-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-160"><a href="#cb8-160" aria-hidden="true" tabindex="-1"></a>    base_dir <span class="op">=</span> <span class="st">&#39;/opt/Downloads/mnist&#39;</span></span>
<span id="cb8-161"><a href="#cb8-161" aria-hidden="true" tabindex="-1"></a>    train_images <span class="op">=</span> parse_images(base_dir <span class="op">+</span> <span class="st">&#39;/train-images-idx3-ubyte.gz&#39;</span>)</span>
<span id="cb8-162"><a href="#cb8-162" aria-hidden="true" tabindex="-1"></a>    train_labels <span class="op">=</span> parse_labels(base_dir <span class="op">+</span> <span class="st">&#39;/train-labels-idx1-ubyte.gz&#39;</span>)</span>
<span id="cb8-163"><a href="#cb8-163" aria-hidden="true" tabindex="-1"></a>    test_images  <span class="op">=</span> parse_images(base_dir <span class="op">+</span> <span class="st">&#39;/t10k-images-idx3-ubyte.gz&#39;</span>)</span>
<span id="cb8-164"><a href="#cb8-164" aria-hidden="true" tabindex="-1"></a>    test_labels  <span class="op">=</span> parse_labels(base_dir <span class="op">+</span> <span class="st">&#39;/t10k-labels-idx1-ubyte.gz&#39;</span>)</span>
<span id="cb8-165"><a href="#cb8-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-166"><a href="#cb8-166" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_images, train_labels, test_images, test_labels</span>
<span id="cb8-167"><a href="#cb8-167" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-168"><a href="#cb8-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-169"><a href="#cb8-169" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</span>
<span id="cb8-170"><a href="#cb8-170" aria-hidden="true" tabindex="-1"></a>    np.random.seed(<span class="dv">0</span>)</span>
<span id="cb8-171"><a href="#cb8-171" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Network parameters    </span></span>
<span id="cb8-172"><a href="#cb8-172" aria-hidden="true" tabindex="-1"></a>    L2_reg <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb8-173"><a href="#cb8-173" aria-hidden="true" tabindex="-1"></a>    input_shape <span class="op">=</span> (<span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb8-174"><a href="#cb8-174" aria-hidden="true" tabindex="-1"></a>    layer_specs <span class="op">=</span> [conv_layer((<span class="dv">5</span>, <span class="dv">5</span>), <span class="dv">6</span>),</span>
<span id="cb8-175"><a href="#cb8-175" aria-hidden="true" tabindex="-1"></a>                   maxpool_layer((<span class="dv">2</span>, <span class="dv">2</span>)),</span>
<span id="cb8-176"><a href="#cb8-176" aria-hidden="true" tabindex="-1"></a>                   conv_layer((<span class="dv">5</span>, <span class="dv">5</span>), <span class="dv">16</span>),</span>
<span id="cb8-177"><a href="#cb8-177" aria-hidden="true" tabindex="-1"></a>                   maxpool_layer((<span class="dv">2</span>, <span class="dv">2</span>)),</span>
<span id="cb8-178"><a href="#cb8-178" aria-hidden="true" tabindex="-1"></a>                   tanh_layer(<span class="dv">12</span>),</span>
<span id="cb8-179"><a href="#cb8-179" aria-hidden="true" tabindex="-1"></a>                   tanh_layer(<span class="dv">84</span>),</span>
<span id="cb8-180"><a href="#cb8-180" aria-hidden="true" tabindex="-1"></a>                   softmax_layer(<span class="dv">10</span>)]</span>
<span id="cb8-181"><a href="#cb8-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-182"><a href="#cb8-182" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training parameters</span></span>
<span id="cb8-183"><a href="#cb8-183" aria-hidden="true" tabindex="-1"></a>    param_scale <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb8-184"><a href="#cb8-184" aria-hidden="true" tabindex="-1"></a>    learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb8-185"><a href="#cb8-185" aria-hidden="true" tabindex="-1"></a>    momentum <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb8-186"><a href="#cb8-186" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb8-187"><a href="#cb8-187" aria-hidden="true" tabindex="-1"></a>    num_epochs <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb8-188"><a href="#cb8-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-189"><a href="#cb8-189" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load and process MNIST data</span></span>
<span id="cb8-190"><a href="#cb8-190" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Loading training data...&quot;</span>)</span>
<span id="cb8-191"><a href="#cb8-191" aria-hidden="true" tabindex="-1"></a>    add_color_channel <span class="op">=</span> <span class="kw">lambda</span> x : x.reshape((x.shape[<span class="dv">0</span>], <span class="dv">1</span>, x.shape[<span class="dv">1</span>], x.shape[<span class="dv">2</span>]))</span>
<span id="cb8-192"><a href="#cb8-192" aria-hidden="true" tabindex="-1"></a>    one_hot <span class="op">=</span> <span class="kw">lambda</span> x, K : np.array(x[:,<span class="va">None</span>] <span class="op">==</span> np.arange(K)[<span class="va">None</span>, :], dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb8-193"><a href="#cb8-193" aria-hidden="true" tabindex="-1"></a>    train_images, train_labels, test_images, test_labels <span class="op">=</span> mnist()</span>
<span id="cb8-194"><a href="#cb8-194" aria-hidden="true" tabindex="-1"></a>    train_images <span class="op">=</span> add_color_channel(train_images) <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb8-195"><a href="#cb8-195" aria-hidden="true" tabindex="-1"></a>    test_images  <span class="op">=</span> add_color_channel(test_images)  <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb8-196"><a href="#cb8-196" aria-hidden="true" tabindex="-1"></a>    train_labels <span class="op">=</span> one_hot(train_labels, <span class="dv">10</span>)</span>
<span id="cb8-197"><a href="#cb8-197" aria-hidden="true" tabindex="-1"></a>    test_labels <span class="op">=</span> one_hot(test_labels, <span class="dv">10</span>)</span>
<span id="cb8-198"><a href="#cb8-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-199"><a href="#cb8-199" aria-hidden="true" tabindex="-1"></a>    test_sampler <span class="op">=</span> np.random.permutation(<span class="bu">len</span>(test_labels))[:<span class="dv">100</span>]</span>
<span id="cb8-200"><a href="#cb8-200" aria-hidden="true" tabindex="-1"></a>    train_sampler <span class="op">=</span> np.random.permutation(<span class="bu">len</span>(train_labels))[:<span class="dv">1000</span>]</span>
<span id="cb8-201"><a href="#cb8-201" aria-hidden="true" tabindex="-1"></a>    train_images <span class="op">=</span> train_images[train_sampler]</span>
<span id="cb8-202"><a href="#cb8-202" aria-hidden="true" tabindex="-1"></a>    train_labels <span class="op">=</span> train_labels[train_sampler]</span>
<span id="cb8-203"><a href="#cb8-203" aria-hidden="true" tabindex="-1"></a>    test_images <span class="op">=</span> test_images[test_sampler]</span>
<span id="cb8-204"><a href="#cb8-204" aria-hidden="true" tabindex="-1"></a>    test_labels <span class="op">=</span> test_labels[test_sampler]</span>
<span id="cb8-205"><a href="#cb8-205" aria-hidden="true" tabindex="-1"></a>    N_data <span class="op">=</span> train_images.shape[<span class="dv">0</span>]</span>
<span id="cb8-206"><a href="#cb8-206" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-207"><a href="#cb8-207" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn <span class="im">import</span> neighbors</span>
<span id="cb8-208"><a href="#cb8-208" aria-hidden="true" tabindex="-1"></a>    train_images_3 <span class="op">=</span> np.reshape(train_images, (<span class="dv">1000</span>,<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>))</span>
<span id="cb8-209"><a href="#cb8-209" aria-hidden="true" tabindex="-1"></a>    test_images_3 <span class="op">=</span> np.reshape(test_images, (<span class="dv">100</span>,<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>))</span>
<span id="cb8-210"><a href="#cb8-210" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> neighbors.KNeighborsClassifier()</span>
<span id="cb8-211"><a href="#cb8-211" aria-hidden="true" tabindex="-1"></a>    clf.fit(train_images_3,train_labels)</span>
<span id="cb8-212"><a href="#cb8-212" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> clf.predict(test_images_3)</span>
<span id="cb8-213"><a href="#cb8-213" aria-hidden="true" tabindex="-1"></a>    res2 <span class="op">=</span> [np.<span class="bu">all</span>(x) <span class="cf">for</span> x <span class="kw">in</span> (res<span class="op">==</span>test_labels)]</span>
<span id="cb8-214"><a href="#cb8-214" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&#39;KNN&#39;</span>)</span>
<span id="cb8-215"><a href="#cb8-215" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> ( np.<span class="bu">sum</span>(res2) <span class="op">/</span> <span class="fl">100.</span>)</span>
<span id="cb8-216"><a href="#cb8-216" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-217"><a href="#cb8-217" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make neural net functions</span></span>
<span id="cb8-218"><a href="#cb8-218" aria-hidden="true" tabindex="-1"></a>    N_weights, pred_fun, loss_fun, <span class="op">\</span></span>
<span id="cb8-219"><a href="#cb8-219" aria-hidden="true" tabindex="-1"></a>        frac_err <span class="op">=</span> make_nn_funs(input_shape, layer_specs, L2_reg)</span>
<span id="cb8-220"><a href="#cb8-220" aria-hidden="true" tabindex="-1"></a>    loss_grad <span class="op">=</span> grad(loss_fun)</span>
<span id="cb8-221"><a href="#cb8-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-222"><a href="#cb8-222" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize weights</span></span>
<span id="cb8-223"><a href="#cb8-223" aria-hidden="true" tabindex="-1"></a>    rs <span class="op">=</span> npr.RandomState()</span>
<span id="cb8-224"><a href="#cb8-224" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> rs.randn(N_weights) <span class="op">*</span> param_scale</span>
<span id="cb8-225"><a href="#cb8-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-226"><a href="#cb8-226" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;    Epoch      |    Train err  |   Test error  &quot;</span>)</span>
<span id="cb8-227"><a href="#cb8-227" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> print_perf(epoch, W):</span>
<span id="cb8-228"><a href="#cb8-228" aria-hidden="true" tabindex="-1"></a>        test_perf  <span class="op">=</span> frac_err(W, test_images, test_labels)</span>
<span id="cb8-229"><a href="#cb8-229" aria-hidden="true" tabindex="-1"></a>        train_perf <span class="op">=</span> frac_err(W, train_images, train_labels)</span>
<span id="cb8-230"><a href="#cb8-230" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">&quot;</span><span class="sc">{0:15}</span><span class="st">|</span><span class="sc">{1:15}</span><span class="st">|</span><span class="sc">{2:15}</span><span class="st">&quot;</span>.<span class="bu">format</span>(epoch, train_perf, test_perf))</span>
<span id="cb8-231"><a href="#cb8-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-232"><a href="#cb8-232" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train with sgd</span></span>
<span id="cb8-233"><a href="#cb8-233" aria-hidden="true" tabindex="-1"></a>    batch_idxs <span class="op">=</span> make_batches(N_data, batch_size)</span>
<span id="cb8-234"><a href="#cb8-234" aria-hidden="true" tabindex="-1"></a>    cur_dir <span class="op">=</span> np.zeros(N_weights)</span>
<span id="cb8-235"><a href="#cb8-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-236"><a href="#cb8-236" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb8-237"><a href="#cb8-237" aria-hidden="true" tabindex="-1"></a>        print_perf(epoch, W)</span>
<span id="cb8-238"><a href="#cb8-238" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idxs <span class="kw">in</span> batch_idxs:</span>
<span id="cb8-239"><a href="#cb8-239" aria-hidden="true" tabindex="-1"></a>            grad_W <span class="op">=</span> loss_grad(W, train_images[idxs], train_labels[idxs])</span>
<span id="cb8-240"><a href="#cb8-240" aria-hidden="true" tabindex="-1"></a>            cur_dir <span class="op">=</span> momentum <span class="op">*</span> cur_dir <span class="op">+</span> (<span class="fl">1.0</span> <span class="op">-</span> momentum) <span class="op">*</span> grad_W</span>
<span id="cb8-241"><a href="#cb8-241" aria-hidden="true" tabindex="-1"></a>            W <span class="op">-=</span> learning_rate <span class="op">*</span> cur_dir</span>
<span id="cb8-242"><a href="#cb8-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-243"><a href="#cb8-243" aria-hidden="true" tabindex="-1"></a>    np.savetxt(<span class="st">&#39;W&#39;</span>,W)</span></code></pre></div>
<pre><code>Loading training data...
KNN
0.82
    Epoch      |    Train err  |   Test error  
              0|          0.895|           0.89
              1|           0.89|            0.9
              2|          0.892|           0.91
              3|          0.891|            0.9
              4|          0.874|           0.88
              5|          0.871|           0.86
              6|          0.874|           0.86
              7|          0.872|           0.86
              8|           0.87|           0.85
              9|          0.851|           0.84
             10|           0.82|           0.75
             11|          0.787|           0.73
             12|          0.757|           0.71
             13|          0.677|           0.71
             14|          0.641|           0.69
             15|          0.621|           0.68
             16|          0.611|           0.64
             17|          0.598|           0.64
             18|          0.567|           0.58
             19|          0.546|           0.59
             20|          0.509|           0.55
             21|          0.467|           0.53
             22|          0.443|            0.5
             23|          0.411|           0.46
             24|          0.403|           0.43
             25|          0.381|           0.41
             26|          0.348|           0.41
             27|          0.348|           0.42
             28|          0.347|           0.42
             29|          0.338|            0.4
             30|          0.289|            0.4
             31|           0.33|           0.39
             32|          0.287|           0.31
             33|          0.292|           0.37
             34|          0.252|           0.28
             35|          0.224|           0.31
             36|           0.22|           0.31
             37|          0.201|           0.29
             38|          0.192|           0.27
             39|          0.165|           0.27
             40|          0.146|           0.24
             41|          0.144|           0.21
             42|          0.139|            0.2
             43|          0.126|           0.19
             44|          0.116|           0.21
             45|          0.107|           0.16
             46|          0.098|           0.16
             47|          0.095|           0.15
             48|           0.09|           0.19
             49|          0.081|           0.17
</code></pre>
<p>Tarih</p>
<p>Convnet’lerin son zamanlarda başarılı olmaya başlamasında bazı
etkenler şunlar: en önemlisi araştırmacıların dışbükey olmayan kayıp
fonksiyonlarından (non-convex loss functions) korkmadan (!) onların
minimumunu rasgele gradyan inişi üzerinden bulabilmeye başlamaları.
Convnet, HMM, Gaussian karışımlar gibi pek çok alanda dışbükey olmayan
bir durum vardır, hatta bir bakıma ciddi her yapay öğrenim alanında bu
durum ortaya çıkar. Bu optimizasyon problemi rasgele gradyan inişi ile
çözülmeye başlandı. Gerçi rasgele gradyan inişi çok basit bir yöntemdir,
fakat araştırmacılar yıllarca bu yöntemi dışbükey olmayan yerlerde
kullanmadılar çünkü yakınsama (convergence) garantileri teorik olarak
mümkün değildi. Kıyasla bir dışbükey fonksiyonun minimumuna varmak doğru
yöntemler ile garantidir. Fakat son araştırmalara göre rasgele gradyan
inişi ile optimizasyon eğitim verisi üzerinden yakınsama garantisi
olmasa bile test verisi üzerinde bazı garantilerin olduğu ortaya çıktı,
ki test skoru eğitim skorundan daha önemli.</p>
<p>Bir diğer gelişme ağ yapısında gizli katmandaki nöron sayısını
genişletince, minimum noktasının pek çok yerde ortaya çıkabilmesi, ve bu
noktaların birbiriyle aşağı yukarı aynı olması. Yani gradyan inişi bu
noktalardan birini bulduğunda iş bitmiş sayılabiliyor. Ayrıca çok boyut
olunca yerel minima’da takılıp kalmadan bir yan boyuta atlayıp
önümüzdeki tepenin etrafından dolaşabilme şansı ortaya çıkıyor [4].</p>
<p>Ek bir gelişme herhangi bir yazılım fonksiyonun türevinin
alınabilmesini sağlayan otomatik türev alma (automatic differentiation)
tekniğinin yaygınlaşması, ki bu teknik çetrefil nöron fonksiyonları
içerebilen convnet’ler için faydalı oldu. Not: dikkat otomatik türev ile
{} fonksiyon hatta <code>if, while</code> komutları içeren bir kod
parçasının bile türevi alınabiliyor (bkz [2] yazısı). Türevler önemli
çünkü gradyan inişi minimuma gidebilmek için türev kullanmalı. Üstteki
kodda dikkat çekmiş olabilir, sinir ağında aslında yaptığımız tek hesap
ileri doğru beslemeli (feed-forward) olan hesap; her veri noktası için
ağın başından başlayarak girdi / işlem / sonuçları ardı ardına bir
sonraki katmana aktarıyoruz, otomatik türev pek çok katmanlı bu
işlemlerin türevini alarak bize o gidişin yaptığı tahminlerin hatasını
düzeltmek için gidilmesi gereken gradyan yönünü veriyor.</p>
<p>Bazı negatifler: Convnet’lerin eğitimi hızlı değildir, ciddi
kullanımlar için GPU, ya da paralel CPU kullanmak şart. Ayrıca
convnet’lerin bir fark yaratması için oldukça çok veriye ihtiyaçları
var, normal ölçek veri ortamında diğer yaklaşımlar da convnet’ten çok
daha kötü değiller. Bir diğeri bir YSA’nın, yapı olarak, istatistiki bir
anlamının olmaması: Bir ağ yapısı var, fakat bu ağın tamamının
olasılıksal olarak irdelenmesi mümkün değil. Kıyasla bir GMM bir
olasılığı temsil eder, bu sebeple yaptığı hesapların mesela güven
aralığını hesaplamak mümkündür. Diğer yanda mesela tıbbi bir uygulama
bağlamında YSA bir tahmin yapınca bundan “ne kadar emin olduğu’’
sorusunun direk cevabı alınamıyor. Bu alanda araştırmalar var tabii,
ilginç bazı buluşlar bunu mümkün kılabilir.</p>
<p>Kaynaklar</p>
<p>[2] Bayramlı, Bilgisayar Bilim, <em>Otomatik Türev</em></p>
<p>[3] <em>Machine learning algorithms</em>,
https://github.com/rushter/MLAlgorithms</p>
<p>[4] LeCun, <em>Who is Afraid of Non-Convex Loss Functions?</em>, <a
href="https://youtu.be/8zdo6cnCW2w">https://youtu.be/8zdo6cnCW2w</a></p>
<p>[5] <em>MNIST Verisi</em>, <a
href="https://github.com/fgnt/mnist">Github</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
