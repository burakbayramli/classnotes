<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Derin Takviyeli Öğrenme, İlke Gradyanları (Deep Reinforcement Learning, Policy Gradients )</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1
id="derin-takviyeli-öğrenme-ilke-gradyanları-deep-reinforcement-learning-policy-gradients">Derin
Takviyeli Öğrenme, İlke Gradyanları (Deep Reinforcement Learning, Policy
Gradients )</h1>
<p>Bilgisayar otomatik olarak oyun oynamayı öğrenebilir mi? Diyelim
herhangi bir bilgisayar oyunu, dama, satranç, ya da eğlence oyunlarından
Pong. Eğer elimizde bir simülasyon ortamı var ise, ve takviyeli öğrenme
teknikleri ile bu sorunun cevabı evet. Simülasyon ortamında bilgisayara
karşı istediğimiz kadar oynayıp RL teknikleri bir oyunu oynamayı
öğrenebilir.</p>
<p>Daha önce [7] yazısında farklı bir yaklaşım gördük, bir değer
fonksiyonu vardı, bu fonksiyona tahtanın son halini veriyorduk, değer
fonksiyonu bize pozisyonun taraflar için ne kadar avantajlı olduğunu
raporluyordu (tek bir sayı). Bu fonksiyon bir kez, ve önceden
kodlanmaktaydı, ve oyun oynayan yapay zeka altüst (minimax) algoritması
ile kendisi için en avantajlı karşı taraf için en avantajsız
pozisyonları bu fonksiyon ile değerlendirerek ve arama yaparak
buluyordu. Fakat değer fonksiyonu yaklaşımının bazı dezavantajları var,
birincisi fonksiyonun deterministik olması. Oyun sırasında değişmiyor,
önceden kodlanmış.</p>
<p>Daha iyi bir yaklaşım olasılıksal bir ilke <span
class="math inline">\(\pi_\theta(a,s)\)</span> kodlamak, atılan adımları
örnekleme ile atmak, ve ilkeyi her oyun sonunda güncellemek. Böylece
oyun sırasında hem oyuncu yeni şeyler denemeye (açık fikirli!) hazır
oluyor, takılıp kalmıyor, oyun durumundan tam emin olunamadığı durumlar
icin bile hazır oluyor, ve kazandıran ilkeler daha yoğun olasılıklara
tekabül ettiği için yine iyi bir oyun oynama becerisine kavuşuyor, ve
kendini sürekli güncelliyor.</p>
<p>İlke <span class="math inline">\(\pi_\theta(a,s)\)</span>, oyun
konumu (state) <span class="math inline">\(s\)</span> ile, yapılacak
hareket (action) ise <span class="math inline">\(a\)</span> ile
belirtilir. Pong örneğinde konum tüm oyunun o andaki piksel görüntüsü
olarak bize bildiriliyor olabilir, hareket ise raketin yukarı mı aşağı
mı gideceği; verili konum <span class="math inline">\(s\)</span> için
<span class="math inline">\(\pi_\theta(a|s)\)</span> (kazanmak için
optimallik bağlamında) mümkün tüm davranışların dağılımını verecek.</p>
<p>Peki ilke fonksiyonunu nasıl güncelleriz? İlke gradyanı (policy
gradient) kavramı ile. İlke bir fonksiyondur, bir softmax fonksiyonu ile
ya da yapay sinir ağı ile temsil edilebilir. YSA’lar her türlü
fonksiyonu temsil edebildikleri için sofistike kabiliyetleri için daha
tercih ediliyorlar (daha önemlisi gradyanları otomatik alınabiliyor,
bunun niye faydalı olduğunu birazdan göreceğiz).</p>
<p><img src="policy.png" /></p>
<p>Güncelleme nasıl olacak? Burada skor fonksiyonunu kavramı gerekli,
optimize etmek istediğimiz bir skor fonksiyonunun beklentisinin optimize
edilmesi, skor fonksiyonu tabii ki ilke fonksiyonuna bağlıdır, yani skor
beklentisi en iyi olacak ilkeyi arıyoruz. Bu beklentinin gradyanını
istiyoruz, çünkü ilkeyi tanımlayan <span
class="math inline">\(\theta\)</span>’yi skor bağlamında öyle
güncelleyeceğiz ki eğer aynı konumu tekrar gelmiş olsak, daha iyi
hareketlerle daha iyi skora erişelim. Aradığımız gradyan (<span
class="math inline">\(s,a\)</span> yerine kısaca <span
class="math inline">\(x\)</span> kullanalım, skor <span
class="math inline">\(Q\)</span> olsun [2]),</p>
<p><span class="math display">\[
\nabla_\theta E_{x \sim \pi_\theta(x)} [Q(x)]
\]</span></p>
<p>Üstteki ifadeyi açalım, beklentinin tanımı üzerinden,</p>
<p><span class="math display">\[ \nabla_\theta E_{x \sim \pi_\theta(s)}
[Q(x)] =
\nabla_\theta \sum_x \pi(x) Q(x)
\]</span></p>
<p>Gradyan içeri nüfuz edebilir,</p>
<p><span class="math display">\[
= \sum_x \nabla_\theta \pi_\theta(x) Q(x)
\]</span></p>
<p><span class="math inline">\(\pi(x)\)</span> ile çarpıp bölersek hiç
bir şey değişmemiş olur,</p>
<p><span class="math display">\[
= \sum_x \pi(x) \frac{\nabla_\theta \pi(x)}{\pi(x)} Q(x)
\]</span></p>
<p>Cebirsel olarak biliyoruz ki <span
class="math inline">\(\nabla_\theta \log(z) =
\frac{1}{z}\nabla_\theta\)</span>, o zaman,</p>
<p><span class="math display">\[
= \sum_x \pi(x) \nabla_\theta \log \pi(x)Q(x)
\]</span></p>
<p>Yine beklenti tanımından hareketle</p>
<p><span class="math display">\[
= E_x \big[ \nabla_\theta \log \pi(x) Q(x) \big]
\]</span></p>
<p><span class="math inline">\(x = (s,a)\)</span> demistik, o zaman
nihai denklem</p>
<p><span class="math display">\[
\nabla_\theta E_{x \sim \pi_\theta(s,a)} [Q(s,a)]
= E_{s,a} \big[ \nabla_\theta \log \pi_\theta(s,a) Q(s,a) \big]
\]</span></p>
<p>Eşitliğin sağ tarafı bize güzel bir kabiliyet sunmuş oldu, orada bir
beklenti var, bu hesabı analitik olarak yapmak çok zor olabilir, fakat
beklentilerin örneklem alarak nasıl hesaplanacağını biliyoruz! Detaylar
için <em>İstatistik, Monte Carlo, Entegraller, MCMC</em> yazısı. O zaman
<span class="math inline">\(v_t \sim Q(s,a)\)</span> örneklemi alırız,
yani oyunu baştan sonra kadar oynarız ve skora bakarız, ve <span
class="math inline">\(\theta\)</span> güncellemesi için [5],</p>
<p><span class="math display">\[ \Delta \theta_t = \alpha \nabla_\theta
\log \pi_\theta (s_t,a_t) v_t\]</span></p>
<p>Oyun oynamak ile örneklemin alakası ne? Oynanan bir oyun mümkün tüm
oyunlar içinden alınan bir örneklem değil midir? Evet. Ayrıca DYSA
durumunda da olası her aksiyonun olasılığını hesaplıyoruz ve bu
olasıklar üzerinden zar atarak bir hareket seçiyoruz. Daha olası olan
daha fazla seçiliyor tabii ama az olası olan da bazen seçilebiliyor.</p>
<p>Tabii mesela Pong oyunu bir sürü adım <span
class="math inline">\(a_1,..,a_n\)</span> sonrası bitiyor, bu durumda en
sondaki kazanç (ya da kaybı) o oyundaki tüm adımlara geriye giderek
uyguluyoruz. Güncelleme sonrası ilke fonksiyonumuz değişiyor, ve bir
oyun daha oynayarak aynı şeyi tekrarlıyoruz.</p>
<p><span class="math inline">\(\pi_\theta (s,a)\)</span>’nin ilke
fonksiyonu olduğunu söyledik, bu fonksiyon DYSA olabilir, ya da daha
basit, sonlu sayıda seçenek üzerinden ayrıksal olasılıkları depolayan
softmax olabilir (bu durum için gradyan türetmesi altta). DYSA durumunda
üstteki formüle göre <span class="math inline">\(\log \pi_\theta
(s,a)\)</span>’un gradyanının gerektiğini görüyoruz, otomatik türev
uzerinden bu gradyan DYSA paketinden rahatça alınabilir.</p>
<p>Pong oyunu kodunu göreceğiz, ama ondan önce daha basit çubuk
dengeleme problemine bakalım [1], kuruluş, oyun açıklaması için [3]. Bir
simulasyon ortamındayız, ve bu ortamda bize bir çubuk veriliyor, ve
çubuğun konumu dört tane sayı üzerinden bildirilir, ödül her adımda
anında alınır (çubuk düşmediyse, ekrandan çıkmadıysa o anda başarı).</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cartpole_train.py - egitim</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf, gym</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reset_graph(seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    tf.reset_default_graph()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    tf.set_random_seed(seed)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discount_rewards(rewards, discount_rate):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    discounted_rewards <span class="op">=</span> np.zeros(<span class="bu">len</span>(rewards))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    cumulative_rewards <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(rewards))):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        cumulative_rewards <span class="op">=</span> rewards[step] <span class="op">+</span> cumulative_rewards <span class="op">*</span> discount_rate</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        discounted_rewards[step] <span class="op">=</span> cumulative_rewards</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> discounted_rewards</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> discount_and_normalize_rewards(all_rewards, discount_rate):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    all_discounted_rewards <span class="op">=</span> [discount_rewards(rewards, discount_rate) <span class="op">\</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                              <span class="cf">for</span> rewards <span class="kw">in</span> all_rewards]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    flat_rewards <span class="op">=</span> np.concatenate(all_discounted_rewards)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    reward_mean <span class="op">=</span> flat_rewards.mean()</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    reward_std <span class="op">=</span> flat_rewards.std()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [(discounted_rewards <span class="op">-</span> reward_mean)<span class="op">/</span>reward_std <span class="cf">for</span> <span class="op">\</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>            discounted_rewards <span class="kw">in</span> all_discounted_rewards]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>reset_graph()</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>n_inputs <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>n_outputs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>initializer <span class="op">=</span> tf.contrib.layers.variance_scaling_initializer()</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.placeholder(tf.float32, shape<span class="op">=</span>[<span class="va">None</span>, n_inputs])</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>hidden <span class="op">=</span> tf.layers.dense(X, n_hidden,</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>                         activation<span class="op">=</span>tf.nn.elu,</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>                         kernel_initializer<span class="op">=</span>initializer)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> tf.layers.dense(hidden, n_outputs)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> tf.nn.sigmoid(logits)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>p_left_and_right <span class="op">=</span> tf.concat(axis<span class="op">=</span><span class="dv">1</span>, values<span class="op">=</span>[outputs, <span class="dv">1</span> <span class="op">-</span> outputs])</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>action <span class="op">=</span> tf.multinomial(tf.log(p_left_and_right), num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">1.</span> <span class="op">-</span> tf.to_float(action)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>cross_entropy <span class="op">=</span> tf.nn.sigmoid_cross_entropy_with_logits(labels<span class="op">=</span>y, logits<span class="op">=</span>logits)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.AdamOptimizer(learning_rate)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>grads_and_vars <span class="op">=</span> optimizer.compute_gradients(cross_entropy)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>gradients <span class="op">=</span> [grad <span class="cf">for</span> grad, variable <span class="kw">in</span> grads_and_vars]</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>gradient_placeholders <span class="op">=</span> []</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>grads_and_vars_feed <span class="op">=</span> []</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> grad, variable <span class="kw">in</span> grads_and_vars:</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    gradient_placeholder <span class="op">=</span> tf.placeholder(tf.float32, shape<span class="op">=</span>grad.get_shape())</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    gradient_placeholders.append(gradient_placeholder)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    grads_and_vars_feed.append((gradient_placeholder, variable))</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>training_op <span class="op">=</span> optimizer.apply_gradients(grads_and_vars_feed)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>saver <span class="op">=</span> tf.train.Saver()        </span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">&quot;CartPole-v0&quot;</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>n_games_per_update <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>n_max_steps <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">250</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>save_iterations <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>discount_rate <span class="op">=</span> <span class="fl">0.95</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>ffile <span class="op">=</span> <span class="st">&quot;/tmp/cartpole.ckpt&quot;</span></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.Session()</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>sess.run(init)</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">range</span>(n_iterations):</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\r</span><span class="st">Iteration: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(iteration))</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>    all_rewards <span class="op">=</span> []</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>    all_gradients <span class="op">=</span> []</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> game <span class="kw">in</span> <span class="bu">range</span>(n_games_per_update):</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>        current_rewards <span class="op">=</span> []</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>        current_gradients <span class="op">=</span> []</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>        obs <span class="op">=</span> env.reset()</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(n_max_steps):</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>            d <span class="op">=</span> {X: obs.reshape(<span class="dv">1</span>, n_inputs)}</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>            action_val, gradients_val <span class="op">=</span> sess.run([action, gradients], feed_dict<span class="op">=</span>d)</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>            obs, reward, done, info <span class="op">=</span> env.step(action_val[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>            current_rewards.append(reward)</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>            current_gradients.append(gradients_val)</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> done: <span class="cf">break</span></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>        all_rewards.append(current_rewards)</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>        all_gradients.append(current_gradients)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>    all_rewards <span class="op">=</span> discount_and_normalize_rewards(all_rewards,</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>                                                 discount_rate<span class="op">=</span>discount_rate)</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>    feed_dict <span class="op">=</span> {}</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> var_index, gradient_placeholder <span class="kw">in</span> <span class="bu">enumerate</span>(gradient_placeholders):</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>        tmp <span class="op">=</span> [reward <span class="op">*</span> all_gradients[game_index][step][var_index] <span class="op">\</span></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>               <span class="cf">for</span> game_index, rewards <span class="kw">in</span> <span class="bu">enumerate</span>(all_rewards) <span class="op">\</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>               <span class="cf">for</span> step, reward <span class="kw">in</span> <span class="bu">enumerate</span>(rewards)]</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>        mean_gradients <span class="op">=</span> np.mean(tmp, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>        feed_dict[gradient_placeholder] <span class="op">=</span> mean_gradients</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>    sess.run(training_op, feed_dict<span class="op">=</span>feed_dict)</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iteration <span class="op">%</span> save_iterations <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>        saver.save(sess, ffile)</span></code></pre></div>
<p>Eğitim fazla sürmüyor. Bittikten sonra alttaki kodla sonucu
görebiliriz. Çubuğun dengeli bir şekilde tutulabildiğini göreceğiz.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cartpole_play.py - oyunu oyna</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reset_graph(seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    tf.reset_default_graph()</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    tf.set_random_seed(seed)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    np.random.seed(seed)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> render_cart_pole(env, obs):</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> env.render(mode<span class="op">=</span><span class="st">&quot;rgb_array&quot;</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_scene(num, frames, patch):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    patch.set_data(frames[num])</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> patch,</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> render_policy_net(model_path, action, X, n_max_steps <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    frames <span class="op">=</span> []</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    env <span class="op">=</span> gym.make(<span class="st">&quot;CartPole-v0&quot;</span>)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    obs <span class="op">=</span> env.reset()</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        saver.restore(sess, model_path)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(n_max_steps):</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> render_cart_pole(env, obs)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>            frames.append(img)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>            action_val <span class="op">=</span> action.<span class="bu">eval</span>(feed_dict<span class="op">=</span>{X: obs.reshape(<span class="dv">1</span>, n_inputs)})</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            obs, reward, done, info <span class="op">=</span> env.step(action_val[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> done:</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    env.close()</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> frames</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>reset_graph()</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>n_inputs <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>n_hidden <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>n_outputs <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>initializer <span class="op">=</span> tf.contrib.layers.variance_scaling_initializer()</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.placeholder(tf.float32, shape<span class="op">=</span>[<span class="va">None</span>, n_inputs])</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>hidden <span class="op">=</span> tf.layers.dense(X, n_hidden,</span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>                         activation<span class="op">=</span>tf.nn.elu,</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>                         kernel_initializer<span class="op">=</span>initializer)</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> tf.layers.dense(hidden, n_outputs)</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>outputs <span class="op">=</span> tf.nn.sigmoid(logits)  <span class="co"># probability of action 0 (left)</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>p_left_and_right <span class="op">=</span> tf.concat(axis<span class="op">=</span><span class="dv">1</span>, values<span class="op">=</span>[outputs, <span class="dv">1</span> <span class="op">-</span> outputs])</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>action <span class="op">=</span> tf.multinomial(tf.log(p_left_and_right), num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="fl">1.</span> <span class="op">-</span> tf.to_float(action)</span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>cross_entropy <span class="op">=</span> tf.nn.sigmoid_cross_entropy_with_logits(labels<span class="op">=</span>y, logits<span class="op">=</span>logits)</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.AdamOptimizer(learning_rate)</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>grads_and_vars <span class="op">=</span> optimizer.compute_gradients(cross_entropy)</span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>gradients <span class="op">=</span> [grad <span class="cf">for</span> grad, variable <span class="kw">in</span> grads_and_vars]</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>gradient_placeholders <span class="op">=</span> []</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>grads_and_vars_feed <span class="op">=</span> []</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> grad, variable <span class="kw">in</span> grads_and_vars:</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>    gradient_placeholder <span class="op">=</span> tf.placeholder(tf.float32, shape<span class="op">=</span>grad.get_shape())</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    gradient_placeholders.append(gradient_placeholder)</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>    grads_and_vars_feed.append((gradient_placeholder, variable))</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a>training_op <span class="op">=</span> optimizer.apply_gradients(grads_and_vars_feed)</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>saver <span class="op">=</span> tf.train.Saver()        </span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">&quot;CartPole-v0&quot;</span>)</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>n_games_per_update <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>n_max_steps <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>n_iterations <span class="op">=</span> <span class="dv">250</span></span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>save_iterations <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>discount_rate <span class="op">=</span> <span class="fl">0.95</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>ffile <span class="op">=</span> <span class="st">&quot;/tmp/cartpole.ckpt&quot;</span></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>frames <span class="op">=</span> render_policy_net(ffile, action, X, n_max_steps<span class="op">=</span><span class="dv">500</span>)</span></code></pre></div>
<p>Pong oyunu kodu alttadır [6]. Bu eğitim paralellik özelliği olmayan
normal bilgisayarda uzun sürüyor (ben birkaç gün eğittim), fakat
TensorFlow çizitini arada sırada kaydedip kaldığı yerden devam
edebildiği için parça parça işletilebilir. Oyunun otomatik nasıl
oynandığını görmek için alttaki <code>env.render</code> satırını aktive
etmek yeterli. Bilgisayarın kendi başına öğrenip oynaması müthiş!</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># pong.py</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>n_obs <span class="op">=</span> <span class="dv">80</span> <span class="op">*</span> <span class="dv">80</span>          </span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="dv">200</span>                  </span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>n_actions <span class="op">=</span> <span class="dv">3</span>            </span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>gamma <span class="op">=</span> <span class="fl">.99</span>              </span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>decay <span class="op">=</span> <span class="fl">0.99</span>             </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>save_path<span class="op">=</span><span class="st">&#39;/home/burak/Downloads/scikit-data/models/pong/pong.ckpt&#39;</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">&quot;Pong-v0&quot;</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>observation <span class="op">=</span> env.reset()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>prev_x <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>xs,rs,ys <span class="op">=</span> [],[],[]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>running_reward <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>reward_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>episode_number <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>tf_model <span class="op">=</span> {}</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.variable_scope(<span class="st">&#39;layer_one&#39;</span>,reuse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    xavier_l1 <span class="op">=</span> tf.truncated_normal_initializer(mean<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>                                                stddev<span class="op">=</span><span class="fl">1.</span><span class="op">/</span>np.sqrt(n_obs),</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>                                                dtype<span class="op">=</span>tf.float32)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>    tf_model[<span class="st">&#39;W1&#39;</span>] <span class="op">=</span> tf.get_variable(<span class="st">&quot;W1&quot;</span>, [n_obs, h], initializer<span class="op">=</span>xavier_l1)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.variable_scope(<span class="st">&#39;layer_two&#39;</span>,reuse<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    xavier_l2 <span class="op">=</span> tf.truncated_normal_initializer(mean<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>                                                stddev<span class="op">=</span><span class="fl">1.</span><span class="op">/</span>np.sqrt(h),</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>                                                dtype<span class="op">=</span>tf.float32)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    tf_model[<span class="st">&#39;W2&#39;</span>] <span class="op">=</span> tf.get_variable(<span class="st">&quot;W2&quot;</span>, [h,n_actions], initializer<span class="op">=</span>xavier_l2)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tf_discount_rewards(tf_r):</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    discount_f <span class="op">=</span> <span class="kw">lambda</span> a, v: a<span class="op">*</span>gamma <span class="op">+</span> v<span class="op">;</span></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    tf_r_reverse <span class="op">=</span> tf.scan(discount_f, tf.reverse(tf_r,[<span class="va">True</span>, <span class="va">False</span>]))</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    tf_discounted_r <span class="op">=</span> tf.reverse(tf_r_reverse,[<span class="va">True</span>, <span class="va">False</span>])</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf_discounted_r</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tf_policy_forward(x): <span class="co">#x ~ [1,D]</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> tf.matmul(x, tf_model[<span class="st">&#39;W1&#39;</span>])</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> tf.nn.relu(h)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>    logp <span class="op">=</span> tf.matmul(h, tf_model[<span class="st">&#39;W2&#39;</span>])</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> tf.nn.softmax(logp)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepro(I):</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>    I <span class="op">=</span> I[<span class="dv">35</span>:<span class="dv">195</span>] </span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>    I <span class="op">=</span> I[::<span class="dv">2</span>,::<span class="dv">2</span>,<span class="dv">0</span>]</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>    I[I <span class="op">==</span> <span class="dv">144</span>] <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>    I[I <span class="op">==</span> <span class="dv">109</span>] <span class="op">=</span> <span class="dv">0</span> </span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    I[I <span class="op">!=</span> <span class="dv">0</span>] <span class="op">=</span> <span class="dv">1</span>   </span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> I.astype(np.<span class="bu">float</span>).ravel()</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>tf_x <span class="op">=</span> tf.placeholder(dtype<span class="op">=</span>tf.float32, shape<span class="op">=</span>[<span class="va">None</span>, n_obs],name<span class="op">=</span><span class="st">&quot;tf_x&quot;</span>)</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>tf_y <span class="op">=</span> tf.placeholder(dtype<span class="op">=</span>tf.float32, shape<span class="op">=</span>[<span class="va">None</span>, n_actions],name<span class="op">=</span><span class="st">&quot;tf_y&quot;</span>)</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>tf_epr <span class="op">=</span> tf.placeholder(dtype<span class="op">=</span>tf.float32, shape<span class="op">=</span>[<span class="va">None</span>,<span class="dv">1</span>], name<span class="op">=</span><span class="st">&quot;tf_epr&quot;</span>)</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>tf_discounted_epr <span class="op">=</span> tf_discount_rewards(tf_epr)</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>tf_mean, tf_variance<span class="op">=</span> tf.nn.moments(tf_discounted_epr, [<span class="dv">0</span>],</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>                                    shift<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="st">&quot;reward_moments&quot;</span>)</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>tf_discounted_epr <span class="op">-=</span> tf_mean</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>tf_discounted_epr <span class="op">/=</span> tf.sqrt(tf_variance <span class="op">+</span> <span class="fl">1e-6</span>)</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>tf_aprob <span class="op">=</span> tf_policy_forward(tf_x)</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> tf.nn.l2_loss(tf_y<span class="op">-</span>tf_aprob)</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> tf.train.RMSPropOptimizer(learning_rate, decay<span class="op">=</span>decay)</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>tf_grads <span class="op">=</span> optimizer.compute_gradients(loss,</span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>                                       var_list<span class="op">=</span>tf.trainable_variables(),</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>                                       grad_loss<span class="op">=</span>tf_discounted_epr)</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>train_op <span class="op">=</span> optimizer.apply_gradients(tf_grads)</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.InteractiveSession()</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>tf.initialize_all_variables().run()</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a>saver <span class="op">=</span> tf.train.Saver(tf.all_variables())</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>load_was_success <span class="op">=</span> <span class="va">True</span> </span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># mevcut TF ciziti varsa yuklemeye ugras, kaldigi yerden devam icin</span></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>    save_dir <span class="op">=</span> <span class="st">&#39;/&#39;</span>.join(save_path.split(<span class="st">&#39;/&#39;</span>)[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>    ckpt <span class="op">=</span> tf.train.get_checkpoint_state(save_dir)</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>    load_path <span class="op">=</span> ckpt.model_checkpoint_path</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>    saver.restore(sess, load_path)</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> <span class="st">&quot;no saved model to load. starting new session&quot;</span></span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>    load_was_success <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> <span class="st">&quot;loaded model: </span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(load_path)</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>    saver <span class="op">=</span> tf.train.Saver(tf.all_variables())</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>    episode_number <span class="op">=</span> <span class="bu">int</span>(load_path.split(<span class="st">&#39;-&#39;</span>)[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>    <span class="co"># oyunu seyretmek icin bir sure egitildikten sonra</span></span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>    <span class="co"># alttaki satiri aktif hale getirebiliriz</span></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>    <span class="co"># env.render() </span></span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>    cur_x <span class="op">=</span> prepro(observation)</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> cur_x <span class="op">-</span> prev_x <span class="cf">if</span> prev_x <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> np.zeros(n_obs)</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>    prev_x <span class="op">=</span> cur_x</span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>    feed <span class="op">=</span> {tf_x: np.reshape(x, (<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>))}</span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>    aprob <span class="op">=</span> sess.run(tf_aprob,feed) <span class="op">;</span> aprob <span class="op">=</span> aprob[<span class="dv">0</span>,:]</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>    action <span class="op">=</span> np.random.choice(n_actions, p<span class="op">=</span>aprob)</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>    label <span class="op">=</span> np.zeros_like(aprob) <span class="op">;</span> label[action] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>    observation, reward, done, info <span class="op">=</span> env.step(action<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>    reward_sum <span class="op">+=</span> reward</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>    xs.append(x) <span class="op">;</span> ys.append(label) <span class="op">;</span> rs.append(reward)</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> done:</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>        running_reward <span class="op">=</span> reward_sum <span class="cf">if</span> running_reward <span class="op">\</span></span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>                         <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> running_reward <span class="op">*</span> <span class="fl">0.99</span> <span class="op">+</span> reward_sum <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>        feed <span class="op">=</span> {tf_x: np.vstack(xs), tf_epr: np.vstack(rs), tf_y: np.vstack(ys)}</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> sess.run(train_op,feed)</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> episode_number <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span> <span class="st">&#39;ep </span><span class="sc">{}</span><span class="st">: reward: </span><span class="sc">{}</span><span class="st">, mean reward: </span><span class="sc">{:3f}</span><span class="st">&#39;</span>.<span class="op">\</span></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>                <span class="bu">format</span>(episode_number, reward_sum, running_reward)</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span> <span class="st">&#39;</span><span class="ch">\t</span><span class="st">ep </span><span class="sc">{}</span><span class="st">: reward: </span><span class="sc">{}</span><span class="st">&#39;</span>.<span class="bu">format</span>(episode_number, reward_sum)</span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>        xs,rs,ys <span class="op">=</span> [],[],[] </span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a>        episode_number <span class="op">+=</span> <span class="dv">1</span> </span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>        observation <span class="op">=</span> env.reset() </span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a>        reward_sum <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> episode_number <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a>            saver.save(sess, save_path, global_step<span class="op">=</span>episode_number)</span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span> <span class="st">&quot;SAVED MODEL #</span><span class="sc">{}</span><span class="st">&quot;</span>.<span class="bu">format</span>(episode_number)</span></code></pre></div>
<p>Softmax</p>
<p>Softmax sonlu sayıda seçenek üzerinden bir dağılım tanımlar,</p>
<p><span class="math display">\[
\pi_\theta(s,a) = \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} }
\]</span></p>
<p>ki <span class="math inline">\(h(s,a,\theta) =
\phi(s,a)^T\theta\)</span>. Şimdi ilke gradyanını softmax ile nasıl
işletiriz onu görelim. Softmax’in kodlaması için bir çözüm ayrıksal
olarak (bir matriste mesela) her <span
class="math inline">\(s,a\)</span> kombinasyonu için gerekli ağırlıkları
tutmak. O zaman spesifik bir <span class="math inline">\(\phi\)</span>
çağrısı sonrası <span class="math inline">\(\theta\)</span> ile bu
katsayılar çarpılır ve sonuç alınır. <span
class="math inline">\(\theta\)</span> ilkenin ne olduğunu, onun özünü
tanımlar. DYSA durumundan bir fark softmax için otomatik türeve gerek
olmadan direk türevi kendimiz hesaplayabiliriz [4]. Üstteki formülün log
gradyanı</p>
<p><span class="math display">\[
\nabla_\theta \log \pi_\theta =
\nabla_\theta \log \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)} }
\]</span></p>
<p><span class="math display">\[ = \nabla_\theta \big[ \log
e^{h(s,a,\theta)} - \log \sum_b e^{h(s,b,\theta)}\big]\]</span></p>
<p>çünkü</p>
<p><span class="math display">\[ \log(\frac{x}{y}) = \log x - \log y
\]</span></p>
<p>Devam edelim</p>
<p><span class="math display">\[ = \nabla_\theta \big[ h(s,a,\theta) -
\log \sum_b e^{h(s,b,\theta)} \big]\]</span></p>
<p>Gradyan her iki terime de uygulanır,</p>
<p><span class="math display">\[
= \phi(s,a) - \sum_b h(s,b,\theta)\frac{e^{h(s,b,\theta)}}{\sum_b
e^{h(s,b,\theta)}}
\]</span></p>
<p><span class="math display">\[
= \phi(s,a) - \sum_b h(s,b,\theta) \pi_\theta(b,s)
\]</span></p>
<p><span class="math display">\[
= \phi(s,a) - E_{\pi_\theta} \big[ \phi(s,\cdot) \big]
\]</span></p>
<p>İlginç ve ilk bakışta anlaşılabilen / akla yatacak (intuitive) bir
sonuca ulaştık. Log gradyanı içinde bulunduğumuz konum ve attığımız adım
için hesaplanan <span class="math inline">\(\phi\)</span>’den mevcut
atılabilecek tüm adımlar üzerinden hesaplanan bir <span
class="math inline">\(\phi\)</span> ortalamasının çıkartılmış hali. Yani
“bu spesifik <span class="math inline">\(\phi\)</span> normalden ne
kadar fazla?’’ sorusunu sormuş oluyorum, ve gradyanın gideceği,
iyileştirme yönünü bu sayı belirliyor. Yani bir <span
class="math inline">\(\phi\)</span> eğer normalden fazla ortaya
çıkıyorsa ve iyi sonuç alıyorsa (skorla çarpım yaptığımızı unutmayalım),
ilkeyi o yönde daha fazla güncelliyoruz ki bu başarılı sonuçları daha
fazla alabilelim.</p>
<p>Kaynaklar</p>
<p>[1] Géron, <em>Hands-On Machine Learning with Scikit-Learn and
TensorFlow</em></p>
<p>[2] Karpathy, <em>Deep Reinforcement Learning: Pong from Pixels</em>,
<a
href="http://karpathy.github.io/2016/05/31/rl/">http://karpathy.github.io/2016/05/31/rl/</a></p>
<p>[3] Bayramlı, <em>OpenAI Gym, Pong, Derin Takviyeli Öğrenme</em>, <a
href="https://burakbayramli.github.io/dersblog/sk/2017/09/openai-gym-pong-derin-takviyeli-ogrenme.html">https://burakbayramli.github.io/dersblog/sk/2017/09/openai-gym-pong-derin-takviyeli-ogrenme.html</a></p>
<p>[3] Bayramlı, <em>OpenAI, Çubuklu Araba, CartPole</em>, <a
href="https://burakbayramli.github.io/dersblog/sk/2017/09/openai-cubuklu-araba-cartpole.html">https://burakbayramli.github.io/dersblog/sk/2017/09/openai-cubuklu-araba-cartpole.html</a></p>
<p>[4] Silver, <em>Monte-Carlo Simulation Balancing</em>, <a
href="http://www.machinelearning.org/archive/icml2009/papers/500.pdf">http://www.machinelearning.org/archive/icml2009/papers/500.pdf</a></p>
<p>[5] Silver, <em>Reinforcement Learning</em>, <a
href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html</a></p>
<p>[6] Greydanus, <em>Solves Pong with Policy Gradients in
Tensorflow</em>, <a
href="https://gist.github.com/greydanus/5036f784eec2036252e1990da21eda18">https://gist.github.com/greydanus/5036f784eec2036252e1990da21eda18</a></p>
<p>[7] Bayramlı, Bilgisayar Bilim, <em>Yapay Zeka ve Müsabaka</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
