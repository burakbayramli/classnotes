<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Ders 2</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="ders-2">Ders 2</h1>
<p>Çoğunlukla özvektör kavramına atıf yapıldığında söylenmek istenen
(<span class="math inline">\(C\)</span> kompleks sayıların kümesi
olsun),</p>
<p><span class="math display">\[ Av = \lambda v, \qquad \lambda \in C
\]</span></p>
<p>ifadesidir, yani “sağ özvektör’’, yani bir matrisi <em>sağdan</em>
çarpınca boyu büyüyen ya da küçülen vektör. Sol özvektörler de mümkün,
bu durumda,</p>
<p><span class="math display">\[ v^TA = \lambda v^T, \qquad \lambda \in
C \]</span></p>
<p><span class="math inline">\(A\)</span>’nin spektrumu <span
class="math inline">\(\sigma(A)\)</span> o matrisin tüm özvektörlerinin
kümesidir. Numpy ile</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.linalg <span class="im">as</span> lin</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> [[<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">3</span>],[<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">6</span>],[<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>]]</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array(A)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>[V,D] <span class="op">=</span> lin.eig(A)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;ozvektorler&#39;</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> D</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;ozdegerler&#39;</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> V</span></code></pre></div>
<pre><code>ozvektorler
[[-0.41963784+0.j          0.72738656+0.j          0.72738656-0.j        ]
 [-0.66394149+0.j         -0.42567121+0.33744486j -0.42567121-0.33744486j]
 [-0.61893924+0.j         -0.25117492-0.33579002j -0.25117492+0.33579002j]]
ozdegerler
[ 13.75351937+0.j          -0.37675969+0.47073926j  -0.37675969-0.47073926j]</code></pre>
<p>Eğer <span class="math inline">\(B = PAP^{-1}\)</span>, ki <span
class="math inline">\(P\)</span> eşsiz olmayacak şekilde, o zaman <span
class="math inline">\(\sigma(B) = \sigma(A)\)</span>. İspatsız
veriyoruz. Yani <span class="math inline">\(P\)</span> ve onun tersi ile
bir matrisi iki taraftan çarpmak, o matrisin spektrumunu
değiştirmiyor.</p>
<p>Eğer <span class="math inline">\(\lambda \in C\)</span> bir özdeğer
ise, onun eşleniği (conjugate) <span
class="math inline">\(\bar{\lambda}\)</span> da bir özdeğerdir. Bu
sebeple reel matris <span class="math inline">\(A\)</span> için <span
class="math inline">\(\sigma{A} = \overline{\sigma{A}}\)</span>.</p>
<p>Bir reel matrisin tüm özdeğerlerinin reel olduğu “güzel’’ bir durum
vardır; bu durum matrisin simetrik olduğu durumdur, yani <span
class="math inline">\(S^T = S\)</span>. Bu güzel durum aslında pratikte
pek çok kez karşımıza çıkar; mesela kovaryans matrisleri olarak.</p>
<p>Simetrik matrisin özgün özdeğerlerine tekabül eden özvektörleri
birbirine dikgendir. İspat, $v_i^T S v_j $ formülüne bakalım, ki <span
class="math inline">\(v_i,v_j\)</span> özvektörler, <span
class="math inline">\(S\)</span> simetrik matris.</p>
<p>Özdeğer eşitliğinden <span class="math inline">\(S v_j = \lambda_j
v_j\)</span> kullanırsam,</p>
<p><span class="math display">\[ v_i^T S v_j  = \lambda_j v_i^T v_j
\]</span></p>
<p>Eğer <span class="math inline">\(v_i^TS = \lambda_i v_i^T\)</span>
kullanırsam,</p>
<p><span class="math display">\[ v_i^T S v_j  = \lambda_i v_i^T v_j
\]</span></p>
<p>elde ederim. İkisini bir araya koyalım,</p>
<p><span class="math display">\[ \lambda_j v_i^T v_j =  \lambda_i v_i^T
v_j \]</span></p>
<p><span class="math display">\[ (\lambda_j-\lambda_i) v_i^T v_j =
0\]</span></p>
<p>Bu eşitliğin doğru olması sadece iki durumda olabilir; ya <span
class="math inline">\(\lambda_i,\lambda_j\)</span> birbiriyle aynıdır,
ya da birbirinden farklıdır ama o zaman <span
class="math inline">\(v_i,v_j\)</span> birbirine dikgen olmalıdır.</p>
<p>[norm atlandı]</p>
<p>Eksi Bakışımlı Matrisler (Skew-symmetric Matrices)</p>
<p>Eğer <span class="math inline">\(A^T = -A\)</span> ise bu matrislere
eksi bakışımlı deniyor [5]. Mesela</p>
<p><span class="math display">\[
\left[\begin{array}{rr}
0 &amp; 2 \\ -2 &amp; 0
\end{array}\right]
\]</span></p>
<p>Bu matrisin devriği ve negatifi aynıdır. Eksi bakışımlı matrislerin
köşegeni sıfır olmalıdır. Bu tür matrislerin ilginç bazı özellikleri
var, mesela, hatırlarsak simetrik matrislerin pür reel spektrumu vardı.
Eksi bakışımlı matrislerin spektrumu pür sanaldır (imaginary).</p>
<p>[köşegenleştirme atlandı]</p>
<p>Çapraz Çarpım (cross-product)</p>
<p>Noktasal çarpım bize bir skalar verir. İki vektör arasındaki çapraz
çarpım bize başka bir vektör verir; <span class="math inline">\(u,v \in
\mathbb{R}^3\)</span> olmak üzere çapraz çarpım,</p>
<p><span class="math display">\[
u \times v =
\left[\begin{array}{r}
u_2v_3 - u_3v_2 \\
u_3v_1 - u_1v_3 \\
u_1v_2 - u_2v_1
\end{array}\right]
\]</span></p>
<p>Yani <span class="math inline">\(\mathbb{R}^3\)</span>’teki iki
vektör <span class="math inline">\(u,v\)</span>’nin çapraz çarpımı
alınabilir, ve <span class="math inline">\(\mathbb{R}^3\)</span>’te yeni
bir vektör elde ederiz. Bu yeni vektör <span
class="math inline">\(u,v\)</span>’ye dikgendir. Sağ el kuralı (alttaki
resimde <span class="math inline">\(u,v\)</span> yerine <span
class="math inline">\(a,b\)</span> kullanılmış)</p>
<p><img src="righthand.png" /></p>
<p>Ayrıca çapraz çarpım simetriktir, <span class="math inline">\(u
\times v = - u \times v\)</span>.</p>
<p>Eksi Bakışımlı Matris (Skew-Symmetric Matrix)</p>
<p>Bilgisayar Görüşü (Computer Vision) alanında oldukça yaygın bir
matris olan eksi bakışımlı bir matris alttadır, ve onu herhangi bir
vektörden oluşturan şapka operatörüyle gösterelim, yani <span
class="math inline">\(u\)</span> için <span
class="math inline">\(\hat{u}\)</span> olsun,</p>
<p><span class="math display">\[
\hat{u} = \left[\begin{array}{rrr}
0 &amp; -u_3 &amp; u_2 \\
u_3 &amp; 0 &amp; -u_1 \\
-u_2 &amp; u_1 &amp; 0
\end{array}\right] \in \mathbb{R}^{3 \times 3}
\]</span></p>
<p>Bu tür matrislerin kertesinin çift sayı olması şarttır.</p>
<p><span class="math inline">\(\hat{u}\)</span>’nun ilginç bir özelliği
var, herhangi bir vektör <span class="math inline">\(v\)</span> ile</p>
<p><span class="math display">\[ \hat{u}v = u \times v \]</span></p>
<p>ki <span class="math inline">\(\times\)</span> bir çapraz çarpım.
Yani, öncelikle, her vektörün bir eksi bakışımlı matris karşılığı var,
ve üstteki operatör ile bu geçişi yapabiliriz, ayrıca ne zaman bir
çapraz çarpım görsek, onu eksi bakışımlı matris çevirimi üzerinen bir
normal matris çarpımı olarak temsil edebiliriz. Bu faydalı çünkü çapraz
çarpımla uğraşmak biraz külfetli olabiliyor.</p>
<p>3 boyut bağlamında <span class="math inline">\(\hat{u}\)</span>’nun
kertesi tabii ki 2’dir; çünkü eksi bakışımlı matrislerinin kertesinin
çift olması şart ise, ve 3 boyutlu durumda bu kerte en fazla 3 olabilir,
ama 3 olamaz çünkü 3 çift sayı değil, o zaman 2 olur.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> skew(a):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> np.array([[<span class="dv">0</span>,<span class="op">-</span>a[<span class="dv">2</span>],a[<span class="dv">1</span>]],[a[<span class="dv">2</span>],<span class="dv">0</span>,<span class="op">-</span>a[<span class="dv">0</span>]],[<span class="op">-</span>a[<span class="dv">1</span>],a[<span class="dv">0</span>],<span class="dv">0</span>]])</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]]).T</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> skew(A)</span></code></pre></div>
<pre><code>[[ 0 -3  2]
 [ 3  0 -1]
 [-2  1  0]]</code></pre>
<p><span class="math inline">\(A^3\)</span></p>
<p>Diyelim ki <span class="math inline">\(A\)</span> bir <span
class="math inline">\(3 \times 3\)</span> eksi bakışımlı matris, ve
<span class="math inline">\(a = (a_1,a_2,a_3)\)</span> vektörü üzerinden
oluşturulmuş. <span class="math inline">\(A^3\)</span> nedir?</p>
<p><span class="math display">\[A \cdot x = u \times x\]</span></p>
<p>olduğunu biliyoruz. O zaman</p>
<p><span class="math display">\[A^3 x = a \times ( a \times ( a \times
x))\]</span></p>
<p>demektir. Eşitliğin sağ tarafına bakarsak, eğer <span
class="math inline">\(a=e\)</span>, ki <span
class="math inline">\(e\)</span> bir birim vektörü olsun, yani <span
class="math inline">\(||e||=1\)</span>, o zaman norm’ların ilişkisi şu
şekilde olurdu,</p>
<p><span class="math display">\[||e\times(e\times (e\times
x)))\|=||e\times (e\times x))|| =||e\times x||\]</span></p>
<p>Neden?</p>
<p><span class="math display">\[ ||e \times x|| = ||e||||x|| \sin \theta
= ||x|| \sin \theta\]</span></p>
<p>O zaman,</p>
<p><span class="math display">\[||e\times (e\times x))|| = ||e|||| e
\times x|| \sin 90 = ||e \times x||\]</span></p>
<p>Böyle gider. Yani norm eşitlikleri doğru.</p>
<p>Eğer <span class="math inline">\(a = ||a||e\)</span> kabul
edersek,</p>
<p><span class="math display">\[||a\times(a\times (a\times x)))|| =
||a||^3 ||e\times(e\times (e\times x)))||\]</span></p>
<p><span class="math display">\[ =\|a\|^3\|e\times x\| \]</span></p>
<p><span class="math display">\[ =\|a\|^2\|a\times x\| \]</span></p>
<p><span class="math display">\[ =(a^Ta)\|a\times x\| \]</span></p>
<p>Yani <span class="math inline">\(A^3x = \pm (a^Ta) Ax\)</span>; eksi
mi artı mı? Eksi işareti olduğunu sağ el kuralıyla görebiliriz. Demek ki
<span class="math inline">\(A^3 = -(a^Ta) \cdot A\)</span>.</p>
<p>Simetrik matrisler için</p>
<p><span class="math display">\[ A = UDU^T \]</span></p>
<p>olduğunu biliyoruz. Eksi bakışımlı matrisler için</p>
<p><span class="math display">\[ S = UBU^T \]</span></p>
<p>olur ki <span class="math inline">\(B\)</span> bir blok köşegen
matristir, <span class="math inline">\(U\)</span> dikgen. <span
class="math inline">\(S\)</span>’in özdeğerleri pür sanaldır. Blok
köşegen matris <span
class="math inline">\(diag(a_1Z,a_2Z,...,z_mZ,0,..0)\)</span>, ve <span
class="math inline">\(Z = \left[\begin{array}{cc} 0 &amp; 1 \\ -1 &amp;
0 \end{array}\right]\)</span>. Köşegende blok olması garip gelebilir,
fakat tek sayılar yerine çapraz yönde birkaç sayının “üst üste’’ olduğu
bir durum bu. Basit matris çarpımı ile kolay bir şekilde kontrol
edilebilir ki</p>
<p><span class="math inline">\(Z^2 = -I, \qquad Z^3 = -Z, \qquad Z^4 =
I\)</span></p>
<p>Matris Üstelleri (Matrix Exponentials) [1, sf. 482]</p>
<p><span class="math inline">\(t \in \mathbb{R}\)</span> ve bir kare
matris <span class="math inline">\(n \times n\)</span> matrisi <span
class="math inline">\(A\)</span> için, matris üsteli,</p>
<p><span class="math display">\[ U(t) = e^{tA} = \exp(tA) \]</span></p>
<p>alttaki problem için özgün bir <span class="math inline">\(n \times
n\)</span> çözümüdür;</p>
<p><span class="math display">\[ \frac{dU}{dt} = AU, \qquad U(0) = I
\]</span></p>
<p>Dikkat: matris üsteli <span class="math inline">\(\exp\)</span>
fonksiyonun teker teker matris öğeleri üzerinde işletilmiş hali
değildir.</p>
<p>Matris üstelleri bir seri olarak ta gösterilebilir,</p>
<p><span class="math display">\[ e^{tA} = \sum_{n=0}^{\infty}
\frac{t^n}{n!} A^n =
I + tA + \frac{t^2}{2}A^2 + \frac{t^3}{6}A^3 + ...
\]</span></p>
<p>Bu seri yakınsayan (converging) bir seridir.</p>
<p>Örnek</p>
<p><span class="math display">\[ A = \left[\begin{array}{rrr}
0 &amp; 1 \\ 0 &amp; 0
\end{array}\right] \]</span></p>
<p>için</p>
<p><span class="math display">\[ e^{tA} = \left[\begin{array}{rrr}
1 &amp; t \\ 0 &amp; 1
\end{array}\right] \]</span></p>
<p>Örnek</p>
<p><span class="math display">\[ A = \left[\begin{array}{rrr}
1 &amp; 0 \\ 0 &amp; 1
\end{array}\right] \]</span></p>
<p>için</p>
<p><span class="math display">\[ e^{tA} = \left[\begin{array}{rrr}
e^t &amp; 0 \\ 0 &amp; e^t
\end{array}\right] \]</span></p>
<p>Teori</p>
<p>Eğer <span class="math inline">\(A\)</span> bir eksi bakışımlı matris
ise, <span class="math inline">\(Q(t) = e^{tA}\)</span> muntazam
(proper) bir dikgen matristir.</p>
<p>İspat</p>
<p>Dikgenlik tersi ile devriğin aynı olması demektir, o zaman üstteki
eşitlikte sol tarafın tersi, sağ tarafın devriği aynı olmalı, yani <span
class="math inline">\(Q(t)^{-1}\)</span> ile <span
class="math inline">\(e^{tA^T}\)</span>.</p>
<p><span class="math display">\[ Q(t)^{-1} = e^{-tA} = e^{tA^T} =
(e^{tA})^T = Q(t)^T \]</span></p>
<p>Hakikaten de öyle.</p>
<p>Not: Bir diğer ispata göre <em>tüm</em> dikgen matrisler bir eksi
bakışımlı matrisin <span class="math inline">\(e\)</span>’nin üsteli
alınarak oluşturulabilir. Buradaki nüansa dikkat, tüm eksi bakışımlı
matrislerin üsteli dikgendir demek ile <em>tüm</em> dikgen matrisler
eksi bakışımlı matrislerin üsteli alınmış halidir demek farklı.</p>
<p>Üstteki kavramları kullanarak dönüş (rotation) yapmayı sağlayan
Rodriguez formülü [6]’da bulunabilir.</p>
<p>SVD</p>
<p>Bu işlemin özdeğer / özvektör hesabının karesel olmayan matrisler
durumundaki genelleştirilmiş hali olduğu düşünülebilir. Pek çok lineer
cebir işlemi, mesela tersini alma, kerte hesabı, vs. SVD bağlamında
incelenebilir. Genelleştirme dedik, eğer <span
class="math inline">\(A\)</span> karesel değilse özvektörleri
hesaplayamayız, ama <span class="math inline">\(A^TA\)</span>
kareseldir, ve bu matrisin özvektörleri <span
class="math inline">\(A\)</span>’nin SVD’si ile yakından alakalıdır.</p>
<p>[İspat atlandı]</p>
<p>Geometrik olarak <span class="math inline">\(A = U \Sigma
V^T\)</span> ile gösterilen SVD’nin bir <span class="math inline">\(x
\in \mathbb{R}^n\)</span>’yi <span class="math inline">\(A\)</span>
uzerinden transform ettiğimiz durumda, <span class="math inline">\(y =
Ax\)</span> diyelim, <span class="math inline">\(y\)</span>’nin <span
class="math inline">\(U\)</span>’da bazındaki kordinatlarının <span
class="math inline">\(V\)</span> bazındaki kordinatları ile bir ilişki
ortaya çıkarttığını söyleyebiliriz; bu ilişki <span
class="math inline">\(\Sigma\)</span>’nin öğeleri üzerinden bir
ölçeklemeden ibarettir. Yani</p>
<p><span class="math display">\[ y = Ax = U\Sigma V^Tx
\quad \Leftrightarrow \quad
U^Ty = \Sigma V^T x
\]</span></p>
<p>[birim küre ellipsoid eşlemesi atlandı]</p>
<p>Genelleştirilmiş Ters (Generalized -Moore Penrose- Inverse)</p>
<p>Lineer sistem çözerken <span class="math inline">\(Ax = b\)</span>
için eğer <span class="math inline">\(A\)</span>’nin tersi
alınabiliyorsa, çözüm kolay, <span class="math inline">\(x^\ast =
A^{-1}b\)</span>. Fakat <span class="math inline">\(A\)</span>’nin tersi
alınamıyorsa, ki bu <span class="math inline">\(A\)</span> karesel
olmadığında otomatik olarak doğru olacaktır, ne yapacağız?
Genelleştirilmiş tersi alma, ya da sözde ters (pseudoinverse) işlemi
burada ise yarar. Sözde ters için, her <span
class="math inline">\(A\)</span> için bir SVD olduğuna göre, <span
class="math inline">\(A = U \Sigma V^T\)</span>, ve <span
class="math inline">\(\Sigma\)</span>’nın sıfır olmayan eşsiz
değerlerinin tersi alınır (yani öğe <span
class="math inline">\(\sigma_i\)</span>, <span
class="math inline">\(1/\sigma_i\)</span> olur), sıfır değerlerine
dokunulmaz, bu sonuçlar yeni bir <span
class="math inline">\(\Sigma^{\dagger}\)</span>’in köşegenine dizilir,
“sözde ters’’ bu matris olur,</p>
<p><span class="math display">\[ \Sigma^{\dagger} =
\left[\begin{array}{cc}
\Sigma_1^{-1} &amp; 0 \\ 0 &amp; 0
\end{array}\right]\]</span></p>
<p>Ve bu ters işlemi tüm <span class="math inline">\(A\)</span>’nin
tersini almak için kullanılır, ki bu sözde tersi de boyutu <span
class="math inline">\(n \times m\)</span> olan bir <span
class="math inline">\(A^{\dagger}\)</span> ile gösteriyoruz (İngilizce
“<span class="math inline">\(A\)</span>-dagger’’ olarak telafuz
ediliyor, biz”<span class="math inline">\(A\)</span>-kama’’
diyelim),</p>
<p><span class="math display">\[
A^{\dagger} = V \Sigma^{\dagger} U^T  
\qquad (1)
\]</span></p>
<p>Bu noktaya nasıl geldiğimize dikkat, eğer SVD sonucunun pür tersini
alabilseydik,</p>
<p><span class="math display">\[ A^{-1} = V^{-T}\Sigma^{-1}U^{-1}
\]</span></p>
<p>Dikgen matrisler için <span class="math inline">\(Q^{-1}=Q^T\)</span>
olduğu için</p>
<p><span class="math display">\[ = V\Sigma^{-1}U^{T} \]</span></p>
<p><span class="math inline">\(\Sigma^{-1}\)</span> olmadığı için yerine
<span class="math inline">\(\Sigma^{\dagger}\)</span> kullanıyoruz ve
(1)’e erişiyoruz.</p>
<p>Bazı özellikler,</p>
<p><span class="math display">\[ A A^{\dagger} A = A \]</span></p>
<p>ya da</p>
<p><span class="math display">\[ A^{\dagger} A  A^{\dagger}
=  A^{\dagger} \]</span></p>
<p>Peki sözde tersi alma işlemini denklem sistemi çözmekte nasıl
kullanırız?</p>
<p>Lineer sistem çözümü bağlamında durumunda 3 türlü sonuç olabileceğini
görmüştük. Eğer sonsuz tane çözüm varsa, bu büyük bir ihtimalle
problemin tam kısıtlanmamış (constrained) olması ile alakalıdır. Tabii
hiçbir çözüm olmayabilir, ve size para veren kişi sizden hala çözüm
beklemektedir (!), bu durumda <span
class="math inline">\(Ax=b\)</span>’yi çözmek yerine ona en yakın
olabilecek şeyi çözebiliriz, yani <span class="math inline">\(|Ax -
b|^2\)</span>’yi minimize etmeyi seçebiliriz, <span
class="math inline">\(Ax\)</span>’i <span
class="math inline">\(b\)</span>’yi mümkün olduğu kadar yaklaştırırız.
Burada sözde ters ise yarar, çünkü <span class="math inline">\(x^\ast =
A^{\dagger}b\)</span> ile hesaplanan çözüm aynı zamanda <span
class="math inline">\(|Ax - b|^2\)</span>’yi minimize eder! Bu çözüm En
Az Kareler (Least Squares) çözümü olarak ta bilinir, tabii burada sistem
aşırı belirtilmiş (overdetermined) değil, eksik belirtilmiş
(underdetermined) durumda. Not: Ayrıca sözde ters ile bulunan <span
class="math inline">\(x^\ast\)</span>’in mümkün tüm çözümler arasında
“norm’ü en az olan <span class="math inline">\(x^\ast\)</span>’i bulduğu
da’’ söylenir.</p>
<p>Eğer çözüm özgün ise, sözde ters yine işler, özgün çözümü bulur. Yani
her halükarda sözde ters tüm problemlerimizi çözer.</p>
<p>Lineer cebir’i böylece gözden geçirmiş olduk. Artık dersimizin ana
konularına başlayabiliriz.</p>
<p>Hareketli bir Sahneyi Temsil Etmek</p>
<p>Burada sahne dış dünya, yani kamera ile hareket ederken gördüğümüz
şeyler.</p>
<p>Hareket ederken kamera pek çok resim alabilir, tabii bu dersimiz uzun
zamandır yapılan araştırmalara dayanıyor, ve bu araştırmalar çoğunlukla
iki resim durumuna odaklandılar; fakat günümüzde bir kamera saniyede
mesela 30 tane resim çekebilir, bu durum için gerekli matematiği de
göreceğiz. Önce iki resimle başlayacağız, ve bu matematiğin daha genel,
çok resimli haline de kendimizi hazırlayacağız.</p>
<p>3D’de Yeniden Oluşturmak (3D Reconstruction)</p>
<p>Durağan olduğu kabul edilen 3D dış dünyayı pek çok açıdan ama iki
boyutlu resmi ile tekrar oluşturma çabasının bilimde uzun bir tarihi
var. Bu problem klasik bir “kötü konumlanmış (ill-posed)’’ problemdir,
çünkü yeniden oluşturulan sonuç tipik olarak özgün değildir (pek çok
farklı yeniden oluşturma mümkündür). Bu yüzden ek bazı kısıtlamalar
getirmek gerekir. Bu alanda hala yapılacak çok iş olduğunu belirtmek
isterim, yani bilim dalımız oldukça bakir [araştırmacılar, atlayın].</p>
<p>Dış dünyanın gördüğümüz imajdaki nasıl oluştuğunu perspektif izdüşümü
(perspective projection) üzerinden modelleyeceğiz, bu modelleme kamera
modeli olarak iğne deliği kamera (pinhole camera) modelini kullanır. Bu
modeli şöyle hayal etmek mümkün, karanlık bir odadayız, duvarda tek bir
delik var, ve bu oda dışındaki tüm görüntüler bu delik üzerinden odaya
giriyor. Perspektif izdüşüm ilk kez Öklit tarafından, I.O. 400 yılında
araştırıldı; bu hakikaten çok ilginç, yani sonuçta bugün bilgisayarlar
ile araştırdığımız bu konunun temelindeki bazı kavramların ne kadar
önceden beri bilindiği şaşırtıcı olabiliyor.</p>
<p>Ardından bu konu Rönesans sırasında çok yoğun araştırıldı, bu
zamanlarda yapılan resimlerdeki derinliği temsil etme çabaları bugüne
kalan eserlerden hepimiz biliyoruz. Sanatçılar, bilimciler iki boyut
üzerinde derinliği, üç boyutluluğu gösterebilmek için kafa yordular, ve
perspektif izdüşümün araştırılması 17. ve 18. yüzyılda perspektif
geometrisi adında bir yeni alana dönüştü.</p>
<p>Çoklu Bakış Açıdan Tekrar Oluşturma (Multiview Reconstruction)
alanında yapılan ilk araştırma oldukça eskiye gidiyor, ki bu da
şaşırtıcı. Kruppa bu konuyu 1913’te araştırdı, iki farklı kameranın aynı
objeye dönük iki resmine odaklandı, kendine şu soruyu sordu, “bu iki
resimde en az kaç tane noktaya bakmalıyım ki 3 boyutta bir model
oluşturabileyim’’. Kruppa gösterdi ki en az 5 nokta sonlu miktarda çözüm
bulmak için yeterli, ki kameranın hareketi de buna dahil. Tabii bulunan
özgün tek çözüm değildir, ama daha önce söylediğimiz gibi bu problem
kötü konumlanmış bir problemdir, ama en azından çözüm sonsuz tane değil,
sonlu sayıda.</p>
<p>İki görüntüden hareket ve yapıyı çıkartabilen ilk lineer algoritma
Lonquet-Higgins tarafından 1981’de bulundu, bu algoritma eş kutupsal
kısıtlama (epipolar constraint) kullanarak bu işi becerdi. Bu derste eş
kutupsal kısıtlama konusunu öğreneceğiz. Bu buluş pek çok takip eden
diğer buluşa ilham verdi, 80 ve 90’li yıllarda ek buluşlar yapıldı.
Ardından alanımızın klasik kitaplarından Zisserman ve arkadaşlarının
yazdığı Çoklu Bakış Açı Geometrisi (<em>Multiple View Geometry</em>)
adlı kitap var, ve iş giderek bizim bu derste kullandığımız en güncel
olan kitaba geliyor, <em>An Invitation to 3D Vision</em>.</p>
<p>Derste işlediğimiz konu farklı isimlerde ortaya çıkabiliyor,
hareketten yapı çıkartmak (structure from motion) ismini gördük, bir
diğer isim görsel (visual) SLAM, ki SLAM kısaltması “aynı anda yer
belirlemek ve haritalamak (simultaneous localization and mapping)’’
kelimelerinden geliyor. Robotik alanındakiler bu kelimeyi çok
kullanırlar, bir robotun dış dünyada hem etrafını haritalaması, hem de
aynı anda o harita içindeki yerini kestirebilmesi, hesaplaması bu alanın
baş problemlerinden. Tabii görsel SLAM bunu görsel olarak yapabilmek;
çünkü çok farklı şekillerde SLAM yapılabiliyor, mesela 1. ders başında
söylediğimiz gibi lazer algılayıcılarla SLAM yapılabilir, hatta sonar
algılayıcılarla bile, ya da tüm bunları bir algılayıcı füzyonu (sensor
fusion) üzerinden birleştirerek.</p>
<p>Kaynaklar</p>
<p>[1] Olver, <em>Applied Linear Algebra</em></p>
<p>[2] Bayramlı, Lineer Cebir, <em>Ders 15</em></p>
<p>[3] Sastry, <em>An Invitation to 3-D Vision</em></p>
<p>[4] Zissermann, <em>Multiple View Geometry</em></p>
<p>[5] Bayramlı, Lineer Cebir, <em>Ders 5</em></p>
<p>[6] Bayramlı, <em>Fizik - Döndürme (Rotation)</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
