<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="ders-2">Ders 2</h1>
<p>Çoğunlukla özvektör kavramına atıf yapıldığında söylenmek istenen (<span class="math inline">\(C\)</span> kompleks sayıların kümesi olsun),</p>
<p><span class="math display">\[ Av = \lambda v, \qquad \lambda \in C \]</span></p>
<p>ifadesidir, yani &quot;sağ özvektör'', yani bir matrisi <em>sağdan</em> çarpınca boyu büyüyen ya da küçülen vektör. Sol özvektörler de mümkün, bu durumda,</p>
<p><span class="math display">\[ v^TA = \lambda v^T, \qquad \lambda \in C \]</span></p>
<p><span class="math inline">\(A\)</span>'nin spektrumu <span class="math inline">\(\sigma(A)\)</span> o matrisin tüm özvektörlerinin kümesidir. Numpy ile</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy.linalg <span class="im">as</span> lin
A <span class="op">=</span> [[<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">3</span>],[<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">6</span>],[<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>]]
A <span class="op">=</span> np.array(A)
[V,D] <span class="op">=</span> lin.eig(A)
<span class="bu">print</span> <span class="st">&#39;ozvektorler&#39;</span>
<span class="bu">print</span> D
<span class="bu">print</span> <span class="st">&#39;ozdegerler&#39;</span>
<span class="bu">print</span> V</code></pre></div>
<pre><code>ozvektorler
[[-0.41963784+0.j          0.72738656+0.j          0.72738656-0.j        ]
 [-0.66394149+0.j         -0.42567121+0.33744486j -0.42567121-0.33744486j]
 [-0.61893924+0.j         -0.25117492-0.33579002j -0.25117492+0.33579002j]]
ozdegerler
[ 13.75351937+0.j          -0.37675969+0.47073926j  -0.37675969-0.47073926j]</code></pre>
<p>Eğer <span class="math inline">\(B = PAP^{-1}\)</span>, ki <span class="math inline">\(P\)</span> eşsiz olmayacak şekilde, o zaman <span class="math inline">\(\sigma(B) = \sigma(A)\)</span>. İspatsız veriyoruz. Yani <span class="math inline">\(P\)</span> ve onun tersi ile bir matrisi iki taraftan çarpmak, o matrisin spektrumunu değiştirmiyor.</p>
<p>Eğer <span class="math inline">\(\lambda \in C\)</span> bir özdeğer ise, onun eşleniği (conjugate) <span class="math inline">\(\bar{\lambda}\)</span> da bir özdeğerdir. Bu sebeple reel matris <span class="math inline">\(A\)</span> için <span class="math inline">\(\sigma{A} = \overline{\sigma{A}}\)</span>.</p>
<p>Bir reel matrisin tüm özdeğerlerinin reel olduğu &quot;güzel'' bir durum vardır; bu durum matrisin simetrik olduğu durumdur, yani <span class="math inline">\(S^T = S\)</span>. Bu güzel durum aslında pratikte pek çok kez karşımıza çıkar; mesela kovaryans matrisleri olarak.</p>
<p>Simetrik matrisin özgün özdeğerlerine tekabül eden özvektörleri birbirine dikgendir. İspat, $v_i^T S v_j $ formülüne bakalım, ki <span class="math inline">\(v_i,v_j\)</span> özvektörler, <span class="math inline">\(S\)</span> simetrik matris.</p>
<p>Özdeğer eşitliğinden <span class="math inline">\(S v_j = \lambda_j v_j\)</span> kullanırsam,</p>
<p><span class="math display">\[ v_i^T S v_j  = \lambda_j v_i^T v_j \]</span></p>
<p>Eğer <span class="math inline">\(v_i^TS = \lambda_i v_i^T\)</span> kullanırsam,</p>
<p><span class="math display">\[ v_i^T S v_j  = \lambda_i v_i^T v_j \]</span></p>
<p>elde ederim. İkisini bir araya koyalım,</p>
<p><span class="math display">\[ \lambda_j v_i^T v_j =  \lambda_i v_i^T v_j \]</span></p>
<p><span class="math display">\[ (\lambda_j-\lambda_i) v_i^T v_j = 0\]</span></p>
<p>Bu eşitliğin doğru olması sadece iki durumda olabilir; ya <span class="math inline">\(\lambda_i,\lambda_j\)</span> birbiriyle aynıdır, ya da birbirinden farklıdır ama o zaman <span class="math inline">\(v_i,v_j\)</span> birbirine dikgen olmalıdır.</p>
<p>[norm atlandı]</p>
<p>Eksi Bakışımlı Matrisler (Skew-symmetric Matrices)</p>
<p>Eğer <span class="math inline">\(A^T = -A\)</span> ise bu matrislere eksi bakışımlı deniyor [5]. Mesela</p>
<p><span class="math display">\[ 
\left[\begin{array}{rr}
0 &amp; 2 \\ -2 &amp; 0
\end{array}\right]
 \]</span></p>
<p>Bu matrisin devriği ve negatifi aynıdır. Eksi bakışımlı matrislerin köşegeni sıfır olmalıdır. Bu tür matrislerin ilginç bazı özellikleri var, mesela, hatırlarsak simetrik matrislerin pür reel spektrumu vardı. Eksi bakışımlı matrislerin spektrumu pür sanaldır (imaginary).</p>
<p>[köşegenleştirme atlandı]</p>
<p>Çapraz Çarpım (cross-product)</p>
<p>Noktasal çarpım bize bir skalar verir. İki vektör arasındaki çapraz çarpım bize başka bir vektör verir; <span class="math inline">\(u,v \in \mathbb{R}^3\)</span> olmak üzere çapraz çarpım,</p>
<p><span class="math display">\[ 
u \times v = 
\left[\begin{array}{r}
u_2v_3 - u_3v_2 \\
u_3v_1 - u_1v_3 \\
u_1v_2 - u_2v_1 
\end{array}\right]
\]</span></p>
<p>Yani <span class="math inline">\(\mathbb{R}^3\)</span>'teki iki vektör <span class="math inline">\(u,v\)</span>'nin çapraz çarpımı alınabilir, ve <span class="math inline">\(\mathbb{R}^3\)</span>'te yeni bir vektör elde ederiz. Bu yeni vektör <span class="math inline">\(u,v\)</span>'ye dikgendir. Sağ el kuralı (alttaki resimde <span class="math inline">\(u,v\)</span> yerine <span class="math inline">\(a,b\)</span> kullanılmış)</p>
<div class="figure">
<img src="righthand.png" />

</div>
<p>Ayrıca çapraz çarpım simetriktir, <span class="math inline">\(u \times v = - u \times v\)</span>.</p>
<p>Eksi Bakışımlı Matris (Skew-Symmetric Matrix)</p>
<p>Bilgisayar Görüşü (Computer Vision) alanında oldukça yaygın bir matris olan eksi bakışımlı bir matris alttadır, ve onu herhangi bir vektörden oluşturan şapka operatörüyle gösterelim, yani <span class="math inline">\(u\)</span> için <span class="math inline">\(\hat{u}\)</span> olsun,</p>
<p><span class="math display">\[ 
\hat{u} = \left[\begin{array}{rrr}
0 &amp; -u_3 &amp; u_2 \\
u_3 &amp; 0 &amp; -u_1 \\
-u_2 &amp; u_1 &amp; 0
\end{array}\right] \in \mathbb{R}^{3 \times 3}
 \]</span></p>
<p>Bu tür matrislerin kertesinin çift sayı olması şarttır.</p>
<p><span class="math inline">\(\hat{u}\)</span>'nun ilginç bir özelliği var, herhangi bir vektör <span class="math inline">\(v\)</span> ile</p>
<p><span class="math display">\[ \hat{u}v = u \times v \]</span></p>
<p>ki <span class="math inline">\(\times\)</span> bir çapraz çarpım. Yani, öncelikle, her vektörün bir eksi bakışımlı matris karşılığı var, ve üstteki operatör ile bu geçişi yapabiliriz, ayrıca ne zaman bir çapraz çarpım görsek, onu eksi bakışımlı matris çevirimi üzerinen bir normal matris çarpımı olarak temsil edebiliriz. Bu faydalı çünkü çapraz çarpımla uğraşmak biraz külfetli olabiliyor.</p>
<p>3 boyut bağlamında <span class="math inline">\(\hat{u}\)</span>'nun kertesi tabii ki 2'dir; çünkü eksi bakışımlı matrislerinin kertesinin çift olması şart ise, ve 3 boyutlu durumda bu kerte en fazla 3 olabilir, ama 3 olamaz çünkü 3 çift sayı değil, o zaman 2 olur.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np

<span class="kw">def</span> skew(a):
   <span class="cf">return</span> np.array([[<span class="dv">0</span>,<span class="op">-</span>a[<span class="dv">2</span>],a[<span class="dv">1</span>]],[a[<span class="dv">2</span>],<span class="dv">0</span>,<span class="op">-</span>a[<span class="dv">0</span>]],[<span class="op">-</span>a[<span class="dv">1</span>],a[<span class="dv">0</span>],<span class="dv">0</span>]])</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">A <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>]]).T
<span class="bu">print</span> skew(A)</code></pre></div>
<pre><code>[[ 0 -3  2]
 [ 3  0 -1]
 [-2  1  0]]</code></pre>
<p><span class="math inline">\(A^3\)</span></p>
<p>Diyelim ki <span class="math inline">\(A\)</span> bir <span class="math inline">\(3 \times 3\)</span> eksi bakışımlı matris, ve <span class="math inline">\(a = (a_1,a_2,a_3)\)</span> vektörü üzerinden oluşturulmuş. <span class="math inline">\(A^3\)</span> nedir?</p>
<p><span class="math display">\[A \cdot x = u \times x\]</span></p>
<p>olduğunu biliyoruz. O zaman</p>
<p><span class="math display">\[A^3 x = a \times ( a \times ( a \times x))\]</span></p>
<p>demektir. Eşitliğin sağ tarafına bakarsak, eğer <span class="math inline">\(a=e\)</span>, ki <span class="math inline">\(e\)</span> bir birim vektörü olsun, yani <span class="math inline">\(||e||=1\)</span>, o zaman norm'ların ilişkisi şu şekilde olurdu,</p>
<p><span class="math display">\[||e\times(e\times (e\times x)))\|=||e\times (e\times x))|| =||e\times x||\]</span></p>
<p>Neden?</p>
<p><span class="math display">\[ ||e \times x|| = ||e||||x|| \sin \theta = ||x|| \sin \theta\]</span></p>
<p>O zaman,</p>
<p><span class="math display">\[||e\times (e\times x))|| = ||e|||| e \times x|| \sin 90 = ||e \times x||\]</span></p>
<p>Böyle gider. Yani norm eşitlikleri doğru.</p>
<p>Eğer <span class="math inline">\(a = ||a||e\)</span> kabul edersek,</p>
<p><span class="math display">\[||a\times(a\times (a\times x)))|| = ||a||^3 ||e\times(e\times (e\times x)))||\]</span></p>
<p><span class="math display">\[ =\|a\|^3\|e\times x\| \]</span></p>
<p><span class="math display">\[ =\|a\|^2\|a\times x\| \]</span></p>
<p><span class="math display">\[ =(a^Ta)\|a\times x\| \]</span></p>
<p>Yani <span class="math inline">\(A^3x = \pm (a^Ta) Ax\)</span>; eksi mi artı mı? Eksi işareti olduğunu sağ el kuralıyla görebiliriz. Demek ki <span class="math inline">\(A^3 = -(a^Ta) \cdot A\)</span>.</p>
<p>Simetrik matrisler için</p>
<p><span class="math display">\[ A = UDU^T \]</span></p>
<p>olduğunu biliyoruz. Eksi bakışımlı matrisler için</p>
<p><span class="math display">\[ S = UBU^T \]</span></p>
<p>olur ki <span class="math inline">\(B\)</span> bir blok köşegen matristir, <span class="math inline">\(U\)</span> dikgen. <span class="math inline">\(S\)</span>'in özdeğerleri pür sanaldır. Blok köşegen matris <span class="math inline">\(diag(a_1Z,a_2Z,...,z_mZ,0,..0)\)</span>, ve <span class="math inline">\(Z = \left[\begin{array}{cc} 0 &amp; 1 \\ -1 &amp; 0 \end{array}\right]\)</span>. Köşegende blok olması garip gelebilir, fakat tek sayılar yerine çapraz yönde birkaç sayının &quot;üst üste'' olduğu bir durum bu. Basit matris çarpımı ile kolay bir şekilde kontrol edilebilir ki</p>
<p><span class="math inline">\(Z^2 = -I, \qquad Z^3 = -Z, \qquad Z^4 = I\)</span></p>
<p>Matris Üstelleri (Matrix Exponentials) [1, sf. 482]</p>
<p><span class="math inline">\(t \in \mathbb{R}\)</span> ve bir kare matris <span class="math inline">\(n \times n\)</span> matrisi <span class="math inline">\(A\)</span> için, matris üsteli,</p>
<p><span class="math display">\[ U(t) = e^{tA} = \exp(tA) \]</span></p>
<p>alttaki problem için özgün bir <span class="math inline">\(n \times n\)</span> çözümüdür;</p>
<p><span class="math display">\[ \frac{dU}{dt} = AU, \qquad U(0) = I \]</span></p>
<p>Dikkat: matris üsteli <span class="math inline">\(\exp\)</span> fonksiyonun teker teker matris öğeleri üzerinde işletilmiş hali değildir.</p>
<p>Matris üstelleri bir seri olarak ta gösterilebilir,</p>
<p><span class="math display">\[ e^{tA} = \sum_{n=0}^{\infty} \frac{t^n}{n!} A^n = 
I + tA + \frac{t^2}{2}A^2 + \frac{t^3}{6}A^3 + ...
\]</span></p>
<p>Bu seri yakınsayan (converging) bir seridir.</p>
<p>Örnek</p>
<p><span class="math display">\[ A = \left[\begin{array}{rrr}
0 &amp; 1 \\ 0 &amp; 0
\end{array}\right] \]</span></p>
<p>için</p>
<p><span class="math display">\[ e^{tA} = \left[\begin{array}{rrr}
1 &amp; t \\ 0 &amp; 1
\end{array}\right] \]</span></p>
<p>Örnek</p>
<p><span class="math display">\[ A = \left[\begin{array}{rrr}
1 &amp; 0 \\ 0 &amp; 1
\end{array}\right] \]</span></p>
<p>için</p>
<p><span class="math display">\[ e^{tA} = \left[\begin{array}{rrr}
e^t &amp; 0 \\ 0 &amp; e^t
\end{array}\right] \]</span></p>
<p>Teori</p>
<p>Eğer <span class="math inline">\(A\)</span> bir eksi bakışımlı matris ise, <span class="math inline">\(Q(t) = e^{tA}\)</span> muntazam (proper) bir dikgen matristir.</p>
<p>İspat</p>
<p>Dikgenlik tersi ile devriğin aynı olması demektir, o zaman üstteki eşitlikte sol tarafın tersi, sağ tarafın devriği aynı olmalı, yani <span class="math inline">\(Q(t)^{-1}\)</span> ile <span class="math inline">\(e^{tA^T}\)</span>.</p>
<p><span class="math display">\[ Q(t)^{-1} = e^{-tA} = e^{tA^T} = (e^{tA})^T = Q(t)^T \]</span></p>
<p>Hakikaten de öyle.</p>
<p>Not: Bir diğer ispata göre <em>tüm</em> dikgen matrisler bir eksi bakışımlı matrisin <span class="math inline">\(e\)</span>'nin üsteli alınarak oluşturulabilir. Buradaki nüansa dikkat, tüm eksi bakışımlı matrislerin üsteli dikgendir demek ile <em>tüm</em> dikgen matrisler eksi bakışımlı matrislerin üsteli alınmış halidir demek farklı.</p>
<p>Üstteki kavramları kullanarak dönüş (rotation) yapmayı sağlayan Rodriguez formülü [6]'da bulunabilir.</p>
<p>SVD</p>
<p>Bu işlemin özdeğer / özvektör hesabının karesel olmayan matrisler durumundaki genelleştirilmiş hali olduğu düşünülebilir. Pek çok lineer cebir işlemi, mesela tersini alma, kerte hesabı, vs. SVD bağlamında incelenebilir. Genelleştirme dedik, eğer <span class="math inline">\(A\)</span> karesel değilse özvektörleri hesaplayamayız, ama <span class="math inline">\(A^TA\)</span> kareseldir, ve bu matrisin özvektörleri <span class="math inline">\(A\)</span>'nin SVD'si ile yakından alakalıdır.</p>
<p>[İspat atlandı]</p>
<p>Geometrik olarak <span class="math inline">\(A = U \Sigma V^T\)</span> ile gösterilen SVD'nin bir <span class="math inline">\(x \in \mathbb{R}^n\)</span>'yi <span class="math inline">\(A\)</span> uzerinden transform ettiğimiz durumda, <span class="math inline">\(y = Ax\)</span> diyelim, <span class="math inline">\(y\)</span>'nin <span class="math inline">\(U\)</span>'da bazındaki kordinatlarının <span class="math inline">\(V\)</span> bazındaki kordinatları ile bir ilişki ortaya çıkarttığını söyleyebiliriz; bu ilişki <span class="math inline">\(\Sigma\)</span>'nin öğeleri üzerinden bir ölçeklemeden ibarettir. Yani</p>
<p><span class="math display">\[ y = Ax = U\Sigma V^Tx 
\quad \Leftrightarrow \quad
U^Ty = \Sigma V^T x
\]</span></p>
<p>[birim küre ellipsoid eşlemesi atlandı]</p>
<p>Genelleştirilmiş Ters (Generalized -Moore Penrose- Inverse)</p>
<p>Lineer sistem çözerken <span class="math inline">\(Ax = b\)</span> için eğer <span class="math inline">\(A\)</span>'nin tersi alınabiliyorsa, çözüm kolay, <span class="math inline">\(x^\ast = A^{-1}b\)</span>. Fakat <span class="math inline">\(A\)</span>'nin tersi alınamıyorsa, ki bu <span class="math inline">\(A\)</span> karesel olmadığında otomatik olarak doğru olacaktır, ne yapacağız? Genelleştirilmiş tersi alma, ya da sözde ters (pseudoinverse) işlemi burada ise yarar. Sözde ters için, her <span class="math inline">\(A\)</span> için bir SVD olduğuna göre, <span class="math inline">\(A = U \Sigma V^T\)</span>, ve <span class="math inline">\(\Sigma\)</span>'nın sıfır olmayan eşsiz değerlerinin tersi alınır (yani öğe <span class="math inline">\(\sigma_i\)</span>, <span class="math inline">\(1/\sigma_i\)</span> olur), sıfır değerlerine dokunulmaz, bu sonuçlar yeni bir <span class="math inline">\(\Sigma^{\dagger}\)</span>'in köşegenine dizilir, &quot;sözde ters'' bu matris olur,</p>
<p><span class="math display">\[ \Sigma^{\dagger} = \left[\begin{array}{cc}
\Sigma_1^{-1} &amp; 0 \\ 0 &amp; 0 
\end{array}\right]\]</span></p>
<p>Ve bu ters işlemi tüm <span class="math inline">\(A\)</span>'nin tersini almak için kullanılır, ki bu sözde tersi de boyutu <span class="math inline">\(n \times m\)</span> olan bir <span class="math inline">\(A^{\dagger}\)</span> ile gösteriyoruz (İngilizce &quot;<span class="math inline">\(A\)</span>-dagger'' olarak telafuz ediliyor, biz &quot;<span class="math inline">\(A\)</span>-kama'' diyelim),</p>
<p><span class="math display">\[ 
A^{\dagger} = V \Sigma^{\dagger} U^T  
\qquad (1)
\]</span></p>
<p>Bu noktaya nasıl geldiğimize dikkat, eğer SVD sonucunun pür tersini alabilseydik,</p>
<p><span class="math display">\[ A^{-1} = V^{-T}\Sigma^{-1}U^{-1} \]</span></p>
<p>Dikgen matrisler için <span class="math inline">\(Q^{-1}=Q^T\)</span> olduğu için</p>
<p><span class="math display">\[ = V\Sigma^{-1}U^{T} \]</span></p>
<p><span class="math inline">\(\Sigma^{-1}\)</span> olmadığı için yerine <span class="math inline">\(\Sigma^{\dagger}\)</span> kullanıyoruz ve (1)'e erişiyoruz.</p>
<p>Bazı özellikler,</p>
<p><span class="math display">\[ A A^{\dagger} A = A \]</span></p>
<p>ya da</p>
<p><span class="math display">\[ A^{\dagger} A  A^{\dagger} =  A^{\dagger} \]</span></p>
<p>Peki sözde tersi alma işlemini denklem sistemi çözmekte nasıl kullanırız?</p>
<p>Lineer sistem çözümü bağlamında durumunda 3 türlü sonuç olabileceğini görmüştük. Eğer sonsuz tane çözüm varsa, bu büyük bir ihtimalle problemin tam kısıtlanmamış (constrained) olması ile alakalıdır. Tabii hiçbir çözüm olmayabilir, ve size para veren kişi sizden hala çözüm beklemektedir (!), bu durumda <span class="math inline">\(Ax=b\)</span>'yi çözmek yerine ona en yakın olabilecek şeyi çözebiliriz, yani <span class="math inline">\(|Ax - b|^2\)</span>'yi minimize etmeyi seçebiliriz, <span class="math inline">\(Ax\)</span>'i <span class="math inline">\(b\)</span>'yi mümkün olduğu kadar yaklaştırırız. Burada sözde ters ise yarar, çünkü <span class="math inline">\(x^\ast = A^{\dagger}b\)</span> ile hesaplanan çözüm aynı zamanda <span class="math inline">\(|Ax - b|^2\)</span>'yi minimize eder! Bu çözüm En Az Kareler (Least Squares) çözümü olarak ta bilinir, tabii burada sistem aşırı belirtilmiş (overdetermined) değil, eksik belirtilmiş (underdetermined) durumda. Not: Ayrıca sözde ters ile bulunan <span class="math inline">\(x^\ast\)</span>'in mümkün tüm çözümler arasında &quot;norm'ü en az olan <span class="math inline">\(x^\ast\)</span>'i bulduğu da'' söylenir.</p>
<p>Eğer çözüm özgün ise, sözde ters yine işler, özgün çözümü bulur. Yani her halükarda sözde ters tüm problemlerimizi çözer.</p>
<p>Lineer cebir'i böylece gözden geçirmiş olduk. Artık dersimizin ana konularına başlayabiliriz.</p>
<p>Hareketli bir Sahneyi Temsil Etmek</p>
<p>Burada sahne dış dünya, yani kamera ile hareket ederken gördüğümüz şeyler.</p>
<p>Hareket ederken kamera pek çok resim alabilir, tabii bu dersimiz uzun zamandır yapılan araştırmalara dayanıyor, ve bu araştırmalar çoğunlukla iki resim durumuna odaklandılar; fakat günümüzde bir kamera saniyede mesela 30 tane resim çekebilir, bu durum için gerekli matematiği de göreceğiz. Önce iki resimle başlayacağız, ve bu matematiğin daha genel, çok resimli haline de kendimizi hazırlayacağız.</p>
<p>3D'de Yeniden Oluşturmak (3D Reconstruction)</p>
<p>Durağan olduğu kabul edilen 3D dış dünyayı pek çok açıdan ama iki boyutlu resmi ile tekrar oluşturma çabasının bilimde uzun bir tarihi var. Bu problem klasik bir &quot;kötü konumlanmış (ill-posed)'' problemdir, çünkü yeniden oluşturulan sonuç tipik olarak özgün değildir (pek çok farklı yeniden oluşturma mümkündür). Bu yüzden ek bazı kısıtlamalar getirmek gerekir. Bu alanda hala yapılacak çok iş olduğunu belirtmek isterim, yani bilim dalımız oldukça bakir [araştırmacılar, atlayın].</p>
<p>Dış dünyanın gördüğümüz imajdaki nasıl oluştuğunu perspektif izdüşümü (perspective projection) üzerinden modelleyeceğiz, bu modelleme kamera modeli olarak iğne deliği kamera (pinhole camera) modelini kullanır. Bu modeli şöyle hayal etmek mümkün, karanlık bir odadayız, duvarda tek bir delik var, ve bu oda dışındaki tüm görüntüler bu delik üzerinden odaya giriyor. Perspektif izdüşüm ilk kez Öklit tarafından, I.O. 400 yılında araştırıldı; bu hakikaten çok ilginç, yani sonuçta bugün bilgisayarlar ile araştırdığımız bu konunun temelindeki bazı kavramların ne kadar önceden beri bilindiği şaşırtıcı olabiliyor.</p>
<p>Ardından bu konu Rönesans sırasında çok yoğun araştırıldı, bu zamanlarda yapılan resimlerdeki derinliği temsil etme çabaları bugüne kalan eserlerden hepimiz biliyoruz. Sanatçılar, bilimciler iki boyut üzerinde derinliği, üç boyutluluğu gösterebilmek için kafa yordular, ve perspektif izdüşümün araştırılması 17. ve 18. yüzyılda perspektif geometrisi adında bir yeni alana dönüştü.</p>
<p>Çoklu Bakış Açıdan Tekrar Oluşturma (Multiview Reconstruction) alanında yapılan ilk araştırma oldukça eskiye gidiyor, ki bu da şaşırtıcı. Kruppa bu konuyu 1913'te araştırdı, iki farklı kameranın aynı objeye dönük iki resmine odaklandı, kendine şu soruyu sordu, &quot;bu iki resimde en az kaç tane noktaya bakmalıyım ki 3 boyutta bir model oluşturabileyim''. Kruppa gösterdi ki en az 5 nokta sonlu miktarda çözüm bulmak için yeterli, ki kameranın hareketi de buna dahil. Tabii bulunan özgün tek çözüm değildir, ama daha önce söylediğimiz gibi bu problem kötü konumlanmış bir problemdir, ama en azından çözüm sonsuz tane değil, sonlu sayıda.</p>
<p>İki görüntüden hareket ve yapıyı çıkartabilen ilk lineer algoritma Lonquet-Higgins tarafından 1981'de bulundu, bu algoritma eş kutupsal kısıtlama (epipolar constraint) kullanarak bu işi becerdi. Bu derste eş kutupsal kısıtlama konusunu öğreneceğiz. Bu buluş pek çok takip eden diğer buluşa ilham verdi, 80 ve 90'li yıllarda ek buluşlar yapıldı. Ardından alanımızın klasik kitaplarından Zisserman ve arkadaşlarının yazdığı Çoklu Bakış Açı Geometrisi (<em>Multiple View Geometry</em>) adlı kitap var, ve iş giderek bizim bu derste kullandığımız en güncel olan kitaba geliyor, <em>An Invitation to 3D Vision</em>.</p>
<p>Derste işlediğimiz konu farklı isimlerde ortaya çıkabiliyor, hareketten yapı çıkartmak (structure from motion) ismini gördük, bir diğer isim görsel (visual) SLAM, ki SLAM kısaltması &quot;aynı anda yer belirlemek ve haritalamak (simultaneous localization and mapping)'' kelimelerinden geliyor. Robotik alanındakiler bu kelimeyi çok kullanırlar, bir robotun dış dünyada hem etrafını haritalaması, hem de aynı anda o harita içindeki yerini kestirebilmesi, hesaplaması bu alanın baş problemlerinden. Tabii görsel SLAM bunu görsel olarak yapabilmek; çünkü çok farklı şekillerde SLAM yapılabiliyor, mesela 1. ders başında söylediğimiz gibi lazer algılayıcılarla SLAM yapılabilir, hatta sonar algılayıcılarla bile, ya da tüm bunları bir algılayıcı füzyonu (sensor fusion) üzerinden birleştirerek.</p>
<p>Kaynaklar</p>
<p>[1] Olver, <em>Applied Linear Algebra</em></p>
<p>[2] Bayramlı, Lineer Cebir, <em>Ders 15</em></p>
<p>[3] Sastry, <em>An Invitation to 3-D Vision</em></p>
<p>[4] Zissermann, <em>Multiple View Geometry</em></p>
<p>[5] Bayramlı, Lineer Cebir, <em>Ders 5</em></p>
<p>[6] Bayramlı, <em>Fizik - Döndürme (Rotation)</em></p>
</body>
</html>
