<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Çok Değişkenli Bernoulli Karışımı (Mixture of Multivariate Bernoulli)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1
id="çok-değişkenli-bernoulli-karışımı-mixture-of-multivariate-bernoulli">Çok
Değişkenli Bernoulli Karışımı (Mixture of Multivariate Bernoulli)</h1>
<p>Eğer verimizi, her biri verinin değişik bir bölümünü, yönünü temsil
eden bir “dağılım grubu’’ yani karışım ile modellemek istiyorsak,
karışım modellemesi kullanılabilir. Mesela boy ve ağırlık verisinde
bayanlar ve erkekler ayrı dağılımlara sahip olabilir, bu durumu modele
dahil etmek modelin tahmin gücünü arttırır. Karışım modellerinin güzel
bir tarafı kümeleme teknikleri ile başta”bilinmeyen’’ kümelerinin neye
benzediğini bulmaları, ayrıca her veri noktasının bu kümelere
olasılıksal olarak aidiyetini, “yakınlığını’’ hesaplamamızı mümkün
kılmaları.</p>
<p>Formel olarak bir karışım dağılımı <span
class="math inline">\(f\)</span> her biri ayrı bir dağılım olan <span
class="math inline">\(f_1,f_2,...,f_K\)</span> ile <span
class="math inline">\(K\)</span> öğeden oluşan, bir yeni dağılımdır
diyoruz, eğer</p>
<p><span class="math display">\[ f(x) = \sum_{k=1}^{K} \lambda_k
f_k(x)  \]</span></p>
<p>ise, ve <span class="math inline">\(\lambda_k\)</span> karışım
oranları, <span class="math inline">\(\lambda_k &gt; 0, \sum_k \lambda_k
= 1\)</span> olacak şekilde.</p>
<p>Üstteki model üzerinden zar atılabilecek bir model aynı zamanda (tüm
olasılıksal dağılımlar simule edilebilir tabii, ama üstteki için
simulasyon oldukça direk), <span class="math inline">\(\lambda\)</span>
içindeki olasılıklara göre zar atıp bir karışım öğesi seçilir, daha
sonra bu öğenin dağılımına gidilip ona zar attırılır. Bunun
olabileceğini ispatlamak için, <span class="math inline">\(Z\)</span>
rasgele değişkeninin <span class="math inline">\(\lambda_k\)</span> ile
dağıldığını (ayrıksal dağılım) düşünelim, yani</p>
<p><span class="math display">\[ Z \sim Mult(\lambda_1,..,\lambda_k)
\]</span></p>
<p><span class="math inline">\(f_k(x)\)</span> bir diğer açıdan <span
class="math inline">\(f(x|Z=k)\)</span>’dir, notasyonel olarak böyle. O
zaman,</p>
<p><span class="math display">\[ = \sum_{k=1}^{K} f(x|Z=k)\lambda_k
\]</span></p>
<p><span class="math display">\[ = \sum_{k=1}^{K}
f(x|Z=k)P(Z=k)  \]</span></p>
<p><span class="math display">\[ = \sum_{k=1}^{K} f(x,k)  \]</span></p>
<p><span class="math display">\[ = f(x)  \]</span></p>
<p>Yani <span class="math inline">\(\lambda\)</span> olasılıklarına göre
<span class="math inline">\(f_k\)</span> seçmek üstteki ifadedeki
koşullu olasılık durumuna karşılık geliyor, koşullu olasılık <span
class="math inline">\(P(A|B)\)</span> <span
class="math inline">\(B\)</span>’nin verildiği / bilindiği durumda <span
class="math inline">\(A\)</span>’nin olasılığı hatırlayacağımız
üzere.</p>
<p>Karışımın içindeki dağılımlar parametrik dağılımlar olduğu zaman
onları nasıl hesapsal olarak kestiririz? Bir dağılımın parametrelerini
kestirebilmek için en iyi yöntemlerden biri maksimum olurluk (maximum
likelihood) yöntemi. Olurluk eldeki verinin belli dağılım parametreleri
üzerinden olasılığı, yani “verinin olasılığı’’. Örneklemlerin bağımsız
olduğundan hareketle <span
class="math inline">\(x_1,x_2,...,x_N\)</span> verisi için olurluk,</p>
<p><span class="math display">\[ \prod_{i=1}^{N} f(x_i;\theta)
\]</span></p>
<p>Her zaman olduğu gibi çarpımı toplam haline döndürmek için log
alırız,</p>
<p><span class="math display">\[ \ell(\theta) = \sum_{i=1}^{N} \log
f(x_i;\theta) \]</span> Karışımları da dahil edersek,</p>
<p><span class="math display">\[ = \sum_{i=1}^{N} \log \sum_{k=1}^{K}
\lambda_k f(x_i;\theta_k)
\qquad (2)
\]</span></p>
<p>Şimdi log olurluğun mesela <span
class="math inline">\(\theta_j\)</span>’ye göre türevini almayı
deneyelim, yani <span class="math inline">\(j\)</span>’inci öğenin
parametresine göre bir kısmi türev.</p>
<p><span class="math display">\[ \frac{\partial \ell}{\partial
\theta_j}  =
\sum_{i=1}^{N}  \frac{1}{\sum_{k=1}^{K} \lambda_k f(x_i;\theta_k) }
\lambda_j
\frac{\partial f(x_i;\theta_j)}{\partial \theta_j}
\]</span></p>
<p>Bölüm ve bölene <span class="math inline">\(f(x_i;\theta_j)\)</span>
ekleyelim, bu sonucu değiştirmez,</p>
<p><span class="math display">\[ = \sum_{i=1}^{N}  
\frac{\lambda_j f(x_i;\theta_j)}{\sum_{k=1}^{K} \lambda_k
f(x_i;\theta_k)}
\frac{1}{f(x_i;\theta_j)}
\frac{\partial f(x_i;\theta_j)}{\partial \theta_j}
\]</span></p>
<p><span class="math display">\[ = \sum_{i=1}^{N}  
\frac{\lambda_j f(x_i;\theta_j)}{\sum_{k=1}^{K} \lambda_k
f(x_i;\theta_k)}
\frac{\partial \log f(x_i;\theta_j)}{\partial \theta_j}
\]</span></p>
<p>Eğer elimizdeki, karışım olmayan, basit bir parametrik model olsaydı,
log olurluk şuna benzeyecekti,</p>
<p><span class="math display">\[ \frac{\partial \log
f(x_i;\theta_j)}{\partial \theta_j} \]</span></p>
<p>Bu formül iki üstteki formülün en sağındaki çarpan sadece. Demek ki
“karışım olmak’’ log olurluğu bir tür belli ağırlıklara göre ortalanan
(weighted) normal olurluk haline getirdi. Karışımın log olurluğunu
maksimize etmek istiyorsak, bu ağırlığı alınmış olurluğu maksimize
etmemiz gerekli. Bu ağırlığın alındığı kısmı iki üstteki formülden çekip
çıkartırsak,</p>
<p><span class="math display">\[ w_{ij} = \frac{\lambda_j
f(x_i;\theta_j)}{\sum_{k=1}^{K} \lambda_k f(x_i;\theta_k)} \]</span></p>
<p>Bu ağırlık hesabı <span class="math inline">\(i,j\)</span> için
yapılacak. Bu noktaya niçin geldik hatırlayalım, olurluk üzerinden
parametreleri hesaplamak istiyoruz. Fakat üstteki formülde <span
class="math inline">\(w_{ij}\)</span> hesabı için <span
class="math inline">\(\theta_j\)</span>’in bilinmesi gerekiyor!</p>
<p>Ne yapacağız? Şu <span class="math inline">\(w_{ij}\)</span>’ye
yakından bakalım. Daha önce belirttiğimiz gibi <span
class="math inline">\(\lambda_j\)</span> <span
class="math inline">\(Z\)</span>’nin <span
class="math inline">\(j\)</span> olma olasılığı, o zaman bölünendeki
ifade <span class="math inline">\(X = x_i\)</span> <span
class="math inline">\(Z=j\)</span> olmasının ortak (joint) dağılımıdır,
yani <span class="math inline">\(P(Z=j,X=x_i)\)</span> diyelim. Koşullu
dağılım durumundan başlayarak bu sonuca nasıl erişildiğini görmüştük.
Bölendeki ifade ise <span class="math inline">\(f(x_i)\)</span>’dir, bir
kısmı dağılımdır - tüm <span class="math inline">\(k\)</span>’ler
üzerinden olasılığın bir bölümü toplanarak kısmen çıkartılmış halidir
(marginalized out) - o zaman tüm bölümden ele geçen sonuç <span
class="math inline">\(Z=j\)</span>’nin <span
class="math inline">\(X=x_i\)</span> verildiği, koşullu
olasılığıdır,</p>
<p><span class="math display">\[  
w_{ij}
= \frac{\lambda_j f(x_i;\theta_j)}{\sum_{k=1}^{K} \lambda_k
f(x_i;\theta_k)}
= P(Z=j | X=x_i;\theta)
\qquad (1)
\]</span></p>
<p>O zaman</p>
<p><span class="math display">\[ \frac{\partial \ell}{\partial
\theta_j}  = \sum_{i=1}^{N}
w_{ij} \frac{\partial \log f(x_i;\theta_j)}{\partial \theta_j}
\]</span></p>
<p><span class="math inline">\(w_{ij}\)</span> ile, veriye göre, <span
class="math inline">\(Z\)</span>’nin sonsal (posterior) hesaplamış
oluyoruz. Yani karışımsal modeli hesaplarken bir ağırlıksal olurluk
hesabı yapıyoruz, ki bu ağırlıklar sonsal dağılımlardan gelen değerlere
ihtiyaç duyuyor. Ama bu sonsal dağılımlar da aslında hesaplamaya
çalıştığımız parametrelere ihtiyaç duyuyor, yani bir kördüğüm!</p>
<p>Ama şöyle bir deyiş vardır; kimisine kördüğüm gibi gözüken, bir
başkasına ardışıl yaklaşıksal prosedür gibi gözükür (succcessive
approximation procedure) [hoca şakadan uydurdu bu deyişi, ama teknik
doğru]. Demek istiyorum ki eğer kördüğümde takılı kaldıysak, bir taraf
için tahmin yapıp diğer tarafı hesaplarız, sonra bu hesaplanan değerleri
kullanarak ilk tarafı hesaplarız. Bunu sürekli devam ettiririz.</p>
<p>Ünlü Beklenti-Maksimizasyon (Expectation-Maximization -EM-) prosedürü
tam da bunu yapıyor. Detaylar için [3, sf. 450]. EM özyinesel çalışan
bir rutindir, birkaç adımda sonuca erişir, ve her adımda olurluğu
iyileştirmesi ve yerel maksimuma erişmesi garantidir; Tabii başlangıç
noktasına göre bu yerel maksimum tamamın (global) maksimumu olmayabilir,
o zaman EM yerel maksimumda takılıp kalmış olur (stuck at local maxima),
bu sebeple EM’i birkaç değişik rasgele başlangıç noktasından başlatıp en
iyi yerel maksimimumu, yani en iyi olurluğu veren parametreleri bulmak
iyi bir yaklaşımdır.</p>
<p><img src="localmax.png" /></p>
<p><span class="math inline">\(w_{ij}\)</span>’ye Değişik bir Yönden
Erişmek</p>
<p><span class="math inline">\(\theta_j\)</span> hesabı için
formülasyonu biraz değiştirmek lazım. Tüm ortak dağılımı yazalım, ayrıca
<span class="math inline">\(z_{ik}\)</span> değişkenini katalım, <span
class="math inline">\(Z\)</span> değişkeni multinom idi, onu 0/1
değerleri içeren vektörel olarak tasarlayalım, yani <span
class="math inline">\(z\)</span> veri noktası <span
class="math inline">\(i\)</span> ve bileşen <span
class="math inline">\(k\)</span> için, <span
class="math inline">\(Z_i\)</span> ise <span
class="math inline">\(i\)</span>’inci nokta için</p>
<p><span class="math display">\[  P(X_i = x_i, Z_i=k) =  
\prod_{k=1}^{K} \big( f(x_i;\theta_k)P(Z_i=k) \big)^{z_{ik}}
\]</span></p>
<p>Şimdi log alalım,</p>
<p><span class="math display">\[  = \sum_{k=1}^{K} z_{ij} \ln \big(
f(x_i;\theta_k)P(Z_i=k) \big) \]</span></p>
<p>Tüm veri noktaları için</p>
<p><span class="math display">\[  \ell(\theta) =
\sum_{i=1}^{N} \sum_{k=1}^{K} z_{ij} \ln \big( f(x_i;\theta_k)P(Z_i=k)
\big) \]</span></p>
<p><span class="math display">\[
= \sum_{i=1}^{N} \sum_{k=1}^{K} z_{ik}
\big( \ln f(x_i;\theta_j) + \ln(\lambda_j) \big)   
\]</span></p>
<p>Şimdi bu ifadenin beklentisini almamız lazım; bunun sebebi EM’in
yakınsaması (convergence) ile alakalı [3, sf. 450]. Beklentiyi “eksik’’
olan yani bilinmeyen küme ataması üzerinden alıyoruz, <span
class="math inline">\(\theta_k\)</span>,<span
class="math inline">\(P(Z_i=k)\)</span> ve <span
class="math inline">\(x_i\)</span> sabit olarak kalıyor,</p>
<p><span class="math display">\[
E[l(\theta)] = \sum_{i=1}^{N} \sum_{k=1}^{K} E[z_{ik}]
\big( \ln f(x_i;\theta_j) + \ln(\lambda_j) \big)   
\]</span></p>
<p>Hesaplanacak tek şey burada <span
class="math inline">\(E[z_{ik}]\)</span>. Peki bu beklenti nedir?</p>
<p><span class="math display">\[ E[z_{ik}] = 1 \cdot P(z_{ik}=1 | x_i) +
0 \cdot P(z_{ik}=1 | x_i)  \]</span></p>
<p><span class="math display">\[=  P(z_{ik}=1 | x_i)  \]</span></p>
<p>Bu formül (1)’deki formülün aynısıdır! Yeni notasyon üzerinden tabii;
o zaman</p>
<p><span class="math display">\[  E[z_{ik}] = w_{ik} \]</span></p>
<p>Yani</p>
<p><span class="math display">\[
E[l(\theta)] = \sum_{i=1}^{N} \sum_{k=1}^{K} w_{ik}
\big( \ln f(x_i;\theta_j) + \ln(\lambda_j) \big)
\qquad (4)
\]</span></p>
<p>EM Hesap Adımları</p>
<p><span class="math inline">\(w_{ij}\)</span> hesabına EM’in “beklenti
adımı (expectation step)’’ ismi veriliyor, çünkü görüldüğü gibi beklenti
alıyoruz. Bu adım için <span class="math inline">\(\theta\)</span>’nin
bilindiği farz edilir, bilinmiyorsa, ki hesap döngüsünün ilk adımında
durum böyledir, o zaman rasgele <span
class="math inline">\(\theta\)</span> kullanılır. Döngünün diğer
adımlarında döngünün bir önceki adımındaki değerler kullanılır.</p>
<p>Maksimizasyon adımı için bilinen <span
class="math inline">\(w_{ij}\)</span> için <span
class="math inline">\(\theta\)</span>’nin hesaplanması gerekir; bu adıma
maksimizasyon adı verilmesi de mantıklı, çünkü altta da görüleceği
üzere, kısmi türevler alıp sıfıra eşitleyerek maksimal değerler
hesaplayacağız.</p>
<p>Bu hesap şöyle: Eğer (4) çok değişkenli Bernoulli modeli içinse, ki
<span class="math inline">\(x_{id}\)</span> <span
class="math inline">\(i\)</span>’inci veri noktasının <span
class="math inline">\(D\)</span> boyutlu Bernoulli için <span
class="math inline">\(d\)</span>’inci hücresinin değeri, <span
class="math inline">\(\theta_{jd}\)</span> ise <span
class="math inline">\(j\)</span>’inci karışım öğesinin <span
class="math inline">\(D\)</span> boyut içinden <span
class="math inline">\(d\)</span>’inci olasılık değeri olsun, <span
class="math inline">\(f\)</span> içinde yerine koyunca ve <span
class="math inline">\(f\)</span> üzerinde log etki yapınca çarpım yine
toplam olur,</p>
<p><span class="math display">\[
= \sum_{i=1}^{N} \sum_{k=1}^{K} w_{ik}
\bigg[ \ln(\lambda_k)  +
\sum_{d=1}^{D} \ln \big( \theta_{kd}^{x_{id}} (1-\theta_{kd})^{1-x_{id}}
\big)
\bigg]
\]</span></p>
<p><span class="math display">\[
E[l(\theta)]= \sum_{i=1}^{N} \sum_{k=1}^{K} w_{ik}
\bigg[ \ln(\lambda_k)  +
\sum_{d=1}^{D} x_{id} \ln \theta_{kd} + (1-x_{id}) \ln (1-\theta_{kd})
\bigg]
\]</span></p>
<p>Şimdi <span class="math inline">\(\theta_{kd}\)</span> hesabı için
ona göre türevi alıp sıfıra eşitleriz,</p>
<p><span class="math display">\[
\frac{\partial }{\partial \theta_{kd}} E[l(\theta)] =
w_{ik} \sum_{i=1}^{N}
x_{id} \frac{\partial }{\partial \theta_{kd}} (\ln \theta_{kd}) +
\frac{\partial }{\partial \theta_{kd}}\big[ (1-x_{id}) \ln
(1-\theta_{kd})\big]
= 0
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{N}  w_{ik}
(
\frac{x_{id}}{\theta_{kd}}  -
\frac{1-x_{id}}{1-\theta_{kd}}
) = 0
\]</span></p>
<p><span class="math display">\[
\sum_{i=1}^{N} \frac{w_{ik}  x_{id}}{\theta_{kd}} =
\sum_{i=1}^{N} \frac{w_{ik}-w_{ik}x_{id}}{1-\theta_{kd}}
\]</span></p>
<p><span class="math display">\[
\frac{1}{\theta_{kd}}\sum_{i=1}^N w_{ik}  x_{id} =
\frac{1}{1-\theta_{kd}}\sum_{i=1}^{N} w_{ik}-w_{ik}x_{id}
\]</span></p>
<p><span class="math display">\[
\frac{1-\theta_{kd}}{\theta_{kd}}\sum_{i=1}^N w_{ik}  x_{id} =
\sum_{i=1}^{N} w_{ik}-w_{ik}x_{id}
\]</span></p>
<p><span class="math display">\[
\frac{1-\theta_{kd}}{\theta_{kd}} =
\frac{\sum_i w_{ik}-\sum_i w_{ik}x_{id}}{\sum_i w_{ik}  x_{id}}
\]</span></p>
<p><span class="math display">\[
\frac{1}{\theta_{kd}} - 1=
\frac{\sum_i w_{ik}}{\sum_i w_{ik}  x_{id}} - 1
\]</span></p>
<p><span class="math display">\[
\hat{\theta}_{kd}=
\frac{\sum_i w_{ik}  x_{id}}{\sum_i w_{ik}}
\]</span></p>
<p>Ya da</p>
<p><span class="math display">\[
\hat{\theta}_{k}=
\frac{\sum_i w_{ik}  x_{i}}{\sum_i w_{ik}}
\]</span></p>
<p><span class="math inline">\(\lambda_j\)</span> Hesabı</p>
<p>Şimdi <span class="math inline">\(\lambda_j\)</span>’ye göre bir
türev almamız, sıfıra eşitlememiz ve çözmemiz lazım. Tek bir pürüz <span
class="math inline">\(\sum_k \lambda_k = 1\)</span> olması şartı, yani
tüm ağırlıkların toplamı 1’e eşit olmalı ve bu şartı bir şekilde
denklemlere dahil etmemiz lazım. Lagrange çarpan tekniği burada
kullanılır [1, sf. 395].</p>
<p><span class="math display">\[ \frac{\partial }{\partial \lambda_j}
\big[ \ell(\theta)  + \alpha (\sum_k \lambda_k - 1) \big]
\]</span></p>
<p>Ondan önce olurluğun <span
class="math inline">\(\lambda_j\)</span>’ye göre kısmi türevi lazım, (1)
formülüne dönersek, ve kısmi türevi alırsak,</p>
<p><span class="math display">\[
\frac{\partial \ell}{\partial \lambda_j}  =
\sum_{i=1}^{N}
\frac{f(x_i;\theta_j)}{\sum_{k=1}^{K} \lambda_k f(x_i;\theta_k) }
=
\sum_{i=1}^{N}
\frac{f(x_i;\theta_j)}{f(x_i) }
\]</span></p>
<p>O zaman iki üstteki türev su hale gelir, sıfıra da eşitlersek,</p>
<p><span class="math display">\[  
\sum_{i=1}^{N} \frac{f(x_i;\theta_j)}{f(x_i) } + \alpha = 0
\]</span></p>
<p>Biraz düzenleyip iki tarafı da <span
class="math inline">\(\lambda_j\)</span> ile çarpalım,</p>
<p><span class="math display">\[
\sum_{i=1}^{N} \frac{f(x_i;\theta_j) \lambda_j}{f(x_i) } = - \alpha
\lambda_j
\]</span></p>
<p>Eşitliğin sol tarafında toplam içinde yine (1)’de görülen <span
class="math inline">\(w_{ij}\)</span>’ye eriştik! Yerine koyalım,</p>
<p><span class="math display">\[
\sum_{i=1}^{N} w_{ij} = - \alpha \lambda_j
\qquad (3)
\]</span></p>
<p>Şimdi tüm öğeler / kümeler üzerinden bir toplam alalım (yani <span
class="math inline">\(\sum_k\)</span>’yi her iki tarafa da
uygulayalım),</p>
<p><span class="math display">\[
\sum_{k=1}^{K} \sum_{i=1}^{N} w_{ij} = - \alpha  \sum_{k=1}^{K}
\lambda_j
\]</span></p>
<p><span class="math inline">\(\sum_k \lambda_j = 1\)</span>, <span
class="math inline">\(\sum_j w_{ij} = 1\)</span> olduğu için,</p>
<p><span class="math display">\[
N = - \alpha
\]</span></p>
<p>Üstteki formülü (3) içine koyarsak, ve tekrar düzenlersek,</p>
<p><span class="math display">\[
\lambda_j = \frac{\sum_{i=1}^{N} w_{ij}}{N}
\]</span></p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loginnerprodexp(t,a):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    eps<span class="op">=</span><span class="fl">1e-15</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    t[t<span class="op">&gt;</span><span class="fl">0.</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    tmp <span class="op">=</span> np.dot(t,np.exp(a)) <span class="op">+</span> eps</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    b<span class="op">=</span>np.log(tmp)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> b</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logsumexp(a):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.log(np.<span class="bu">sum</span>(np.exp(a), axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> do_EMmixtureBernoulli(Y,K,<span class="bu">iter</span>,tol):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    N,D<span class="op">=</span>Y.shape</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    OMY<span class="op">=</span><span class="dv">1</span><span class="op">+</span>(<span class="op">-</span><span class="dv">1</span><span class="op">*</span>Y) <span class="co">#  &quot;One minus Y&quot;, (1-Y)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    tmp<span class="op">=</span>np.random.rand(N,K)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    tmp2<span class="op">=</span>np.<span class="bu">sum</span>(tmp,axis<span class="op">=</span><span class="dv">1</span>).reshape((N,<span class="dv">1</span>))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    tmp3<span class="op">=</span>np.tile(tmp2,(<span class="dv">1</span>,K))</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    lR<span class="op">=</span>np.log(np.divide(tmp, tmp3))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    L <span class="op">=</span> []</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">iter</span>):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lPi log Mixture params Kx1</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    lPi<span class="op">=</span>np.tile(<span class="op">-</span><span class="dv">1</span> <span class="op">*</span> np.log(N),(K,<span class="dv">1</span>))<span class="op">+</span>logsumexp(lR).T.reshape((K,<span class="dv">1</span>))</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        const<span class="op">=</span>np.tile(logsumexp(lR).T.reshape((K,<span class="dv">1</span>)),(<span class="dv">1</span>,D))</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lP log Bernoulli params KxD</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        lP<span class="op">=</span>loginnerprodexp(Y.T,lR).T <span class="op">-</span> const</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># lOMP log(1-P), also KxD</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        lOMP<span class="op">=</span>loginnerprodexp(OMY.T,lR).T<span class="op">-</span>const</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># *** E-step        </span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    lR<span class="op">=</span>np.tile(lPi.T,(N,<span class="dv">1</span>))<span class="op">+</span>np.dot(Y,lP.T) <span class="op">+</span> np.dot(OMY,lOMP.T) <span class="co"># + const</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        Z<span class="op">=</span>logsumexp(lR.T)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        lR<span class="op">=</span>lR<span class="op">-</span>np.tile(Z.T.reshape((N,<span class="dv">1</span>)),(<span class="dv">1</span>,K))</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        L.append(np.<span class="bu">sum</span>(Z))</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i<span class="op">&gt;</span><span class="dv">1</span>):</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> np.<span class="bu">abs</span>(L[i]<span class="op">-</span>L[i<span class="op">-</span><span class="dv">1</span>]) <span class="op">&lt;</span> tol: <span class="cf">break</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>                        </span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    iters <span class="op">=</span> i</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lR,lPi,lP,L,iters</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> EMmixtureBernoulli(Y,K,<span class="bu">iter</span>,tol,attempts):</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    Lbest <span class="op">=</span> <span class="op">-</span>np.inf</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    eps<span class="op">=</span><span class="fl">1e-15</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># EM&#39;i farkli noktalardan birkac kere (attempts kadar) baslat</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># En iyi sonucun sonuclarini elde tut</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> attempt <span class="kw">in</span> <span class="bu">range</span>(attempts):</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        lRtmp,lPitmp,lPtmp,L,iters <span class="op">=</span> do_EMmixtureBernoulli(Y,K,<span class="bu">iter</span>,eps)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> L[iters]<span class="op">&gt;</span>Lbest:</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            lR<span class="op">=</span>lRtmp</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>            lPi<span class="op">=</span>lPitmp</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>            lP<span class="op">=</span>lPtmp</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            Lbest<span class="op">=</span>L[iters]</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>            itersbest<span class="op">=</span>iters</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    aic <span class="op">=</span> <span class="op">-</span><span class="dv">2</span><span class="op">*</span>Lbest <span class="op">+</span> <span class="dv">2</span><span class="op">*</span>lP.shape[<span class="dv">0</span>]<span class="op">*</span>lP.shape[<span class="dv">1</span>]</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lR, lPi, lP, Lbest, aic</span></code></pre></div>
<p>Kodda kullanılan log-toplam-exp numarası için <em>Ekler</em>’e
bakılabilir.</p>
<p>Örnek olarak ikisel olarak siyah/beyaz olarak kodlanmış üç tane
farklı sayının 8x8 boyutundaki imajlarını içeren veriyi kullanabiliriz.
Küme sayısını 3 olarak verdik.</p>
<p>Veriden bazı örnekler görelim,</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.loadtxt(<span class="st">&#39;binarydigits.txt&#39;</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(Y[<span class="dv">4</span>,:].reshape((<span class="dv">8</span>,<span class="dv">8</span>),order<span class="op">=</span><span class="st">&#39;C&#39;</span>), cmap<span class="op">=</span>plt.cm.gray)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;mixbern_04.png&#39;</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(Y[<span class="dv">7</span>,:].reshape((<span class="dv">8</span>,<span class="dv">8</span>),order<span class="op">=</span><span class="st">&#39;C&#39;</span>), cmap<span class="op">=</span>plt.cm.gray)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;mixbern_05.png&#39;</span>)</span></code></pre></div>
<p><img src="mixbern_04.png" /> <img src="mixbern_05.png" /></p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> mixbern</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>K<span class="op">=</span><span class="dv">3</span><span class="op">;</span> <span class="bu">iter</span><span class="op">=</span><span class="dv">40</span><span class="op">;</span> eps<span class="op">=</span><span class="fl">1e-15</span><span class="op">;</span> attempts<span class="op">=</span><span class="dv">5</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>lR,lPi,lP,lbest,aic <span class="op">=</span> mixbern.EMmixtureBernoulli(Y,K,<span class="bu">iter</span>,eps,attempts)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span>  np.argmax(lR.T,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> labels</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;log olurluk&#39;</span>, lbest, <span class="st">&#39;aic&#39;</span>, aic</span></code></pre></div>
<pre><code>[0 0 0 2 2 1 2 0 2 2 1 1 2 1 0 0 0 1 0 1 1 0 0 1 0 2 0 2 1 1 1 2 0 0 0 0 0
 0 1 2 0 0 0 0 0 1 2 0 0 2 2 2 1 2 1 2 2 0 0 1 2 1 2 1 0 1 0 0 2 2 2 1 0 2
 2 2 0 1 1 2 2 0 1 0 2 0 0 2 2 0 0 2 0 2 1 2 0 1 0 2]
log olurluk -3049.95050527 aic 6483.90101054</code></pre>
<p>Elde edilen sonuçlara göre, ve paylaştığımız say resimlerindeki
sıraya bakarsak, mesela ilk üç sayı imajını birbirine benziyor olması
lazım. Yine aynı sırada gidersek Daha sonra 4. ve 6. sayıların birbirine
benziyor olması lazım, ve 8. imajın ilk üç imaja benziyor olması lazım,
vs. Resimlere bakınca bunun hakikaten böyle olduğunu görüyoruz. Demek ki
kümeleme başarıyla gerçekleştirilmiş.</p>
<p>Her veri noktasının üyeliğini için <span
class="math inline">\(w_{ij}\)</span>’ye baktık (kodda <code>lR</code>,
üyeliğin log’u), <span class="math inline">\(i\)</span> hangi kümeye en
fazla yakın ise (yüksek olasılık) bunu bir aidiyet olarak kabul
ettik.</p>
<p>Daha ilginç bir hesap şu; her <span
class="math inline">\(\theta_k\)</span> (kodda <code>lP</code>, log’u
alınmış parametreler) artık bir kümeyi “temsil’’ ediyor (multinom bir
değişken bu hatırlarsak) ve bu dağılımların her biri, bir nevi”şablon’’
haline dönüşmüş olmalı; öyle ya, <span class="math inline">\(Z\)</span>
ile zar atıyoruz bir dağılım seçiyoruz, sonra o dağılıma bir daha zar
attırıyoruz, ve herhangi bir sayının imajını üretmek istiyorsak şablon
gerçeğine oldukça yakın olmalı! Yani mantıki olarak düşünürsek, eğer
model veriye iyi uymuş ise, her şablon dağılımının 0,7,5 sayılarının
şeklini aşağı yukarı temsil etmesini bekleriz. Kontrol edelim,</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>dim <span class="op">=</span> (<span class="dv">8</span>,<span class="dv">8</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>templates <span class="op">=</span> np.exp(lP)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>digit0 <span class="op">=</span> np.reshape(templates[<span class="dv">0</span>,:], dim,order<span class="op">=</span><span class="st">&#39;C&#39;</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(digit0, cmap<span class="op">=</span>plt.cm.gray)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;mixbern_01.png&#39;</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>digit1 <span class="op">=</span> np.reshape(templates[<span class="dv">1</span>,:], dim,order<span class="op">=</span><span class="st">&#39;C&#39;</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(digit1, cmap<span class="op">=</span>plt.cm.gray)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;mixbern_02.png&#39;</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>digit2 <span class="op">=</span> np.reshape(templates[<span class="dv">2</span>,:], dim, order<span class="op">=</span><span class="st">&#39;C&#39;</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>plt.imshow(digit2, cmap<span class="op">=</span>plt.cm.gray)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;mixbern_03.png&#39;</span>)</span></code></pre></div>
<p><img src="mixbern_01.png" /> <img src="mixbern_02.png" /> <img
src="mixbern_03.png" /></p>
<p>Hakikaten de şeklen benziyorlar!</p>
<p>Kodlar <a href="mixbern.py">mixbern.py</a></p>
<p>Kaynaklar</p>
<p>[1] Zaki, <em>Data Mining and Analysis: Fundamental Concepts and
Algorithms</em></p>
<p>[2] Alfons Juan, Enrique Vidal, <em>Bernoulli mixture models for
binary images</em></p>
<p>[3] Shalizi, <em>Advanced Data Analysis from an Elementary Point of
View</em></p>
<p>[4] Bishop, C., <em>Pattern Recognition and Machine Learning</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
