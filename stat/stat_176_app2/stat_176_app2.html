<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Ekler</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="ekler">Ekler</h1>
<p>Binom ve <span class="math inline">\(p\)</span> İçin Maksimum Olurluk
Tahmini [1]</p>
<p><span class="math display">\[ L(p;x) = \prod_{i=1}^n f(x_i;p) =
\prod_{i=1}^n {n \choose x} p^x(1-p)^{1-x} \]</span></p>
<p>Log alalım</p>
<p><span class="math display">\[ \log L(p;x) =
\sum_{i=1}^n \log {n \choose x} + x \log p + (1-x) \log (1-p)
\]</span></p>
<p><span class="math inline">\(p\)</span>’ye göre türevi alalım, bu
sırada kombinasyon ifadesi <span class="math inline">\({n \choose
x}\)</span> içinde <span class="math inline">\(p\)</span> olmadığı için
o yokolacaktır,</p>
<p><span class="math display">\[ \frac{\partial \log L(p)}{\partial p} =
\frac{x}{p} - \frac{n-x}{1-p}
\]</span></p>
<p>Maksimum değeri bulmak için sıfıra eşitleyelim ve <span
class="math inline">\(p\)</span> için çözelim,</p>
<p><span class="math display">\[ 0 = \frac{x}{p} - \frac{n-x}{1-p}
\]</span></p>
<p><span class="math display">\[  \frac{x}{p} =
\frac{n-x}{1-p}  \]</span></p>
<p><span class="math display">\[ p(n-x)  = x(1-p) \]</span></p>
<p><span class="math display">\[ pn - px = x-px \]</span></p>
<p><span class="math display">\[ pn = x \]</span></p>
<p><span class="math display">\[ p = \frac{x}{n} \]</span></p>
<p>Yani <span class="math inline">\(p\)</span> için maksimum olurluk
tahmini <span class="math inline">\(x/n\)</span>.</p>
<p>Bernoulli dağılımı Binom dağılımına çok benzer, sadece onun baş
kısmında kombinasyon ifadesi yoktur. Fakat o ifade <span
class="math inline">\(p\)</span>’ye göre türevde nasıl olsa yokolacağına
göre Bernoulli dağılımı için de tahmin edici aynıdır.</p>
<p>Bayes Usulü Güven Aralığı (Confidence Intervals)</p>
<p>Bayes ile bu hesabı yapmak için bir dağılımı baz almak lazım. Eğer
sonuç olarak bir tek sayı değil, bir dağılım elde edersek bu dağılım
üzerinde güvenlik hesaplarını yaparız. Mesela sonuç, sonsal dağılım
(posterior) bir Gaussian dağılım ise, bu dağılımın yüzde 95 ağırlığının
nerede olduğu, ve nasıl hesaplandığı bellidir.</p>
<p>Bayes Teorisi</p>
<p><span class="math display">\[ P(A \mid B)  = \frac{P(B \mid
A)P(A)}{P(B)} \]</span></p>
<p>Veri analizi bağlamında diyelim ki deneyler yaparak tahmini olarak
hesaplamak (estimate) istediğimiz bir parametre var, bu bir protonun
kütlesi ya da bir ameliyat sonrası hayatta kalma oranı olabilir. Bu
durumlarda iki ayrı “olaydan” bahsetmemiz gerekir, B olayı spesifik bazı
ölçümlerin elde edilmesi “olayıdır”, mesela ölçüm üç sayıdan oluşuyorsa,
biz bir ölçümde spesifik olarak <span
class="math inline">\(\{0.2,4,5.4\}\)</span> değerlerini elde etmişiz.
İkinci olay bilmediğimiz parametrenin belli bir değere sahip olması
olacak. O zaman Bayes Teorisinin şu şekilde tekrar yazabiliriz,</p>
<p><span class="math display">\[ P(parametre \mid veri ) \propto P(veri
\mid parametre)P(parametre) \]</span></p>
<p><span class="math inline">\(\propto\)</span> işareti orantılı olmak
(proportional to) anlamına geliyor. Böleni attık çünkü o bir sabit
(tamamen veriye bağlı, tahmini hesaplamak istediğimiz parametreye bağlı
değil). Tabii bu durumda sol ve sağ taraf birbirine eşit olmaz, o yüzden
eşitlik yerine orantılı olmak işaretini kullandık. Bu çerçevede “belli
bir sayısal sabit çerçevesinde birbirine eşit (equal within a numeric
constant)” gibi cümleler de görülebilir.</p>
<p>Örnek</p>
<p>Diyelim ki bir bozuk para ile 10 kere yazı-tura attık, ve sonuç
altta</p>
<p>T H H H H T T H H H</p>
<p>Bu veriye bakarak paranın hileli olup olmadığını anlamaya
çalışacağız. Bayes ifadesini bu veriye göre yazalım,</p>
<p><span class="math display">\[ P(p | \{ \textrm{T H H H H T T H H H}
\} \propto
P(\{ \textrm{T H H H H T T H H H} | p) P(p) \}
\]</span></p>
<p><span class="math inline">\(P(p)\)</span> ifadesi ne anlama gelir?
Aslında bu ifadeyi <span class="math inline">\(P([Dagilim] = p)\)</span>
olarak görmek daha iyi, artık <span class="math inline">\(p\)</span>
parametresini bir dağılımdan gelen bir özgün değer olarak gördüğümüze
göre, o dağılımın belli bir <span class="math inline">\(p\)</span>’ye
eşit olduğu zamanı modelliyoruz burada. Her halükarda <span
class="math inline">\(P(p)\)</span> dağılımını, yani onsel (prior)
olasılığı bilmiyoruz, hesaptan önce her değerin mümkün olduğunu
biliyoruz, o zaman bu onsel dağılımı düz (flat) olarak alırız, yani
<span class="math inline">\(P(p) = 1\)</span>.</p>
<p><span class="math inline">\(P( \{\textrm{T H H H H T T H H H} \} |
p)\)</span> ifadesi göz korkutucu olabilir, ama buradaki her öğenin
bağımsız özdeşçe dağılmış (independent identically distributed) olduğunu
görürsek, ama bu ifadeyi ayrı ayrı <span class="math inline">\(P(
\textrm{T} | p)\)</span> ve <span class="math inline">\(P( \textrm{H} |
p)\)</span> çarpımları olarak görebiliriz. <span
class="math inline">\(P( \textrm{T} | p) = p\)</span> ve <span
class="math inline">\(P( \textrm{H} | p)=1-p\)</span> olduğunu
biliyoruz. O zaman</p>
<p><span class="math display">\[
P(p | \{ \textrm{7 Tura, 3 Yazı} \} \propto
p^7(1-p)^3
\]</span></p>
<p>Grafiklersek,</p>
<p><img src="stat_appendix_01.png" /></p>
<p>Böylece <span class="math inline">\(p\)</span> için bir sonsal
dağılım elde ettik. Artık bu dağılımın yüzde 95 ağırlığının nerede
olduğunu rahatça görebiliriz / hesaplayabiliriz. Dağılımın tepe
noktasının <span class="math inline">\(p=0.7\)</span> civarında olduğu
görülüyor. Bir dağılımla daha fazlasını yapmak ta mümkün, mesela bu
fonksiyonu <span class="math inline">\(p\)</span>’ye bağlı başka bir
fonksiyona karşı entegre etmek mümkün, mesela beklentiyi bu şekilde
hesaplayabiliriz.</p>
<p>Onsel dağılımın her noktaya eşit ağırlık veren birörnek (uniform)
seçilmiş olması, yani problemi çözmeye sıfır bilgiden başlamış olmamız,
yöntemin bir zayıflığı olarak görülmemeli. Yöntemin kuvveti elimizdeki
bilgiyle başlayıp onu net bir şekilde veri ve olurluk üzerinden sonsal
tek dağılıma götürebilmesi. Başlangıç ve sonuç arasındaki bağlantı gayet
net. Fazlası da var; ilgilendiğimiz alanı (domain) öğrendikçe, başta hiç
bilmediğimiz onsel dağılımı daha net, bilgili bir şekilde seçebiliriz ve
bu sonsal dağılımı da daha olması gereken modele daha
yaklaştırabilir.</p>
<p>Moment</p>
<p>Olasılık matematiğinde “moment üreten işlevler” olarak adlandırılan,
başlangıçta pek yararlı gibi gözükmesede bir takım matematiksel
özellikleri olduğu için, ispatlarda oldukça işe yarayan bir kavram
vardır.</p>
<p>Her rasgele değişkenin bir dağılımı olduğunu biliyoruz. Her rasgele
değişkenin de ayrıca bir moment üreten fonksiyonu da vardır. Ayrıca,
moment üreten fonksiyon ile rasgele değişken arasında bire-bir olarak
bir ilişki mevcuttur. “Bu neye yarar?” diye sorulabilir; Cevap olarak,
mesela cebirsel olarak türete türete bir moment’e geldiğimiz düşünelim,
ve tekrar başka bir taraftan, başka bir formülden gene türete türete
tekrar aynı moment işlevine geliyorsak, bu demektir ki, iki taraftan
gelen rasgele değişkenler (ve tekabül eden dağılımları) birbirine
eşittir. Bazı şartlarda moment üreten işlevler ile cebir yapmak, dağılım
fonksiyonlarından daha rahat olmaktadır.</p>
<p>Her rasgele değişken için, moment üreten işlev şöyle bulunur.</p>
<p><span class="math inline">\(X\)</span> rasgele degiskenin moment
ureten operasyonu</p>
<p><span class="math inline">\(M(t)=E(e^{tX})\)</span> olarak
gösterilir</p>
<p>Ayrıksal operasyonlar için</p>
<p><span class="math display">\[ M(t) = \sum_x e^{tx}p(x) \]</span></p>
<p>Sürekli işlevler için</p>
<p><span class="math display">\[ M(t) = \int_{-\infty}^{\infty}
e^{tx}f(x) \mathrm{d} x   \]</span></p>
<p>Kuram</p>
<p>Gelelim yazımızın esas konusu olan kuramımıza.</p>
<p>Eğer <span class="math inline">\(X_1, X_2...X_n\)</span> bağımsız
rasgele değişken ise, ve her değişkenin <span
class="math inline">\(M_i(t)\)</span> <span
class="math inline">\(i=1,2,3,...n\)</span> olarak, öz olarak aynı olan
birer moment üreten işlevi var ise, o zaman,</p>
<p><span class="math display">\[ Y = \sum_{i=1}^n  aX_i \]</span></p>
<p>açılımı</p>
<p><span class="math display">\[ M_y(t) = \prod_{i=1}^n M(a_i t)
\]</span></p>
<p>olacaktır.</p>
<p>İspat</p>
<p><span class="math display">\[ M_y(t) =
E(e^{tY}=E(e^{t(a_1X_1+a_2X_2+..+a_nX_n)} \]</span></p>
<p><span class="math display">\[ = E[\exp(ta_1 X_1 ta_2X_2...+ta_nX_n)]
\]</span></p>
<p><span class="math display">\[ = E[\exp(ta_1X_1)+\exp(ta_2X_2)+ ... +
\exp(ta_nX_n)] \]</span></p>
<p><span class="math display">\[ = E[\exp(ta_1X_1)]+E[\exp(ta_2X_2)]+
... + E[\exp(ta_nX_n)]\]</span></p>
<p>Daha önce belirttiğimiz gibi</p>
<p><span class="math display">\[ M_i(t) = E[\exp(tX_i)] \]</span></p>
<p>olduğuna göre ve <span class="math inline">\(t\)</span> yerine <span
class="math inline">\(ta_i\)</span> koyulduğunu düşünelim</p>
<p><span class="math display">\[ M_y(t) = \prod_{i=1}^n M_y(a_it)
\]</span></p>
<p>olacaktır.</p>
<p>Bunu <span class="math inline">\(M_y(t)= (M_i(a_it))^n\)</span>
şeklinde de gösterebiliriz.</p>
<p>Markov’un Eşitsizliği (Markov’s Inequality)</p>
<p><span class="math inline">\(X\)</span> bir negatif olmayan rasgele
değişken olsun ve farz edelim ki <span
class="math inline">\(E(X)\)</span> mevcut [1]. O zaman her <span
class="math inline">\(t &gt; 0\)</span> için</p>
<p><span class="math display">\[ P(X&gt;t) \le
\frac{E(X)}{t}\]</span></p>
<p>doğru olmalıdır.</p>
<p>İspat</p>
<p><span class="math inline">\(X &gt; 0\)</span> olduğuna göre,</p>
<p><span class="math display">\[
E(X)
= \int_{0}^{\infty} x f(x) \mathrm{d} x
= \int_{0}^{t} x f(x) \mathrm{d} x + \int_{t}^{\infty} x f(x) \mathrm{d}
x =
\]</span></p>
<p><span class="math display">\[
\ge \int_{t}^{\infty} x f(x) \mathrm{d} x \ge t \int_{t}^{\infty} f(x)
\mathrm{d} x
= t P(X &gt; t)
\]</span></p>
<p>Çebişev Eşitsizliği (Chebyshev’s Inequality)</p>
<p>Herhangi bir <span class="math inline">\(t\)</span> değeri için,</p>
<p><span class="math display">\[ P(|X-\mu| &gt; t) \le
\frac{\sigma^2}{t^2} \]</span></p>
<p>ve</p>
<p><span class="math display">\[ P(|Z| \ge k) \le
\frac{1}{k^2}\]</span></p>
<p>ki <span class="math inline">\(Z = (X-\mu)/\sigma\)</span>, ve <span
class="math inline">\(E(X) = \mu\)</span>. Bunun bazı akılda kalabilecek
ilginç sonuçları <span class="math inline">\(P(|Z| &gt; 2) &lt;
1/4\)</span> ve <span class="math inline">\(P(|Z| &gt; 3) &lt;
1/9\)</span> olabilir.</p>
<p>İspat</p>
<p>Yöntem 1</p>
<p>Üstteki Markov’un eşitsizliğini kullanırız, oradan şu sonuca
varırız,</p>
<p><span class="math display">\[
P(|X-\mu| \ge t) = P(|X-\mu|^2 \ge t^2 ) \le \frac{E(X-\mu)^2}{t^2}
= \frac{\sigma^2}{t^2}  
\]</span></p>
<p>İkinci kısım <span class="math inline">\(t=k\sigma\)</span>
kullanılarak elde edilebilir.</p>
<p>Yöntem 2</p>
<p>Olasılık matematiğinde, büyük sayılar kuramı adında anılan ve
olasılık matematiğinin belkemiğini oluşturan kuramı ispatlamak için,
diğer bir kuram olan Çebişev eşitsizliğini de anlamamız gerekiyor.
Çebişev eşitsizliği bir rasgele değişken, onun ortalaması (beklentisi)
ve herhangi bir sabit sayı arasındaki üçlü arasında bir ‘eşitsizlik’
bağlantısı kurar, ve bu bağlantı diğer olasılık işlemlerimizde ispat
verisi olarak işimize yarar.</p>
<p>İspata başlayalım. Entegral ile olasılık hesabı yapmak için bize bir
<span class="math inline">\(x\)</span> uzayı lazım.</p>
<p><span class="math display">\[ \mathbb{R} = {x: |x-\mu| &gt; t}
\]</span></p>
<p>Yani <span class="math inline">\(\mathbb{R}\)</span> uzayı, <span
class="math inline">\(x\)</span> ile ortalamasının farkının, <span
class="math inline">\(t\)</span>’den büyük olduğu bütün sayıların
kümesidir.</p>
<p>O zaman,</p>
<p><span class="math display">\[ P(|X-\mu| &gt; t) = \int_R f(x)
\mathrm{d} x \]</span></p>
<p>Dikkat edelim <span class="math inline">\(P(..)\)</span> içindeki
formül, küme tanımı ile aynı. O yüzden <span
class="math inline">\(P()\)</span> hesabı ortada daha olmayan, ama
varolduğu kesin bir dağılım fonksiyonu tanımlamış da oluyor. Buna <span
class="math inline">\(f(x)\)</span> deriz. <span
class="math inline">\(P()\)</span>’in, <span
class="math inline">\(f(x)\)</span> fonksiyonunun <span
class="math inline">\(R\)</span> üzerinden entegral olduğunu olasılığa
giriş dersinden bilmemiz lazım.</p>
<p>Eger <span class="math inline">\(x \in R\)</span> dersek o zaman</p>
<p><span class="math display">\[ \frac{|x-\mu|^2}{t^2} \ge 1
\]</span></p>
<p>t’nin denkleme bu şekilde nereden geldiği şaşkınlık yaratabilir. Daha
önce tanımlanan şu ibareye dikkat edelim, <span class="math inline">\(x:
|x-u| &gt; t\)</span> diye belirtmiştik. Bu ifadeyi değiştirerek,
yukarıdaki denkleme gelebiliriz.</p>
<p>Devam edersek, elimizdeki 1’den büyük bir değer var. Bu değeri
kullanarak, aşağıdaki tanımı yapmamız doğru olacaktır.</p>
<p><span class="math display">\[
\int_R f(x) \mathrm{d} x \le \int_R \frac{(x-\mu)^2}{t^2}f(x) \mathrm{d}
x \le
\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x) \mathrm{d} x
\]</span></p>
<p>Ortadaki entegral niye birinci entegralden büyük? Çünkü orta
entegraldeki <span class="math inline">\(f(x)dx\)</span> ibaresinden
önce gelen kısmın, her zaman 1’den büyük olacağını belirttiğimize göre,
ikinci entegralin birinciden büyük olması normaldir, çünkü birinci
entegral <span class="math inline">\(f(x)\)</span> olasılık dağılımına
bağlı, entegral ise bir alan hesabıdır ve olasılık dağılımlarının
sonsuzlar arasındaki entegrali her zaman 1 çıkar, kaldı ki üstteki <span
class="math inline">\(x\)</span>’in uzayını daha da daralttık.</p>
<p>Evet…Üçüncü entegral ispata oldukça yaklaştı aslında. Standart sapma
işaretini hala ortada göremiyoruz, fakat son entegraldeki ibare standart
sapma değerini zaten içeriyor. Önce daha önceki olasılık natematiği
bilgimize dayanarak, standart sapmanın tanımını yazıyoruz. Dikkat
edelim, bu ibare şu anki ispatımız dahilinden değil, haricinden önceki
bilgimize dayanarak geldi. Standart sapmanın tanımı şöyledir.</p>
<p><span class="math display">\[ \sigma^2 = \int_{-\infty}^{\infty}
(x-\mu)^2f(x) \mathrm{d} x \]</span></p>
<p>O zaman</p>
<p><span class="math display">\[
\frac{\sigma^2}{t^2}
= \int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x)\mathrm{d} x
\]</span></p>
<p>yani</p>
<p><span class="math display">\[
\int_R f(x) \mathrm{d} x \le \frac{\sigma^2}{t^2} =
\int_{-\infty}^{\infty} \frac{(x-\mu)^2}{t^2}f(x) \mathrm{d} x
\]</span></p>
<p>ki <span class="math inline">\(\int_R f(x) \mathrm{d} x\)</span>
zaten <span class="math inline">\(P(|X-\mu| &gt; t)\)</span> olarak
tanımlanmıştı.</p>
<p>Örnek</p>
<p>Diyelim ki bir tahmin edicimiz var, onu test etmek istiyoruz, bu bir
yapay sinir ağı (YSA) olabilir, ve elimizde <span
class="math inline">\(n\)</span> tane test verisi var. Eğer tahmin
edici, yani YSA, hatalı ise <span class="math inline">\(X_i=1\)</span>
olsun, haklı ise <span class="math inline">\(X_i=0\)</span> olsun. O
zaman gözlenen hata oranı (observed error rate) <span
class="math inline">\(\overline{X}_n = n^{-1}\sum_{i=1}^{n} X_i\)</span>
olacaktır. Rasgele değişken çıktılarına bakarak bunu bir <span
class="math inline">\(p\)</span>’si bilinmeyen bir Bernoulli
dağılımından geliyormuş gibi kabul edebileceğimizi görebiliriz.
İstediğimiz gerçek -ama bilinmeyen- <span
class="math inline">\(p\)</span> hakkında irdeleme yapmak. <span
class="math inline">\(\overline{X}_n\)</span>’in gerçek <span
class="math inline">\(p\)</span>’nin <span
class="math inline">\(\epsilon\)</span> yakınında olmama olasılığı
nedir?</p>
<p>Bernoulli’lerin özelliklerinden biliyoruz ki</p>
<p><span class="math display">\[ V(\overline{X}_n) = V(X_1) / n =
p(1-p)/n\]</span></p>
<p>Çebişev uygulayınca,</p>
<p><span class="math display">\[
P(|\overline{X}_n - p| &gt; \epsilon) \le
\frac{V(\overline{X}_n)}{\epsilon^2}
= \frac{p(1-p)}{n\epsilon^2} \le \frac{1}{4n\epsilon^2}
\]</span></p>
<p>Hatırlarsak Bernoulli için <span
class="math inline">\(E(X)=p\)</span>. Son geçiş mümkün oldu çünkü her
<span class="math inline">\(p\)</span> için <span
class="math inline">\(p(1-p) \le \frac{1}{4}\)</span> olmak zorundadır.
Öyle değil mi? <span class="math inline">\(p(1-p)\)</span>’nin
alabileceği en büyük değer <span class="math inline">\(p=1/2\)</span>
içindir, bundan farklı her <span class="math inline">\(p\)</span> değeri
<span class="math inline">\(1/4\)</span>’ten küçük bir çarpım verir,
mesela <span class="math inline">\(p=1/3\)</span> için <span
class="math inline">\(1/3 \cdot 2/3 = 2/9\)</span>.</p>
<p>O zaman, ve diyelim ki <span class="math inline">\(\epsilon =
.2\)</span> ve <span class="math inline">\(n=100\)</span> için <span
class="math inline">\(0.0625\)</span> sınırını elde ederiz.</p>
<p>Hoeffding’in Eşitsizliği</p>
<p>Bu eşitsizlik Markov’un eşitsizliğine benziyor, ama daha keskin
sonuçlar verebiliyor, yani ufak güven aralıkları elde edebiliyoruz, ki
bu daha fazla kesinlik demektir. Bu eşitsizliği iki bölüm olarak
vereceğiz,</p>
<p><span class="math inline">\(Y_1,Y_2,..,Y_n\)</span> bağımsız
gözlemler olsunlar, ki <span class="math inline">\(E(Y_i)=0\)</span> ve
<span class="math inline">\(a_i \le Y_i \le b_i\)</span> doğru olacak
şekilde. O zaman herhangi bir <span
class="math inline">\(t&gt;0\)</span> için</p>
<ol type="1">
<li>Teori</li>
</ol>
<p><span class="math display">\[
P \bigg(
\sum_{i=1}^{n} Y_i \ge \epsilon \le e^{-t\epsilon}
\prod_{i=1}^{n} e^{{t^2}(b_i-a_i)^2 / 8}
\bigg)
\]</span></p>
<ol start="2" type="1">
<li>Teori</li>
</ol>
<p><span class="math inline">\(X_1,..,X_n \sim Bernoulli(p)\)</span>
olsun. O zaman herhangi bir <span class="math inline">\(\epsilon &gt;
0\)</span> icin</p>
<p><span class="math display">\[ P(|\overline{X}_n -p| &gt; \epsilon )
\le 2e^{-2n\epsilon^2}\]</span></p>
<p>doğru olmalıdır ki, daha önce gördüğümüz gibi, <span
class="math inline">\(\overline{X}_n = n^{-1}\sum_{i=1}^{n} X_i\)</span>
olacak şekilde.</p>
<p>İspat için bkz [1, sf. 67].</p>
<p>Örnek</p>
<p>Diyelim ki <span class="math inline">\(X_1,..,X_n \sim
Bernoulli(p)\)</span>. <span class="math inline">\(n=100\)</span> ve
<span class="math inline">\(\epsilon=.2\)</span> olsun. Çebişev
esitsizligi ile</p>
<p><span class="math display">\[ P(|\overline{X}_n - p| &gt; \epsilon )
\le 0.0625 \]</span></p>
<p>elde etmiştik. Hoeffding’e göre</p>
<p><span class="math display">\[
P(|\overline{X}_n - p| &gt; \epsilon ) \le 2e^{-2 (100)(.2)^2} = 0.00067
\]</span></p>
<p>elde ederiz, ki bu Cebisev’den gelen <span
class="math inline">\(0.0625\)</span>’e göre çok daha ufak bir
değerdir.</p>
<p>Jensen’in Esitsizligi (Jensen’s Inequality)</p>
<p>Teori</p>
<p>Eğer <span class="math inline">\(g\)</span> fonksiyonu dışbükey
(convex) ise o zaman</p>
<p><span class="math display">\[
E g(X) \ge g(E(X))
\]</span></p>
<p>İçbukey için tam tersi geçerli.</p>
<p>Teorinin sözel olarak söylediği eğer <span
class="math inline">\(f\)</span> fonksiyonu dışbükey ise verinin
ortalaması (beklentisi) üzerinde <span class="math inline">\(f\)</span>
işletmek, o verinin <span class="math inline">\(f\)</span> değerlerinin
ortalaması ile aynı olmuyor, daha doğrusu ikinci büyüklük birinci için
bir alt sınır oluşturuyor, birinci en az ikinci kadar.</p>
<p>İspat</p>
<p>Bir <span class="math inline">\(L(x) = a + bx\)</span> çizgisi hayal
edelim, bu çizgi <span class="math inline">\(g(x)\)</span>’e tam <span
class="math inline">\(E(X)\)</span> noktasında teğet olsun [1, sf. 66].
<span class="math inline">\(g\)</span> dışbükey olduğu için her noktada
<span class="math inline">\(L(x)\)</span> çizgisi üzerinde olması
garanti,</p>
<p><img src="jensen.jpg" /></p>
<p>O zaman</p>
<p><span class="math display">\[
E(g(X)) \ge E(L(X))
\]</span></p>
<p><span class="math inline">\(E(L(X))\)</span> formülünü açalım,</p>
<p><span class="math display">\[
E(L(X)) = E(a + bX) = a + bE(X)
\]</span></p>
<p>Birinci geçiş basit beklenti matematiği. Son formül <span
class="math inline">\(L(x)\)</span>’in <span
class="math inline">\(E(X)\)</span> üzerindeki formu olurdu, o zaman</p>
<p><span class="math display">\[
a + bE(X) = L(E(X))
\]</span></p>
<p>diyebiliriz.</p>
<p>Şimdi hatırlıyoruz ki teğet çizgi <span
class="math inline">\(g\)</span> ile tam <span
class="math inline">\(E(X)\)</span> noktasında kesişiyor, o noktada
değerleri aynı yani, o zaman</p>
<p><span class="math display">\[
L(E(X)) = g(E(X))
\]</span></p>
<p>Demek ki</p>
<p><span class="math display">\[
E(g(X)) \ge E(L(X)) = g(E(X))
\]</span></p>
<p>Kısaca</p>
<p><span class="math display">\[
E(g(X)) \ge  g(E(X))
\]</span></p>
<p>Teori ispatlanmış oldu.</p>
<p>Aslında Jensen Eşitsizliğinin daha geniş bir hali ve yorumlaması var,
eğer <span class="math inline">\(g(x)\)</span> genel olarak gayri lineer
ise (sadece dışbükey değil) o zaman <span
class="math inline">\(g(x)\)</span>’in ortalaması <span
class="math inline">\(x\)</span> ortalamalarının üzerindeki <span
class="math inline">\(g\)</span> hesabına eşit değildir [2]. Bir alt
sınır değil direk eşitsizlikten bahsediyoruz. Bu yorumlamanın da pek çok
yerde uygulaması vardır, bu ifadenin ispatı için İstatistik kaynaklarına
başvurulabilir.</p>
<p>Kaynaklar</p>
<p>[1] Wasserman, <em>All of Statistics</em></p>
<p>[2] Denny, <em>The fallacy of the average: on the ubiquity, utility
and continuing novelty of Jensen’s inequality</em> <a
href="https://journals.biologists.com/jeb/article/220/2/139/18635/The-fallacy-of-the-average-on-the-ubiquity-utility">https://journals.biologists.com/jeb/article/220/2/139/18635/The-fallacy-of-the-average-on-the-ubiquity-utility</a></p>
<hr>
<p>z-Tablosu</p>
<p>Nasıl okunur? Z-değeri -0.8994 için z kolonundan aşağı inilir, ve
-0.8 bulunur, x.x9xx yani 9 için .09 kolonuna gidilir ve bu kesişmedeki
değer okunur, .1867, yuvarlanarak .19 da kabul edilebilir.</p>
<p><img src="stat_appendix_02.png" /></p>
<p>z .00 .01 .02 .04 .05 .06 .07 .08 .09</p>
<p>-3.4 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0002</p>
<p>-3.3 .0005 .0005 .0005 .0004 .0004 .0004 .0004 .0004 .0004 .0003</p>
<p>-3.2 .0007 .0007 .0006 .0006 .0006 .0006 .0006 .0005 .0005 .0005</p>
<p>-3.1 .0010 .0009 .0009 .0009 .0008 .0008 .0008 .0008 .0007 .0007</p>
<p>-3.0 .0013 .0013 .0013 .0012 .0012 .0011 .0011 .0011 .0010 .0010</p>
<p>-2.9 .0019 .0018 .0018 .0017 .0016 .0016 .0015 .0015 .0014 .0014</p>
<p>-2.8 .0026 .0025 .0024 .0023 .0023 .0022 .0021 .0021 .0020 .0019</p>
<p>-2.7 .0035 .0034 .0033 .0032 .0031 .0030 .0029 .0028 .0027 .0026</p>
<p>-2.6 .0047 .0045 .0044 .0043 .0041 .0040 .0039 .0038 .0037 .0036</p>
<p>-2.5 .0062 .0060 .0059 .0057 .0055 .0054 .0052 .0051 .0049 .0048</p>
<p>-2.4 .0082 .0080 .0078 .0075 .0073 .0071 .0069 .0068 .0066 .0064</p>
<p>-2.3 .0107 .0104 .0102 .0099 .0096 .0094 .0091 .0089 .0087 .0084</p>
<p>-2.2 .0139 .0136 .0132 .0129 .0125 .0122 .0119 .0116 .0113 .0110</p>
<p>-2.1 .0179 .0174 .0170 .0166 .0162 .0158 .0154 .0150 .0146 .0143</p>
<p>-2.0 .0228 .0222 .0217 .0212 .0207 .0202 .0197 .0192 .0188 .0183</p>
<p>-1.9 .0287 .0281 .0274 .0268 .0262 .0256 .0250 .0244 .0239 .0233</p>
<p>-1.8 .0359 .0351 .0344 .0336 .0329 .0322 .0314 .0307 .0301 .0294</p>
<p>-1.7 .0446 .0436 .0427 .0418 .0409 .0401 .0392 .0384 .0375 .0367</p>
<p>-1.6 .0548 .0537 .0526 .0516 .0505 .0495 .0485 .0475 .0465 .0455</p>
<p>-1.5 .0668 .0655 .0643 .0630 .0618 .0606 .0594 .0582 .0571 .0559</p>
<p>-1.4 .0808 .0793 .0778 .0764 .0749 .0735 .0721 .0708 .0694 .0681</p>
<p>-1.3 .0968 .0951 .0934 .0918 .0901 .0885 .0869 .0853 .0838 .0823</p>
<p>-1.2 .1151 .1131 .1112 .1093 .1075 .1056 .1038 .1020 .1003 .0985</p>
<p>-1.1 .1357 .1335 .1314 .1292 .1271 .1251 .1230 .1210 .1190 .1170</p>
<p>-1.0 .1587 .1562 .1539 .1515 .1492 .1469 .1446 .1423 .1401 .1379</p>
<p>-0.9 .1841 .1814 .1788 .1762 .1736 .1711 .1685 .1660 .1635 .1611</p>
<p>-0.8 .2119 .2090 .2061 .2033 .2005 .1977 .1949 .1922 .1894 .1867</p>
<p>-0.7 .2420 .2389 .2358 .2327 .2296 .2266 .2236 .2206 .2177 .2148</p>
<p>-0.6 .2743 .2709 .2676 .2643 .2611 .2578 .2546 .2514 .2483 .2451</p>
<p>-0.5 .3085 .3050 .3015 .2981 .2946 .2912 .2877 .2843 .2810 .2776</p>
<p>-0.4 .3446 .3409 .3372 .3336 .3300 .3264 .3228 .3192 .3156 .3121</p>
<p>-0.3 .3821 .3783 .3745 .3707 .3669 .3632 .3594 .3557 .3520 .3483</p>
<p>-0.2 .4207 .4168 .4129 .4090 .4052 .4013 .3974 .3936 .3897 .3859</p>
<p>-0.1 .4602 .4562 .4522 .4483 .4443 .4404 .4364 .4325 .4286 .4247</p>
<p>0.0 .5000 .4960 .4920 .4880 .4840 .4801 .4761 .4721 .4681 .4641</p>
<hr>
<p>z .00 .01 .02 .04 .05 .06 .07 .08 .09</p>
<p>0.0 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359</p>
<p>0.1 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .5753</p>
<p>0.2 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .6141</p>
<p>0.3 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6480 .6517</p>
<p>0.4 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879</p>
<p>0.5 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224</p>
<p>0.6 .7257 .7291 .7324 .7357 .7389 .7422 .7454 .7486 .7517 .7549</p>
<p>0.7 .7580 .7611 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .7852</p>
<p>0.8 .7881 .7910 .7939 .7967 .7995 .8023 .8051 .8078 .8106 .8133</p>
<p>0.9 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389</p>
<p>1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621</p>
<p>1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .8830</p>
<p>1.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .9015</p>
<p>1.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9147 .9162 .9177</p>
<p>1.4 .9192 .9207 .9222 .9236 .9251 .9265 .9279 .9292 .9306 .9319</p>
<p>1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9429 .9441</p>
<p>1.6 .9452 .9463 .9474 .9484 .9495 .9505 .9515 .9525 .9535 .9545</p>
<p>1.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .9633</p>
<p>1.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9699 .9706</p>
<p>1.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9761 .9767</p>
<p>2.0 .9772 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817</p>
<p>2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .9857</p>
<p>2.2 .9861 .9864 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890</p>
<p>2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916</p>
<p>2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .9936</p>
<p>2.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952</p>
<p>2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964</p>
<p>2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .9974</p>
<p>2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 .9979 .9980 .9981</p>
<p>2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986</p>
<p>3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .9990</p>
<p>3.1 .9990 .9991 .9991 .9991 .9992 .9992 .9992 .9992 .9993 .9993</p>
<p>3.2 .9993 .9993 .9994 .9994 .9994 .9994 .9994 .9995 .9995 .9995</p>
<p>3.3 .9995 .9995 .9995 .9996 .9996 .9996 .9996 .9996 .9996 .9997</p>
<p>3.4 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9998</p>
<p>Kaynaklar</p>
<p>[1] Gullickson, {}, <a
href="https://web.archive.org/web/20160312151715/http://pages.uoregon.edu/aarong/teaching/G4075_Outline/node13.html">https://web.archive.org/web/20160312151715/http://pages.uoregon.edu/aarong/teaching/G4075_Outline/node13.html</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
