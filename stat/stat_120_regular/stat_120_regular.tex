\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Regresyon, Ridge, Lasso, Çapraz Saðlama, Regülarize Etmek

Konumuz regresyon çeþitleri, ve örnek veri olarak diyabet hastalýðý
olan kiþilerden alýnmýþ bazý temel verilerle hastalýðýn bir sene
sonraki ilerleme miktarý kullanýlacak. Regresyon sayesinde temel
veriler ile hastalýðýn ilerlemesi arasýnda bir baðlantý bulunabilir,
bu sayede hem veri açýklanýr / daha iyi anlaþýlýr (hangi deðiþken
önemlidir, hangisi deðildir), hem de baþka bir hastanýn temel
verilerini kullanarak o hastanýn diyabetinin bir sene sonra ne
olacaðýný tahmin etmek mümkün olur. Kullanýlan temel veriler kiþinin
yaþý, cinsiyeti, vücut kütle endeksi (body mass index) ortalama
tansiyonu ve altý kere alýnmýþ kan serum ölçümleridir.

\begin{minted}[fontsize=\footnotesize]{python}
from pandas import *
diabetes = read_csv("diabetes.csv",sep=';')
diabetes_y = diabetes['response']
diabetes_x = diabetes.drop("response",axis=1)
diabetes_x_train = diabetes_x[:-20]
diabetes_x_test  = diabetes_x[-20:]
diabetes_y_train = diabetes_y[:-20]
diabetes_y_test  = diabetes_y[-20:]
\end{minted}

Ýlk önce basit regresyonu hatýrlayalým. Bu tekniði daha önce pek çok yönden
gördük. {\em Lineer Cebir}, {\em Çok Deðiþkenli Calculus} ders notlarýnda
bu tekniðin türetilmesi mevcut. Formül

$$ \hat{w} = (X^TX) ^{-1} X^{T}y $$

Sayýsal olarak hemen bu hesabý yapabiliriz. Bir hatýrlatma: veri setine $y$
ekseninin nerede kesildiðinin bulunabilmesi için suni bir ekstra kesi,
``intercept'' adlý kolon ekleyeceðiz, bu kolon iki boyutta $y=ax+c$
formülündeki $c$'nin bulunabilmesi içindir. Pandas ile bu ekstra kolonu
eklemek çok basit, ismen mevcut olmayan kolon eriþildiði anda o kolon hemen
yoktan yaratýlýr.

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as la
x_tmp = diabetes_x_train.copy()
x_tmp['intercept'] = 1
xTx = np.dot(x_tmp.T,x_tmp )
ws = np.dot(la.inv(xTx),np.dot(x_tmp.T,diabetes_y_train))
print ws
\end{minted}

\begin{verbatim}
[  3.03499452e-01  -2.37639315e+02   5.10530605e+02   3.27736981e+02
  -8.14131711e+02   4.92814589e+02   1.02848453e+02   1.84606489e+02
   7.43519617e+02   7.60951724e+01   1.52764307e+02]
\end{verbatim}

Ayný hesabý bir de \verb!scikit-learn! paketini kullanarak yapalým. Bu
paketin \verb!LinearRegression! çaðrýsý kesi ekleme iþini otomatik olarak
hallediyor, eðer kesi olmasýn isteseydik, \verb!fit_intercept=False!
diyecektik.

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn import linear_model, cross_validation
lin = linear_model.LinearRegression()
lin.fit(diabetes_x_train, diabetes_y_train)
print lin.coef_
print "score", lin.score(diabetes_x_test, diabetes_y_test), 
\end{minted}

\begin{verbatim}
[  3.03499452e-01  -2.37639315e+02   5.10530605e+02   3.27736981e+02
  -8.14131711e+02   4.92814589e+02   1.02848453e+02   1.84606489e+02
   7.43519617e+02   7.60951724e+01]
score 0.585075302278
\end{verbatim}

Sonuçlar birbirine oldukça yakýn. Þimdi diðer tekniklere gelelim.

Sýrt Regresyonu (Ridge Regression)

Klasik regresyon ile

$$ \hat{w} = \arg \min_w ||y-Xw||^2  $$

problemini çözdüðümüzü biliyoruz, ki $||\cdot||^2$ Öklit normunun karesini
temsil ediyor. Fakat bazý durumlarda $X^TX$'in eþsiz (singular) olmasý mümkün ki
böyle bir durumda $(X^TX)^{-1}$'in tersini almamýz mümkün olmazdý. Eþsizlik ne
zaman ortaya çýkar? Eðer elimizde veri noktasýndan daha fazla boyut var ise
mesela... Diyelim ki veri olarak 10 tane kolon var, ama sadece 9 tane veri
satýrý. Sýrt Regregyonunun çýkýþ noktasý budur.

Fakat ek olarak bu teknik kestirme hesaplarýmýza (estimation) bir yanlýlýk
(bias) eklemek için de kullanýlabilir ve bu meyil kestirme hesaplarýnýn
iyileþmesine faydalý olabilir.

Meyili nasýl ekleriz? Diyelim ki bizim tanýmlayacaðýmýz bir $\lambda$ ile
tüm $ws$'lerin toplamýna bir üst sýnýr tanýmlayabiliriz. Böylelikle
regresyonun bulacaðý katsayýlarýn çok fazla büyümesine bir "ceza" getirmiþ
olacaðýz, ve bu cezayý içeren regresyon hesabý o cezadan kaçýnmak için
mecburen bulacaðý katsayýlarý ufak tutacak, hatta bazýlarýný sýfýra indirebilecek.
Bu azaltmaya istatistikte küçülme (shrinkage) ismi veriliyor. 

Sýrt regresyonu için bu küçültme þöyle

$$ \hat{w}_{sirt} = \arg \min_w ( ||y-Xw||^2 + \lambda||w||^2 )  $$

Görüldüðü üzere $w$'nin büyüklüðünü, bir $\lambda$ katsayýsý üzerinden
minimizasyon problemine dahil ettik, böylece diðer parametreler ile büyüklük te
minimize edilecek. Üstteki taným sýnýrý tanýmlanmamýþ (unconstrained) bir
optimizasyon problemidir. Sýnýrlý olarak

$$ \min_w ||y-Xw||^2  $$
$$ ||w||^2 \le \tau \textrm{ koþuluna göre (subject to) }$$

ki $\lambda$ Lagrange çarpanýdýr. Aslýnda þimdiye kadar üstteki
çevrimin tersini gördük çoðunlukla (yani sýnýrlý problemden sýnýrsýza
gitmeyi), bu gidiþ tarzýný görmek te iyi oldu.

Neyse baþtaki sýnýrsýz problemi çözmek için ifadenin gradyanýný alalým,

$$ \nabla \big( ||y-Xw||^2 + \lambda||w||^2 \big) $$

$$ \nabla \big( (y-Xw)^T (y-Xw) + \lambda w^Tw \big) $$

$$ \nabla \big(  (y^T-w^TX^T)(y-Xw) + \lambda w^Tw  \big) $$

$$ \nabla ( y^Ty - y^TXw - w^TX^Ty + w^TX^TXw + \lambda w^Tw   )  $$

$$  - y^TX - X^Ty + 2X^TXw + 2\lambda w   $$

$$  - 2X^Ty + 2X^TXw + 2\lambda w   $$

$$   2X^TXw + 2\lambda w - 2X^Ty   $$

$$   2(X^TX + \lambda I ) w - 2X^Ty   $$

Minimizasyon için üstteki ifadeyi sýfýra eþitleyebiliriz

$$   2(X^TX + \lambda I ) w - 2X^Ty  = 0 $$

O zaman

$$   (X^TX + \lambda I ) w = X^Ty  $$

$$   \hat{w} = (X^TX + \lambda I)^{-1} X^Ty  $$

Bu son ifade en az kareler (least squares) yani normal regresyon çözüm
formülüne çok benziyor, sadece ek olarak bir $\lambda I$ toplama
iþlemi var.  Demek ki sýrt regresyonunu kullanmak için zaten
yaptýðýmýz hesaba, zaten bizim kendimizin karar verdiði bir $\lambda$
üzerinden $\lambda I$ eklersek, geri kalan tüm iþlemler ayný olacak. 

Kontrol edelim

\begin{minted}[fontsize=\footnotesize]{python}
lam = 0.2
wridge = np.dot(la.inv(xTx+lam*np.eye(xTx.shape[0])),\
                np.dot(x_tmp.T,diabetes_y_train))
print wridge
\end{minted}

\begin{verbatim}
[  16.70807829 -179.42288145  447.64999897  285.41866481  -51.7991733
  -75.09876191 -192.46341288  123.61066573  387.91385823  105.53294479
  152.7637018 ]
\end{verbatim}

Þimdi \verb!scikit-learn! ile ayný hesabý yapalým

\begin{minted}[fontsize=\footnotesize]{python}
ridge = linear_model.Ridge(alpha=0.2)
ridge.fit(diabetes_x_train, diabetes_y_train) 
print ridge.score(diabetes_x_test, diabetes_y_test), ridge.coef_
\end{minted}

\begin{verbatim}
0.553680030106 [  16.69330211 -179.414259    447.63706059  285.40960442  -51.79094255
  -75.08327488 -192.45037659  123.60400024  387.91106403  105.55514774]
\end{verbatim}

Bir yöntem daha var, bu yönteme Lasso ismi veriliyor. Lasso'ya göre cezalandýrma

$$ \sum_{k=1}^{n} w_k^2 \le \lambda $$

üzerinden olur. Bu yöntemin tüm detaylarýna þimdilik
inmeyeceðiz.

Örnek olarak bir $\lambda$ ile onun bulduðu katsayýlara bakalým.

\begin{minted}[fontsize=\footnotesize]{python}
lasso = linear_model.Lasso(alpha=0.3)
lasso.fit(diabetes_x_train, diabetes_y_train)
print lasso.coef_
\end{minted}

\begin{verbatim}
[   0.           -0.          497.3407568   199.17441037   -0.           -0.
 -118.89291549    0.          430.93795945    0.        ]
\end{verbatim}

Lasso bazý katsayýlarý sýfýra indirdi! Bu katsayýlarýn aðýrlýk verdiði
deðiþkenleri, eðer Lasso'ya inanýrsak, modelden tamamen atmak mümkündür.

Bu arada Sýrt ve Lasso yöntemlerinin metotlarýna "regülarize etmek
(regularization)" ismi de veriliyor. 

k-Katlamalý Çapraz Saðlama (k-fold Cross-Validation)

Bir yapay öðrenim algoritmasýný kullanmadan önce veriyi iki parçaya ayýrmak
ise yarar; bu parçalar tipik olarak eðitim verisi (training set) diðeri ise
test verisi (validation set) olarak isimlendirilir. Ýsimlerden belli
olacaðý üzere, algoritma eðitim seti üzerinde eðitilir; ve baþarýsý test
verisi üzerinden rapor edilir. Bir bakýma modelin oluþturulmasý bir set
üzerindendir, sonra "al þimdi hiç görmediðin bir veri seti, bakalým ne
yapacaksýn" sorusunun cevabý, saðlamasý bu þekilde yapýlýr.

Not: AIC istatistiði, standart þartlar altýnda, çapraz saglama ile
eþdeðerdedir, ki bu durumda iki farklý veri öbeðine gerek yok, eðitim
verisi yeterli.

k-Katlamalý Çapraz Saðlama bu iki parçalý eðitim / test kavramýný bir adým
öteye taþýr. Ufak bir k seçeriz, ki bu genellikle 5 ila 10 arasýnda bir
sayý olur, ve tüm verimizi rasgele bir þekilde ama k tane ve eþit
büyüklükte olacak þekilde parçalara ayýrýrýz. Bu parçalara "katlar (folds)"
ismi verilir bazen (ki isim buradan geliyor). Sonra teker teker her parçayý
test verisi yaparýz ve geri kalan tüm parçalarý eðitim verisi olarak
kullanýrýz. Bu iþlemi tüm parçalar için tekrarlarýz.

Bu yaklaþým niye faydalýdýr? Çünkü veriyi rasgele þekillerde bölüp, pek çok
yönden eðitim / test için kullanýnca verinin herhangi bir þekilde bizi
yönlendirmesi / aldatmasý daha az mümkün hale gelir.

Ve iþte bu özelliði, ek olarak, çapraz saðlamayý "model seçmek" için
vazgeçilmez bir araç haline getirir.

Model seçmek nedir? Model seçimi üstteki baðlamda optimal bir $\lambda$
bulmaktýr mesela, yani her modeli temsil eden bir $\lambda$ var ise, en iyi
$\lambda$'yi bulmak, en iyi modeli bulmak anlamýna geliyor, çapraz saðlama
bunu saðlýyor. Çapraz saðlama için \verb!scikit-learn!'un saðladýðý
fonksiyonlar vardýr, önce katlarý tanýmlarýz, sonra bu deðiþtirilmiþ
regresyon fonksiyonlarýna katlama usulünü geçeriz.

\begin{minted}[fontsize=\footnotesize]{python}
k_fold = cross_validation.KFold(n=420, n_folds=7)
\end{minted}

Katlarý üstteki gibi tanýmladýk. 420 tane veri noktasýný 7 kata bol dedik.
Þimdi bu katlarý kullanalým,

\begin{minted}[fontsize=\footnotesize]{python}
ridge_cv = linear_model.RidgeCV(cv=k_fold)
ridge_cv.fit(np.array(diabetes_x), np.array(diabetes_y))
print ridge_cv.alpha_
\end{minted}

\begin{verbatim}
0.1
\end{verbatim} 

Üstteki sonuç $\lambda = 0.1$'i gösteriyor. Bu $\lambda$ daha optimalmýþ
demek ki. Lasso için benzer þekilde

\begin{minted}[fontsize=\footnotesize]{python}
lasso_cv = linear_model.LassoCV(cv=k_fold)
print lasso_cv.fit(diabetes_x, diabetes_y)
\end{minted}

\begin{verbatim}
LassoCV(alphas=None, copy_X=True,
    cv=sklearn.cross_validation.KFold(n=420, n_folds=7), eps=0.001,
    fit_intercept=True, max_iter=1000, n_alphas=100, normalize=False,
    precompute=auto, tol=0.0001, verbose=False)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print lasso_cv.alpha_
\end{minted}

\begin{verbatim}
0.00283958719118
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print lasso_cv.score(diabetes_x_test, diabetes_y_test) 
\end{minted}

\begin{verbatim}
0.597090337358
\end{verbatim}

Þimdi veri setinin bir kýsmý üzerinde teker teker hangi algoritmanýn
daha baþarýlý olduðunu görelim. 

\begin{minted}[fontsize=\footnotesize]{python}
def predict(row):
    j = row; i = row-1
    new_data = diabetes_x[i:j]
    print diabetes_y[i:j], "lasso",lasso_cv.predict(new_data), \
    	    "ridge",ridge_cv.predict(new_data), \
      	    "linear",lin.predict(new_data)	    

predict(-2) # sondan ikinci veri satiri
predict(-3)
predict(-4)
predict(-5)
predict(-8)
\end{minted}

\begin{verbatim}
439    132
Name: response, dtype: int64 lasso [ 122.2361344] ridge [ 127.1821212] linear [ 123.56604986]
438    104
Name: response, dtype: int64 lasso [ 101.85154189] ridge [ 108.89678818] linear [ 102.5713971]
437    178
Name: response, dtype: int64 lasso [ 192.95670241] ridge [ 189.58095011] linear [ 194.03798086]
436    48
Name: response, dtype: int64 lasso [ 52.8903924] ridge [ 57.66611598] linear [ 52.5445869]
433    72
Name: response, dtype: int64 lasso [ 60.42852107] ridge [ 66.3661042] linear [ 61.19831285]
\end{verbatim}

Üstteki sonuçlara göre gerçek deðeri 132 olan 439. satýrda lasso 122.2,
sýrt (ridge) 127.1, basit regresyon ise 123.5 bulmuþ. O veri noktasý için
sýrt yöntemi daha baþarýlý çýktý.

Sonuçlara bakýnca bazen sýrt, bazen normal regresyon baþarýlý çýkýyor.
Hangi yöntem kazanmýþ o zaman? Bir o, bir bu öndeyse, hangi yöntemi
kullanacaðýmýzý nasýl bileceðiz?

Aslýnda her seferinde tek bir metotu kullanmak gerekmiyor. Bu metotlarý bir
takým (ensemble) halinde iþletebiliriz. Her test noktasýný, her seferinde
tüm metotlara sorarýz, gelen sonuçlarýn mesela.. ortalamasýný alýrýz. Bu
þekilde tek baþýna iþleyen tüm metotlardan tutarlý olarak her seferinde
daha iyi sonuca ulaþacak bir sonuç elde edebiliriz. Zaten Kaggle gibi
yarýþmalarda çoðunlukla birinciliði kazanan metotlar bu türden takým
yöntemlerini kullanan metotlar, mesela Netflix yarýþmasýný kNN ve SVD
metotlarýný takým halinde iþleten bir grup kazandý.

Kaynaklar

[1] Figueiredo, {\em Lecture Notes on Linear Regression}, \url{www.lx.it.pt/~mtf/Figueiredo_Linear_Regression.pdf}

[3] Harrington, P., {\em Machine Learning in Action}

[4] Shalizi, {\em Data Analysis from an Elementary Point of View}

\end{document}
