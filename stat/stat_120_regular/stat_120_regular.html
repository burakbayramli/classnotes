<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Regresyon, Ridge, Lasso, Çapraz Sağlama, Regülarize Etmek</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1
id="regresyon-ridge-lasso-çapraz-sağlama-regülarize-etmek">Regresyon,
Ridge, Lasso, Çapraz Sağlama, Regülarize Etmek</h1>
<p>Konumuz regresyon çeşitleri, ve örnek veri olarak diyabet hastalığı
olan kişilerden alınmış bazı temel verilerle hastalığın bir sene sonraki
ilerleme miktarı kullanılacak. Regresyon sayesinde temel veriler ile
hastalığın ilerlemesi arasında bir bağlantı bulunabilir, bu sayede hem
veri açıklanır / daha iyi anlaşılır (hangi değişken önemlidir, hangisi
değildir), hem de başka bir hastanın temel verilerini kullanarak o
hastanın diyabetinin bir sene sonra ne olacağını tahmin etmek mümkün
olur. Kullanılan temel veriler kişinin yaşı, cinsiyeti, vücut kütle
endeksi (body mass index) ortalama tansiyonu ve altı kere alınmış kan
serum ölçümleridir.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>diabetes <span class="op">=</span> read_csv(<span class="st">&quot;diabetes.csv&quot;</span>,sep<span class="op">=</span><span class="st">&#39;;&#39;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>diabetes_y <span class="op">=</span> diabetes[<span class="st">&#39;response&#39;</span>]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>diabetes_x <span class="op">=</span> diabetes.drop(<span class="st">&quot;response&quot;</span>,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>diabetes_x_train <span class="op">=</span> diabetes_x[:<span class="op">-</span><span class="dv">20</span>]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>diabetes_x_test  <span class="op">=</span> diabetes_x[<span class="op">-</span><span class="dv">20</span>:]</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>diabetes_y_train <span class="op">=</span> diabetes_y[:<span class="op">-</span><span class="dv">20</span>]</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>diabetes_y_test  <span class="op">=</span> diabetes_y[<span class="op">-</span><span class="dv">20</span>:]</span></code></pre></div>
<p>İlk önce basit regresyonu hatırlayalım. Bu tekniği daha önce pek çok
yönden gördük. <em>Lineer Cebir</em>, <em>Çok Değişkenli Calculus</em>
ders notlarında bu tekniğin türetilmesi mevcut. Formül</p>
<p><span class="math display">\[ \hat{w} = (X^TX) ^{-1} X^{T}y
\]</span></p>
<p>Sayısal olarak hemen bu hesabı yapabiliriz. Bir hatırlatma: veri
setine <span class="math inline">\(y\)</span> ekseninin nerede
kesildiğinin bulunabilmesi için suni bir ekstra kesi, “intercept’’ adlı
kolon ekleyeceğiz, bu kolon iki boyutta <span
class="math inline">\(y=ax+c\)</span> formülündeki <span
class="math inline">\(c\)</span>’nin bulunabilmesi içindir. Pandas ile
bu ekstra kolonu eklemek çok basit, ismen mevcut olmayan kolon
erişildiği anda o kolon hemen yoktan yaratılır.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.linalg <span class="im">as</span> la</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>x_tmp <span class="op">=</span> diabetes_x_train.copy()</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>x_tmp[<span class="st">&#39;intercept&#39;</span>] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>xTx <span class="op">=</span> np.dot(x_tmp.T,x_tmp )</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>ws <span class="op">=</span> np.dot(la.inv(xTx),np.dot(x_tmp.T,diabetes_y_train))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (ws)</span></code></pre></div>
<pre class="text"><code>[ 3.03499452e-01 -2.37639315e+02  5.10530605e+02  3.27736981e+02
 -8.14131711e+02  4.92814589e+02  1.02848453e+02  1.84606489e+02
  7.43519617e+02  7.60951724e+01  1.52764307e+02]</code></pre>
<p>Aynı hesabı bir de <code>scikit-learn</code> paketini kullanarak
yapalım. Bu paketin <code>LinearRegression</code> çağrısı kesi ekleme
işini otomatik olarak hallediyor, eğer kesi olmasın isteseydik,
<code>fit_intercept=False</code> diyecektik.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> linear_model, model_selection</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>lin <span class="op">=</span> linear_model.LinearRegression()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>lin.fit(diabetes_x_train, diabetes_y_train)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (lin.coef_)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;score&quot;</span>, lin.score(diabetes_x_test, diabetes_y_test), )</span></code></pre></div>
<pre class="text"><code>[ 3.03499452e-01 -2.37639315e+02  5.10530605e+02  3.27736981e+02
 -8.14131711e+02  4.92814589e+02  1.02848453e+02  1.84606489e+02
  7.43519617e+02  7.60951724e+01]
score 0.5850753022781172</code></pre>
<p>Sonuçlar birbirine oldukça yakın. Şimdi diğer tekniklere gelelim.</p>
<p>Sırt Regresyonu (Ridge Regression)</p>
<p>Klasik regresyon ile</p>
<p><span class="math display">\[ \hat{w} = \arg \min_w
||y-Xw||^2  \]</span></p>
<p>problemini çözdüğümüzü biliyoruz, ki <span
class="math inline">\(||\cdot||^2\)</span> Öklit normunun karesini
temsil ediyor. Fakat bazı durumlarda <span
class="math inline">\(X^TX\)</span>’in eşsiz (singular) olması mümkün ki
böyle bir durumda <span class="math inline">\((X^TX)^{-1}\)</span>’in
tersini almamız mümkün olmazdı. Eşsizlik ne zaman ortaya çıkar? Eğer
elimizde veri noktasından daha fazla boyut var ise mesela… Diyelim ki
veri olarak 10 tane kolon var, ama sadece 9 tane veri satırı. Sırt
Regregyonunun çıkış noktası budur.</p>
<p>Fakat ek olarak bu teknik kestirme hesaplarımıza (estimation) bir
yanlılık (bias) eklemek için de kullanılabilir ve bu meyil kestirme
hesaplarının iyileşmesine faydalı olabilir.</p>
<p>Meyili nasıl ekleriz? Diyelim ki bizim tanımlayacağımız bir <span
class="math inline">\(\lambda\)</span> ile tüm <span
class="math inline">\(ws\)</span>’lerin toplamına bir üst sınır
tanımlayabiliriz. Böylelikle regresyonun bulacağı katsayıların çok fazla
büyümesine bir “ceza” getirmiş olacağız, ve bu cezayı içeren regresyon
hesabı o cezadan kaçınmak için mecburen bulacağı katsayıları ufak
tutacak, hatta bazılarını sıfıra indirebilecek. Bu azaltmaya
istatistikte küçülme (shrinkage) ismi veriliyor.</p>
<p>Sırt regresyonu için bu küçültme şöyle</p>
<p><span class="math display">\[ \hat{w}_{sirt} = \arg \min_w (
||y-Xw||^2 + \lambda||w||^2 )  \]</span></p>
<p>Görüldüğü üzere <span class="math inline">\(w\)</span>’nin
büyüklüğünü, bir <span class="math inline">\(\lambda\)</span> katsayısı
üzerinden minimizasyon problemine dahil ettik, böylece diğer
parametreler ile büyüklük te minimize edilecek. Üstteki tanım sınırı
tanımlanmamış (unconstrained) bir optimizasyon problemidir. Sınırlı
olarak</p>
<p><span class="math display">\[ \min_w ||y-Xw||^2  \]</span> <span
class="math display">\[ ||w||^2 \le \tau \textrm{ koşuluna göre (subject
to) }\]</span></p>
<p>ki <span class="math inline">\(\lambda\)</span> Lagrange çarpanıdır.
Aslında şimdiye kadar üstteki çevrimin tersini gördük çoğunlukla (yani
sınırlı problemden sınırsıza gitmeyi), bu gidiş tarzını görmek te iyi
oldu.</p>
<p>Neyse baştaki sınırsız problemi çözmek için ifadenin gradyanını
alalım,</p>
<p><span class="math display">\[ \nabla \big( ||y-Xw||^2 +
\lambda||w||^2 \big) \]</span></p>
<p><span class="math display">\[ \nabla \big( (y-Xw)^T (y-Xw) + \lambda
w^Tw \big) \]</span></p>
<p><span class="math display">\[ \nabla \big(  (y^T-w^TX^T)(y-Xw) +
\lambda w^Tw  \big) \]</span></p>
<p><span class="math display">\[ \nabla ( y^Ty - y^TXw - w^TX^Ty +
w^TX^TXw + \lambda w^Tw   )  \]</span></p>
<p><span class="math display">\[  - y^TX - X^Ty + 2X^TXw + 2\lambda
w   \]</span></p>
<p><span class="math display">\[  - 2X^Ty + 2X^TXw + 2\lambda
w   \]</span></p>
<p><span class="math display">\[   2X^TXw + 2\lambda w -
2X^Ty   \]</span></p>
<p><span class="math display">\[   2(X^TX + \lambda I ) w -
2X^Ty   \]</span></p>
<p>Minimizasyon için üstteki ifadeyi sıfıra eşitleyebiliriz</p>
<p><span class="math display">\[   2(X^TX + \lambda I ) w - 2X^Ty  = 0
\]</span></p>
<p>O zaman</p>
<p><span class="math display">\[   (X^TX + \lambda I ) w =
X^Ty  \]</span></p>
<p><span class="math display">\[   \hat{w} = (X^TX + \lambda I)^{-1}
X^Ty  \]</span></p>
<p>Bu son ifade en az kareler (least squares) yani normal regresyon
çözüm formülüne çok benziyor, sadece ek olarak bir <span
class="math inline">\(\lambda I\)</span> toplama işlemi var. Demek ki
sırt regresyonunu kullanmak için zaten yaptığımız hesaba, zaten bizim
kendimizin karar verdiği bir <span
class="math inline">\(\lambda\)</span> üzerinden <span
class="math inline">\(\lambda I\)</span> eklersek, geri kalan tüm
işlemler aynı olacak.</p>
<p>Kontrol edelim</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>lam <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>wridge <span class="op">=</span> np.dot(la.inv(xTx<span class="op">+</span>lam<span class="op">*</span>np.eye(xTx.shape[<span class="dv">0</span>])),<span class="op">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>                np.dot(x_tmp.T,diabetes_y_train))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (wridge)</span></code></pre></div>
<pre class="text"><code>[  16.70807829 -179.42288145  447.64999897  285.41866481  -51.7991733
  -75.09876191 -192.46341288  123.61066573  387.91385823  105.53294479
  152.7637018 ]</code></pre>
<p>Şimdi <code>scikit-learn</code> ile aynı hesabı yapalım</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> linear_model.Ridge(alpha<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>ridge.fit(diabetes_x_train, diabetes_y_train) </span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (ridge.score(diabetes_x_test, diabetes_y_test), ridge.coef_)</span></code></pre></div>
<pre class="text"><code>0.5536800301058324 [  16.69330211 -179.414259    447.63706059  285.40960442  -51.79094255
  -75.08327488 -192.45037659  123.60400024  387.91106403  105.55514774]</code></pre>
<p>Bir yöntem daha var, bu yönteme Lasso ismi veriliyor. Lasso’ya göre
cezalandırma</p>
<p><span class="math display">\[ \sum_{k=1}^{n} w_k^2 \le \lambda
\]</span></p>
<p>üzerinden olur. Bu yöntemin tüm detaylarına şimdilik inmeyeceğiz.</p>
<p>Örnek olarak bir <span class="math inline">\(\lambda\)</span> ile
onun bulduğu katsayılara bakalım.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>lasso <span class="op">=</span> linear_model.Lasso(alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>lasso.fit(diabetes_x_train, diabetes_y_train)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (lasso.coef_)</span></code></pre></div>
<pre class="text"><code>[   0.           -0.          497.3407568   199.17441037   -0.
   -0.         -118.89291549    0.          430.93795945    0.        ]</code></pre>
<p>Lasso bazı katsayıları sıfıra indirdi! Bu katsayıların ağırlık
verdiği değişkenleri, eğer Lasso’ya inanırsak, modelden tamamen atmak
mümkündür.</p>
<p>Bu arada Sırt ve Lasso yöntemlerinin metotlarına “regülarize etmek
(regularization)” ismi de veriliyor.</p>
<p>k-Katlamalı Çapraz Sağlama (k-fold Cross-Validation)</p>
<p>Bir yapay öğrenim algoritmasını kullanmadan önce veriyi iki parçaya
ayırmak ise yarar; bu parçalar tipik olarak eğitim verisi (training set)
diğeri ise test verisi (validation set) olarak isimlendirilir.
İsimlerden belli olacağı üzere, algoritma eğitim seti üzerinde eğitilir;
ve başarısı test verisi üzerinden rapor edilir. Bir bakıma modelin
oluşturulması bir set üzerindendir, sonra “al şimdi hiç görmediğin bir
veri seti, bakalım ne yapacaksın” sorusunun cevabı, sağlaması bu şekilde
yapılır.</p>
<p>Not: AIC istatistiği, standart şartlar altında, çapraz saglama ile
eşdeğerdedir, ki bu durumda iki farklı veri öbeğine gerek yok, eğitim
verisi yeterli.</p>
<p>k-Katlamalı Çapraz Sağlama bu iki parçalı eğitim / test kavramını bir
adım öteye taşır. Ufak bir k seçeriz, ki bu genellikle 5 ila 10 arasında
bir sayı olur, ve tüm verimizi rasgele bir şekilde ama k tane ve eşit
büyüklükte olacak şekilde parçalara ayırırız. Bu parçalara “katlar
(folds)” ismi verilir bazen (ki isim buradan geliyor). Sonra teker teker
her parçayı test verisi yaparız ve geri kalan tüm parçaları eğitim
verisi olarak kullanırız. Bu işlemi tüm parçalar için tekrarlarız.</p>
<p>Bu yaklaşım niye faydalıdır? Çünkü veriyi rasgele şekillerde bölüp,
pek çok yönden eğitim / test için kullanınca verinin herhangi bir
şekilde bizi yönlendirmesi / aldatması daha az mümkün hale gelir.</p>
<p>Ve işte bu özelliği, ek olarak, çapraz sağlamayı “model seçmek” için
vazgeçilmez bir araç haline getirir.</p>
<p>Model seçmek nedir? Model seçimi üstteki bağlamda optimal bir <span
class="math inline">\(\lambda\)</span> bulmaktır mesela, yani her modeli
temsil eden bir <span class="math inline">\(\lambda\)</span> var ise, en
iyi <span class="math inline">\(\lambda\)</span>’yi bulmak, en iyi
modeli bulmak anlamına geliyor, çapraz sağlama bunu sağlıyor. Çapraz
sağlama için <code>scikit-learn</code>’un sağladığı fonksiyonlar vardır,
önce katları tanımlarız, sonra bu değiştirilmiş regresyon
fonksiyonlarına katlama usulünü geçeriz.</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>k_fold <span class="op">=</span> model_selection.KFold(n_splits<span class="op">=</span><span class="dv">7</span>)</span></code></pre></div>
<p>Katları üstteki gibi tanımladık. 420 tane veri noktasını 7 kata bol
dedik. Şimdi bu katları kullanalım,</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>ridge_cv <span class="op">=</span> linear_model.RidgeCV(cv<span class="op">=</span>k_fold)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>ridge_cv.fit(np.array(diabetes_x), np.array(diabetes_y))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (ridge_cv.alpha_)</span></code></pre></div>
<pre class="text"><code>0.1</code></pre>
<p>Üstteki sonuç <span class="math inline">\(\lambda = 0.1\)</span>’i
gösteriyor. Bu <span class="math inline">\(\lambda\)</span> daha
optimalmış demek ki. Lasso için benzer şekilde</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>lasso_cv <span class="op">=</span> linear_model.LassoCV(cv<span class="op">=</span>k_fold)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (lasso_cv.fit(diabetes_x, diabetes_y))</span></code></pre></div>
<pre class="text"><code>LassoCV(cv=KFold(n_splits=7, random_state=None, shuffle=False))</code></pre>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (lasso_cv.alpha_)</span></code></pre></div>
<pre class="text"><code>0.05705392298174187</code></pre>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (lasso_cv.score(diabetes_x_test, diabetes_y_test) )</span></code></pre></div>
<pre class="text"><code>0.596945426794025</code></pre>
<p>Şimdi veri setinin bir kısmı üzerinde teker teker hangi algoritmanın
daha başarılı olduğunu görelim.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(row):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    j <span class="op">=</span> row<span class="op">;</span> i <span class="op">=</span> row<span class="op">-</span><span class="dv">1</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    new_data <span class="op">=</span> diabetes_x[i:j]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (diabetes_y[i:j], <span class="st">&quot;lasso&quot;</span>,lasso_cv.predict(new_data), <span class="op">\</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;ridge&quot;</span>,ridge_cv.predict(new_data), <span class="op">\</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;linear&quot;</span>,lin.predict(new_data))</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>predict(<span class="op">-</span><span class="dv">2</span>) <span class="co"># sondan ikinci veri satiri</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>predict(<span class="op">-</span><span class="dv">3</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>predict(<span class="op">-</span><span class="dv">4</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>predict(<span class="op">-</span><span class="dv">5</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>predict(<span class="op">-</span><span class="dv">8</span>)</span></code></pre></div>
<pre class="text"><code>439    132
Name: response, dtype: int64 lasso [125.28690776] ridge [127.1821212] linear [123.56604986]
438    104
Name: response, dtype: int64 lasso [109.2916004] ridge [108.89678818] linear [102.5713971]
437    178
Name: response, dtype: int64 lasso [193.36557007] ridge [189.58095011] linear [194.03798086]
436    48
Name: response, dtype: int64 lasso [55.56982967] ridge [57.66611598] linear [52.5445869]
433    72
Name: response, dtype: int64 lasso [62.60106436] ridge [66.3661042] linear [61.19831285]</code></pre>
<p>Üstteki sonuçlara göre gerçek değeri 132 olan 439. satırda lasso
122.2, sırt (ridge) 127.1, basit regresyon ise 123.5 bulmuş. O veri
noktası için sırt yöntemi daha başarılı çıktı.</p>
<p>Sonuçlara bakınca bazen sırt, bazen normal regresyon başarılı
çıkıyor. Hangi yöntem kazanmış o zaman? Bir o, bir bu öndeyse, hangi
yöntemi kullanacağımızı nasıl bileceğiz?</p>
<p>Aslında her seferinde tek bir metotu kullanmak gerekmiyor. Bu
metotları bir takım (ensemble) halinde işletebiliriz. Her test
noktasını, her seferinde tüm metotlara sorarız, gelen sonuçların
mesela.. ortalamasını alırız. Bu şekilde tek başına işleyen tüm
metotlardan tutarlı olarak her seferinde daha iyi sonuca ulaşacak bir
sonuç elde edebiliriz. Zaten Kaggle gibi yarışmalarda çoğunlukla
birinciliği kazanan metotlar bu türden takım yöntemlerini kullanan
metotlar, mesela Netflix yarışmasını kNN ve SVD metotlarını takım
halinde işleten bir grup kazandı.</p>
<p>Kaynaklar</p>
<p>[1] Figueiredo, <em>Lecture Notes on Linear Regression</em>, <a
href="www.lx.it.pt/~mtf/Figueiredo_Linear_Regression.pdf">www.lx.it.pt/~mtf/Figueiredo_Linear_Regression.pdf</a></p>
<p>[3] Harrington, P., <em>Machine Learning in Action</em></p>
<p>[4] Shalizi, <em>Data Analysis from an Elementary Point of
View</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
