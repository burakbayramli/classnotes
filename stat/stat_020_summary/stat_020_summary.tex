\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Büyük Sayýlar Kanunu (Law of Large Numbers)

Bu kanun, örneklem (sample) ile rasgele deðiþkenler, yani matematiksel
olasýlýk daðýlýmlarý arasýnda bir baðlantý görevi görür. Kanun kabaca
bildiðimiz günlük bir gerçeðin matematiksel ispatýdýr. Yazý-tura atarken
yazý çýkma ihtimalinin 1/2 olduðunu biliyoruz; herhalde çoðumuz bu
yazý-tura iþlemin "bir çok kere" tekrarlandýðý durumda, toplam sonucun
aþaðý yukarý yarýsýnýn yazý olacaðýný bilir.

Matematiksel olarak, farzedelim ki her yazý-tura atýþý bir deney
olsun. Deneylerin sonucu $X_1, X_2...X_n$ olarak rasgelen deðiþkenlerle
olsun, bu deðiþkenlerin daðýlýmý ayný (çünkü ayný zar), ve birbirlerinden
baðýmsýzlar (çünkü her deney diðerinden alakasýz). Deðiþkenlerin sonucu 1
ya da 0 deðeri taþýyacak, Yazý=1, Tura=0.

Büyük Sayýlar Kanunu tüm bu deney sonuçlarýnýn, yani rasgele deðiþkenlerin
averajý alýnýrsa, yani $\bar{X} = X_1 + .. + X_n$ ile, elde edilen sonucun
$X_i$'lerin (ayný olan) beklentisine yaklaþacaðýnýn söyler, yani $n$ büyüdükçe
$\bar{X}_n$'in 1/2'ye yaklaþtýðýný ispatlar, yani $E[X_i] = 1/2$
deðerine. Notasyonel olarak $E(X_i) = \mu$ olarak da gösterilebilir.

Özetlemek gerekirse, bir olasýlýk daðýlýmýna sahip olan, görmediðimiz bir
``yerlerde'' olan bir daðýlýmdan bir örneklem alýyoruz, örneklem bir zar
atma iþlemi gibi (simülasyon ile bu deðiþkenleri de doldurabilirdik), sonra
bu deðiþkenlerin averajýný alýyoruz, ve bu averajýn o görmediðimiz
bilmediðimiz ``gerçek'' daðýlýmýn $\mu$ deðerine yaklaþtýðýný görüyoruz. 

Formülsel olarak, herhangi bir $\epsilon > 0$ için,

$$ \lim_{n \to \infty} P(|\bar{X} - \mu| \le \epsilon) = 1$$

ya da

 $$ \lim_{n \to \infty} P(|\bar{X}_n-\mu| > \epsilon) = 0 $$

ya da 

$$ P(|\bar{X}_n-\mu| > \epsilon) \rightarrow 0 $$

Burada ne söylendiðine dikkat edelim, $X_i$ daðýlýmý {\em ne olursa olsun}, yaný
ister Binom, ister Gaussian olsun, {\em örneklem} üzerinden hesaplanan sayýsal
ortalamanýn (empirical mean) formülsel olasýlýk beklentisine yaklaþtýðýný
söylüyoruz! $X_i$'ler en absürt daðýlýmlar olabilirler, bu daðýlýmlarýn
fonksiyonu son derece çetrefil, tek tepeli (unimodal) bile olmayabilir, o
formüller üzerinden beklenti için gereken entegralin belki analitik çözümü bile
mevcut olmayabilir! Ama yine de ortalama, o daðýlýmlarýn beklentisine
yaklaþacaktýr.  Ýstatistik ile olasýlýk teorisi arasýndaki çok önemli bir
baðlantý bu.

Sonuç þaþýrtýcý, fakat bir ek daha yapalým, sezgisel (intuitive) olarak
bakarsak aslýnda sonuç çok þaþýrtýcý olmayabilir. Niye? Diyelim ki genel
veri $N(\mu,\sigma^2)$ þeklinde bir Normal daðýlýmdan geliyor ve örneklem
de bu sebeple ayný daðýlýma sahip. Bu durumda örneklemdeki veri
noktalarýnýn $\mu$'ya yakýn deðerler olmasýný beklemek mantýklý olmaz mý?
Çünkü bu daðýlým ``zar atýnca'' ya da bir genel nüfustan bir ``örnek
toplayýnca'' (ki bunu bir anlamda istatistiksel bir zar atýþý olarak
görebiliriz) onu $\mu,\sigma^2$'e göre atacak. Örneklemi zar atýþý
sonuçlarý olarak gördüðümüze göre elde edilen verilerin bu þekilde olacaðý
þaþýrtýcý olmamalý. Ve bu zar atýþlarýnýn ortalamasýnýn, son derece basit
bir aritmetik bir iþlemle hesaplanýyor olsa bile, $\mu$'ye yaklaþmasý
normal olmalý.

Bu arada, bu argümana tersten bakarsak Monte Carlo entegralinin niye
iþlediðini görebiliriz, bkz [3].

Özellikle örneklem ile genel nüfus (population) arasýnda kurulan baðlantýya
dikkat edelim. Ýstatiðin önemli bir bölümünün bu baðlantý olduðu
söylenebilir. Her örneklem, bilmediðimiz ama genel nüfusu temsil eden bir
daðýlýmla ayný daðýlýma sahip olan $X_i$'dir dedik, ve bu aynýlýktan ve
baðýmsýzlýktan yola çýkarak bize genel nüfus hakkýnda bir ipucu saðlayan
bir kanun geliþtirdik (ve birazdan ispatlayacaðýz).

Ispata baþlayalým.

$X_1,X_2,..,X_n$ bagimsiz degiskenler olsun. 

$$ E(X_i) = \mu $$

$$ Var(X_i) = \sigma $$

$$ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i  $$

$\bar{X}_n$ de bir rasgele deðiþkendir, çünku $\bar{X}_n$ deðiþkeni her
$X_i$ daðýlýmýyla alakalý.

Ýspata devam etmek için $\bar{X}_n$ daðýlýmýnýn beklentisini bulmamýz gerekiyor. 

$$ E(\bar{X}_n) = E(\frac{1}{n} \sum_{i=1}^n X_i)  $$

E doðrusal bir iþleç (linear operatör) olduðu için dýþarýdan içeri doðru
nüfuz eder. 

$$ = \frac{1}{n} \sum_{i=1}^n E(X_i) = \frac{1}{n}n\mu = \mu $$

Dikkat edelim, bu {\em ortalamanýn} beklentisi, ortalamanýn kendisinin
hangi deðere yaklaþacaðýný hala göstermiyor. Eðer öyle olsaydý iþimiz
bitmiþ olurdu :) Daha yapacak çok iþ var.

Þimdi $\bar{X}_n$ daðýlýmýnýn standart sapmasýný da bulalým. Diðer bir
olasýlýk kuramýna göre

$$ Y = a + bX $$

$$ Var(Y) = b^2Var(X) $$

oldugunu biliyoruz. O zaman,

$$ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i  $$

$$ Var(\bar{X}_n) = Var(\frac{1}{n}\sum_{i=1}^nX_i) = 
\frac{1}{n^2}\sum_{i=1}^n Var(X_i)
$$

$$ 
Var(\bar{X}_n) = \frac{1}{n^2}\sum_{i=1}^n \sigma^2 = 
\frac{1}{n^2}n\sigma^2 = \frac{\sigma^2}{n} 
\mlabel{3}
$$

Artýk Çebiþev kuramýný kullanmaya hazýrýz. Ispatlamaya calistigimiz neydi?
$n \rightarrow \infty$ iken,

 $$ P(|\bar{X}_n-\mu| > \epsilon) \rightarrow 0 $$

Çebiþev'den

$$ P(|\bar{X}_n-\mu| > \epsilon) \le \frac{Var(\bar{X}_n)}{\epsilon^2} $$

$$ P(|\bar{X}_n-\mu| > \epsilon) \le \frac{\sigma^2}{n\epsilon^2}
\rightarrow 0 $$

$\sigma^2 / n\epsilon^2$'in sýfýra gitmesi normal çünkü $n$ sonsuza gidiyor.

Peki $P(|\bar{X}_n-\mu| > \epsilon)$'nin sýfýra gittiðini gösterdik mi? 

$\sigma^2 / n\epsilon^2$'nin sýfýra gittiðini gösterdik. $\sigma^2 /
n\epsilon^2$ de $P(|\bar{X}_n-\mu| > \epsilon)$'den büyük olduðuna göre, demek
ki o da sýfýra iner.

Çebiþev Eþitsizliðinin ispatý ek bölümde bulunabilir.

$\square$

Büyük Sayýlar Kanunu örneklem ortalamasýnýn ve varyansýnýn $X_i$'in
beklentisi ve varyansý ile baðlantý kurar. Merkezi Limit Teorisi bir adým
daha atar, ve der ki ``$\bar{X}$'in daðýlýmý Gaussian daðýlým olmalýdýr
yani normal eðrisi þeklinde çýkmalýdýr!''. Teorinin detaylarý bu bölümde
bulunabilir. 

Merkezi Limit Teorisi (Central Limit Theorem -CLT-)

Büyük Sayýlar Kanunu örneklem ortalamasýnýn gerçek nüfus beklentisine
yaklaþacaðýný ispatladý. Örneklem herhangi bir daðýlýmdan
gelebiliyordu. CLT bu teoriyi bir adým ilerletiyor ve diyor ki kendisi de
bir rasgele deðiþken olan örneklem ortalamasý $\bar{X}$ Normal daðýlýma
sahiptir! Daha detaylandýrmal gerekirse, 

Diyelim ki $X_1,..,X_i$ örneklemi birbirinden baðýmsýz, ayný daðýlýmlý ve
ortalamasý $\mu$, standart sapmasý $\sigma$ olan (ki o da ayný daðýlýma
sahip) bir nüfustan geliyorlar. Örneklem ortalamasý $\bar{X}$, ki bu
rasgele deðiþkenin beklentisinin $\mu$, ve (3)'e göre standart sapmasýnýn
$\sigma / \sqrt{n}$ olduðunu biliyoruz. Dikkat: $\bar{X}$'in kendisinden
deðil, {\em beklentisinden} bahsediyoruz, BSK'deki ayný durum, yani
ortalama daðýlýmýnýn ortalamasý. Teori der ki $n$ büyüdükçe $\bar{X}$
daðýlýmý (bu sefer kendisi) bir $N(\mu, \sigma/\sqrt{n})$ daðýlýmýna
yaklaþýr.

Bu ifade genelde standart normal olarak gösterilir, herhangi bir normal
daðýlýmý standart normal'e dönüþtürmeyi daha önce görmüþtük zaten,
beklentiyi çýkartýp standart sapmaya bölüyoruz, o zaman örneklem daðýlýmý
$\bar{X}$,

$$ Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} $$

daðýlýmýna yaklaþýr diyoruz, ki $Z = N(0,1)$ daðýlýmýdýr, beklentisi sýfýr,
standart sapmasý 1 deðerindedir. 

Bu teorinin ispatýný þimdilik vermeyeceðiz. 

Parametre Tahmin Ediciler (Estimators) 

Maksimum Olurluk (maximum likelihood) kavramýný kullanarak ilginç bazý
sonuçlara eriþmek mümkün; bu sayede daðýlým fonksiyonlarý ve veri arasýnda
bazý sonuçlar elde edebiliriz. Maksimum olurluk nedir? MO ile verinin her
noktasý teker teker olasýlýk fonksiyonuna geçilir, ve elde edilen olasýlýk
sonuçlarý birbiri ile çarpýlýr. Çoðunlukla formül içinde bilinmeyen
bir(kaç) parametre vardýr, ve bu çarpým sonrasý, içinde bu parametre(ler)
olan yeni bir formül ortaya çýkar. Bu nihai formülün kýsmi türevi alýnýp
sýfýra eþitlenince cebirsel bazý teknikler ile bilinmeyen parametre
bulunabilir. Bu sonuç eldeki veri baðlamýnda en mümkün (olur) parametre
deðeridir. Öyle ya, mesela Gaussian $N(10,2)$ daðýlýmý var ise, 60,90 gibi
deðerlerin ``olurluðu'' düþüktür. Gaussin üzerinde örnek,

$$
f(x;\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} 
\exp \bigg\{ - \frac{1}{2\sigma^2}(x-\mu)^2  \bigg\}
, \ x \in \mathbb{R}
$$
 
Çarpým sonrasý

$$ f(x_1,..,x_n;\mu,\sigma) = 
\prod \frac{1}{\sigma\sqrt{2\pi}} 
\exp \bigg\{ - \frac{1}{2\sigma^2}(x_i-\mu)^2  \bigg\}
$$

$$ =
\frac{(2\pi)^{-n/2}}{\sigma^n}
\exp \bigg\{ - \frac{\sum (x_i-\mu)^2}{2\sigma^2}  \bigg\}
$$

Üstel kýsým $-n/2$ nereden geldi? Çünkü bölen olan karekökü üste çýkardýk,
böylece $-1/2$ oldu, $n$ çünkü $n$ tane veri noktasý yüzünden formül $n$
kere çarpýlýyor. Veri noktalarý $x_i$ içinde. Eðer log, yani $\ln$ alýrsak
$\exp$'den kurtuluruz, ve biliyoruz ki log olurluðu maksimize etmek normal
olurluðu maksimize etmek ile ayný þeydir, çünkü $\ln$ transformasyonu
monoton bir transformasyondur. Ayrýca olurluk içbukeydir (concave) yani
kesin tek bir maksimumu vardýr. 

$$ \ln f = -\frac{1}{2} n \ln (2\pi) 
- n \ln \sigma - 
\frac{\sum (x_i-\mu)^2}{2\sigma^2}  
$$

Türevi alýp sýfýra eþitleyelim

$$ \frac{\partial (\ln f)}{\partial \mu} =
\frac{\sum (x_i-\mu)^2}{2\sigma^2}   = 0 
$$

$$ \hat{\mu} = \frac{\sum x_i }{n} $$

Bu sonuç (1)'deki formül, yani örneklem ortalamasý ile ayný! Fakat buradan
hemen bir baðlantýya zýplamadan önce þunu hatýrlayalým - örneklem
ortalamasý formülünü {\em biz} tanýmladýk. ``Taným'' diyerek bir ifade
yazdýk, ve budur dedik. Þimdi sonradan, verinin daðýlýmýnýn Gaussian
olduðunu farzederek, bu verinin mümkün kýlabileceði en optimal parametre
deðeri nedir diye hesap ederek ayný formüle eriþtik, fakat bu bir anlamda
bir güzel raslantý oldu.. Daha doðrusu bu aynýlýk Gaussian / Normal
daðýlýmlarýnýn ``normalliði'' ile alakalý muhakkak, fakat örnekleme
ortalamasý hiçbir daðýlým faraziyesi yapmýyor, herhangi bir daðýlýmdan
geldiði bilinen ya da bilinmeyen bir veri üzerinde kullanýlabiliyor. Bunu
unutmayalým. Ýstatistikte matematiðin lakaytlaþmasý (sloppy) kolaydýr, o
sebeple neyin taným, neyin hangi faraziyeye göre optimal, neyin nüfus
(population) neyin örneklem (sample) olduðunu hep hatýrlamamýz lazým.

Devam edelim, maksimum olurluk ile $\hat{\sigma}$ hesaplayalým,

$$ \frac{\partial (\ln f)}{\partial \sigma} =
-\frac{n}{\sigma} + \frac{\sum (x_i-\mu)^2}{2\sigma^3}   = 0 
$$

Cebirsel birkac duzenleme sonrasi ve $\mu$ yerine yeni hesapladigimiz
$\hat{\mu}$ kullanarak,

$$ \hat{\sigma}^2 = \frac{\sum (x_i-\hat{\mu})^2}{n} $$

Bu da örneklem varyansý ile ayný! 

Yansýzlýk (Unbiasedness)

Tahmin edicilerin kendileri de birer rasgele deðiþken olduðu için her
örneklem için deðiþik deðerler verirler. Diyelim ki $\theta$ için bir
tahmin edici $\hat{\theta}$ hesaplýyoruz, bu $\hat{\theta}$ gerçek $\theta$
için bazý örneklemler için çok küçük, bazý örneklemler için çok büyük
sonuçlar (tahminler) verebilecektir. Kabaca ideal durumun, az çýkan
tahminlerin çok çýkan tahminleri bir þekilde dengelemesi olduðunu tahmin
edebiliriz, yani tahmin edicinin üreteceði pek çok deðerin $\theta$'yý bir
þekilde ``ortalamasý'' iyi olacaktýr.

\includegraphics[height=5cm]{unbias.png}

Bu durumu þöyle açýklayalým, madem tahmin ediciler birer rasgele deðiþken,
o zaman bir daðýlým fonksiyonlarý var. Ve üstteki resimde örnek olarak
$\hat{\theta_1},\hat{\theta_2}$ olarak iki tahmin edici gösteriliyor mesela
ve onlara tekabül eden yoðunluklar $f_{\hat{\theta_1}},
f_{\hat{\theta_1}}$. Ýdeal durum soldaki resimdir, yoðunluðun fazla olduðu
yer gerçek $\theta$'ya yakýn olmasý. Bu durumu matematiksel olarak nasýl
belirtiriz? Beklenti ile!

Taným

$Y_1,..,Y_n$ üzerindeki $\theta$ tahmin edicisi $\hat{\theta} $'den alýnmýþ
rasgele örneklem. Eðer tüm $\theta$'lar için $E(\hat{\theta}) = \theta$
iþe, bu durumda tahmin edicinin yansýz olduðu söylenir.

Örnek olarak maksimum olurluk ile önceden hesapladýðýmýz $\hat{\sigma}$
tahmin edicisine bakalým. Bu ifade

$$ \hat{\sigma}^2 = \frac{1}{n}\sum (Y_i-\hat{\mu})^2 $$

ya da 

$$ \hat{\sigma}^2 = \frac{1}{n}\sum_i (Y_i-\bar{Y})^2 $$

ile belirtildi. Tahmin edici $\hat{\sigma}^2$, $\sigma^2$ için yansýz midir?
Tanýmýmýza göre eðer tahmin edici yansýz ise $E(\hat{\sigma}^2) = \sigma^2$
olmalýdýr.

Not: Faydalý olacak bazý eþitlikler, daha önceden gördüðümüz

$$ Var(X) = E(X^2) - (E(X)^2)$$

ve sayýsal ortalama $\bar{Y}$'nin beklentisi $E({\bar{Y}}) = E(Y_i)$, ve
$Var(\bar{Y}) = 1/n Var(Y_i)$.

Baþlayalým,

$$ E(\hat{\sigma}^2) = E\bigg(\frac{1}{n}\sum_i (Y_i-\bar{Y})^2 \bigg)$$

Parantez içindeki $1/n$ sonrasýndaki ifadeyi açarsak,


$$ \sum_i (Y_i-\bar{Y})^2  =  \sum_i (Y_i^2-2Y_i\bar{Y}+ \bar{Y}^2)$$

$$ = \sum_iY_i^2 -2\sum_i Y_i\bar{Y} + n\bar{Y}^2  $$

$\sum_i Y_i$'nin hemen yanýnda $\bar{Y}$ görüyoruz. Fakat $\bar{Y}$'nin kendisi
zaten $1/n \sum_i Y_i$ demek deðil midir? Ya da, toplam içinde her $i$ için
deðiþmeyecek $\bar{Y}$'yi toplam dýþýna çekersek, $\bar{Y}\sum_iY_i$ olur, bu da
$\bar{Y} \cdot n \bar{Y}$ demektir ya da $n\bar{Y}^2$,

$$ = \sum_iY_i^2 -2 n\bar{Y}^2 + n\bar{Y}^2  $$

$$ = \sum_iY_i^2 -n\bar{Y}^2  $$

Dikkat, artýk $-n\bar{Y}^2$ toplama iþleminin {\em dýþýnda}. Þimdi beklentiye
geri dönelim,

$$ = E \bigg( \frac{1}{n} \bigg( \sum_iY_i^2 -n\bar{Y}^2 \bigg) \bigg) $$

$1/n$ dýþarý çekilir, beklenti toplamdan içeri nüfuz eder,

$$ = \frac{1}{n} \bigg(  \sum_i  E(Y_i^2) -n E(\bar{Y}^2) \bigg) $$

Daha önce demiþtik ki (genel baðlamda)

$$ Var(X) = E(X^2) - (E(X)^2)$$

Bu örnek için harfleri deðiþtirirsek,

$$ Var(Y_i) = E(Y_i^2) - E(Y_i)^2$$

Yani

$$ E(Y_i^2) = Var(Y_i) + E(Y_i)^2 $$

$E(Y_i) = \mu$ oldugunu biliyoruz,

$$ E(Y_i^2) = Var(Y_i) + \mu^2 $$

Aynýsýný $ E(\bar{Y}^2)$ için kullanýrsak,

$$  E(\bar{Y}^2) = Var(\bar{Y}) + E(\bar{Y})^2 $$

$E(\bar{Y}) = \mu$, 

$$  E(\bar{Y}^2) = Var(\bar{Y}) + \mu^2 $$

$$
= \frac{1}{n} \bigg(  \sum_i Var(Y_i) + \mu^2   
-n (Var(\bar{Y}) + \mu^2 ) \bigg) 
$$

$Var(Y_i) = \sigma$, ve baþta verdiðimiz eþitlikler ile beraber

$$
= \frac{1}{n} \bigg(  \sum_i (\sigma^2 + \mu^2)
-n (\frac{\sigma^2}{n} + \mu^2 ) \bigg) 
$$

Tekrar hatýrlatalým, $\sum_i$ sadece ilk iki terim için geçerli, o zaman,
ve sabit deðerleri $n$ kadar topladýðýmýza göre bu aslýnda bir çarpým
iþlemi olur,

$$ 
= \frac{1}{n} \bigg(  n\sigma^2 + n\mu^2   
-n (\frac{\sigma^2}{n} + \mu^2 ) \bigg) 
$$

$$ 
=  \sigma^2 + \mu^2 -\frac{\sigma^2}{n} - \mu^2 
$$

$$ 
=  \sigma^2 -\frac{\sigma^2}{n} 
$$

$$ 
=  \frac{n\sigma^2}{n} -\frac{\sigma^2}{n} 
$$


$$ 
=  \frac{n\sigma^2 - \sigma^2}{n} 
$$

$$ 
=  \frac{\sigma^2(n-1)}{n} 
$$

$$ = \sigma^2 \frac{n-1}{n} $$

Görüldüðü gibi eriþtiðimiz sonuç $\sigma^2$ deðil, demek ki bu tahmin edici
yansýz deðil. Kontrol tamamlandý.

Fakat eriþtiðimiz son denklem bize baþka bir þey gösteriyor, eðer üstteki
sonucu $\frac{n}{n-1}$ ile çarpsaydýk, $\sigma^2$ elde etmez miydik? O
zaman yanlý tahmin ediciyi yansýz hale çevirmek için, onu $\frac{n}{n-1}$
ile çarparýz ve

$$ \frac{n}{n-1} \frac{1}{n}\sum_i (Y_i-\bar{Y})^2 $$

$$ =  \frac{1}{n-1}\sum_i (Y_i-\bar{Y})^2 $$

Üstteki ifade $\sigma^2$'nin yansýz tahmin edicisidir. 

Hesap için kullandýðýnýz kütüphanelerin yanlý mý yansýz mý hesap yaptýðýný
bilmek iyi olur, mesela Numpy versiyon 1.7.1 itibariyle yanlý standart
sapma hesabý yapýyor, fakat Pandas yansýz olaný kullanýyor (Pandas
versiyonu daha iyi)

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
arr = np.array([1,2,3])
print 'numpy', np.std(arr)
print 'pandas', float(pd.DataFrame(arr).std())
\end{minted}

\begin{verbatim}
numpy 0.816496580928
pandas 1.0
\end{verbatim}

Kaynaklar

[1] Wolfram Mathworld, {\em Maximum Likelihood}, \url{http://mathworld.wolfram.com/MaximumLikelihood.html}

[2] {\em Introduction to Probability and Statistics Using R}

[3] Bayramli, Istatistik, {\em Monte Carlo, Entegraller, MCMC}

\end{document}
