<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>

  <title>Büyük Sayılar, Veri</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="büyük-sayılar-veri">Büyük Sayılar, Veri</h1>
<p>Büyük Sayılar Kanunu (Law of Large Numbers)</p>
<p>Bu kanun, örneklem (sample) ile rasgele değişkenler, yani matematiksel olasılık dağılımları arasında bir bağlantı görevi görür. Kanun kabaca bildiğimiz günlük bir gerçeğin matematiksel ispatıdır. Yazı-tura atarken yazı çıkma ihtimalinin 1/2 olduğunu biliyoruz; herhalde çoğumuz bu yazı-tura işlemin &quot;bir çok kere&quot; tekrarlandığı durumda, toplam sonucun aşağı yukarı yarısının yazı olacağını bilir.</p>
<p>Matematiksel olarak, farzedelim ki her yazı-tura atışı bir deney olsun. Deneylerin sonucu <span class="math inline">\(X_1, X_2...X_n\)</span> olarak rasgelen değişkenlerle olsun, bu değişkenlerin dağılımı aynı (çünkü aynı zar), ve birbirlerinden bağımsızlar (çünkü her deney diğerinden alakasız). Değişkenlerin sonucu 1 ya da 0 değeri taşıyacak, Yazı=1, Tura=0.</p>
<p>Büyük Sayılar Kanunu tüm bu deney sonuçlarının, yani rasgele değişkenlerin averajı alınırsa, yani <span class="math inline">\(\bar{X} = X_1 + .. + X_n\)</span> ile, elde edilen sonucun <span class="math inline">\(X_i\)</span>'lerin (aynı olan) beklentisine yaklaşacağının söyler, yani <span class="math inline">\(n\)</span> büyüdükçe <span class="math inline">\(\bar{X}_n\)</span>'in 1/2'ye yaklaştığını ispatlar, yani <span class="math inline">\(E[X_i] = 1/2\)</span> değerine. Notasyonel olarak <span class="math inline">\(E(X_i) = \mu\)</span> olarak da gösterilebilir.</p>
<p>Özetlemek gerekirse, bir olasılık dağılımına sahip olan, görmediğimiz bir &quot;yerlerde'' olan bir dağılımdan bir örneklem alıyoruz, örneklem bir zar atma işlemi gibi (simülasyon ile bu değişkenleri de doldurabilirdik), sonra bu değişkenlerin averajını alıyoruz, ve bu averajın o görmediğimiz bilmediğimiz &quot;gerçek'' dağılımın <span class="math inline">\(\mu\)</span> değerine yaklaştığını görüyoruz.</p>
<p>Formülsel olarak, herhangi bir <span class="math inline">\(\epsilon &gt; 0\)</span> için,</p>
<p><span class="math display">\[ \lim_{n \to \infty} P(|\bar{X} - \mu| \le \epsilon) = 1\]</span></p>
<p>ya da</p>
<p><span class="math display">\[ \lim_{n \to \infty} P(|\bar{X}_n-\mu| &gt; \epsilon) = 0 \]</span></p>
<p>ya da</p>
<p><span class="math display">\[ P(|\bar{X}_n-\mu| &gt; \epsilon) \rightarrow 0 \]</span></p>
<p>Burada ne söylendiğine dikkat edelim, <span class="math inline">\(X_i\)</span> dağılımı <em>ne olursa olsun</em>, yanı ister Binom, ister Gaussian olsun, <em>örneklem</em> üzerinden hesaplanan sayısal ortalamanın (empirical mean) formülsel olasılık beklentisine yaklaştığını söylüyoruz! <span class="math inline">\(X_i\)</span>'ler en absürt dağılımlar olabilirler, bu dağılımların fonksiyonu son derece çetrefil, tek tepeli (unimodal) bile olmayabilir, o formüller üzerinden beklenti için gereken entegralin belki analitik çözümü bile mevcut olmayabilir! Ama yine de ortalama, o dağılımların beklentisine yaklaşacaktır. İstatistik ile olasılık teorisi arasındaki çok önemli bir bağlantı bu.</p>
<p>Sonuç şaşırtıcı, fakat bir ek daha yapalım, sezgisel (intuitive) olarak bakarsak aslında sonuç çok şaşırtıcı olmayabilir. Niye? Diyelim ki genel veri <span class="math inline">\(N(\mu,\sigma^2)\)</span> şeklinde bir Normal dağılımdan geliyor ve örneklem de bu sebeple aynı dağılıma sahip. Bu durumda örneklemdeki veri noktalarının <span class="math inline">\(\mu\)</span>'ya yakın değerler olmasını beklemek mantıklı olmaz mı? Çünkü bu dağılım &quot;zar atınca'' ya da bir genel nüfustan bir &quot;örnek toplayınca'' (ki bunu bir anlamda istatistiksel bir zar atışı olarak görebiliriz) onu <span class="math inline">\(\mu,\sigma^2\)</span>'e göre atacak. Örneklemi zar atışı sonuçları olarak gördüğümüze göre elde edilen verilerin bu şekilde olacağı şaşırtıcı olmamalı. Ve bu zar atışlarının ortalamasının, son derece basit bir aritmetik bir işlemle hesaplanıyor olsa bile, <span class="math inline">\(\mu\)</span>'ye yaklaşması normal olmalı.</p>
<p>Bu arada, bu argümana tersten bakarsak Monte Carlo entegralinin niye işlediğini görebiliriz, bkz [3].</p>
<p>Özellikle örneklem ile genel nüfus (population) arasında kurulan bağlantıya dikkat edelim. İstatiğin önemli bir bölümünün bu bağlantı olduğu söylenebilir. Her örneklem, bilmediğimiz ama genel nüfusu temsil eden bir dağılımla aynı dağılıma sahip olan <span class="math inline">\(X_i\)</span>'dir dedik, ve bu aynılıktan ve bağımsızlıktan yola çıkarak bize genel nüfus hakkında bir ipucu sağlayan bir kanun geliştirdik (ve birazdan ispatlayacağız).</p>
<p>İspata başlayalım.</p>
<p><span class="math inline">\(X_1,X_2,..,X_n\)</span> bağımsız değişkenler olsun.</p>
<p><span class="math display">\[ E(X_i) = \mu \]</span></p>
<p><span class="math display">\[ Var(X_i) = \sigma \]</span></p>
<p><span class="math display">\[ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i  \]</span></p>
<p><span class="math inline">\(\bar{X}_n\)</span> de bir rasgele değişkendir, çünku <span class="math inline">\(\bar{X}_n\)</span> değişkeni her <span class="math inline">\(X_i\)</span> dağılımıyla alakalı.</p>
<p>İspata devam etmek için <span class="math inline">\(\bar{X}_n\)</span> dağılımının beklentisini bulmamız gerekiyor.</p>
<p><span class="math display">\[ E(\bar{X}_n) = E(\frac{1}{n} \sum_{i=1}^n X_i)  \]</span></p>
<p>E doğrusal bir işleç (linear operatör) olduğu için dışarıdan içeri doğru nüfuz eder.</p>
<p><span class="math display">\[ = \frac{1}{n} \sum_{i=1}^n E(X_i) = \frac{1}{n}n\mu = \mu \]</span></p>
<p>Dikkat edelim, bu <em>ortalamanın</em> beklentisi, ortalamanın kendisinin hangi değere yaklaşacağını hala göstermiyor. Eğer öyle olsaydı işimiz bitmiş olurdu :) Daha yapacak çok iş var.</p>
<p>Şimdi <span class="math inline">\(\bar{X}_n\)</span> dağılımının standart sapmasını da bulalım. Diğer bir olasılık kuramına göre</p>
<p><span class="math display">\[ Y = a + bX \]</span></p>
<p><span class="math display">\[ Var(Y) = b^2Var(X) \]</span></p>
<p>oldugunu biliyoruz. O zaman,</p>
<p><span class="math display">\[ \bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i  \]</span></p>
<p><span class="math display">\[ Var(\bar{X}_n) = Var(\frac{1}{n}\sum_{i=1}^nX_i) = 
\frac{1}{n^2}\sum_{i=1}^n Var(X_i)
\]</span></p>
<p><span class="math display">\[ 
Var(\bar{X}_n) = \frac{1}{n^2}\sum_{i=1}^n \sigma^2 = 
\frac{1}{n^2}n\sigma^2 = \frac{\sigma^2}{n} 
\qquad (3)
\]</span></p>
<p>Artık Çebişev kuramını kullanmaya hazırız. Ispatlamaya calistigimiz neydi? <span class="math inline">\(n \rightarrow \infty\)</span> iken,</p>
<p><span class="math display">\[ P(|\bar{X}_n-\mu| &gt; \epsilon) \rightarrow 0 \]</span></p>
<p>Çebişev'den</p>
<p><span class="math display">\[ P(|\bar{X}_n-\mu| &gt; \epsilon) \le \frac{Var(\bar{X}_n)}{\epsilon^2} \]</span></p>
<p><span class="math display">\[ P(|\bar{X}_n-\mu| &gt; \epsilon) \le \frac{\sigma^2}{n\epsilon^2}
\rightarrow 0 \]</span></p>
<p><span class="math inline">\(\sigma^2 / n\epsilon^2\)</span>'in sıfıra gitmesi normal çünkü <span class="math inline">\(n\)</span> sonsuza gidiyor.</p>
<p>Peki <span class="math inline">\(P(|\bar{X}_n-\mu| &gt; \epsilon)\)</span>'nin sıfıra gittiğini gösterdik mi?</p>
<p><span class="math inline">\(\sigma^2 / n\epsilon^2\)</span>'nin sıfıra gittiğini gösterdik. <span class="math inline">\(\sigma^2 / n\epsilon^2\)</span> de <span class="math inline">\(P(|\bar{X}_n-\mu| &gt; \epsilon)\)</span>'den büyük olduğuna göre, demek ki o da sıfıra iner.</p>
<p>Çebişev Eşitsizliğinin ispatı ek bölümde bulunabilir.</p>
<p><span class="math inline">\(\square\)</span></p>
<p>Büyük Sayılar Kanunu örneklem ortalamasının ve varyansının <span class="math inline">\(X_i\)</span>'in beklentisi ve varyansı ile bağlantı kurar. Merkezi Limit Teorisi bir adım daha atar, ve der ki &quot;<span class="math inline">\(\bar{X}\)</span>'in dağılımı Gaussian dağılım olmalıdır yani normal eğrisi şeklinde çıkmalıdır!''. Teorinin detayları bu bölümde bulunabilir.</p>
<p>Merkezi Limit Teorisi (Central Limit Theorem -CLT-)</p>
<p>Büyük Sayılar Kanunu örneklem ortalamasının gerçek nüfus beklentisine yaklaşacağını ispatladı. Örneklem herhangi bir dağılımdan gelebiliyordu. CLT bu teoriyi bir adım ilerletiyor ve diyor ki kendisi de bir rasgele değişken olan örneklem ortalaması <span class="math inline">\(\bar{X}\)</span> Normal dağılıma sahiptir! Daha detaylandırmal gerekirse,</p>
<p>Diyelim ki <span class="math inline">\(X_1,..,X_i\)</span> örneklemi birbirinden bağımsız, aynı dağılımlı ve ortalaması <span class="math inline">\(\mu\)</span>, standart sapması <span class="math inline">\(\sigma\)</span> olan (ki o da aynı dağılıma sahip) bir nüfustan geliyorlar. Örneklem ortalaması <span class="math inline">\(\bar{X}\)</span>, ki bu rasgele değişkenin beklentisinin <span class="math inline">\(\mu\)</span>, ve (3)'e göre standart sapmasının <span class="math inline">\(\sigma / \sqrt{n}\)</span> olduğunu biliyoruz. Dikkat: <span class="math inline">\(\bar{X}\)</span>'in kendisinden değil, <em>beklentisinden</em> bahsediyoruz, BSK'deki aynı durum, yani ortalama dağılımının ortalaması. Teori der ki <span class="math inline">\(n\)</span> büyüdükçe <span class="math inline">\(\bar{X}\)</span> dağılımı (bu sefer kendisi) bir <span class="math inline">\(N(\mu, \sigma/\sqrt{n})\)</span> dağılımına yaklaşır.</p>
<p>Bu ifade genelde standart normal olarak gösterilir, herhangi bir normal dağılımı standart normal'e dönüştürmeyi daha önce görmüştük zaten, beklentiyi çıkartıp standart sapmaya bölüyoruz, o zaman örneklem dağılımı <span class="math inline">\(\bar{X}\)</span>,</p>
<p><span class="math display">\[ Z = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \]</span></p>
<p>dağılımına yaklaşır diyoruz, ki <span class="math inline">\(Z = N(0,1)\)</span> dağılımıdır, beklentisi sıfır, standart sapması 1 değerindedir.</p>
<p>Bu teorinin ispatını şimdilik vermeyeceğiz.</p>
<p>Parametre Tahmin Ediciler (Estimators)</p>
<p>Maksimum Olurluk (maximum likelihood) kavramını kullanarak ilginç bazı sonuçlara erişmek mümkün; bu sayede dağılım fonksiyonları ve veri arasında bazı sonuçlar elde edebiliriz. Maksimum olurluk nedir? MO ile verinin her noktası teker teker olasılık fonksiyonuna geçilir, ve elde edilen olasılık sonuçları birbiri ile çarpılır. Çoğunlukla formül içinde bilinmeyen bir(kaç) parametre vardır, ve bu çarpım sonrası, içinde bu parametre(ler) olan yeni bir formül ortaya çıkar. Bu nihai formülün kısmi türevi alınıp sıfıra eşitlenince cebirsel bazı teknikler ile bilinmeyen parametre bulunabilir. Bu sonuç eldeki veri bağlamında en mümkün (olur) parametre değeridir. Öyle ya, mesela Gaussian <span class="math inline">\(N(10,2)\)</span> dağılımı var ise, 60,90 gibi değerlerin &quot;olurluğu'' düşüktür. Gaussin üzerinde örnek,</p>
<p><span class="math display">\[
f(x;\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}} 
\exp \bigg\{ - \frac{1}{2\sigma^2}(x-\mu)^2  \bigg\}
, \ x \in \mathbb{R}
\]</span></p>
<p>Çarpım sonrası</p>
<p><span class="math display">\[ f(x_1,..,x_n;\mu,\sigma) = 
\prod \frac{1}{\sigma\sqrt{2\pi}} 
\exp \bigg\{ - \frac{1}{2\sigma^2}(x_i-\mu)^2  \bigg\}
\]</span></p>
<p><span class="math display">\[ =
\frac{(2\pi)^{-n/2}}{\sigma^n}
\exp \bigg\{ - \frac{\sum (x_i-\mu)^2}{2\sigma^2}  \bigg\}
\]</span></p>
<p>Üstel kısım <span class="math inline">\(-n/2\)</span> nereden geldi? Çünkü bölen olan karekökü üste çıkardık, böylece <span class="math inline">\(-1/2\)</span> oldu, <span class="math inline">\(n\)</span> çünkü <span class="math inline">\(n\)</span> tane veri noktası yüzünden formül <span class="math inline">\(n\)</span> kere çarpılıyor. Veri noktaları <span class="math inline">\(x_i\)</span> içinde. Eğer log, yani <span class="math inline">\(\ln\)</span> alırsak <span class="math inline">\(\exp\)</span>'den kurtuluruz, ve biliyoruz ki log olurluğu maksimize etmek normal olurluğu maksimize etmek ile aynı şeydir, çünkü <span class="math inline">\(\ln\)</span> transformasyonu monoton bir transformasyondur. Ayrıca olurluk içbukeydir (concave) yani kesin tek bir maksimumu vardır.</p>
<p><span class="math display">\[ \ln f = -\frac{1}{2} n \ln (2\pi) 
- n \ln \sigma - 
\frac{\sum (x_i-\mu)^2}{2\sigma^2}  
\]</span></p>
<p>Türevi alıp sıfıra eşitleyelim</p>
<p><span class="math display">\[ \frac{\partial (\ln f)}{\partial \mu} =
\frac{\sum (x_i-\mu)^2}{2\sigma^2}   = 0 
\]</span></p>
<p><span class="math display">\[ \hat{\mu} = \frac{\sum x_i }{n} \]</span></p>
<p>Bu sonuç (1)'deki formül, yani örneklem ortalaması ile aynı! Fakat buradan hemen bir bağlantıya zıplamadan önce şunu hatırlayalım - örneklem ortalaması formülünü <em>biz</em> tanımladık. &quot;Tanım'' diyerek bir ifade yazdık, ve budur dedik. Şimdi sonradan, verinin dağılımının Gaussian olduğunu farzederek, bu verinin mümkün kılabileceği en optimal parametre değeri nedir diye hesap ederek aynı formüle eriştik, fakat bu bir anlamda bir güzel raslantı oldu.. Daha doğrusu bu aynılık Gaussian / Normal dağılımlarının &quot;normalliği'' ile alakalı muhakkak, fakat örnekleme ortalaması hiçbir dağılım faraziyesi yapmıyor, herhangi bir dağılımdan geldiği bilinen ya da bilinmeyen bir veri üzerinde kullanılabiliyor. Bunu unutmayalım. İstatistikte matematiğin lakaytlaşması (sloppy) kolaydır, o sebeple neyin tanım, neyin hangi faraziyeye göre optimal, neyin nüfus (population) neyin örneklem (sample) olduğunu hep hatırlamamız lazım.</p>
<p>Devam edelim, maksimum olurluk ile <span class="math inline">\(\hat{\sigma}\)</span> hesaplayalım,</p>
<p><span class="math display">\[ \frac{\partial (\ln f)}{\partial \sigma} =
-\frac{n}{\sigma} + \frac{\sum (x_i-\mu)^2}{2\sigma^3}   = 0 
\]</span></p>
<p>Cebirsel birkac duzenleme sonrasi ve <span class="math inline">\(\mu\)</span> yerine yeni hesapladigimiz <span class="math inline">\(\hat{\mu}\)</span> kullanarak,</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{\sum (x_i-\hat{\mu})^2}{n} \]</span></p>
<p>Bu da örneklem varyansı ile aynı!</p>
<p>Yansızlık (Unbiasedness)</p>
<p>Tahmin edicilerin kendileri de birer rasgele değişken olduğu için her örneklem için değişik değerler verirler. Diyelim ki <span class="math inline">\(\theta\)</span> için bir tahmin edici <span class="math inline">\(\hat{\theta}\)</span> hesaplıyoruz, bu <span class="math inline">\(\hat{\theta}\)</span> gerçek <span class="math inline">\(\theta\)</span> için bazı örneklemler için çok küçük, bazı örneklemler için çok büyük sonuçlar (tahminler) verebilecektir. Kabaca ideal durumun, az çıkan tahminlerin çok çıkan tahminleri bir şekilde dengelemesi olduğunu tahmin edebiliriz, yani tahmin edicinin üreteceği pek çok değerin <span class="math inline">\(\theta\)</span>'yı bir şekilde &quot;ortalaması'' iyi olacaktır.</p>
<div class="figure">
<img src="unbias.png" />

</div>
<p>Bu durumu şöyle açıklayalım, madem tahmin ediciler birer rasgele değişken, o zaman bir dağılım fonksiyonları var. Ve üstteki resimde örnek olarak <span class="math inline">\(\hat{\theta_1},\hat{\theta_2}\)</span> olarak iki tahmin edici gösteriliyor mesela ve onlara tekabül eden yoğunluklar <span class="math inline">\(f_{\hat{\theta_1}}, f_{\hat{\theta_1}}\)</span>. İdeal durum soldaki resimdir, yoğunluğun fazla olduğu yer gerçek <span class="math inline">\(\theta\)</span>'ya yakın olması. Bu durumu matematiksel olarak nasıl belirtiriz? Beklenti ile!</p>
<p>Tanım</p>
<p><span class="math inline">\(Y_1,..,Y_n\)</span> üzerindeki <span class="math inline">\(\theta\)</span> tahmin edicisi <span class="math inline">\(\hat{\theta}\)</span>'den alınmış rasgele örneklem. Eğer tüm <span class="math inline">\(\theta\)</span>'lar için <span class="math inline">\(E(\hat{\theta}) = \theta\)</span> işe, bu durumda tahmin edicinin yansız olduğu söylenir.</p>
<p>Örnek olarak maksimum olurluk ile önceden hesapladığımız <span class="math inline">\(\hat{\sigma}\)</span> tahmin edicisine bakalım. Bu ifade</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{1}{n}\sum (Y_i-\hat{\mu})^2 \]</span></p>
<p>ya da</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{1}{n}\sum_i (Y_i-\bar{Y})^2 \]</span></p>
<p>ile belirtildi. Tahmin edici <span class="math inline">\(\hat{\sigma}^2\)</span>, <span class="math inline">\(\sigma^2\)</span> için yansız midir? Tanımımıza göre eğer tahmin edici yansız ise <span class="math inline">\(E(\hat{\sigma}^2) = \sigma^2\)</span> olmalıdır.</p>
<p>Not: Faydalı olacak bazı eşitlikler, daha önceden gördüğümüz</p>
<p><span class="math display">\[ Var(X) = E(X^2) - (E(X)^2)\]</span></p>
<p>ve sayısal ortalama <span class="math inline">\(\bar{Y}\)</span>'nin beklentisi <span class="math inline">\(E({\bar{Y}}) = E(Y_i)\)</span>, ve <span class="math inline">\(Var(\bar{Y}) = 1/n Var(Y_i)\)</span>.</p>
<p>Başlayalım,</p>
<p><span class="math display">\[ E(\hat{\sigma}^2) = E\bigg(\frac{1}{n}\sum_i (Y_i-\bar{Y})^2 \bigg)\]</span></p>
<p>Parantez içindeki <span class="math inline">\(1/n\)</span> sonrasındaki ifadeyi açarsak,</p>
<p><span class="math display">\[ \sum_i (Y_i-\bar{Y})^2  =  \sum_i (Y_i^2-2Y_i\bar{Y}+ \bar{Y}^2)\]</span></p>
<p><span class="math display">\[ = \sum_iY_i^2 -2\sum_i Y_i\bar{Y} + n\bar{Y}^2  \]</span></p>
<p><span class="math inline">\(\sum_i Y_i\)</span>'nin hemen yanında <span class="math inline">\(\bar{Y}\)</span> görüyoruz. Fakat <span class="math inline">\(\bar{Y}\)</span>'nin kendisi zaten <span class="math inline">\(1/n \sum_i Y_i\)</span> demek değil midir? Ya da, toplam içinde her <span class="math inline">\(i\)</span> için değişmeyecek <span class="math inline">\(\bar{Y}\)</span>'yi toplam dışına çekersek, <span class="math inline">\(\bar{Y}\sum_iY_i\)</span> olur, bu da <span class="math inline">\(\bar{Y} \cdot n \bar{Y}\)</span> demektir ya da <span class="math inline">\(n\bar{Y}^2\)</span>,</p>
<p><span class="math display">\[ = \sum_iY_i^2 -2 n\bar{Y}^2 + n\bar{Y}^2  \]</span></p>
<p><span class="math display">\[ = \sum_iY_i^2 -n\bar{Y}^2  \]</span></p>
<p>Dikkat, artık <span class="math inline">\(-n\bar{Y}^2\)</span> toplama işleminin <em>dışında</em>. Şimdi beklentiye geri dönelim,</p>
<p><span class="math display">\[ = E \bigg( \frac{1}{n} \bigg( \sum_iY_i^2 -n\bar{Y}^2 \bigg) \bigg) \]</span></p>
<p><span class="math inline">\(1/n\)</span> dışarı çekilir, beklenti toplamdan içeri nüfuz eder,</p>
<p><span class="math display">\[ = \frac{1}{n} \bigg(  \sum_i  E(Y_i^2) -n E(\bar{Y}^2) \bigg) \]</span></p>
<p>Daha önce demiştik ki (genel bağlamda)</p>
<p><span class="math display">\[ Var(X) = E(X^2) - (E(X)^2)\]</span></p>
<p>Bu örnek için harfleri değiştirirsek,</p>
<p><span class="math display">\[ Var(Y_i) = E(Y_i^2) - E(Y_i)^2\]</span></p>
<p>Yani</p>
<p><span class="math display">\[ E(Y_i^2) = Var(Y_i) + E(Y_i)^2 \]</span></p>
<p><span class="math inline">\(E(Y_i) = \mu\)</span> oldugunu biliyoruz,</p>
<p><span class="math display">\[ E(Y_i^2) = Var(Y_i) + \mu^2 \]</span></p>
<p>Aynısını <span class="math inline">\(E(\bar{Y}^2)\)</span> için kullanırsak,</p>
<p><span class="math display">\[  E(\bar{Y}^2) = Var(\bar{Y}) + E(\bar{Y})^2 \]</span></p>
<p><span class="math inline">\(E(\bar{Y}) = \mu\)</span>,</p>
<p><span class="math display">\[  E(\bar{Y}^2) = Var(\bar{Y}) + \mu^2 \]</span></p>
<p><span class="math display">\[
= \frac{1}{n} \bigg(  \sum_i Var(Y_i) + \mu^2   
-n (Var(\bar{Y}) + \mu^2 ) \bigg) 
\]</span></p>
<p><span class="math inline">\(Var(Y_i) = \sigma\)</span>, ve başta verdiğimiz eşitlikler ile beraber</p>
<p><span class="math display">\[
= \frac{1}{n} \bigg(  \sum_i (\sigma^2 + \mu^2)
-n (\frac{\sigma^2}{n} + \mu^2 ) \bigg) 
\]</span></p>
<p>Tekrar hatırlatalım, <span class="math inline">\(\sum_i\)</span> sadece ilk iki terim için geçerli, o zaman, ve sabit değerleri <span class="math inline">\(n\)</span> kadar topladığımıza göre bu aslında bir çarpım işlemi olur,</p>
<p><span class="math display">\[ 
= \frac{1}{n} \bigg(  n\sigma^2 + n\mu^2   
-n (\frac{\sigma^2}{n} + \mu^2 ) \bigg) 
\]</span></p>
<p><span class="math display">\[ 
=  \sigma^2 + \mu^2 -\frac{\sigma^2}{n} - \mu^2 
\]</span></p>
<p><span class="math display">\[ 
=  \sigma^2 -\frac{\sigma^2}{n} 
\]</span></p>
<p><span class="math display">\[ 
=  \frac{n\sigma^2}{n} -\frac{\sigma^2}{n} 
\]</span></p>
<p><span class="math display">\[ 
=  \frac{n\sigma^2 - \sigma^2}{n} 
\]</span></p>
<p><span class="math display">\[ 
=  \frac{\sigma^2(n-1)}{n} 
\]</span></p>
<p><span class="math display">\[ = \sigma^2 \frac{n-1}{n} \]</span></p>
<p>Görüldüğü gibi eriştiğimiz sonuç <span class="math inline">\(\sigma^2\)</span> değil, demek ki bu tahmin edici yansız değil. Kontrol tamamlandı.</p>
<p>Fakat eriştiğimiz son denklem bize başka bir şey gösteriyor, eğer üstteki sonucu <span class="math inline">\(\frac{n}{n-1}\)</span> ile çarpsaydık, <span class="math inline">\(\sigma^2\)</span> elde etmez miydik? O zaman yanlı tahmin ediciyi yansız hale çevirmek için, onu <span class="math inline">\(\frac{n}{n-1}\)</span> ile çarparız ve</p>
<p><span class="math display">\[ \frac{n}{n-1} \frac{1}{n}\sum_i (Y_i-\bar{Y})^2 \]</span></p>
<p><span class="math display">\[ =  \frac{1}{n-1}\sum_i (Y_i-\bar{Y})^2 \]</span></p>
<p>Üstteki ifade <span class="math inline">\(\sigma^2\)</span>'nin yansız tahmin edicisidir.</p>
<p>Hesap için kullandığınız kütüphanelerin yanlı mı yansız mı hesap yaptığını bilmek iyi olur, mesela Numpy versiyon 1.7.1 itibariyle yanlı standart sapma hesabı yapıyor, fakat Pandas yansız olanı kullanıyor (Pandas versiyonu daha iyi)</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
arr <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>])
<span class="bu">print</span> <span class="st">&#39;numpy&#39;</span>, np.std(arr)
<span class="bu">print</span> <span class="st">&#39;pandas&#39;</span>, <span class="bu">float</span>(pd.DataFrame(arr).std())</code></pre></div>
<pre><code>numpy 0.816496580928
pandas 1.0</code></pre>
<p>Kaynaklar</p>
<p>[1] Wolfram Mathworld, <em>Maximum Likelihood</em>, <a href="http://mathworld.wolfram.com/MaximumLikelihood.html" class="uri">http://mathworld.wolfram.com/MaximumLikelihood.html</a></p>
<p>[2] <em>Introduction to Probability and Statistics Using R</em></p>
<p>[3] Bayramlı, Istatistik, <em>Monte Carlo, Entegraller, MCMC</em></p>
</body>
</html>
