\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Tahmin Aralýklarý (Prediction Interval)

Lineer Regresyon yazýsýnda regresyon katsayýlarý $\beta$'yi veriden
hesaplamayý öðrendik. Bu bir anlamda alttaki denklemde verili $y,A$ ile
geri kalanlarý tahmin etmektir. 

$$ y = A\beta + \epsilon  $$

ki

$$ \epsilon \sim Normal(\mathbf{0}, \sigma^2 \mathbf{I}) $$

Yani katsayýlarýn $A$ ile çarpýmlarý artý gürültü ($\sigma$ ile parametrize
edilmiþ bir Gaussian üzerinden) bu sonucu verecektir. Tahmin edici,

$$ \hat{\beta} = (A^TA)^{-1}A^Ty $$

olarak bilinir. Bu formülü pek çok yazýda gördük, mesela [3]. O zaman

$$ \hat{\beta} = (A^TA)^{-1}A^T(A\beta + \epsilon) $$

$$ 
\hat{\beta} = \beta  + (A^TA)^{-1}A^T \epsilon 
\mlabel{1}
$$

Eðer $E( \hat{\beta} )$ hesaplamak istersek, 

$$ E( \hat{\beta} ) = E(  \beta  + (A^TA)^{-1}A^T \epsilon )$$

Fakat $E(\epsilon) = 0$ olduðu için üstteki hemen $ E( \hat{\beta} ) =
\beta $ haline geliyor. Vektör rasgele deðiþkenler üzerinde varyans, ya da
kovaryans hesabýný daha önce görmüþtük, bu hesabý $\hat{\beta}$ üzerinde
uygularsak,

$$ Var(\hat{\beta}) = 
E \big[
(\hat{\beta} - E(\hat{\beta}))  
(\hat{\beta} - E(\hat{\beta}))^T 
\big]
$$

Biraz önce $E( \hat{\beta} ) = \beta $ demiþtik, o zaman üstteki 

$$ 
Var(\hat{\beta}) = 
E \big[
(\hat{\beta} - \beta)  
(\hat{\beta} - \beta)^T 
\big]
\mlabel{2}
$$

olur. Üstte $\hat{\beta} - \beta$ var, bu (1)'den $\beta$ çýkartýlýyor
anlamýna gelir, o zaman oradaki $\beta$ kaybolur, geriye 

$$ \hat{\beta} - \beta = \beta  + (A^TA)^{-1}A^T \epsilon - \beta 
$$

$$ = (A^TA)^{-1}A^T \epsilon   $$

Üstteki ifadeyi (2) içine koyalým, 

$$ E \bigg[
\big( (A^TA)^{-1}A^T \epsilon \big)
\big( (A^TA)^{-1}A^T \epsilon \big)^T
\bigg]
 $$

Beklenti içini açalým, 

$$  = E [(A^TA)^{-1}A^T \epsilon \epsilon^T A (A^TA)^{-1}] $$

Tersi iþleminin devriði kayboldu çünkü $A^TA$ simetriktir, onun tersi de
simetriktir, simetrik matrisin devriði yine kendisidir.

$$  = (A^TA)^{-1}A^TA E[\epsilon \epsilon^T]  (A^TA)^{-1} $$

$$  = E[\epsilon \epsilon^T]  (A^TA)^{-1} $$

$$ 
Var(\hat{\beta})  = \sigma^2  (A^TA)^{-1} 
\mlabel{3}
$$

Yeni bir tahmin $a$ için 

$$ \hat{y}_a = a^T \hat{\beta}  $$

$\beta$ yerine $\hat{\beta}$ kullandýk. Þimdi tüm ifadenin varyansýna
bakalým,

$$ Var(\hat{y}_a) = Var(a^T \hat{\beta}) $$

Bundan önce $Var(a^T \hat{\beta}) = \big[a^T(A^TA)^{-1}a \big] \sigma^2$ olduðunu ispatlamak lazým, 
[1, sf 617] olduðu gibi - öncelikle $Var(a^T \hat{\beta})$ formülünde $a$ ve $\hat{\beta}$ nin birer
vektör olduðunu hatýrlayalým, o zaman $a^T \hat{\beta}$ bir noktasal
çarpýmdýr, yani $a_1\hat{\beta}_1 + ... + a_n\hat{\beta}_n$.  Demek ki

$$ Var(a^T \hat{\beta}) =  Var(a_1\hat{\beta}_1 + ... + a_n\hat{\beta}_n)$$

Þimdi [4] bölümünden hatýrlayacaðýmýz üzere,

$$ Var(X_1+ .. + X_n ) = Var(X_1) + .. + Var(X_n) + 2 \sum _{i<j}^{} Cov(X_i,X_j) $$

Bizim elimizde $a_i\hat{\beta}_i$'lar var tabii, o zaman

$$ Var(a^T \hat{\beta})  = Var(a_1\hat{\beta}_1) + .. + Var(a_n\hat{\beta}_n) + 
2 \sum _{i<j}^{}  Cov(a_i \hat{\beta}_i,a_j \hat{\beta}_j) 
$$

$$ Var(a_i\hat{\beta}_i) = a_i^2Var(\hat{\beta}_i)$$ 

olduðunu hatýrlayalým, o zaman iki üstteki

$$ =  a_1^2Var(\hat{\beta}_1) + .. +  a_n^2Var(\hat{\beta}_n) + 
2 \sum _{i<j} Cov(a_i \hat{\beta}_i,a_j \hat{\beta}_j) 
$$

Peki $Var(\hat{\beta}_i)$ nedir? (3)'u hatýrlayalým, buradaki matris
çarpýmýndan hareketle, her $Var(\hat{\beta}_i) = c_{ii} \sigma^2$
diyebiliriz ki $c_{ii}$, $(A^TA)^{-1}$ matrisinin (köþegeninde bulunan) bir
öðesidir.


$$ 
=  a_1^2c_{11} \sigma^2 + .. +  a_n^2c_{nn} \sigma^2 + 
2 \sum _{i<j} Cov(a_i \hat{\beta}_i,a_j \hat{\beta}_j) 
$$

Ayný þekilde $ Cov(a_i\hat{\beta}_i,a_j \hat{\beta}_j) = 2a_ia_jc_{ij}\sigma^2$ diyebiliriz,

$$ =  a_1^2c_{11} \sigma^2 + .. +  a_n^2 c_{nn} \sigma^2 + 2 \sum _{i<j}
a_ia_jc_{ij}\sigma^2 $$

$$ =  \big[a_1^2c_{11}  + .. +  a_n^2 c_{nn}  + 2 \sum _{i<j}  a_ia_jc_{ij}\big]\sigma^2 $$

Üstteki ifadeyi rahat bir þekilde $\big[a^T(A^TA)^{-1}a \big] \sigma^2$ olarak yazabiliriz.

Þimdi güven aralýðý yaratmanýn zamaný geldi. Hatýrlayalým ki
$\hat{\beta}_1,\hat{\beta}_2,,.$ tahmin edicilerinin kendileri birer
rasgele deðiþkendir, ve bu deðiþkenler Normal daðýlýma sahiptirler. O zaman
$a^T\hat{\beta}$ da normal olarak daðýlmýþtýr ve bu daðýlýmýn beklentisinin
$E(a^T\hat{\beta}) = a^T\beta$ olduðunu biliyoruz (dikkat eþitliðin saðýnda
þapkasýz $\beta$ var). O zaman ``gerçek'' $\beta$ için bir güvenlik aralýðý
oluþturmak için $a^T\hat{\beta} - a^T\beta $'nin da Normal olarak
daðýlmasýnýn zorunlu olduðundan hareketle,

$$ Z = 
\frac{a^T\hat{\beta} -a^T \beta }
{ \sqrt{ Var(a^T \hat{\beta}) } } = 
\frac{a^T\hat{\beta} -a^T \beta }
{ \sigma \sqrt{ a^T(A^TA)^{-1}a  } }
$$

Böylece bir standart normal yarattýk, ve bu formülü daha önce güvenlik
aralýðý için yaptýðýmýz gibi düzenlersek, 

$$ a^T\hat{\beta} \pm z_{\alpha/2} \sigma \sqrt{ a^T(A^TA)^{-1}a  } $$

Daha önce gördüðümüz gibi $\sigma$ yerine $S$ koyabiliriz, o zaman Öðrenci
T daðýlýmý elde ederiz (yazýnýn sonunda $\sigma,S$ teorik baðlantýsýnýn
sebepleri bulunabilir),


$$ T = 
\frac{a^T\hat{\beta} -a^T \beta }
{ S \sqrt{ a^T(A^TA)^{-1}a  } }
$$

ki bu güven aralýðý

$$ a^T\hat{\beta} \pm t_{\alpha/2} S \sqrt{ a^T(A^TA)^{-1}a  } $$

olarak hesaplanabilecektir, T daðýlýmýnýn serbestlik derecesi
$n-(k+1)$'dir, ki $n$ eldeki veri nokta sayýsý, $k$ iþe kaç $\beta$
deðiþkeninin olduðudur. 

Örnek

Basit bir örnek üzerinde görelim ([1, sf 620]'den alýndý), 

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
import numpy as np
df = pd.read_csv('11.1.csv',sep=' ')
print df.head()
import statsmodels.formula.api as smf
results = smf.ols('y ~ x', data=df).fit()
mse = np.sum(results.resid**2) / (len(df)-2)
s = np.sqrt(mse)
print 'mse', mse, 's', s
print results.summary()
\end{minted}

\begin{verbatim}
   x  y
0 -2  0
1 -1  0
2  0  1
3  1  1
4  2  3
mse 0.366666666667 s 0.605530070819
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.817
Model:                            OLS   Adj. R-squared:                  0.756
Method:                 Least Squares   F-statistic:                     13.36
Date:                Mon, 11 May 2015   Prob (F-statistic):             0.0354
Time:                        17:26:07   Log-Likelihood:                -3.3094
No. Observations:                   5   AIC:                             10.62
Df Residuals:                       3   BIC:                             9.838
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept      1.0000      0.271      3.693      0.034         0.138     1.862
x              0.7000      0.191      3.656      0.035         0.091     1.309
==============================================================================
Omnibus:                          nan   Durbin-Watson:                   2.509
Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.396
Skew:                          -0.174   Prob(JB):                        0.821
Kurtosis:                       1.667   Cond. No.                         1.41
==============================================================================

\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
A = df[['x']]
A['intercept'] = 1.
A = A[['intercept','x']]
ATA_inv = lin.inv(np.dot(A.T,A))
print ATA_inv
beta_hat = np.array(results.params)
a = np.array([[1,1]]).T
\end{minted}

\begin{verbatim}
[[ 0.2  0. ]
 [ 0.   0.1]]
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
pm = np.dot(np.dot(a.T, ATA_inv),a)[0][0]
pred = np.dot(a.T,beta_hat)[0]
print pm, pred
\end{minted}

\begin{verbatim}
0.3 1.7
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
from scipy.stats.distributions import t
t95_val = t.ppf(0.95,len(df)-2)
print 'tval', t95_val
print t95_val*s*pm
print 'Yuzde 90 guven araligi', \
      (pred - np.array([1,-1])*t95_val*s*np.sqrt(pm))
\end{minted}

\begin{verbatim}
tval 2.3533634348
0.427509698202
Yuzde 90 guven araligi [ 0.91947765  2.48052235]
\end{verbatim}

Görüldüðü gibi [1, sf 620] ile ayný sonucu aldýk.

Baþkanlýk Yarýþý Tahminleri

Daha önce [5] yazýsýnda gördüðümüz 2016 baþkanlýk yarýþý tahminini þimdi bu
yeni yöntemimizi kullanarak yapalým.

\begin{minted}[fontsize=\footnotesize]{python}
import statsmodels.formula.api as smf
import pandas as pd
df1 = pd.read_csv('../stat_linreg/prez.csv',sep=',')
regr = 'incumbent_vote ~ gdp_growth + net_approval + two_terms'
results1 = smf.ols(regr, data=df1).fit()
A1 = df1.copy()
A1['intercept'] = 1.
A1 = A1[['intercept','gdp_growth','net_approval','two_terms']]
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
from scipy.stats.distributions import t
t975_val1 = t.ppf(0.975,len(df1)-2)
beta_hat1 = np.array(results1.params)
ATA_inv1 = lin.inv(np.dot(A1.T,A1))
a1 = np.array([[1., 2.0, 0., 1]]).T
pm1 = np.dot(np.dot(a1.T, ATA_inv1),a1)[0][0]
pred1 = np.dot(a1.T,beta_hat1)[0]
mse1 = np.sum(results1.resid**2) / (len(df1)-2)
s1 = np.sqrt(mse1)
print 'Yuzde 95 Guven Araligi', \
      (pred1 - np.array([1,-1])*t975_val1*s1*np.sqrt(pm1))
\end{minted}

\begin{verbatim}
Yuzde 95 Guven Araligi [ 46.95198025  49.64353527]
\end{verbatim}

Yani Demokratlarýn kazanma þansý neredeyse hiç yok gibi. Önceki baþkanlýk
yarýþý tahmini {\em katsayýlarýn} güven aralýklarýný kullanmýþtý; þimdi
nihai tahminin güven aralýðýna baktýk. Aradaki fark þudur - katsayýlarýn
güven aralýklarýný kullandýðýmýzda onlarý en kötüleri birarada ve en
iyileri birada olacak þekilde yanyana kullanmýþ olduk; bu tür bir kullaným
bu katsayýlarýn arasýndaki korelasyonu dikkate almaz, çünkü, belki bir
katsayý X'in en kötümser olduðu noktada katsayý Y daha iyimser bir tahminde
bulunacaktýr, çünkü aradaki baðlantý böyledir...? Bu durumlar ilk
kullanýmda yakalanamazdý.  Bu sebeple ilk yöntemle hesaplanan güven aralýðý
ikincisine nazaran daha geniþ olacaktý, ki bunun olduðunu gördük.

$\sigma,\hat{\sigma},S$ Ýliþkileri

Öncelikle mümkün bazý notasyonel karýþýklýðý düzeltmeye uðraþalým;
kitaplarda $\sigma,\hat{\sigma}$ kullanýmý tek boyutlu verinin nüfus
standart hatasý ve onun tahmin edicisi (estimatör) için de kullanýlýyor. Bu
yazýda bu farklý, bu yazýdaki $\sigma$ bir lineer modelin hatasýný temsil
eden $\sigma$. 

Bu tür bir $\sigma$'nin tahmin edicisi $\hat{\sigma}$ þu þekilde tanýmlý, 

$$ \hat{\sigma}^2 = \frac{1}{n} \sum _{i=1}^{n} (Y_i - \hat{Y}_i)^2 $$

Ýspat için [2, sf 557-558]. Fakat üstteki kodda $n-2$ kullanýmý görüyoruz,
bu nereden geliyor? Bunun için $n/(n-2) \hat{\sigma}^2$ formülünün
$\sigma^2$ için bir yansýz tahmin edici (unbiaþed estimatör) olduðunu
bilmemiz lazým. Ýspat için bakýnýz [2, sf 560]. Yansýzlýk tanýmý için {\em
  Örneklem Daðýlýmlarý} yazýsý.

Tüm bunlarý biraraya koyarsak, $Y_i - \hat{Y}_i$ regresyondan bize
döndürülen \verb!resid! dizini, ve bu ``artýklarýn'' karelerini alýp
toplayýnca (ki artýklar tahmin ile gerçek verinin arasýndaki fark), ve
onlarý $n-2$ ile bölünce $\sigma^2$ için bir yansýz tahmin edici $S^2$'yi
nasýl elde ettiðimizi görebiliriz herhalde.

Ek

$\hat{\beta}$ Daðýlýmý

Lineer regresyonu $Y=X\beta + \epsilon$ olarak modellediðini farzedelim, ki
$X,Y,\beta$ çok boyutlu deðiþkenler / matris / vektör ve $\epsilon \sim
N(0,\sigma I)$ yani cok boyutlu bir Gaussian.  Soru su: Acaba $\beta$'nýn
tahmin edicisi $\hat{\beta}$'nin daðýlýmý nedir?

Tahmin edici hesabi

$$ \hat{\beta} = (X^TX)^{-1}X^TY $$

olduðunu biliyoruz. $Y$'yi yerine koyarsak,

$$  = (X^TX)^{-1}X^T(X\beta + \epsilon) $$

$$  = \cancel{(X^TX)^{-1}X^TX}\beta + (X^TX)^{-1}X^T\epsilon $$

$$  = \beta + (X^TX)^{-1}X^T\epsilon $$

Bir yan not, biliyoruz ki çok boyutlu Gaussian mesela $G \sim N(\phi,\rho)$'a
$BG + A$ þekilde ilgin (affine) transform uygulayýnca sonuç 
$N(\phi+A, B\rho B^T)$ oluyor. Burada $\epsilon$ bir çok boyutlu
Gaussian. O zaman üstteki transformu hesaplayabiliriz. $\beta$ toplamý
basit, esas iki taraftan $(X^TX)^{-1}X^T$ ve onun devriði
ile çarpýlan standart sapmaya ne olacak ona bakalým,

$$ (X^TX)^{-1}X^T \sigma X(X^{-1}X^{-T})^{T}  $$

$$ = (X^TX)^{-1}X^T \sigma X X^{-1}X^{-T}  $$

$$ = \sigma (X^TX)^{-1}  $$

Sonra $\beta$ toplamýný hatýrlarýz, yani $\hat{\beta} \sim N(\beta, \sigma
(X^TX)^{-1} )$ olarak daðýlmýþtýr, demek ki katsayýlarýmýzýn regresyon 
tahmini ``gerçek'' katsayýlar etrafýnda  merkezlenen bir Gaussian'dýr.

Kaynaklar

[1] Wackerly, {\em Mathematical Statistics}, 7th Edition

[2] Larsen, {\em Introduction to Mathematical Statistics and Its Applications}, 5th Edition

[3] Bayramli, Lineer Cebir, {\em Ders 15,16}

[4] Bayramli, Istatistik, {\em Beklenti, Kovaryans ve Korelasyon} 

[5] Bayramli, Istatistik, {\em Lineer Regresyon}

\end{document}


