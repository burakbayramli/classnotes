<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Tahmin Aralıkları (Prediction Interval)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h1 id="tahmin-aralıkları-prediction-interval">Tahmin Aralıkları (Prediction Interval)</h1>
<p>Lineer Regresyon yazısında regresyon katsayıları <span class="math inline">\(\beta\)</span>'yi veriden hesaplamayı öğrendik. Bu bir anlamda alttaki denklemde verili <span class="math inline">\(y,A\)</span> ile geri kalanları tahmin etmektir.</p>
<p><span class="math display">\[ y = A\beta + \epsilon  \]</span></p>
<p>ki</p>
<p><span class="math display">\[ \epsilon \sim Normal(\mathbf{0}, \sigma^2 \mathbf{I}) \]</span></p>
<p>Yani katsayıların <span class="math inline">\(A\)</span> ile çarpımları artı gürültü (<span class="math inline">\(\sigma\)</span> ile parametrize edilmiş bir Gaussian üzerinden) bu sonucu verecektir. Tahmin edici,</p>
<p><span class="math display">\[ \hat{\beta} = (A^TA)^{-1}A^Ty \]</span></p>
<p>olarak bilinir. Bu formülü pek çok yazıda gördük, mesela [3]. O zaman</p>
<p><span class="math display">\[ \hat{\beta} = (A^TA)^{-1}A^T(A\beta + \epsilon) \]</span></p>
<p><span class="math display">\[ 
\hat{\beta} = \beta  + (A^TA)^{-1}A^T \epsilon 
\qquad (1)
\]</span></p>
<p>Eğer <span class="math inline">\(E( \hat{\beta} )\)</span> hesaplamak istersek,</p>
<p><span class="math display">\[ E( \hat{\beta} ) = E(  \beta  + (A^TA)^{-1}A^T \epsilon )\]</span></p>
<p>Fakat <span class="math inline">\(E(\epsilon) = 0\)</span> olduğu için üstteki hemen <span class="math inline">\(E( \hat{\beta} ) = \beta\)</span> haline geliyor. Vektör rasgele değişkenler üzerinde varyans, ya da kovaryans hesabını daha önce görmüştük, bu hesabı <span class="math inline">\(\hat{\beta}\)</span> üzerinde uygularsak,</p>
<p><span class="math display">\[ Var(\hat{\beta}) = 
E \big[
(\hat{\beta} - E(\hat{\beta}))  
(\hat{\beta} - E(\hat{\beta}))^T 
\big]
\]</span></p>
<p>Biraz önce <span class="math inline">\(E( \hat{\beta} ) = \beta\)</span> demiştik, o zaman üstteki</p>
<p><span class="math display">\[ 
Var(\hat{\beta}) = 
E \big[
(\hat{\beta} - \beta)  
(\hat{\beta} - \beta)^T 
\big]
\qquad (2)
\]</span></p>
<p>olur. Üstte <span class="math inline">\(\hat{\beta} - \beta\)</span> var, bu (1)'den <span class="math inline">\(\beta\)</span> çıkartılıyor anlamına gelir, o zaman oradaki <span class="math inline">\(\beta\)</span> kaybolur, geriye</p>
<p><span class="math display">\[ \hat{\beta} - \beta = \beta  + (A^TA)^{-1}A^T \epsilon - \beta 
\]</span></p>
<p><span class="math display">\[ = (A^TA)^{-1}A^T \epsilon   \]</span></p>
<p>Üstteki ifadeyi (2) içine koyalım,</p>
<p><span class="math display">\[ E \bigg[
\big( (A^TA)^{-1}A^T \epsilon \big)
\big( (A^TA)^{-1}A^T \epsilon \big)^T
\bigg]
 \]</span></p>
<p>Beklenti içini açalım,</p>
<p><span class="math display">\[  = E [(A^TA)^{-1}A^T \epsilon \epsilon^T A (A^TA)^{-1}] \]</span></p>
<p>Tersi işleminin devriği kayboldu çünkü <span class="math inline">\(A^TA\)</span> simetriktir, onun tersi de simetriktir, simetrik matrisin devriği yine kendisidir.</p>
<p><span class="math display">\[  = (A^TA)^{-1}A^TA E[\epsilon \epsilon^T]  (A^TA)^{-1} \]</span></p>
<p><span class="math display">\[  = E[\epsilon \epsilon^T]  (A^TA)^{-1} \]</span></p>
<p><span class="math display">\[ 
Var(\hat{\beta})  = \sigma^2  (A^TA)^{-1} 
\qquad (3)
\]</span></p>
<p>Yeni bir tahmin <span class="math inline">\(a\)</span> için</p>
<p><span class="math display">\[ \hat{y}_a = a^T \hat{\beta}  \]</span></p>
<p><span class="math inline">\(\beta\)</span> yerine <span class="math inline">\(\hat{\beta}\)</span> kullandık. Şimdi tüm ifadenin varyansına bakalım,</p>
<p><span class="math display">\[ Var(\hat{y}_a) = Var(a^T \hat{\beta}) \]</span></p>
<p>Bundan önce <span class="math inline">\(Var(a^T \hat{\beta}) = \big[a^T(A^TA)^{-1}a \big] \sigma^2\)</span> olduğunu ispatlamak lazım, [1, sf 617] olduğu gibi - öncelikle <span class="math inline">\(Var(a^T \hat{\beta})\)</span> formülünde <span class="math inline">\(a\)</span> ve <span class="math inline">\(\hat{\beta}\)</span> nin birer vektör olduğunu hatırlayalım, o zaman <span class="math inline">\(a^T \hat{\beta}\)</span> bir noktasal çarpımdır, yani <span class="math inline">\(a_1\hat{\beta}_1 + ... + a_n\hat{\beta}_n\)</span>. Demek ki</p>
<p><span class="math display">\[ Var(a^T \hat{\beta}) =  Var(a_1\hat{\beta}_1 + ... + a_n\hat{\beta}_n)\]</span></p>
<p>Şimdi [4] bölümünden hatırlayacağımız üzere,</p>
<p><span class="math display">\[ Var(X_1+ .. + X_n ) = Var(X_1) + .. + Var(X_n) + 2 \sum_{i&lt;j}^{} Cov(X_i,X_j) \]</span></p>
<p>Bizim elimizde <span class="math inline">\(a_i\hat{\beta}_i\)</span>'lar var tabii, o zaman</p>
<p><span class="math display">\[ Var(a^T \hat{\beta})  = Var(a_1\hat{\beta}_1) + .. + Var(a_n\hat{\beta}_n) + 
2 \sum_{i&lt;j}^{}  Cov(a_i \hat{\beta}_i,a_j \hat{\beta}_j) 
\]</span></p>
<p><span class="math display">\[ Var(a_i\hat{\beta}_i) = a_i^2Var(\hat{\beta}_i)\]</span></p>
<p>olduğunu hatırlayalım, o zaman iki üstteki</p>
<p><span class="math display">\[ =  a_1^2Var(\hat{\beta}_1) + .. +  a_n^2Var(\hat{\beta}_n) + 
2 \sum_{i&lt;j} Cov(a_i \hat{\beta}_i,a_j \hat{\beta}_j) 
\]</span></p>
<p>Peki <span class="math inline">\(Var(\hat{\beta}_i)\)</span> nedir? (3)'u hatırlayalım, buradaki matris çarpımından hareketle, her <span class="math inline">\(Var(\hat{\beta}_i) = c_{ii} \sigma^2\)</span> diyebiliriz ki <span class="math inline">\(c_{ii}\)</span>, <span class="math inline">\((A^TA)^{-1}\)</span> matrisinin (köşegeninde bulunan) bir öğesidir.</p>
<p><span class="math display">\[ 
=  a_1^2c_{11} \sigma^2 + .. +  a_n^2c_{nn} \sigma^2 + 
2 \sum_{i&lt;j} Cov(a_i \hat{\beta}_i,a_j \hat{\beta}_j) 
\]</span></p>
<p>Aynı şekilde <span class="math inline">\(Cov(a_i\hat{\beta}_i,a_j \hat{\beta}_j) = 2a_ia_jc_{ij}\sigma^2\)</span> diyebiliriz,</p>
<p><span class="math display">\[ =  a_1^2c_{11} \sigma^2 + .. +  a_n^2 c_{nn} \sigma^2 + 2 \sum_{i&lt;j}
a_ia_jc_{ij}\sigma^2 \]</span></p>
<p><span class="math display">\[ =  \big[a_1^2c_{11}  + .. +  a_n^2 c_{nn}  + 2 \sum_{i&lt;j}  a_ia_jc_{ij}\big]\sigma^2 \]</span></p>
<p>Üstteki ifadeyi rahat bir şekilde <span class="math inline">\(\big[a^T(A^TA)^{-1}a \big] \sigma^2\)</span> olarak yazabiliriz.</p>
<p>Şimdi güven aralığı yaratmanın zamanı geldi. Hatırlayalım ki <span class="math inline">\(\hat{\beta}_1,\hat{\beta}_2,,.\)</span> tahmin edicilerinin kendileri birer rasgele değişkendir, ve bu değişkenler Normal dağılıma sahiptirler. O zaman <span class="math inline">\(a^T\hat{\beta}\)</span> da normal olarak dağılmıştır ve bu dağılımın beklentisinin <span class="math inline">\(E(a^T\hat{\beta}) = a^T\beta\)</span> olduğunu biliyoruz (dikkat eşitliğin sağında şapkasız <span class="math inline">\(\beta\)</span> var). O zaman &quot;gerçek'' <span class="math inline">\(\beta\)</span> için bir güvenlik aralığı oluşturmak için <span class="math inline">\(a^T\hat{\beta} - a^T\beta\)</span>'nin da Normal olarak dağılmasının zorunlu olduğundan hareketle,</p>
<p><span class="math display">\[ Z = 
\frac{a^T\hat{\beta} -a^T \beta }
{ \sqrt{ Var(a^T \hat{\beta}) } } = 
\frac{a^T\hat{\beta} -a^T \beta }
{ \sigma \sqrt{ a^T(A^TA)^{-1}a  } }
\]</span></p>
<p>Böylece bir standart normal yarattık, ve bu formülü daha önce güvenlik aralığı için yaptığımız gibi düzenlersek,</p>
<p><span class="math display">\[ a^T\hat{\beta} \pm z_{\alpha/2} \sigma \sqrt{ a^T(A^TA)^{-1}a  } \]</span></p>
<p>Daha önce gördüğümüz gibi <span class="math inline">\(\sigma\)</span> yerine <span class="math inline">\(S\)</span> koyabiliriz, o zaman Öğrenci T dağılımı elde ederiz (yazının sonunda <span class="math inline">\(\sigma,S\)</span> teorik bağlantısının sebepleri bulunabilir),</p>
<p><span class="math display">\[ T = 
\frac{a^T\hat{\beta} -a^T \beta }
{ S \sqrt{ a^T(A^TA)^{-1}a  } }
\]</span></p>
<p>ki bu güven aralığı</p>
<p><span class="math display">\[ a^T\hat{\beta} \pm t_{\alpha/2} S \sqrt{ a^T(A^TA)^{-1}a  } \]</span></p>
<p>olarak hesaplanabilecektir, T dağılımının serbestlik derecesi <span class="math inline">\(n-(k+1)\)</span>'dir, ki <span class="math inline">\(n\)</span> eldeki veri nokta sayısı, <span class="math inline">\(k\)</span> işe kaç <span class="math inline">\(\beta\)</span> değişkeninin olduğudur.</p>
<p>Örnek</p>
<p>Basit bir örnek üzerinde görelim ([1, sf 620]'den alındı),</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">import</span> numpy <span class="im">as</span> np
df <span class="op">=</span> pd.read_csv(<span class="st">&#39;11.1.csv&#39;</span>,sep<span class="op">=</span><span class="st">&#39; &#39;</span>)
<span class="bu">print</span> df.head()
<span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf
results <span class="op">=</span> smf.ols(<span class="st">&#39;y ~ x&#39;</span>, data<span class="op">=</span>df).fit()
mse <span class="op">=</span> np.<span class="bu">sum</span>(results.resid<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="bu">len</span>(df)<span class="op">-</span><span class="dv">2</span>)
s <span class="op">=</span> np.sqrt(mse)
<span class="bu">print</span> <span class="st">&#39;mse&#39;</span>, mse, <span class="st">&#39;s&#39;</span>, s
<span class="bu">print</span> results.summary()</code></pre></div>
<pre><code>   x  y
0 -2  0
1 -1  0
2  0  1
3  1  1
4  2  3
mse 0.366666666667 s 0.605530070819
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.817
Model:                            OLS   Adj. R-squared:                  0.756
Method:                 Least Squares   F-statistic:                     13.36
Date:                Mon, 11 May 2015   Prob (F-statistic):             0.0354
Time:                        17:26:07   Log-Likelihood:                -3.3094
No. Observations:                   5   AIC:                             10.62
Df Residuals:                       3   BIC:                             9.838
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept      1.0000      0.271      3.693      0.034         0.138     1.862
x              0.7000      0.191      3.656      0.035         0.091     1.309
==============================================================================
Omnibus:                          nan   Durbin-Watson:                   2.509
Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.396
Skew:                          -0.174   Prob(JB):                        0.821
Kurtosis:                       1.667   Cond. No.                         1.41
==============================================================================
</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy.linalg <span class="im">as</span> lin
A <span class="op">=</span> df[[<span class="st">&#39;x&#39;</span>]]
A[<span class="st">&#39;intercept&#39;</span>] <span class="op">=</span> <span class="fl">1.</span>
A <span class="op">=</span> A[[<span class="st">&#39;intercept&#39;</span>,<span class="st">&#39;x&#39;</span>]]
ATA_inv <span class="op">=</span> lin.inv(np.dot(A.T,A))
<span class="bu">print</span> ATA_inv
beta_hat <span class="op">=</span> np.array(results.params)
a <span class="op">=</span> np.array([[<span class="dv">1</span>,<span class="dv">1</span>]]).T</code></pre></div>
<pre><code>[[ 0.2  0. ]
 [ 0.   0.1]]</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pm <span class="op">=</span> np.dot(np.dot(a.T, ATA_inv),a)[<span class="dv">0</span>][<span class="dv">0</span>]
pred <span class="op">=</span> np.dot(a.T,beta_hat)[<span class="dv">0</span>]
<span class="bu">print</span> pm, pred</code></pre></div>
<pre><code>0.3 1.7</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> scipy.stats.distributions <span class="im">import</span> t
t95_val <span class="op">=</span> t.ppf(<span class="fl">0.95</span>,<span class="bu">len</span>(df)<span class="op">-</span><span class="dv">2</span>)
<span class="bu">print</span> <span class="st">&#39;tval&#39;</span>, t95_val
<span class="bu">print</span> t95_val<span class="op">*</span>s<span class="op">*</span>pm
<span class="bu">print</span> <span class="st">&#39;Yuzde 90 guven araligi&#39;</span>, <span class="op">\</span>
      (pred <span class="op">-</span> np.array([<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>])<span class="op">*</span>t95_val<span class="op">*</span>s<span class="op">*</span>np.sqrt(pm))</code></pre></div>
<pre><code>tval 2.3533634348
0.427509698202
Yuzde 90 guven araligi [ 0.91947765  2.48052235]</code></pre>
<p>Görüldüğü gibi [1, sf 620] ile aynı sonucu aldık.</p>
<p>Başkanlık Yarışı Tahminleri</p>
<p>Daha önce [5] yazısında gördüğümüz 2016 başkanlık yarışı tahminini şimdi bu yeni yöntemimizi kullanarak yapalım.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf
<span class="im">import</span> pandas <span class="im">as</span> pd
df1 <span class="op">=</span> pd.read_csv(<span class="st">&#39;../stat_linreg/prez.csv&#39;</span>,sep<span class="op">=</span><span class="st">&#39;,&#39;</span>)
regr <span class="op">=</span> <span class="st">&#39;incumbent_vote ~ gdp_growth + net_approval + two_terms&#39;</span>
results1 <span class="op">=</span> smf.ols(regr, data<span class="op">=</span>df1).fit()
A1 <span class="op">=</span> df1.copy()
A1[<span class="st">&#39;intercept&#39;</span>] <span class="op">=</span> <span class="fl">1.</span>
A1 <span class="op">=</span> A1[[<span class="st">&#39;intercept&#39;</span>,<span class="st">&#39;gdp_growth&#39;</span>,<span class="st">&#39;net_approval&#39;</span>,<span class="st">&#39;two_terms&#39;</span>]]</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy.linalg <span class="im">as</span> lin
<span class="im">from</span> scipy.stats.distributions <span class="im">import</span> t
t975_val1 <span class="op">=</span> t.ppf(<span class="fl">0.975</span>,<span class="bu">len</span>(df1)<span class="op">-</span><span class="dv">2</span>)
beta_hat1 <span class="op">=</span> np.array(results1.params)
ATA_inv1 <span class="op">=</span> lin.inv(np.dot(A1.T,A1))
a1 <span class="op">=</span> np.array([[<span class="fl">1.</span>, <span class="fl">2.0</span>, <span class="fl">0.</span>, <span class="dv">1</span>]]).T
pm1 <span class="op">=</span> np.dot(np.dot(a1.T, ATA_inv1),a1)[<span class="dv">0</span>][<span class="dv">0</span>]
pred1 <span class="op">=</span> np.dot(a1.T,beta_hat1)[<span class="dv">0</span>]
mse1 <span class="op">=</span> np.<span class="bu">sum</span>(results1.resid<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> (<span class="bu">len</span>(df1)<span class="op">-</span><span class="dv">2</span>)
s1 <span class="op">=</span> np.sqrt(mse1)
<span class="bu">print</span> <span class="st">&#39;Yuzde 95 Guven Araligi&#39;</span>, <span class="op">\</span>
      (pred1 <span class="op">-</span> np.array([<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>])<span class="op">*</span>t975_val1<span class="op">*</span>s1<span class="op">*</span>np.sqrt(pm1))</code></pre></div>
<pre><code>Yuzde 95 Guven Araligi [ 46.95198025  49.64353527]</code></pre>
<p>Yani Demokratların kazanma şansı neredeyse hiç yok gibi. Önceki başkanlık yarışı tahmini <em>katsayıların</em> güven aralıklarını kullanmıştı; şimdi nihai tahminin güven aralığına baktık. Aradaki fark şudur - katsayıların güven aralıklarını kullandığımızda onları en kötüleri birarada ve en iyileri birada olacak şekilde yanyana kullanmış olduk; bu tür bir kullanım bu katsayıların arasındaki korelasyonu dikkate almaz, çünkü, belki bir katsayı X'in en kötümser olduğu noktada katsayı Y daha iyimser bir tahminde bulunacaktır, çünkü aradaki bağlantı böyledir...? Bu durumlar ilk kullanımda yakalanamazdı. Bu sebeple ilk yöntemle hesaplanan güven aralığı ikincisine nazaran daha geniş olacaktı, ki bunun olduğunu gördük.</p>
<p><span class="math inline">\(\sigma,\hat{\sigma},S\)</span> İlişkileri</p>
<p>Öncelikle mümkün bazı notasyonel karışıklığı düzeltmeye uğraşalım; kitaplarda <span class="math inline">\(\sigma,\hat{\sigma}\)</span> kullanımı tek boyutlu verinin nüfus standart hatası ve onun tahmin edicisi (estimatör) için de kullanılıyor. Bu yazıda bu farklı, bu yazıdaki <span class="math inline">\(\sigma\)</span> bir lineer modelin hatasını temsil eden <span class="math inline">\(\sigma\)</span>.</p>
<p>Bu tür bir <span class="math inline">\(\sigma\)</span>'nin tahmin edicisi <span class="math inline">\(\hat{\sigma}\)</span> şu şekilde tanımlı,</p>
<p><span class="math display">\[ \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 \]</span></p>
<p>İspat için [2, sf 557-558]. Fakat üstteki kodda <span class="math inline">\(n-2\)</span> kullanımı görüyoruz, bu nereden geliyor? Bunun için <span class="math inline">\(n/(n-2) \hat{\sigma}^2\)</span> formülünün <span class="math inline">\(\sigma^2\)</span> için bir yansız tahmin edici (unbiaşed estimatör) olduğunu bilmemiz lazım. İspat için bakınız [2, sf 560]. Yansızlık tanımı için {} yazısı.</p>
<p>Tüm bunları biraraya koyarsak, <span class="math inline">\(Y_i - \hat{Y}_i\)</span> regresyondan bize döndürülen <code>resid</code> dizini, ve bu &quot;artıkların'' karelerini alıp toplayınca (ki artıklar tahmin ile gerçek verinin arasındaki fark), ve onları <span class="math inline">\(n-2\)</span> ile bölünce <span class="math inline">\(\sigma^2\)</span> için bir yansız tahmin edici <span class="math inline">\(S^2\)</span>'yi nasıl elde ettiğimizi görebiliriz herhalde.</p>
<p>Ek</p>
<p><span class="math inline">\(\hat{\beta}\)</span> Dağılımı</p>
<p>Lineer regresyonu <span class="math inline">\(Y=X\beta + \epsilon\)</span> olarak modellediğini farzedelim, ki <span class="math inline">\(X,Y,\beta\)</span> çok boyutlu değişkenler / matris / vektör ve <span class="math inline">\(\epsilon \sim N(0,\sigma I)\)</span> yani cok boyutlu bir Gaussian. Soru su: Acaba <span class="math inline">\(\beta\)</span>'nın tahmin edicisi <span class="math inline">\(\hat{\beta}\)</span>'nin dağılımı nedir?</p>
<p>Tahmin edici hesabi</p>
<p><span class="math display">\[ \hat{\beta} = (X^TX)^{-1}X^TY \]</span></p>
<p>olduğunu biliyoruz. <span class="math inline">\(Y\)</span>'yi yerine koyarsak,</p>
<p><span class="math display">\[  = (X^TX)^{-1}X^T(X\beta + \epsilon) \]</span></p>
<p><span class="math display">\[  = \cancel{(X^TX)^{-1}X^TX}\beta + (X^TX)^{-1}X^T\epsilon \]</span></p>
<p><span class="math display">\[  = \beta + (X^TX)^{-1}X^T\epsilon \]</span></p>
<p>Bir yan not, biliyoruz ki çok boyutlu Gaussian mesela <span class="math inline">\(G \sim N(\phi,\rho)\)</span>'a <span class="math inline">\(BG + A\)</span> şekilde ilgin (affine) transform uygulayınca sonuç <span class="math inline">\(N(\phi+A, B\rho B^T)\)</span> oluyor. Burada <span class="math inline">\(\epsilon\)</span> bir çok boyutlu Gaussian. O zaman üstteki transformu hesaplayabiliriz. <span class="math inline">\(\beta\)</span> toplamı basit, esas iki taraftan <span class="math inline">\((X^TX)^{-1}X^T\)</span> ve onun devriği ile çarpılan standart sapmaya ne olacak ona bakalım,</p>
<p><span class="math display">\[ (X^TX)^{-1}X^T \sigma X(X^{-1}X^{-T})^{T}  \]</span></p>
<p><span class="math display">\[ = (X^TX)^{-1}X^T \sigma X X^{-1}X^{-T}  \]</span></p>
<p><span class="math display">\[ = \sigma (X^TX)^{-1}  \]</span></p>
<p>Sonra <span class="math inline">\(\beta\)</span> toplamını hatırlarız, yani <span class="math inline">\(\hat{\beta} \sim N(\beta, \sigma (X^TX)^{-1} )\)</span> olarak dağılmıştır, demek ki katsayılarımızın regresyon tahmini &quot;gerçek'' katsayılar etrafında merkezlenen bir Gaussian'dır.</p>
<p>Kaynaklar</p>
<p>[1] Wackerly, <em>Mathematical Statistics</em>, 7th Edition</p>
<p>[2] Larsen, <em>Introduction to Mathematical Statistics and Its Applications</em>, 5th Edition</p>
<p>[3] Bayramlı, Lineer Cebir, <em>Ders 15,16</em></p>
<p>[4] Bayramlı, Istatistik, <em>Beklenti, Kovaryans ve Korelasyon</em></p>
<p>[5] Bayramlı, Istatistik, <em>Lineer Regresyon</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
