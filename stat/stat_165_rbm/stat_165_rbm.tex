\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Kýsýtlý Boltzmann Makinalarý (Restricted Boltzmann Machines -RBM-)

RBM aynen Boltzman Makinalarýnda (BM) örneðinde olduðu gibi bir
daðýlýmdýr. Verilen $x,h$ için bir olasýlýk deðeri geri döndürebilir.
$$ p(x,h;W) = \exp (-E(x,h)) / Z $$

Standart RBM için $h,x$ ikiseldir (binary). Gizli (hidden) tabaka $h$, ve
``görünen (visible)'' tabaka $x$ vardýr. $Z$ aynen önce gördüðümüz BM'de
olduðu gibi normalizasyon sabitidir. Spesifik bir RBM'i tanýmlayan þey onun
$W$ matrisidir. Gizli deðiþkenler bazen karýþýklýk yaratabiliyor, bu
deðiþkenler aynen görünen deðiþkenler gibi deðiþkendirler. Yani belli
$h$'lerin ``olasýlýðý'' sorulabilir, ya da onlar üretilebilir. Fakat RBM'i
eðitirken sadece görünen kýsmý tarafýndan eðitiriz. Gizli tabaka bu sýrada
örneklem ile arada sýrada içi doldurulur, bu tabii ki $W$'ye baðlý olarak
yapýlacaktýr. Gizli tabaka daha düþük boyutlu olduðu, ve 0/1 deðerlerine
sahip olmasý mecbur olduðu için bu git/gel bir tür özetleme yapar ki
öðrenim bu sýrada ortaya çýkar.

Devam edelim, $E$ tanýmýna ``enerji'' olarak ta atýf yapýlabiliyor.

$$ E(x,h) = -h^TWx - c^Tx - b^Th $$

BM'lerden farklý olarak RBM'de $c,b$ deðiþkenleri var. Bu deðiþkenler
yanlýlýk (bias) için, yani veri içindeki genel eðilimi saptamalarý için
modele konulmuþtur. Ayrýca $h^TWx$ terimi var, bu BM'deki $x^TWx$'den biraz
farklý, daha önce belirttiðimiz gibi, $h$ üzerinden $x$'ler arasýnda
baðlantý yapýyor. BM ile tüm $x$ öðeleri birbirine baðlanabiliyordu, RBM
ile $h$ katmanýnda baðlantýlar paylaþýlýyor. Bu $h$ üzerinden baðlantý
zorunluluðu RBM'in özetleme alanýný azaltarak genelleme oluþturmasýný
saðlýyor. Bu yüzden onlara ``kýsýtlý'' Boltzmann makinalarý adý
veriliyor. Gizli deðiþkenlerin kendi aralarýnda, ve görünen deðiþkenlerin
kendi aralarýnda direk baðlantýya izin verilmemiþtir, ki bu daha önce
bahsedilen kýsýtlamanýn bir diðer yönü. Baðlantýlara, $W$ üzerinden sadece
gizli ve görünen deðiþkenler (tabakalar) arasýnda izin verilmiþtir. Bu
ayrýca matematiksel olarak bazý kolaylýklar saðlýyor, bu konuyu birazdan
iþleyeceðiz.

Formül alttaki gibi de açýlabilir,

$$ = - \sum_j \sum_k W_{j,k}h_jx_k - \sum_k c_kx_k - \sum_j b_jh_j  $$

\includegraphics[height=4cm]{rbm_01.png}
\includegraphics[height=4cm]{rbm_02.png}

Tekrar vurgulayalým, $h,x$ deðiþkenleri olasýlýk teorisinden bilinen
rasgele deðiþkenlerdir, yani hem $x$'e hem de $h$'e ``zar attýrabiliriz'' /
bu deðiþkenler üzerinden örneklem toplayabiliriz.

Ayrýca, RBM'ler aynen BM'ler gibi bir olasýlýk yoðunluk fonksiyonu
üzerinden tanýmlanýrlar, önceki formülde gördüðümüz gibi, tüm mümkün
deðerleri üzerinden entegralleri (ya da toplamlarý) alýnýnca sonuç 1 olur,
vs.

Devam edelim, ana formülden hareketle cebirsel olarak þunlar da doðrudur,

$$ p(x,h;W) = \exp (-E(x,h)) / Z $$

$$ 
\mlabel{2}
= \exp (h^TWx + c^Tx + b^Th ) / Z $$

$$ = \exp (h^TWx) \exp (c^Tx) \exp(b^Th) / Z $$

çünkü bir toplam üzerindeki $\exp$, ayrý ayrý $\exp$'lerin çarpýmý
olur. Ayný mantýkla, eðer ana formülü matris / vektör yerine ayrý
deðiþkenler olarak görmek istersek,

$$ 
p(x,h;W) = \frac{1}{Z}
\prod_j \prod_k \exp (W_{jk}h_jx_k) \prod_k \exp(c_kx_k) \prod_j \exp(b_jh_j) 
 $$

Notasyonu kolaylaþtýrmak amacýyla $b,c$ terimlerini $W$ içine absorbe
edebiliriz, $x_0=1$ ve $h_0=1$ deðerlerini mecbur tutarsak ve $w_{0,:}=c$
ve $w_{:,0}=b$ dersek, yani $W$'nin sýfýrýncý satýrýnýn tamamýnýn $c$
olduðunu, sýfýrýncý kolonunun tamamýnýn $b$ olduðunu kabul edersek
RBM ana formülünü tekrar elde etmiþ oluruz, fakat artýk

$$ E(x,h) = -h^TWx $$

$$ = - \sum_j \sum_k W_{j,k}h_jx_k  $$

ve

$$ p(x,h;W)  = \exp (h^TWx) / Z $$

yeterli olacaktýr. Bir diðer kolaylýk $x,h$ yerine tek deðiþken kullanmak,

Eðer $y \equiv (x,h)$ olarak alýrsak ($\equiv$ tabiri ``taným'' anlamýna gelir), 


$$ P(x,h;W) = \frac{1}{Z(W)} \exp 
\bigg[ 
\frac{1}{2} y^T W y
\bigg]
$$

Aslýnda açýk konuþmak gerekirse ``enerji'' gibi kavramlarla uðraþmak, ya da
içinde eksi terimler içeren bir grup deðiþkenin tekrar eksisini almak ve
eksilerin etkisini nötralize etmiþ olmaya gerek yok, bunun yerine baþtan
(2)'deki ifadeyle yola çýkmak daha kýsa olur. Ýçinde enerji olan
açýklamalarý biraz da literatürde görülebilecek anlatýmlara açýklýk
getirmek için yaptýk.

Þimdi $h$ üzerinden marjinalize edersek,

$$ P(x;W) = \sum_h \frac{1}{Z(W)} \exp 
\bigg[ 
\frac{1}{2} y^T W y
\bigg]
$$


$$  
\mlabel{1}
P(x;W) = \frac{1}{Z(W)}  \sum_h \exp 
\bigg[ 
\frac{1}{2} y^T W y
\bigg]
$$


Ve $Z(W)$ 

$$ Z(W) = \sum_{h,x} \exp 
\bigg[ 
\frac{1}{2} y^T W y
\bigg]
$$

(1) denkleminde bölümünden sonraki kýsma $Z_x(W)$ dersek, sanki ayný $\exp$
denkleminin $x$'ler üzerinden marjinalize edilmiþ hali olarak
gösterebiliriz onu, ve böylece daha kýsa bir formül kullanabiliriz,

$$  
P(x;W) = \frac{1}{Z(W)}  
\underbrace{
\sum_h \exp 
\bigg[ 
\frac{1}{2} y^T W y
\bigg]
}_{Z_x(W)}
$$

O zaman 

$$  
P(x;W) = \frac{Z_x(W)}{Z(W)} 
$$

elde ederiz. Veri üzerinden maksimum olurluk için, yine log üzerinden bir
hesap yaparýz, BM için yapmýþtýk bunu,

$$  
\mathcal{L} = 
\ln \big( \prod_{n=1}^{N} P(x^{n};W) \big) = 
\sum_{n=1}^{N} \ln P(x^{n};W) 
$$

$$ 
= \sum_{n=1}^{N} \ln \frac{Z_{x^{(n)}}(W)}{Z(W)}  
= \sum_{n=1}^{N}  \big(\ln Z_{x^{(n)}} - \ln Z \big) 
$$

$$ 
\mlabel{3}
\frac{\partial \mathcal{L} }{\partial w_{ij}} = 
\sum_{n=1}^{N}  \big( \frac{\partial \ln Z_{x^{(n)}} }{\partial w_{ij}}
- \frac{\partial \ln Z }{\partial w_{ij}} \big)
$$

Parantez içindeki 1. türevi alalým,

$$ 
\frac{\partial \ln Z_{x^{(n)}} }{\partial w_{ij}} = 
\frac{\partial }{\partial w_{ij}}  
\ln \bigg[ 
\sum_h \exp \big( \frac{1}{2} y^{n^T} W y^n \big) 
\bigg]
$$

$$ 
= \frac{1}{Z_{x^{(n)}}}  \bigg[ \sum_h \frac{\partial }{\partial w_{ij}} \exp \big( \frac{1}{2} y^{n^T} W y^n  \big) \bigg]
$$

$$ 
= \frac{1}{Z_{x^{(n)}}}  
\bigg[ 
\sum_h  \exp \big( \frac{1}{2} y^{n^T} W y^n  \big) 
\frac{\partial }{\partial w_{ij}} y^{n^T} W y^n 
\bigg]
$$

$$ 
= \frac{1}{Z_{x^{(n)}}}  \sum_h  \exp \big( \frac{1}{2} y^{n^T} W y^n  \big) y_iy_j
$$

$$ 
= \sum_h  \frac{1}{Z_{x^{(n)}}}  \exp \big( \frac{1}{2} y^{n^T} W y^n  \big) y_iy_j
$$

$Z_{x^{(n)}}$'nin ne olduðunu hatýrlarsak, $\exp$ ifadesinin $h$ üzerinden
marjinalize edilmiþ hali,

$$ 
= \sum_h  \frac{\exp \big( \frac{1}{2} y^{n^T} W y^n  \big)}
{\sum_h \exp \big( \frac{1}{2} y^T W y \big) } 
y_iy_j
$$

Eðer bölümün üstünü ve altýný $Z$ ile bolþek,

$$ 
= \sum_h  
\frac{\exp \big( \frac{1}{2} y^{n^T} W y^n  \big) / Z} 
{\sum_h \exp \big( \frac{1}{2} y^T W y \big) / Z} 
y_iy_j
$$

Üst kýsým $P(y;W)$ yani $P(x,h;W) $ alt kýsým $P(x;W)$ olmaz mý? Evet! Ve,

$$ P(h|x^n;W) = \frac{P(x^n,h;W)}{P(x^n;W)}  $$

olduðuna göre, 

$$ =  \sum_h P(h|x^n;W) y_iy_j $$

elde ederiz. Bunu da $<y_iy_j>_{P(h|x^n;W)}$ olarak yazabiliriz. 

Þimdi parantez içindeki 2. türevi alalým, yani $\frac{\partial \ln Z }{\partial w_{ij}} $,

$$ 
\frac{\partial \ln Z }{\partial w_{ij}}  = 
\sum_{h,x} \frac{1}{Z}  \exp \big( \frac{1}{2} y^{T} W y  \big) y_iy_j =
\sum_{h,x} P(y;W)  y_iy_j
$$

ki bu son ifadeyi de $< y_iy_j >_{P(y;W)}$ olarak yazabiliriz. Tamamýný,
yani (3) ifadesini, artýk þöyle yazabiliriz,

$$
\sum_{n=1}^{N}  \big( \frac{\partial \ln Z_{x^{(n)}} }{\partial w_{ij}} - 
\frac{\partial \ln Z }{\partial w_{ij}} \big)
= \sum_{n=1}^{N}  < y_iy_j >_{P(h|x^n;W)} - < y_iy_j >_{P(y;W)}
\mlabel{4}
$$

Bu formülü de BM için yaptýðýmýz gibi bir gradyan güncelleme formülüne
dönüþtürebiliriz. Güncelleme formülünün hangi hesaplarý gerektirdiðine
gelince; Ýlk terim tüm $h$'ler üzerinden ki hesabý basit, ikincisi ise tüm
mümkün $x,h$'ler üzerinden bir olasýlýk hesabý ve örnekleme
gerektirecek. Bu durum çetin hesap (intractable) denen bir durum, özellikle
$x,h$ þartý için; daha önce BM için bu problemi Gibbs örneklemesi ile
çözmüþtük. Ayný çözümü burada da uygulayabiliriz, fakat belki daha iyi bir
yaklaþým þu olacak.

CD Yöntemi (Contrastive Divergence) 

RBM'leri eðitmek için kullanýlan en popüler yöntem CD yöntemidir. Bu
tekniði anlatmadan önce bazý matematiksel kolaylýklarý bilmek gerekli.

RBM grafiðine bakarsak, eðer $x$ biliniyor ise bu $h$ deðiþkenlerini
baðýmsýz hale getirir (koþullu olasýlýk kuralý), ve ayný þekilde $h$
biliniyor ise $x$ baðýmsýz hale gelir. Bunu görsel olarak bile anlamak çok
kolay, elimizle tüm $x$'leri kapatalým mesela ve $h$ düðümlerine bakalým,
aralarýnda hiçbir baðlantý yoktur deðil mi? Ayný þekilde $h$ kapatýnca
$x$'ler ``baðlantýsýz'' hale gelir. 

Bu baðýmsýzlýktan yola çýkarak, daha önce BM için yaptýðýmýz gibi,
olasýlýklar þu basit formüllere dönüþür,

$$ P(h_i=1|x) = \sigma \bigg( \sum _{j=1}^{m} w_{ij} x_j \bigg) $$

$$ P(x_i=1|h) = \sigma \bigg( \sum _{i=1}^{n} w_{ij} h_i \bigg) $$

ve tabii ki $\sigma(x) = 1 / (1+e^{-x})$. Daha önce 1 olma olasýlýðýný
nasýl örnekleme çevireceðimizi de görmüþtük zaten. 

Þimdi CD'nin ne olduðuna gelelim. Eðer RBM için gereken örneklemeyi klasik
Gibbs ile yaparsak örnekleme zincirini ``yeterince uzun süre'' iþletmek
gerekir ki daðýlýmýn olasý noktalarý gezilmiþ olsun. Fakat, özellikle
yüksek boyutlu durumlarda, tüm $x,h$ kombinasyonlarýný düþünürsek bu çok
büyük bir alandýr ve gezme iþlemi çok, çok uzun zaman alabilir. Bunun
yerine, ve üstteki baðýmsýzlýk formüllerinden hareketle CD yöntemi
bulunmuþtur, bu yönteme göre örnekleme verinin {\em kendisinden} baþlatýlýr
(kýyasla pür Gibbs rasgele bir noktadan), döngünün mesela ilk adýmýnda
$x^0$ (ki bu tüm verinin tamamý), baz alýnarak $p(h^0|v^0)$ hesaplanýr
(üstteki sigmoid), onun üzerinden $h^0$ örneklemi alýnýr, sonra $h^0$ baz
alýnýr ve $x^1$ üretilir, bu böyle devam eder. Böylece mümkün $h$ ve
$x$'ler gezilmiþ olur. Not: Sürekli verinin kendisine dönmenin de bazý
dezavantajlarý var, ki bunu yapmadan pür Gibbs örneklemesine daha yakýn bir
yaklaþým Kalýcý (Persistent) CD adlý yöntemdir (tabii baþka yaklaþýksal
numaralar kullanarak).

Literatürde þu þekildeki resim bolca görülebilir,

\includegraphics[height=5cm]{rbm_03.png}

Bu yöntem pür Gibbs örneklemesine kýyasla çok daha hýzlý iþler ve iyi
sonuçlar verir. Teorik olarak niye iþlediði [1,2,4] makalelerinde
bulunabilir. CD aslýnda (4) hedef formülünü deðil baþka bir hedefi optimize
ediyor, fakat sonuç orijinal gradyan adýmlarýnýn yapmak istediðine
yakýn. [3] baz alýnarak, þu þekilde kodlanabilir,

\inputminted[fontsize=\footnotesize]{python}{rbm.py}

RBM ve Sýnýflama 

Sýnýflama (classification) iþlemi yapmak için BM örneðinde bir
normalizasyon sabiti hesaplamýþtýk. Burada deðiþik bir yoldan gideceðiz; ki
bu yol ileride Derin Öðrenim için faydalý olacak. 

Eðittikten sonra bir RBM, içindeki $W$'ye göre, herhangi bir ``görünür''
veri noktasý $x$ için bir gizli bir $h$ üretebilir. Bunu üstteki
formülasyondan zaten biliyoruz. Ayrýca, $h$ genellikle daha az boyutta
olduðuna göre (hatta olmasa bile) bu $h$ üretiminin bir tür transformasyon
olduðu, veri üzerinde bir ``özetleme'' yaptýðý iddia edilebilir. O zaman
teorik olarak, görünür veri yerine, görünür veriden üretilen gizli veriyi
kullanýrsak ve bu veriyi alýp baþka bir sýnýflayýcýya verirsek, mesela
lojistik regresyon gibi, bu $h$'ler ve etiketler üzerinden denetimli
(supervised) bir eðitim yapabiliriz. Yani, önce RBM eðitiyoruz, tüm verinin
$h$ karþýlýðýný alýyoruz, sonra bunlarý lojistik regresyona
veriyoruz. Alttaki kodda bunun örneðinin görebiliriz.

Bu kod, ayrýca, k-Katlama (k-fold) tekniðini uyguluyor, veriyi 3 parçaya
bölüp sýrasýyla tüm parçalarý birer kez test, diðerlerini eðitim verisi
yapýyor, böylece verinin tamamý üzerinden eðitim/test yapmýþ
olunuyor. Sonuç,

\inputminted[fontsize=\footnotesize]{python}{test_rbmkfold.py}

\begin{minted}[fontsize=\footnotesize]{python}
! python test_rbmkfold.py
\end{minted}

\begin{verbatim}
1.0
\end{verbatim}

Baþarý yüzde 100! Altta karþýlaþtýrma için KNN tekniði kullandýk,

\inputminted[fontsize=\footnotesize]{python}{test_knnkfold.py}

\begin{minted}[fontsize=\footnotesize]{python}
! python test_knnkfold.py
\end{minted}

\begin{verbatim}
0.98009506833
\end{verbatim}

Kaynaklar

[1] Hinton, G., 
    {\em Training Products of Experts by Minimizing Contrastive Divergence}

[2] Louppe, G., 
    {\em Collaborative filtering, Scalable approaches using restricted Boltzmann machines}, Master Tezi, 2010

[3] \url{https://github.com/echen/restricted-boltzmann-machines}

[4] Tieleman, Hinton, {\em Using Fast Weights to Improve Persistent Contrastive Divergence}

[5] Larochelle, H., {\em Neural networks [5.1] : Restricted Boltzmann machine - definition}, 
    \url{https://www.youtube.com/watch?v=p4Vh_zMw-HQ}

\end{document}
