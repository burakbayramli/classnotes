<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Değişim Noktası Analizi (Changepoint Analysis)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="değişim-noktası-analizi-changepoint-analysis">Değişim Noktası Analizi (Changepoint Analysis)</h1>
<p>İngiltere'de 1851 ve 1962 yılları arasında kömür madenlerinde olan kazaların sayısı yıllık olarak kayıtlıdır. Acaba bu kazaların dağılımına bakarak, değişimin olduğu seneyi bulabilir miyiz? Böyle bir değişim anı neyi gösterir? Belki madenlerle alakalı regülasyonlarda, denetimlerde bir değişiklik olmuştur, ve kaza oranı azalmıştır [1, 2], [3, sf. 141]. Veriye bakalım.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
coal <span class="op">=</span> pd.read_csv(<span class="st">&#39;coal.txt&#39;</span>,header<span class="op">=</span><span class="va">None</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">coal.hist(bins<span class="op">=</span><span class="dv">7</span>)
plt.savefig(<span class="st">&#39;stat_coal_02.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_coal_02.png" />

</div>
<p>Eğer veride bir değişim noktası var ise, bu durum veride iki fark bölge olduğunu gösterir, ki bu bölgelerin iki farklı dağılımla temsil edileceğini tahmin edebiliriz.</p>
<p>Aynı zaman diliminde vuku bulan olay toplamlarının (event counts) Poisson dağılımına sahip olduğunu biliyoruz. O zaman, belki de ilk yapmamız gereken bu veriye iki tane Poisson uydurmak, yani veriyi iki Poisson dağılımının karışımı olarak temsil etmek. Karışımlar konusu [5] yazısında görülebilir, buradaki tek fark Bernoulli yerine Poisson kullanılacak olması. İdeal olarak uydurma operasyonu için Beklenti-Maksimizasyon (Expectation-Maximization -EM-) kullanılır. Fakat denklemleri türetmek zaman alabilir, biz şuradaki tavsiyeyi [4, sf. 11] takip ederek bu örnek için uydurmayı bir gayrı lineer optimizasyon paketi <code>lmfit</code> ile yapacağız (tavsiyenin R kodu <code>coal.r</code> içinde).</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> scipy.stats.distributions <span class="im">import</span> poisson
<span class="im">from</span> lmfit <span class="im">import</span> Parameters, minimize
<span class="im">from</span> lmfit.printfuncs <span class="im">import</span> report_fit

<span class="kw">def</span> f(pars,x):
    m1 <span class="op">=</span> pars[<span class="st">&#39;m1&#39;</span>].value
    lam1 <span class="op">=</span> pars[<span class="st">&#39;lam1&#39;</span>].value
    lam2 <span class="op">=</span> pars[<span class="st">&#39;lam2&#39;</span>].value
    model <span class="op">=</span> m1<span class="op">*</span>poisson(lam1).pmf(x) <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>m1)<span class="op">*</span>poisson(lam2).pmf(x) 
    <span class="cf">return</span> model
    
<span class="kw">def</span> residual(pars,y,x):
    <span class="cf">return</span> <span class="op">-</span>np.log(f(pars,x).T[<span class="dv">0</span>])

fit_params <span class="op">=</span> Parameters()
fit_params.add(<span class="st">&#39;m1&#39;</span>, value<span class="op">=</span><span class="fl">0.5</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>,<span class="bu">max</span><span class="op">=</span><span class="fl">1.</span>)
fit_params.add(<span class="st">&#39;lam1&#39;</span>, value<span class="op">=</span><span class="fl">1.0</span>, <span class="bu">min</span><span class="op">=</span><span class="fl">1.</span>,<span class="bu">max</span><span class="op">=</span><span class="fl">7.</span>)
fit_params.add(<span class="st">&#39;lam2&#39;</span>, value<span class="op">=</span><span class="fl">2.0</span>, <span class="bu">min</span><span class="op">=</span><span class="fl">2.</span>,<span class="bu">max</span><span class="op">=</span><span class="fl">7.</span>)

out <span class="op">=</span> minimize(residual, fit_params, args<span class="op">=</span>(coal,coal,))
report_fit(fit_params)</code></pre></div>
<pre><code>[[Variables]]
    m1:     0.51428096 +/- 0.406949 (79.13%) (init= 0.5)
    lam1:   1.00000004 +/- 0.557045 (55.70%) (init= 1)
    lam2:   3.35150806 +/- 1.791094 (53.44%) (init= 2)
[[Correlations]] (unreported correlations are &lt;  0.100)
    C(m1, lam1)                  =  0.905 
    C(m1, lam2)                  =  0.878 
    C(lam1, lam2)                =  0.772 </code></pre>
<p>Sonuçlar yaklaşık <span class="math inline">\(\lambda_1=1,\lambda_2=3\)</span> (tam sayıya yuvarladık, çünkü olay sayısı tam sayı olmalı). Bu iki dağılımı verinini normalize edilmiş histogramı üzerinde gösterirsek,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> scipy.stats.distributions <span class="im">import</span> poisson
coal.hist(bins<span class="op">=</span><span class="dv">7</span>,normed<span class="op">=</span><span class="va">True</span>)
plt.hold(<span class="va">True</span>)
p <span class="op">=</span> poisson(<span class="fl">1.0</span>)
x <span class="op">=</span> np.arange(<span class="dv">1</span>,<span class="dv">10</span>)
plt.plot(x, p.pmf(x))
p <span class="op">=</span> poisson(<span class="fl">3.0</span>)
plt.hold(<span class="va">True</span>)
plt.plot(x, p.pmf(x))
plt.savefig(<span class="st">&#39;stat_coal_03.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_coal_03.png" />

</div>
<p>Peki bu bulguyu şimdi değişim noktası keşfine nasıl çevireceğiz? Dikkat, üstteki iki dağılımın ayrıldığı <span class="math inline">\(\lambda\)</span> anı değil aradığımız, verideki senesel akış içinde hangi sene sonrası bir dağılımın diğerinin yerine geçtiği.</p>
<p>Şöyle bir yaklaşım olabilir mi acaba: bir döngü içinde potansiyel ayraç noktası olabilecek tüm seneler için veriyi iki parçaya ayırırız. Sıfır hipotezi nedir? Bu veri parçaları üstteki bulduğumuz Poisson dağılımlarından geliyor. O zaman şöyle devam ederiz: Üstteki optimizasyondan elimizde her iki dağılımın beklentisi, yani <span class="math inline">\(\lambda\)</span> değerleri var, ve Poisson dağılımlarının bir avantajı beklentisinin ve varyansının aynı olması! Şimdi, eğer her iki parçanın sayısal ortalamasını ve sıfır hipoteze göre bilinen <span class="math inline">\(\mu,\sigma^2\)</span> (her ikisi de <span class="math inline">\(\lambda\)</span>) üzerinden standardize edersek, yani <span class="math inline">\(N(0,1)\)</span> haline getirirsek, elimize iki tane <span class="math inline">\(N(0,1)\)</span> geçer, diyelim ki <span class="math inline">\(Z_1,Z_2\)</span>. Bunların karelerinin toplamının chi kare olacağını biliyoruz. Sıfır hipotezine göre böyle olmalı. O zaman bundan &quot;sapma'' sıfır hipotezinden ne kadar uzaklaşıldığını gösterir, bu bağlamda en yüksek p-değerini veren ayraç noktası bize değişim anını verir.</p>
<p>Daha detaylı matematiği vermek gerekirse; Merkezi Limit Teori'sine göre birbirinden bağımsız, aynı dağılımlı <span class="math inline">\(X_1,..,X_n\)</span>'in, ki her birinin beklentisi <span class="math inline">\(E(X_i) = \mu\)</span> ve varyansı <span class="math inline">\(Var(X_i)=\sigma^2\)</span>, o zaman sayısal ortalama <span class="math inline">\(\bar{X}\)</span> üzerinden, ve <span class="math inline">\(n \to \infty\)</span></p>
<p><span class="math display">\[ Z = \frac{\bar{X} - \mu }{\sigma \sqrt{n}}   \]</span></p>
<p>yani standard normal <span class="math inline">\(Z \sim N(0,1)\)</span>. Daha önce belirttiğimiz gibi Poisson için <span class="math inline">\(\mu = \sigma^2\)</span>.</p>
<p>Gerekli olan diğer teori: <span class="math inline">\(\chi_{n}^2 \sim Z_1^2 + ... + Z_n^2\)</span>, yani <span class="math inline">\(n\)</span> tane standart normalın toplamı yaklaşık olarak serbestlik derecesi <span class="math inline">\(n\)</span> olan chi kare dağılımı. Bu iki bilgiyi yan yana koyarsak, ve üstte bahsettiğimiz döngüyü yazarsak,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> scipy.stats <span class="im">import</span> chi2
<span class="co"># buyuk olan lambda degerini ilk parca icin kullaniyoruz, cunku</span>
<span class="co"># test ettigimiz kaza oranlarinin once fazla sonra az olmasi</span>
lam1 <span class="op">=</span> <span class="fl">3.</span><span class="op">;</span> lam2 <span class="op">=</span> <span class="fl">1.</span>
dof <span class="op">=</span> <span class="dv">2</span>
res <span class="op">=</span> []
cutoffs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">20</span>,<span class="dv">80</span>)
<span class="cf">for</span> cutoff <span class="kw">in</span> cutoffs:
     p1 <span class="op">=</span> coal[<span class="dv">0</span>:cutoff]<span class="op">;</span> p2 <span class="op">=</span> coal[cutoff<span class="op">+</span><span class="dv">1</span>:]
     z1 <span class="op">=</span> (p1.mean()<span class="op">-</span>lam1) <span class="op">/</span> lam1<span class="op">*</span>np.sqrt(<span class="bu">len</span>(p1))
     z2 <span class="op">=</span> (p2.mean()<span class="op">-</span>lam2) <span class="op">/</span> lam2<span class="op">*</span>np.sqrt(<span class="bu">len</span>(p2))
     chi <span class="op">=</span> z1<span class="op">**</span><span class="dv">2</span><span class="op">+</span>z2<span class="op">**</span><span class="dv">2</span>
     res.append(<span class="bu">float</span>(<span class="dv">1</span><span class="op">-</span>chi2.cdf(chi,dof)))

<span class="bu">print</span> <span class="dv">1851</span> <span class="op">+</span> cutoffs[np.array(res).argmax()]</code></pre></div>
<pre><code>1885</code></pre>
<p>Tarihten biliyoruz ki değişimin sebebi büyük ihtimalle İngiltere'de 1887 yılında kanunlaşan <em>Kömür Madenleri Yasası</em>'dır [3]. Yakınlık fena değil.</p>
<p>Ödev: Verinin iki tane Poisson karışımıyla temsil edilmesi gerektiğinden emin olmak istiyorsak, AIC kullanarak tek Poisson uyumu, daha sonra karışımın uyumu için ayrı ayrı AIC'leri hesaplayarak hangisinin daha düşük olduğuna göre bu kararı verebiliriz.</p>
<p>Bayes ve MCMC</p>
<p>Bir değişik yöntem Bayes yaklaşımını kullanarak ve hesapsal olarak Markov Chain Monte Carlo (MCMC) tekniği. Kazaların sayısının tümünü iki Poisson dağılımının ortak dağılımı (joint distribution) üzerinden modelleyeceğiz, ve bu dağılımların birinci Poisson'dan ikincisine geçtiği anı hesaplamaya uğraşacağız.</p>
<p>Poisson dağılımı</p>
<p><span class="math display">\[ p(y|\theta) = \frac{e^{-\theta}\theta^y}{y!} \]</span></p>
<p>Eldeki n tane veri noktası <span class="math inline">\(y=y_0, y_1,...,y_n\)</span>'nin hep birlikte <span class="math inline">\(\theta\)</span> ile tanımlı bir Poisson dağılımından gelip gelmediğinin ne kadar mümkün olduğu (likelihood) hesabı şöyledir:</p>
<p><span class="math display">\[ p(y|\theta) = \frac{e^{-n\theta}\theta^{\sum y_i}}{\prod y_i!}  \]</span></p>
<p>Formülün bölünen kısmındaki tüm y noktaları toplanıyor, bölen kısminde ise tüm y değerleri teker teker faktoryel hesabı sonrası birbiri ile çarpılıyor.</p>
<p>Şimdi yukarıdaki <span class="math inline">\(\theta\)</span> değişkeni de noktasal bir değer yerine bir &quot;dağılıma&quot;, mesela <span class="math inline">\(\theta\)</span> Gamma dağılımına sahip olabilirdi: <span class="math inline">\(Gamma(\alpha, \beta)\)</span>. Formülde <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> sabit değerlerdir (fonksiyon değişkeni değil). Gamma olasılık formülü şöyledir:</p>
<p><span class="math display">\[ p(\theta) = \frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta} \]</span></p>
<p>O zaman <span class="math inline">\(p(y|\theta)\)</span> formülünü bulmak için Bayes teorisini kullanmamız gerekecekti. Bayes teorisi bilindiği gibi</p>
<p><span class="math display">\[ p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} \]</span></p>
<p><span class="math display">\[ p(\theta|y) \propto p(y|\theta)p(\theta) \]</span></p>
<p>İkinci formüle dikkat, eşitlik yerine orantılı olma (proportional to) işaretini kullanıyor. Sebep: bölen kısmındaki p(y)'yi kaldırdık, sonuç olarak soldaki <span class="math inline">\(p(\theta|y)\)</span> değeri artık bir dağılım değil -- bu bir bakımdan önemli ama örnekleme amacı için bir fark yaratmıyor, basitleştirme amacıyla bunu yaptık, böylece <span class="math inline">\(p(y)\)</span>'yi hesaplamamız gerekmeyecek, ama örnekleme üzerinden diğer tüm hesapları hala yapabiliriz. Tamam.</p>
<p>Şimdi Bayes Teorisini Gamma önsel (apriori) ve Poisson olurluğu (likelihood) üzerinden kullanırsak,</p>
<p><span class="math display">\[ 
p(\theta|y) = \frac{\beta^\alpha}{\Gamma(\alpha)}
\theta^{\alpha-1}e^{-\beta\theta} \times
\frac{e^{-n\theta}\theta^{\sum y}}{\prod y!} 
\]</span></p>
<p>Benzer terimleri yanyana getirelim:</p>
<p><span class="math display">\[ 
p(\theta|y) = \frac{\beta^\alpha}{\Gamma(\alpha)\prod y!}
\theta^{\alpha-1}\theta^{\sum y}e^{-\beta\theta} e^{-n\theta} 
\]</span></p>
<p>Şimdi sol taraftaki bölümü atalım; yine üsttekine benzer numara, bu kısım gidince geri galan dağılım olamayacak, ama ona &quot;oranlı&quot; başka bir formül olacak.</p>
<p><span class="math display">\[ p(\theta|y)  \propto  \theta^{\alpha-1}\theta^{\sum y}e^{-\beta\theta} e^{-n\theta} \]</span></p>
<p><span class="math display">\[  \propto \theta^{\alpha-1+\sum y}e^{-(\beta+n)\theta}  \]</span></p>
<p>Bu dağılım nedir? Formülün sağ tarafı Gamma dağılımının formülüne benzemiyor mu? Evet, formülün sağ tarafı <span class="math inline">\(Gamma(\alpha+\sum y, \beta + n)\)</span> dağılımı, yani ona orantılı olan bir formül. Yani Bayes teorisi üzerinden şunu anlamış olduk; eğer önsel dağılım Gamma ise, Poisson mümkünlük bizi tekrar Gamma sonuç dağılımına götürüyor. Gamma'dan başlayınca tekrar Gamma'ya ulaşıyoruz. Bu bir rahatlık, bir kolaylık, bir matematiksel numara olarak kullanılabilir. Sonsal (posterior) dağılımların şekli, hesaplanma, cebirsel işlemler açısından önemli, eğer temiz, kısa, öz olurlarsa hesap işlerimiz kolaylaşır.</p>
<p>Not: Hatta üzerinde çalıştığımız problem sebebiyle eğer Poisson mümkünlük olacağını biliyorsak, sadece bu sebeple bile önsel dağılımı, üstteki kolaylık bilindiği için, özellikle Gamma seçebiliriz, çünkü biliriz ki Gamma ile başlarsak elimize tekrar Gamma geçecektir.</p>
<p>Şimdi kömür madeni verisine gelelim. Bu madendeki kazaların sayısının Poisson dağılımından geldiğini öne sürüyoruz, ve kazaların &quot;iki türlü&quot; olduğunu bildiğimizden hareketle, birinci tur kazaların ikinci tur kazalardan değişik Poisson parametresi kullandığını öne süreceğiz.</p>
<p>O zaman değişim anını, değişim senesini nasıl hesaplarız?</p>
<p>Kazaların ilk k senede ortalama <span class="math inline">\(\theta\)</span> ile, ve k ve n arasındaki senelerde ortalama <span class="math inline">\(\lambda\)</span> Poisson ile dağıldığını söyleyelim: Yani</p>
<p><span class="math display">\[ Y_i = Poisson(\theta) \qquad i=1,..,k   \]</span></p>
<p><span class="math display">\[ Y_i = Poisson(\lambda) \qquad i=k+1,..,n \]</span></p>
<p>Burada <span class="math inline">\(Y_i\)</span> sene i sırasında olan kazaların sayısını belirtiyor. Bayes kuralını hatırlarsak <span class="math inline">\(\theta\)</span> ve <span class="math inline">\(\lambda\)</span> parametrelerine önsel dağılım atayacağız. Bu dağılım Gamma olacak. Yani <span class="math inline">\(\theta \sim Gamma(a_1, b_1)\)</span> ve <span class="math inline">\(\lambda \sim Gamma(a_2, b_2)\)</span>.</p>
<p>Ayrıca k değerini de bilmiyoruz, k değeri yani &quot;değişim noktası&quot; Poisson dağılımların birinden ötekine geçtiği andır. Bu seneyi bulmaya çalışıyoruz. Şimdi tüm verinin, tüm seneleri kapsayacak şekilde modelini kurmaya başlayalım. k parametresinin aynen öteki parametreler gibi bir önsel dağılımı olacak (ki sonradan elimize k için de bir sonsal dağılımı geçecek), ama bu parametre elimizdeki 112 senenin herhangi birinde &quot;eşit olasılıkta&quot; olabileceği için onun önsel dağılımı Gamma değil <span class="math inline">\(k \sim Unif(1,112)\)</span> olacak. Yani ilk başta her senenin olasılığı birbiriyle eşit, her sene <span class="math inline">\(\frac{1}{112}\)</span> olasılık değeri taşıyor.</p>
<p>Bu modelin tamamının olurluğu nedir?</p>
<p><span class="math display">\[ L(\theta, \lambda, k | y) = \frac{1}{112} \times \displaystyle \prod_{i=1}^k
\frac{e^{-\theta}\theta^{y_i}}{y_i!}  \times \displaystyle \prod_{i=k+1}^n
\frac{e^{-\lambda}\lambda^{y_i}}{y_i!} 
 \]</span></p>
<p>Sonsal geçişini yapınca yukarıda olduğu gibi Gamma dağılımlarını elde ederiz:</p>
<p><span class="math display">\[ L(\theta, \lambda, k | y)  \propto 
\theta^{a_1-1+\sum_{i=1}^{k} y_i}e^{-(b_1+k)\theta} 
\lambda^{a_2-1+\sum_{i=k+1}^n y_i}e^{-(b_2+n-k)\lambda} 
 \]</span></p>
<p><span class="math inline">\(\frac{1}{112}\)</span>'yi bir sabit olduğu için formülden attık, bu durum orantılı hali etkilemiyor. Üstteki formül içindeki Gamma dağılımlarını görebiliyoruz, hemen yerlerine koyalım:</p>
<p><span class="math display">\[ L(\theta, \lambda, k | y)  \propto 
Gamma(a_1 + \sum_{i=1}^{k} y_i, b_1+k) \
Gamma(a_2 + \sum_{i=k+1}^{n} y_i, b_2+n-k)
 \]</span></p>
<p>Gibbs örneklemeye gelelim. Bu örneklemeye göre şartsal dağılım (conditional distribution) formülü bulunmaya uğraşılır, hangi değişkenlerin verili olduğuna göre, o değişkenler sabit kabul edilebilir, ve orantısal formülden atılabilir. Bu her değişken için teker teker yapılır.</p>
<p>Sonra hesap sırasında her şartsal dağılıma teker teker zar attırılır, ve elde edilen değer, bu sefer diğer şartsal dağılımlara değer olarak geçilir. Bu işlem sonuca erişilinceye kadar özyineli (iterative) olarak tekrar edilir (mesela 1000 kere). O zaman,</p>
<p><span class="math display">\[ \theta | Y_1,..,Y_n,k \sim Gamma(a_1 + \sum_{i=1}^{k} y_i, b_1+k) \]</span></p>
<p><span class="math display">\[ \lambda | Y_1,..,Y_n,k \sim Gamma(a_2 + \sum_{i=k+1}^{n} y_i, b_2+n-k) \]</span></p>
<p><span class="math display">\[ 
p(k | Y_1,..,Y_n) \propto \theta^{\sum_{i=1}^{k} y_i}e^{-k\theta} 
\lambda^{\sum_{i=k+1}^n y_i}e^{k\lambda} 
 \]</span></p>
<p>En son formülde içinde k olan terimleri tuttuk, gerisini attık. Formül <span class="math inline">\(e\)</span> terimleri birleştirilerek biraz daha basitleştirilebilir:</p>
<p><span class="math display">\[ p(k | Y_1,..,Y_n) \propto
\theta^{\sum_{i=1}^{k} y_i} \lambda^{\sum_{i=k+1}^n y_i}e^{(\lambda-\theta)k} 
 \]</span></p>
<p>Bir basitleştirme daha şöyle olabilir</p>
<p><span class="math display">\[ K = \sum_{i=1}^{k} y_i  \]</span></p>
<p><span class="math display">\[ \lambda^{\sum_{i=k+1}^n y_i} = \lambda^{\sum_{i=1}^n y_i - \sum_{i=1}^k y_i} \]</span></p>
<p>Üstel işlemlerde eksi işareti, üstel değişken ayrılınca bölüm işlemine dönüşür:</p>
<p><span class="math display">\[ = \frac{\lambda^{\sum_{i=1}^n y_i}}{\lambda ^{\sum_{i=1}^k y_i}} \]</span></p>
<p><span class="math display">\[ = \frac{\lambda^{\sum_{i=1}^n y_i}}{\lambda ^{K}} \]</span></p>
<p><span class="math display">\[ p(k | Y_1,..,Y_n) \propto 
\theta^{K} \frac{\lambda^{\sum_{i=1}^n y_i}}{\lambda ^{K}} e^{(\lambda-\theta)k} 
 \]</span></p>
<p><span class="math display">\[ = \bigg(\frac{\theta}{\lambda}\bigg)^{K} \lambda^{\sum_{i=1}^n  y_i} e^{(\lambda-\theta)k} \]</span></p>
<p><span class="math inline">\(\lambda^{\sum_{i=1}^n y_i}\)</span> terimi <span class="math inline">\(k\)</span>'ye değil <span class="math inline">\(n\)</span>'ye bağlı olduğu için o da final formülden atılabilir</p>
<p><span class="math display">\[  
p(k | Y_1,..,Y_n) \propto \bigg(\frac{\theta}{\lambda}\bigg)^{K} 
e^{(\lambda-\theta)k}  
\]</span></p>
<p><span class="math inline">\(p(k)\)</span> için ortaya çıkan bu formüle bakarsak, elimizde verilen her k değeri için bir olasılık döndürecek bir formül var. Daha önceki Gamma örneğinde formüle bakarak elimizde hemen bir Gamma dağılımı olduğunu söyleyebilmiştik. Bu kodlama sırasında işimize yarayacak bir şeydi, hesaplama için bir dağılıma &quot;zar attırmamız&quot; gerekiyor, ve Gamma örneğinde hemen Python Numpy kütüphanesindeki random.gamma çağrısına Gamma'dan gelen rasgele sayılar ürettirebiliriz. Üstteki formüle bakarsak, hangi dağılıma zar attıracağız?</p>
<p>Cevap şöyle: <span class="math inline">\(p(k|..)\)</span> pdf fonsiyonundaki k değişkeni <span class="math inline">\(1,..,119\)</span> arasındaki tam sayı değerleri alabilir, o zaman ortada bir ayrıksal (discrete) dağılım var demektir. Ve her k noktası için olabilecek olasılık değerini üstteki <span class="math inline">\(p(k|..)\)</span> formülüne hesaplattırabiliyorsak, ayrıksal bir dağılımı her nokta için üstteki çağrı, ve bu sonuçları normalize ederek (vektörün her elemanını vektörün toplamına bölerek) bir dağılım şekline dönüştürebiliriz. Daha sonra bu &quot;vektörsel dağılım&quot; üzerinden zar attırırız. Python kodundaki <code>w_choice</code> ya da R dilindeki <code>sample</code> çağrısı bu işi yapar.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> math
<span class="im">import</span> random

np.random.seed(<span class="dv">0</span>)<span class="op">;</span> random.seed(<span class="dv">0</span>)

<span class="co"># samples indexes from a sequence of probability table</span>
<span class="co"># based on those probabilities</span>
<span class="kw">def</span> w_choice(lst):
    n <span class="op">=</span> random.uniform(<span class="dv">0</span>, <span class="dv">1</span>)
    <span class="cf">for</span> item, weight <span class="kw">in</span> <span class="bu">enumerate</span>(lst):
        <span class="cf">if</span> n <span class="op">&lt;</span> weight:
            <span class="cf">break</span>
        n <span class="op">=</span> n <span class="op">-</span> weight
    <span class="cf">return</span> item

<span class="co">#</span>
<span class="co"># hyperparameters: a1, a2, b1, b2</span>
<span class="co">#</span>
<span class="kw">def</span> coal(n,x,init,a1,a2,b1,b2):
    nn<span class="op">=</span><span class="bu">len</span>(x)
    theta<span class="op">=</span>init[<span class="dv">0</span>]
    lam<span class="op">=</span>init[<span class="dv">1</span>]
    k <span class="op">=</span> init[<span class="dv">2</span>]
    z<span class="op">=</span>np.zeros((nn,))
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):
        ca <span class="op">=</span> a1 <span class="op">+</span> <span class="bu">sum</span>(x[<span class="dv">0</span>:k])
        theta <span class="op">=</span> np.random.gamma(ca, <span class="dv">1</span><span class="op">/</span><span class="bu">float</span>(k <span class="op">+</span> b1), <span class="dv">1</span>) 
        ca <span class="op">=</span> a2 <span class="op">+</span> <span class="bu">sum</span>(x[(k<span class="op">+</span><span class="dv">1</span>):nn])
        lam <span class="op">=</span> np.random.gamma(ca, <span class="dv">1</span><span class="op">/</span><span class="bu">float</span>(nn<span class="op">-</span>k <span class="op">+</span> b2), <span class="dv">1</span>)
        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(nn):
            z[j]<span class="op">=</span>math.exp((lam<span class="op">-</span>theta)<span class="op">*</span>(j<span class="op">+</span><span class="dv">1</span>)) <span class="op">*</span> (theta<span class="op">/</span>lam)<span class="op">**</span><span class="bu">sum</span>(x[<span class="dv">0</span>:j])
        <span class="co"># sample</span>
        zz <span class="op">=</span> z <span class="op">/</span> <span class="bu">sum</span>(z)
        k <span class="op">=</span> w_choice(zz)
    <span class="bu">print</span> <span class="bu">float</span>(theta), <span class="bu">float</span>(lam), <span class="bu">float</span>(k)
                
data <span class="op">=</span> np.loadtxt(<span class="st">&quot;coal.txt&quot;</span>)
coal(<span class="dv">1100</span>, data, init<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">30</span>], a1<span class="op">=</span><span class="dv">1</span>,a2<span class="op">=</span><span class="dv">1</span>,b1<span class="op">=</span><span class="dv">1</span>,b2<span class="op">=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>3.32561369453 0.931821137936 42.0</code></pre>
<p>Kodları işletince elimize k = 42 değeri geçecek, yani değişim anı 1851+42 = 1893 senesidir. Kaynaklar:</p>
<p>[1] Ioana A. Cosma, Ludger Evers, <em>Markov Chain Monte Carlo Methods (Lecture)</em></p>
<p>[2] Koop, <em>Bayesian Econometric Methods</em></p>
<p>[3] Anderson, A. (1911). Labour legislation. In H. Chisholm (Ed.), <em>Encyclopedia britannica (11th ed., Vol. 16, sf. 7-28)</em></p>
<p>[4] Zuccini, <em>Hidden Markov Models for Time Series An Introduction Using R</em></p>
<p>[5] Bayramlı, Istatistik, <em>Çok Değişkenli Bernoulli Karışımı</em></p>
<p>[6] <em>Bayesian estimation of changepoints</em>, <a href="https://ruivieira.dev/bayesian-estimation-of-changepoints.html" class="uri">https://ruivieira.dev/bayesian-estimation-of-changepoints.html</a></p>
<p>[7] <em>Coal-Mine Accidents: Their Causes and Prevention</em>, <a href="https://pubs.usgs.gov/bul/0333/report.pdf" class="uri">https://pubs.usgs.gov/bul/0333/report.pdf</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
