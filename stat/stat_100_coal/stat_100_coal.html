<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Değişim Noktası Analizi (Changepoint Analysis)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="değişim-noktası-analizi-changepoint-analysis">Değişim Noktası
Analizi (Changepoint Analysis)</h1>
<p>İngiltere’de 1851 ve 1962 yılları arasında kömür madenlerinde olan
kazaların sayısı yıllık olarak kayıtlıdır. Acaba bu kazaların dağılımına
bakarak, değişimin olduğu seneyi bulabilir miyiz? Böyle bir değişim anı
neyi gösterir? Belki madenlerle alakalı regülasyonlarda, denetimlerde
bir değişiklik olmuştur, ve kaza oranı azalmıştır [1, 2], [3, sf. 141].
Veriye bakalım.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>coal <span class="op">=</span> pd.read_csv(<span class="st">&#39;coal.txt&#39;</span>,header<span class="op">=</span><span class="va">None</span>)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>coal.hist(bins<span class="op">=</span><span class="dv">7</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_coal_02.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_coal_02.png" /></p>
<p>Eğer veride bir değişim noktası var ise, bu durum veride iki fark
bölge olduğunu gösterir, ki bu bölgelerin iki farklı dağılımla temsil
edileceğini tahmin edebiliriz.</p>
<p>Aynı zaman diliminde vuku bulan olay toplamlarının (event counts)
Poisson dağılımına sahip olduğunu biliyoruz. O zaman, belki de ilk
yapmamız gereken bu veriye iki tane Poisson uydurmak, yani veriyi iki
Poisson dağılımının karışımı olarak temsil etmek. Karışımlar konusu [5]
yazısında görülebilir, buradaki tek fark Bernoulli yerine Poisson
kullanılacak olması. İdeal olarak uydurma operasyonu için
Beklenti-Maksimizasyon (Expectation-Maximization -EM-) kullanılır. Fakat
denklemleri türetmek zaman alabilir, biz şuradaki tavsiyeyi [4, sf. 11]
takip ederek bu örnek için uydurmayı bir gayrı lineer optimizasyon
paketi <code>lmfit</code> ile yapacağız (tavsiyenin R kodu
<code>coal.r</code> içinde).</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats.distributions <span class="im">import</span> poisson</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lmfit <span class="im">import</span> Parameters, minimize</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> lmfit.printfuncs <span class="im">import</span> report_fit</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(pars,x):</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    m1 <span class="op">=</span> pars[<span class="st">&#39;m1&#39;</span>].value</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    lam1 <span class="op">=</span> pars[<span class="st">&#39;lam1&#39;</span>].value</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    lam2 <span class="op">=</span> pars[<span class="st">&#39;lam2&#39;</span>].value</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> m1<span class="op">*</span>poisson(lam1).pmf(x) <span class="op">+</span> (<span class="dv">1</span><span class="op">-</span>m1)<span class="op">*</span>poisson(lam2).pmf(x) </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> residual(pars,y,x):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.log(f(pars,x).T[<span class="dv">0</span>])</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>fit_params <span class="op">=</span> Parameters()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>fit_params.add(<span class="st">&#39;m1&#39;</span>, value<span class="op">=</span><span class="fl">0.5</span>, <span class="bu">min</span><span class="op">=</span><span class="dv">0</span>,<span class="bu">max</span><span class="op">=</span><span class="fl">1.</span>)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>fit_params.add(<span class="st">&#39;lam1&#39;</span>, value<span class="op">=</span><span class="fl">1.0</span>, <span class="bu">min</span><span class="op">=</span><span class="fl">1.</span>,<span class="bu">max</span><span class="op">=</span><span class="fl">7.</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>fit_params.add(<span class="st">&#39;lam2&#39;</span>, value<span class="op">=</span><span class="fl">2.0</span>, <span class="bu">min</span><span class="op">=</span><span class="fl">2.</span>,<span class="bu">max</span><span class="op">=</span><span class="fl">7.</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> minimize(residual, fit_params, args<span class="op">=</span>(coal,coal,))</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>report_fit(fit_params)</span></code></pre></div>
<pre class="text"><code>[[Variables]]
    m1:    0.50000000 (init = 0.5)
    lam1:  1.00000000 (init = 1)
    lam2:  2.00000000 (init = 2)</code></pre>
<p>Sonuçlar yaklaşık <span
class="math inline">\(\lambda_1=1,\lambda_2=3\)</span> (tam sayıya
yuvarladık, çünkü olay sayısı tam sayı olmalı). Bu iki dağılımı verinini
normalize edilmiş histogramı üzerinde gösterirsek,</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats.distributions <span class="im">import</span> poisson</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>coal.hist(bins<span class="op">=</span><span class="dv">7</span>,density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> poisson(<span class="fl">1.0</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">1</span>,<span class="dv">10</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.plot(x, p.pmf(x))</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> poisson(<span class="fl">3.0</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>plt.plot(x, p.pmf(x))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_coal_03.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_coal_03.png" /></p>
<p>Peki bu bulguyu şimdi değişim noktası keşfine nasıl çevireceğiz?
Dikkat, üstteki iki dağılımın ayrıldığı <span
class="math inline">\(\lambda\)</span> anı değil aradığımız, verideki
senesel akış içinde hangi sene sonrası bir dağılımın diğerinin yerine
geçtiği.</p>
<p>Şöyle bir yaklaşım olabilir mi acaba: bir döngü içinde potansiyel
ayraç noktası olabilecek tüm seneler için veriyi iki parçaya ayırırız.
Sıfır hipotezi nedir? Bu veri parçaları üstteki bulduğumuz Poisson
dağılımlarından geliyor. O zaman şöyle devam ederiz: Üstteki
optimizasyondan elimizde her iki dağılımın beklentisi, yani <span
class="math inline">\(\lambda\)</span> değerleri var, ve Poisson
dağılımlarının bir avantajı beklentisinin ve varyansının aynı olması!
Şimdi, eğer her iki parçanın sayısal ortalamasını ve sıfır hipoteze göre
bilinen <span class="math inline">\(\mu,\sigma^2\)</span> (her ikisi de
<span class="math inline">\(\lambda\)</span>) üzerinden standardize
edersek, yani <span class="math inline">\(N(0,1)\)</span> haline
getirirsek, elimize iki tane <span class="math inline">\(N(0,1)\)</span>
geçer, diyelim ki <span class="math inline">\(Z_1,Z_2\)</span>. Bunların
karelerinin toplamının chi kare olacağını biliyoruz. Sıfır hipotezine
göre böyle olmalı. O zaman bundan “sapma’’ sıfır hipotezinden ne kadar
uzaklaşıldığını gösterir, bu bağlamda en yüksek p-değerini veren ayraç
noktası bize değişim anını verir.</p>
<p>Daha detaylı matematiği vermek gerekirse; Merkezi Limit Teori’sine
göre birbirinden bağımsız, aynı dağılımlı <span
class="math inline">\(X_1,..,X_n\)</span>’in, ki her birinin beklentisi
<span class="math inline">\(E(X_i) = \mu\)</span> ve varyansı <span
class="math inline">\(Var(X_i)=\sigma^2\)</span>, o zaman sayısal
ortalama <span class="math inline">\(\bar{X}\)</span> üzerinden, ve
<span class="math inline">\(n \to \infty\)</span></p>
<p><span class="math display">\[ Z = \frac{\bar{X} - \mu }{\sigma
\sqrt{n}}   \]</span></p>
<p>yani standard normal <span class="math inline">\(Z \sim
N(0,1)\)</span>. Daha önce belirttiğimiz gibi Poisson için <span
class="math inline">\(\mu = \sigma^2\)</span>.</p>
<p>Gerekli olan diğer teori: <span class="math inline">\(\chi_{n}^2 \sim
Z_1^2 + ... + Z_n^2\)</span>, yani <span
class="math inline">\(n\)</span> tane standart normalın toplamı yaklaşık
olarak serbestlik derecesi <span class="math inline">\(n\)</span> olan
chi kare dağılımı. Bu iki bilgiyi yan yana koyarsak, ve üstte
bahsettiğimiz döngüyü yazarsak,</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># buyuk olan lambda degerini ilk parca icin kullaniyoruz, cunku</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># test ettigimiz kaza oranlarinin once fazla sonra az olmasi</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>lam1 <span class="op">=</span> <span class="fl">3.</span><span class="op">;</span> lam2 <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>dof <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> []</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>cutoffs <span class="op">=</span> <span class="bu">range</span>(<span class="dv">20</span>,<span class="dv">80</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cutoff <span class="kw">in</span> cutoffs:</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>     p1 <span class="op">=</span> coal[<span class="dv">0</span>:cutoff]<span class="op">;</span> p2 <span class="op">=</span> coal[cutoff<span class="op">+</span><span class="dv">1</span>:]</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>     z1 <span class="op">=</span> (p1.mean()<span class="op">-</span>lam1) <span class="op">/</span> lam1<span class="op">*</span>np.sqrt(<span class="bu">len</span>(p1))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>     z2 <span class="op">=</span> (p2.mean()<span class="op">-</span>lam2) <span class="op">/</span> lam2<span class="op">*</span>np.sqrt(<span class="bu">len</span>(p2))</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>     chi <span class="op">=</span> z1<span class="op">**</span><span class="dv">2</span><span class="op">+</span>z2<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>     res.append(<span class="bu">float</span>(<span class="dv">1</span><span class="op">-</span>chi2.cdf(chi,dof)))</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="dv">1851</span> <span class="op">+</span> cutoffs[np.array(res).argmax()])</span></code></pre></div>
<pre class="text"><code>1885</code></pre>
<p>Tarihten biliyoruz ki değişimin sebebi büyük ihtimalle İngiltere’de
1887 yılında kanunlaşan <em>Kömür Madenleri Yasası</em>’dır [3].
Yakınlık fena değil.</p>
<p>Ödev: Verinin iki tane Poisson karışımıyla temsil edilmesi
gerektiğinden emin olmak istiyorsak, AIC kullanarak tek Poisson uyumu,
daha sonra karışımın uyumu için ayrı ayrı AIC’leri hesaplayarak
hangisinin daha düşük olduğuna göre bu kararı verebiliriz.</p>
<h3 id="bayes-ve-mcmc">Bayes ve MCMC</h3>
<p>Bir değişik yöntem Bayes yaklaşımını kullanarak ve hesapsal olarak
Markov Chain Monte Carlo (MCMC) tekniği. Kazaların sayısının tümünü iki
Poisson dağılımının ortak dağılımı (joint distribution) üzerinden
modelleyeceğiz, ve bu dağılımların birinci Poisson’dan ikincisine
geçtiği anı hesaplamaya uğraşacağız.</p>
<p>Poisson dağılımı</p>
<p><span class="math display">\[ p(y|\theta) =
\frac{e^{-\theta}\theta^y}{y!} \]</span></p>
<p>Eldeki n tane veri noktası <span class="math inline">\(y=y_0,
y_1,...,y_n\)</span>’nin hep birlikte <span
class="math inline">\(\theta\)</span> ile tanımlı bir Poisson
dağılımından gelip gelmediğinin ne kadar mümkün olduğu (likelihood)
hesabı şöyledir:</p>
<p><span class="math display">\[ p(y|\theta) =
\frac{e^{-n\theta}\theta^{\sum y_i}}{\prod y_i!}  \]</span></p>
<p>Formülün bölünen kısmındaki tüm y noktaları toplanıyor, bölen
kısminde ise tüm y değerleri teker teker faktoryel hesabı sonrası
birbiri ile çarpılıyor.</p>
<p>Şimdi yukarıdaki <span class="math inline">\(\theta\)</span>
değişkeni de noktasal bir değer yerine bir “dağılıma”, mesela <span
class="math inline">\(\theta\)</span> Gamma dağılımına sahip olabilirdi:
<span class="math inline">\(Gamma(\alpha, \beta)\)</span>. Formülde
<span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(\beta\)</span> sabit değerlerdir (fonksiyon
değişkeni değil). Gamma olasılık formülü şöyledir:</p>
<p><span class="math display">\[ p(\theta) =
\frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}
\]</span></p>
<p>O zaman <span class="math inline">\(p(y|\theta)\)</span> formülünü
bulmak için Bayes teorisini kullanmamız gerekecekti. Bayes teorisi
bilindiği gibi</p>
<p><span class="math display">\[ p(\theta|y) =
\frac{p(y|\theta)p(\theta)}{p(y)} \]</span></p>
<p><span class="math display">\[ p(\theta|y) \propto
p(y|\theta)p(\theta) \]</span></p>
<p>İkinci formüle dikkat, eşitlik yerine orantılı olma (proportional to)
işaretini kullanıyor. Sebep: bölen kısmındaki p(y)’yi kaldırdık, sonuç
olarak soldaki <span class="math inline">\(p(\theta|y)\)</span> değeri
artık bir dağılım değil – bu bir bakımdan önemli ama örnekleme amacı
için bir fark yaratmıyor, basitleştirme amacıyla bunu yaptık, böylece
<span class="math inline">\(p(y)\)</span>’yi hesaplamamız gerekmeyecek,
ama örnekleme üzerinden diğer tüm hesapları hala yapabiliriz. Tamam.</p>
<p>Şimdi Bayes Teorisini Gamma önsel (apriori) ve Poisson olurluğu
(likelihood) üzerinden kullanırsak,</p>
<p><span class="math display">\[
p(\theta|y) = \frac{\beta^\alpha}{\Gamma(\alpha)}
\theta^{\alpha-1}e^{-\beta\theta} \times
\frac{e^{-n\theta}\theta^{\sum y}}{\prod y!}
\]</span></p>
<p>Benzer terimleri yanyana getirelim:</p>
<p><span class="math display">\[
p(\theta|y) = \frac{\beta^\alpha}{\Gamma(\alpha)\prod y!}
\theta^{\alpha-1}\theta^{\sum y}e^{-\beta\theta} e^{-n\theta}
\]</span></p>
<p>Şimdi sol taraftaki bölümü atalım; yine üsttekine benzer numara, bu
kısım gidince geri galan dağılım olamayacak, ama ona “oranlı” başka bir
formül olacak.</p>
<p><span class="math display">\[
p(\theta|y)  \propto  \theta^{\alpha-1}\theta^{\sum y}e^{-\beta\theta}
e^{-n\theta} \]</span></p>
<p><span class="math display">\[  \propto \theta^{\alpha-1+\sum
y}e^{-(\beta+n)\theta}  \]</span></p>
<p>Bu dağılım nedir? Formülün sağ tarafı Gamma dağılımının formülüne
benzemiyor mu? Evet, formülün sağ tarafı <span
class="math inline">\(Gamma(\alpha+\sum y, \beta + n)\)</span> dağılımı,
yani ona orantılı olan bir formül. Yani Bayes teorisi üzerinden şunu
anlamış olduk; eğer önsel dağılım Gamma ise, Poisson mümkünlük bizi
tekrar Gamma sonuç dağılımına götürüyor. Gamma’dan başlayınca tekrar
Gamma’ya ulaşıyoruz. Bu bir rahatlık, bir kolaylık, bir matematiksel
numara olarak kullanılabilir. Sonsal (posterior) dağılımların şekli,
hesaplanma, cebirsel işlemler açısından önemli, eğer temiz, kısa, öz
olurlarsa hesap işlerimiz kolaylaşır.</p>
<p>Not: Hatta üzerinde çalıştığımız problem sebebiyle eğer Poisson
mümkünlük olacağını biliyorsak, sadece bu sebeple bile önsel dağılımı,
üstteki kolaylık bilindiği için, özellikle Gamma seçebiliriz, çünkü
biliriz ki Gamma ile başlarsak elimize tekrar Gamma geçecektir.</p>
<p>Şimdi kömür madeni verisine gelelim. Bu madendeki kazaların sayısının
Poisson dağılımından geldiğini öne sürüyoruz, ve kazaların “iki türlü”
olduğunu bildiğimizden hareketle, birinci tur kazaların ikinci tur
kazalardan değişik Poisson parametresi kullandığını öne süreceğiz.</p>
<p>O zaman değişim anını, değişim senesini nasıl hesaplarız?</p>
<p>Kazaların ilk k senede ortalama <span
class="math inline">\(\theta\)</span> ile, ve k ve n arasındaki
senelerde ortalama <span class="math inline">\(\lambda\)</span> Poisson
ile dağıldığını söyleyelim: Yani</p>
<p><span class="math display">\[ Y_i = Poisson(\theta) \qquad
i=1,..,k   \]</span></p>
<p><span class="math display">\[ Y_i = Poisson(\lambda) \qquad
i=k+1,..,n \]</span></p>
<p>Burada <span class="math inline">\(Y_i\)</span> sene i sırasında olan
kazaların sayısını belirtiyor. Bayes kuralını hatırlarsak <span
class="math inline">\(\theta\)</span> ve <span
class="math inline">\(\lambda\)</span> parametrelerine önsel dağılım
atayacağız. Bu dağılım Gamma olacak. Yani <span
class="math inline">\(\theta \sim Gamma(a_1, b_1)\)</span> ve <span
class="math inline">\(\lambda \sim Gamma(a_2, b_2)\)</span>.</p>
<p>Ayrıca k değerini de bilmiyoruz, k değeri yani “değişim noktası”
Poisson dağılımların birinden ötekine geçtiği andır. Bu seneyi bulmaya
çalışıyoruz. Şimdi tüm verinin, tüm seneleri kapsayacak şekilde modelini
kurmaya başlayalım. k parametresinin aynen öteki parametreler gibi bir
önsel dağılımı olacak (ki sonradan elimize k için de bir sonsal dağılımı
geçecek), ama bu parametre elimizdeki 112 senenin herhangi birinde “eşit
olasılıkta” olabileceği için onun önsel dağılımı Gamma değil <span
class="math inline">\(k \sim Unif(1,112)\)</span> olacak. Yani ilk başta
her senenin olasılığı birbiriyle eşit, her sene <span
class="math inline">\(\frac{1}{112}\)</span> olasılık değeri
taşıyor.</p>
<p>Bu modelin tamamının olurluğu nedir?</p>
<p><span class="math display">\[ L(\theta, \lambda, k | y) =
\frac{1}{112} \times \displaystyle \prod_{i=1}^k
\frac{e^{-\theta}\theta^{y_i}}{y_i!}  \times \displaystyle
\prod_{i=k+1}^n
\frac{e^{-\lambda}\lambda^{y_i}}{y_i!}
\]</span></p>
<p>Sonsal geçişini yapınca yukarıda olduğu gibi Gamma dağılımlarını elde
ederiz:</p>
<p><span class="math display">\[ L(\theta, \lambda, k | y)  \propto
\theta^{a_1-1+\sum_{i=1}^{k} y_i}e^{-(b_1+k)\theta}
\lambda^{a_2-1+\sum_{i=k+1}^n y_i}e^{-(b_2+n-k)\lambda}
\]</span></p>
<p><span class="math inline">\(\frac{1}{112}\)</span>’yi bir sabit
olduğu için formülden attık, bu durum orantılı hali etkilemiyor. Üstteki
formül içindeki Gamma dağılımlarını görebiliyoruz, hemen yerlerine
koyalım:</p>
<p><span class="math display">\[ L(\theta, \lambda, k | y)  \propto
Gamma(a_1 + \sum_{i=1}^{k} y_i, b_1+k) \
Gamma(a_2 + \sum_{i=k+1}^{n} y_i, b_2+n-k)
\]</span></p>
<p>Gibbs örneklemeye gelelim. Bu örneklemeye göre şartsal dağılım
(conditional distribution) formülü bulunmaya uğraşılır, hangi
değişkenlerin verili olduğuna göre, o değişkenler sabit kabul
edilebilir, ve orantısal formülden atılabilir. Bu her değişken için
teker teker yapılır.</p>
<p>Sonra hesap sırasında her şartsal dağılıma teker teker zar attırılır,
ve elde edilen değer, bu sefer diğer şartsal dağılımlara değer olarak
geçilir. Bu işlem sonuca erişilinceye kadar özyineli (iterative) olarak
tekrar edilir (mesela 1000 kere). O zaman,</p>
<p><span class="math display">\[ \theta | Y_1,..,Y_n,k \sim Gamma(a_1 +
\sum_{i=1}^{k} y_i, b_1+k) \]</span></p>
<p><span class="math display">\[ \lambda | Y_1,..,Y_n,k \sim Gamma(a_2 +
\sum_{i=k+1}^{n} y_i, b_2+n-k) \]</span></p>
<p><span class="math display">\[
p(k | Y_1,..,Y_n) \propto \theta^{\sum_{i=1}^{k} y_i}e^{-k\theta}
\lambda^{\sum_{i=k+1}^n y_i}e^{k\lambda}
\]</span></p>
<p>En son formülde içinde k olan terimleri tuttuk, gerisini attık.
Formül <span class="math inline">\(e\)</span> terimleri birleştirilerek
biraz daha basitleştirilebilir:</p>
<p><span class="math display">\[ p(k | Y_1,..,Y_n) \propto
\theta^{\sum_{i=1}^{k} y_i} \lambda^{\sum_{i=k+1}^n
y_i}e^{(\lambda-\theta)k}
\]</span></p>
<p>Bir basitleştirme daha şöyle olabilir</p>
<p><span class="math display">\[ K = \sum_{i=1}^{k} y_i  \]</span></p>
<p><span class="math display">\[ \lambda^{\sum_{i=k+1}^n y_i} =
\lambda^{\sum_{i=1}^n y_i - \sum_{i=1}^k y_i} \]</span></p>
<p>Üstel işlemlerde eksi işareti, üstel değişken ayrılınca bölüm
işlemine dönüşür:</p>
<p><span class="math display">\[ = \frac{\lambda^{\sum_{i=1}^n
y_i}}{\lambda ^{\sum_{i=1}^k y_i}} \]</span></p>
<p><span class="math display">\[ = \frac{\lambda^{\sum_{i=1}^n
y_i}}{\lambda ^{K}} \]</span></p>
<p><span class="math display">\[ p(k | Y_1,..,Y_n) \propto
\theta^{K} \frac{\lambda^{\sum_{i=1}^n y_i}}{\lambda ^{K}}
e^{(\lambda-\theta)k}
\]</span></p>
<p><span class="math display">\[ =
\bigg(\frac{\theta}{\lambda}\bigg)^{K} \lambda^{\sum_{i=1}^n  y_i}
e^{(\lambda-\theta)k} \]</span></p>
<p><span class="math inline">\(\lambda^{\sum_{i=1}^n y_i}\)</span>
terimi <span class="math inline">\(k\)</span>’ye değil <span
class="math inline">\(n\)</span>’ye bağlı olduğu için o da final
formülden atılabilir</p>
<p><span class="math display">\[  
p(k | Y_1,..,Y_n) \propto \bigg(\frac{\theta}{\lambda}\bigg)^{K}
e^{(\lambda-\theta)k}  
\]</span></p>
<p><span class="math inline">\(p(k)\)</span> için ortaya çıkan bu
formüle bakarsak, elimizde verilen her k değeri için bir olasılık
döndürecek bir formül var. Daha önceki Gamma örneğinde formüle bakarak
elimizde hemen bir Gamma dağılımı olduğunu söyleyebilmiştik. Bu kodlama
sırasında işimize yarayacak bir şeydi, hesaplama için bir dağılıma “zar
attırmamız” gerekiyor, ve Gamma örneğinde hemen Python Numpy
kütüphanesindeki random.gamma çağrısına Gamma’dan gelen rasgele sayılar
ürettirebiliriz. Üstteki formüle bakarsak, hangi dağılıma zar
attıracağız?</p>
<p>Cevap şöyle: <span class="math inline">\(p(k|..)\)</span> pdf
fonsiyonundaki k değişkeni <span class="math inline">\(1,..,119\)</span>
arasındaki tam sayı değerleri alabilir, o zaman ortada bir ayrıksal
(discrete) dağılım var demektir. Ve her k noktası için olabilecek
olasılık değerini üstteki <span class="math inline">\(p(k|..)\)</span>
formülüne hesaplattırabiliyorsak, ayrıksal bir dağılımı her nokta için
üstteki çağrı, ve bu sonuçları normalize ederek (vektörün her elemanını
vektörün toplamına bölerek) bir dağılım şekline dönüştürebiliriz. Daha
sonra bu “vektörsel dağılım” üzerinden zar attırırız. Python kodundaki
<code>w_choice</code> ya da R dilindeki <code>sample</code> çağrısı bu
işi yapar.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)<span class="op">;</span> random.seed(<span class="dv">0</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co"># samples indexes from a sequence of probability table</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># based on those probabilities</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> w_choice(lst):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> random.uniform(<span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> item, weight <span class="kw">in</span> <span class="bu">enumerate</span>(lst):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n <span class="op">&lt;</span> weight:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> n <span class="op">-</span> weight</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> item</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># hyperparameters: a1, a2, b1, b2</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> coal(n,x,init,a1,a2,b1,b2):</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    nn<span class="op">=</span><span class="bu">len</span>(x)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    theta<span class="op">=</span>init[<span class="dv">0</span>]</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    lam<span class="op">=</span>init[<span class="dv">1</span>]</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> init[<span class="dv">2</span>]</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    z<span class="op">=</span>np.zeros((nn,))</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        ca <span class="op">=</span> a1 <span class="op">+</span> <span class="bu">sum</span>(x[<span class="dv">0</span>:k])</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> np.random.gamma(ca, <span class="dv">1</span><span class="op">/</span><span class="bu">float</span>(k <span class="op">+</span> b1), <span class="dv">1</span>) </span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        ca <span class="op">=</span> a2 <span class="op">+</span> <span class="bu">sum</span>(x[(k<span class="op">+</span><span class="dv">1</span>):nn])</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        lam <span class="op">=</span> np.random.gamma(ca, <span class="dv">1</span><span class="op">/</span><span class="bu">float</span>(nn<span class="op">-</span>k <span class="op">+</span> b2), <span class="dv">1</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(nn):</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>            z[j]<span class="op">=</span>math.exp((lam<span class="op">-</span>theta)<span class="op">*</span>(j<span class="op">+</span><span class="dv">1</span>)) <span class="op">*</span> (theta<span class="op">/</span>lam)<span class="op">**</span><span class="bu">sum</span>(x[<span class="dv">0</span>:j])</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sample</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        zz <span class="op">=</span> z <span class="op">/</span> <span class="bu">sum</span>(z)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> w_choice(zz)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="bu">float</span>(theta), <span class="bu">float</span>(lam), <span class="bu">float</span>(k))</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.loadtxt(<span class="st">&quot;coal.txt&quot;</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>coal(<span class="dv">1100</span>, data, init<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">30</span>], a1<span class="op">=</span><span class="dv">1</span>,a2<span class="op">=</span><span class="dv">1</span>,b1<span class="op">=</span><span class="dv">1</span>,b2<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<pre class="text"><code>3.3256136945319414 0.9318211379358856 42.0</code></pre>
<p>Kodları işletince elimize k = 42 değeri geçecek, yani değişim anı
1851+42 = 1893 senesidir.</p>
<h3 id="pymc">PyMC</h3>
<p>PyMC ile kurulan model, daha önce el ile yazılmış Gibbs
örnekleyicisiyle aynı Bayesyen değişim noktası (changepoint)
formülasyonunu yeniden üretmektedir. Her iki yaklaşımda da veri, yıllık
kömür madeni kazası sayılarından oluşur ve bu sayılar tek bir değişim
noktasına sahip Poisson süreci olarak modellenir.</p>
<p>Burada <span class="math inline">\(\theta\)</span> ve <span
class="math inline">\(\lambda\)</span> sırasıyla değişim noktasından
önce ve sonra geçerli olan Poisson oranlarıdır. <span
class="math inline">\(k\)</span> ise değişim noktasının konumudur.
Parametreler için öncül dağılımlar bağımsız alınmıştır: <span
class="math inline">\(\theta\)</span> ve <span
class="math inline">\(\lambda\)</span> için Gamma öncülleri, <span
class="math inline">\(k\)</span> için ise tüm olası yıllar üzerinde
birörnek (uniform) ayrık bir öncül kullanılmıştır.</p>
<p>El kodlu Gibbs örnekleyicisinde, her iterasyonda tam koşullu
dağılımlardan örnekleme yapılmıştır:</p>
<ul>
<li><p><span class="math inline">\(\theta\)</span> ve <span
class="math inline">\(\lambda\)</span>, Poisson–Gamma eşlenik yapısı
sayesinde koşullu Gamma dağılımlarından örneklenmiştir,</p></li>
<li><p><span class="math inline">\(k\)</span> ise her olası konum için
koşullu olasılıklar hesaplanarak bu olasılıklara göre rastgele
seçilmiştir.</p></li>
</ul>
<p>Bu yaklaşım, koşullu dağılımların analitik biçimde elde edilmesine ve
bu dağılımlar arasında dönüşümlü örnekleme yapılmasına dayanıyordu.</p>
<p>PyMC ile kurulan modelde ise aynı yapının tamamı sembolik olarak
tanımlanmıştır. Bu tanımlama sonrasında PyMC, ek bir kodlama
gerektirmeden ortak olasılık (joint posterior) dağılımını kendisi kurar.
Örnekleme aşamasında ise sürekli parametreler <span
class="math inline">\(( \theta, \lambda )\)</span> için NUTS (No-U-Turn
Sampler), ayrık değişken <span class="math inline">\(k\)</span> için ise
Metropolis algoritmasını otomatik olarak kullanır. Böylece, Gibbs
örnekleyicisinde elle yapılan koşullu örnekleme işlemleri PyMC
tarafından dahili olarak yürütülür.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.loadtxt(<span class="st">&quot;coal.txt&quot;</span>, dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>years <span class="op">=</span> <span class="dv">1851</span> <span class="op">+</span> np.arange(n)    <span class="co"># for reporting results</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># hiperparametreler</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>a1 <span class="op">=</span> a2 <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> b2 <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> pm.Model() <span class="im">as</span> model:</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iki Poisson orani icin onseller</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> pm.Gamma(<span class="st">&quot;theta&quot;</span>, alpha<span class="op">=</span>a1, beta<span class="op">=</span>b1)   <span class="co"># before the changepoint</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    lam   <span class="op">=</span> pm.Gamma(<span class="st">&quot;lam&quot;</span>,   alpha<span class="op">=</span>a2, beta<span class="op">=</span>b2)   <span class="co"># after the changepoint</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Degisim noktasi k icin ayriksal birornek onsel  (indeks in 0..n-1)</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> pm.DiscreteUniform(<span class="st">&quot;k&quot;</span>, lower<span class="op">=</span><span class="dv">0</span>, upper<span class="op">=</span>n <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.arange(n)                <span class="co"># numpy array of shape (n,)</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> pm.math.switch(idx <span class="op">&lt;=</span> k, theta, lam)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> pm.Poisson(<span class="st">&quot;y&quot;</span>, mu<span class="op">=</span>mu, observed<span class="op">=</span>data)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    graphviz <span class="op">=</span> pm.model_to_graphviz(model)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    graphviz.graph_attr.update(dpi<span class="op">=</span><span class="st">&quot;300&quot;</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    graphviz.render(<span class="st">&quot;stat_coal_04&quot;</span>, <span class="bu">format</span><span class="op">=</span><span class="st">&quot;jpg&quot;</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    nuts <span class="op">=</span> pm.NUTS(<span class="bu">vars</span><span class="op">=</span>[theta, lam], target_accept<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    metro <span class="op">=</span> pm.Metropolis(<span class="bu">vars</span><span class="op">=</span>[k])</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pm.sample(</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>       draws<span class="op">=</span><span class="dv">3000</span>,</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>       tune<span class="op">=</span><span class="dv">2000</span>,</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>       step<span class="op">=</span>[nuts, metro],</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>       random_seed<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>       return_inferencedata<span class="op">=</span><span class="va">True</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(az.summary(trace, var_names<span class="op">=</span>[<span class="st">&quot;theta&quot;</span>, <span class="st">&quot;lam&quot;</span>, <span class="st">&quot;k&quot;</span>], round_to<span class="op">=</span><span class="dv">3</span>))</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>k_samples <span class="op">=</span> trace.posterior[<span class="st">&quot;k&quot;</span>].values.flatten().astype(<span class="bu">int</span>)</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>k_mode <span class="op">=</span> np.bincount(k_samples).argmax()</span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Sonsal modal k indeksi = </span><span class="sc">{</span>k_mode<span class="sc">}</span><span class="ss">, sene = </span><span class="sc">{</span><span class="dv">1851</span> <span class="op">+</span> k_mode<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a><span class="co"># plot posterior of k</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>az.plot_posterior(trace, var_names<span class="op">=</span>[<span class="st">&quot;k&quot;</span>], hdi_prob<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Degisim noktasi indek k icin sonsallik&quot;</span>)</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_coal_01.jpg&#39;</span>)</span></code></pre></div>
<pre class="text"><code>                                                                                
                       Step   Grad                 Acce…   Sam…                 
  Pro…   Dra…   Div…   size   eva…   Tun…   Sca…   Rate    Spe…   Elap…   Rem…  
 -----------------------------------------------------------------------------
        5000   0      1.0…   3      Fal…   4.59   0.30    141…   0:00…   0:0…  
                                                           dra…                 
        5000   0      0.9…   3      Fal…   4.29   0.28    132…   0:00…   0:0…  
                                                           dra…                 
        5000   0      1.0…   7      Fal…   4.29   0.12    131…   0:00…   0:0…  
                                                           dra…                 
        5000   0      0.9…   7      Fal…   4.72   0.00    120…   0:00…   0:0…  
                                                           dra…                 
                                                                                
         mean     sd  hdi_3%  hdi_97%  ...  mcse_sd  ess_bulk  ess_tail  r_hat
theta   3.062  0.284   2.524    3.579  ...    0.002  7874.187  8317.128  1.000
lam     0.923  0.115   0.716    1.145  ...    0.001  7809.059  7082.257  1.000
k      39.162  2.423  34.000   43.000  ...    0.045  1977.323  1858.838  1.004

[3 rows x 9 columns]
Sonsal modal k indeksi = 40, sene = 1891</code></pre>
<p><img width='400' src='stat_coal_04.jpg'/></p>
<p>Elde edilen sonuçlar yorum bakımından tamamen aynıdır: değişim
noktası için ardıl dağılım yaklaşık olarak 1891 yılına karşılık gelir;
<span class="math inline">\(\theta\)</span> ve <span
class="math inline">\(\lambda\)</span> için ardıl ortalamalar sırasıyla
yaklaşık 3.06 ve 0.92 olarak bulunmuştur. Bu sonuçlar, el ile yazılmış
Gibbs örnekleyicisinde elde edilen klasik kömür madeni kazası analizi
sonuçlarıyla örtüşmektedir.</p>
<p>Fark esasen uygulamadadır: PyMC modeli aynı Bayesyen yapıyı çok daha
kısa, okunabilir bir şekilde ifade eder; örnekleme, yakınsama tanısı ve
ardıl tahmin (posterior predictive) işlemleri ise otomatik olarak ve
güvenilir biçimde yürütülür.</p>
<p><img src="stat_coal_01.jpg" /></p>
<p>Kaynaklar</p>
<p>[1] Ioana A. Cosma, Ludger Evers, <em>Markov Chain Monte Carlo
Methods (Lecture)</em></p>
<p>[2] Koop, <em>Bayesian Econometric Methods</em></p>
<p>[3] Anderson, A. (1911). Labour legislation. In H. Chisholm (Ed.),
<em>Encyclopedia britannica (11th ed., Vol. 16, sf. 7-28)</em></p>
<p>[4] Zuccini, <em>Hidden Markov Models for Time Series An Introduction
Using R</em></p>
<p>[5] Bayramlı, Istatistik, <em>Çok Değişkenli Bernoulli
Karışımı</em></p>
<p>[6] <em>Bayesian estimation of changepoints</em>, <a
href="https://ruivieira.dev/bayesian-estimation-of-changepoints.html">https://ruivieira.dev/bayesian-estimation-of-changepoints.html</a></p>
<p>[7] <em>Coal-Mine Accidents: Their Causes and Prevention</em>, <a
href="https://pubs.usgs.gov/bul/0333/report.pdf">https://pubs.usgs.gov/bul/0333/report.pdf</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
