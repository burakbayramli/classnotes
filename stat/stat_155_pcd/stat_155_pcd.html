<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Kalıcı CD (Persistent Contrastive Divergence -PCD-)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="kalıcı-cd-persistent-contrastive-divergence--pcd-">Kalıcı CD
(Persistent Contrastive Divergence -PCD-)</h1>
<p>Kısıtlı Boltzman Makinaları (RBM) yazısında gösterilen eğitim CD
(contrastive divergence) üzerinden idi. Amaç alttaki formülde, özellikle
eksiden sonraki terimi yaklaşıksal olarak hesaplamaktır.</p>
<p><span class="math display">\[
\sum_{n=1}^{N}  &lt; y_iy_j &gt;_{P(h|x^n;W)} - &lt; y_iy_j
&gt;_{P(x,h;W)}
\]</span></p>
<p>Bu terime basında eksi olduğu için negatif parçacıklar (negatıve
partıcles) ismi de veriliyor.</p>
<p>Şimdi RBM’de gördüğümüz CD’yi hatırlayalım, CD bir tür “tek adımlık
Gibbs örneklemesi’’ yapıyordu; bu tek adım örnekleme sonrasında bir
sonraki adım öncesi, veri, tekrar başlangıç noktası olarak zincire
veriliyordu. Yani her CD adımının başlangıcı illa ki verinin kendisi
olacaktır. Bu usul Gibbs’in veriden uzaklaşma şansı çok azdır. Fakat
çoğu ilginç yapay öğrenim verisi çok dorukludur (multimodal),
optimizasyon bağlamında düşünülürse birden fazla tepe (ya da çukur)
noktası içerir. Eğer eldeki veri, eğitimi bu noktalara yeterince
kanalize edemiyorsa o noktalar öğrenilmemiş olur. Bazen verinin (bile)
söylediğinden değişik yönleri gezebilen bir prosedür bu çokdoruklu alanı
gezmesi açısından daha başarılı olabilecektir.</p>
<p>PCD bu eksikleri düzeltmeye çabalar. PCD’ye göre modelden gelen
“negatif parçacıkların’’ örneklemesi arka planda, kendi başlarına
ilerler, ve bu zincir hiçbir zaman veriye, ya da başka bir şeye set
edilmez (hatta zincirin başlangıç noktası bile veriden alakasız olarak,
rasgele seçilir). Bu yönteme göre <span class="math inline">\(h^0,x^0,
h^1, x^1, ...\)</span> üretimi neredeyse tamamen”kapalı devre’’ kendi
kendine ilerleyen bir süreç olacaktır. Diğer yanda pozitif parçacıklar
veriden geliyor (ve tabii ki her gradyan adımı sonrası değişen <span
class="math inline">\(W\)</span> hem pozitif hem negatif parçacıkları
etkiler), ve bu al/ver ilişkisi, hatta bir bakıma model ile verinin
kapışmasının PCD’yi daha avantajlı hale getirdiği iddia edilir, ki PCD,
CD’den genellikle daha iyi öğrenim sağlar [5].</p>
<p>CD’ye kıyasla PCD’nin Gibbs ya da genel olarak MCMC örneklemesinin
prensibine daha yakın durduğu iddia edilebilir, çünkü PCD ile bir
örneklem zinciri kesintisiz olarak devam ettirilir.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> gen_even_slices</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RBM:  </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_hidden, num_visible, learning_rate,max_epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>               batch_size<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.num_hidden <span class="op">=</span> num_hidden</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.num_visible <span class="op">=</span> num_visible</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.weights <span class="op">=</span> <span class="fl">0.1</span> <span class="op">*</span> np.random.randn(<span class="va">self</span>.num_visible, <span class="va">self</span>.num_hidden)    </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.weights <span class="op">=</span> np.insert(<span class="va">self</span>.weights, <span class="dv">0</span>, <span class="dv">0</span>, axis <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.weights <span class="op">=</span> np.insert(<span class="va">self</span>.weights, <span class="dv">0</span>, <span class="dv">0</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.max_epochs <span class="op">=</span> max_epochs</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.batch_size <span class="op">=</span> batch_size</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> run_visible(<span class="va">self</span>, data):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    num_examples <span class="op">=</span> data.shape[<span class="dv">0</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    hidden_states <span class="op">=</span> np.ones((num_examples, <span class="va">self</span>.num_hidden <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.insert(data, <span class="dv">0</span>, <span class="dv">1</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    hidden_activations <span class="op">=</span> np.dot(data, <span class="va">self</span>.weights)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    hidden_probs <span class="op">=</span> <span class="va">self</span>._logistic(hidden_activations)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    hidden_states[:,:] <span class="op">=</span> hidden_probs <span class="op">&gt;</span> <span class="op">\</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        np.random.rand(num_examples, <span class="va">self</span>.num_hidden <span class="op">+</span> <span class="dv">1</span>)  </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    hidden_states <span class="op">=</span> hidden_states[:,<span class="dv">1</span>:]</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hidden_states</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>          </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> run_hidden(<span class="va">self</span>, data):</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    num_examples <span class="op">=</span> data.shape[<span class="dv">0</span>]</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    visible_states <span class="op">=</span> np.ones((num_examples, <span class="va">self</span>.num_visible <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.insert(data, <span class="dv">0</span>, <span class="dv">1</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    visible_activations <span class="op">=</span> np.dot(data, <span class="va">self</span>.weights.T)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    visible_probs <span class="op">=</span> <span class="va">self</span>._logistic(visible_activations)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>    visible_states[:,:] <span class="op">=</span> visible_probs <span class="op">&gt;</span> <span class="op">\</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        np.random.rand(num_examples, <span class="va">self</span>.num_visible <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    visible_states <span class="op">=</span> visible_states[:,<span class="dv">1</span>:]</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> visible_states</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> _logistic(<span class="va">self</span>, x):</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> _fit(<span class="va">self</span>, v_pos):</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    h_pos <span class="op">=</span> <span class="va">self</span>.run_visible(v_pos)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    v_neg <span class="op">=</span> <span class="va">self</span>.run_hidden(<span class="va">self</span>.h_samples_)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>    h_neg <span class="op">=</span> <span class="va">self</span>.run_visible(v_neg)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> <span class="bu">float</span>(<span class="va">self</span>.learning_rate) <span class="op">/</span> v_pos.shape[<span class="dv">0</span>]</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    v_pos <span class="op">=</span> np.insert(v_pos, <span class="dv">0</span>, <span class="dv">1</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    h_pos <span class="op">=</span> np.insert(h_pos, <span class="dv">0</span>, <span class="dv">1</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    v_neg <span class="op">=</span> np.insert(v_neg, <span class="dv">0</span>, <span class="dv">1</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>    h_neg <span class="op">=</span> np.insert(h_neg, <span class="dv">0</span>, <span class="dv">1</span>, axis <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    update <span class="op">=</span> np.dot(v_pos.T, h_pos).T</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>    update <span class="op">-=</span> np.dot(h_neg.T, v_neg)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.weights <span class="op">+=</span> lr <span class="op">*</span> update.T</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>    h_neg[np.random.rand(h_neg.shape[<span class="dv">0</span>], h_neg.shape[<span class="dv">1</span>]) <span class="op">&lt;</span> h_neg] <span class="op">=</span> <span class="fl">1.0</span> </span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.h_samples_ <span class="op">=</span> np.floor(h_neg, h_neg)[:,<span class="dv">1</span>:]</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> fit(<span class="va">self</span>, data):</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>    num_examples <span class="op">=</span> data.shape[<span class="dv">0</span>]</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.h_samples_ <span class="op">=</span> np.zeros((<span class="va">self</span>.batch_size, <span class="va">self</span>.num_hidden))</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    n_batches <span class="op">=</span> <span class="bu">int</span>(np.ceil(<span class="bu">float</span>(num_examples) <span class="op">/</span> <span class="va">self</span>.batch_size))</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>    batch_slices <span class="op">=</span> <span class="bu">list</span>(gen_even_slices(n_batches <span class="op">*</span> <span class="va">self</span>.batch_size,</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>                                        n_batches, num_examples))</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> iteration <span class="kw">in</span> <span class="bu">xrange</span>(<span class="dv">1</span>, <span class="va">self</span>.max_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_slice <span class="kw">in</span> batch_slices:</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._fit(data[batch_slice])</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:    </span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> RBM(num_hidden<span class="op">=</span><span class="dv">2</span>, num_visible<span class="op">=</span><span class="dv">3</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>,batch_size<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>    model.fit(X)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> model.weights</span></code></pre></div>
<p>Üstte görülen kod daha önce RBM için kullanılan kodla benzeşiyor,
sadece <code>fit</code> değişik, ve <code>_fit</code> eklendi. Bu kodda
miniparça (minibatch) kavramı da var, her gradyan adımı ufak verinin
mini parçaları üzerinden atılır. Bu parçalar hakikaten ufak, mesela 10
ila 100 satırlık veri arasındadırlar ve bu ilginç bir durumu ortaya
çıkartır, özellikle negatif parçacıklar için, ki bu parçacıklar <span
class="math inline">\(W\)</span> bağlantısı haricinde kendi başlarına
ilerler, çok az veri noktası ile işlem yapabilmektedirler.</p>
<p>Metot <code>fit</code> içinde <code>self.h_samples_</code>
değişkenine dikkat, bu değişken PCD’nin “kalıcı’’ olmasını sağlar, her
<code>_fit</code> çağrı sonrası negatif parçacık örneklemesi
<code>self.h_samples_</code> ’in bıraktığı yerden başlar.</p>
<p>RBM için kullandığımız aynı veri seti üzerine k-katlama ile test
edelim,</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cross_validation <span class="im">import</span> KFold</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, rbmp, sys</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/binarydigits.txt&#39;</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.ravel(np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/bindigitlabels.txt&#39;</span>))</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> []</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> KFold(n<span class="op">=</span><span class="bu">len</span>(X),n_folds<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> train, test <span class="kw">in</span> cv:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    X_train, Y_train <span class="op">=</span> X[train], Y[train]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    X_test, Y_test <span class="op">=</span> X[test], Y[test]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> rbmp.RBM(num_hidden<span class="op">=</span><span class="dv">40</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, max_epochs<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>                 num_visible<span class="op">=</span><span class="dv">64</span>, batch_size<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    r.fit(X_train)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> LogisticRegression(C<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    clf.fit(r.run_visible(X_train), Y_train)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    res3 <span class="op">=</span> clf.predict(r.run_visible(X_test))</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    scores.append(np.<span class="bu">sum</span>(res3<span class="op">==</span>Y_test) <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(Y_test)))        </span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> np.mean(scores)</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span> python test_rbmkfold.py</span></code></pre></div>
<pre><code>0.989898989899</code></pre>
<p>Daha çetrefil bir veri seti MNIST veri setine [2] bakalım. Veri 28x28
boyutunda ikisel veri olarak kodlanmış rakamların el yazısından alınmış
resimlerini içerir. Veri seti ünlü çünkü Derin Öğrenim’in ilk büyük
başarıları bu veri seti üzerinde paylaşıldı. MNIST’i aldıktan sonra
eğitim / test kısımlarının ilk 1000 tanesi üzerinde algoritmamızı
kullanırsak, tek komşulu KNN (yani 1-NN) yüzde 85.4 başarı sonucunu
verir. Alttaki parametreler üzerinden PCD ile RBM’in başarısı yüzde 86
olacaktır.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, gzip, sys</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> neighbors</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cross_validation <span class="im">import</span> train_test_split</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> gzip.<span class="bu">open</span>(<span class="st">&#39;/tmp/mnist.pkl.gz&#39;</span>, <span class="st">&#39;rb&#39;</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>train_set, valid_set, test_set <span class="op">=</span> cPickle.load(f)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>f.close()</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>X_train,y_train <span class="op">=</span> train_set</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>X_test,y_test <span class="op">=</span> valid_set</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train[:S]<span class="op">;</span> y_train <span class="op">=</span> y_train[:S]</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test[:S]<span class="op">;</span> y_test <span class="op">=</span> y_test[:S]</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> X_train.shape</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> neighbors.KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train, y_train)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;KNN&#39;</span>, clf.score(X_test, y_test)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> rbmp</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> rbmp.RBM(num_hidden<span class="op">=</span><span class="dv">500</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, max_epochs<span class="op">=</span><span class="dv">200</span>,</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>             num_visible<span class="op">=</span><span class="dv">784</span>,batch_size<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>r.fit(X_train)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> LogisticRegression(C<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>clf.fit(r.run_visible(X_train), y_train)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>res3 <span class="op">=</span> clf.predict(r.run_visible(X_test))</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;RBM&#39;</span>, np.<span class="bu">sum</span>(res3<span class="op">==</span>y_test) <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(y_test))</span></code></pre></div>
<p>Kaynaklar</p>
<p>[1] Tieleman, <em>Using Fast Weights to Improve Persistent
Contrastive Divergence</em>,<a
href="http://videolectures.net/icml09_tieleman_ufw/">http://videolectures.net/icml09_tieleman_ufw/</a></p>
<p>[2] Montreal Institute for Learning Algorithms, <em>MNIST Data</em>,
<a
href="http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz">http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz</a></p>
<p>[3] Bengio, Y., <em>Learning Deep Architectures for AI</em></p>
<p>[4] Larochelle, H., <em>Neural networks [5.6] : Restricted Boltzmann
machine - persistent CD</em>, <a
href="https://www.youtube.com/watch?v=S0kFFiHzR8M">https://www.youtube.com/watch?v=S0kFFiHzR8M</a></p>
<p>[5] Murphy, K. <em>Machine Learning A Probabilistic
Perspective</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
