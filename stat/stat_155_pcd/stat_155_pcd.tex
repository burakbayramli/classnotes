\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Kalıcı CD (Persistent Contrastive Divergence -PCD-)

Kısıtlı Boltzman Makinaları (RBM) yazısında gösterilen eğitim CD
(contrastive divergence) üzerinden idi. Amaç alttaki formülde, özellikle
eksiden sonraki terimi yaklaşıksal olarak hesaplamaktır. 

$$ 
\sum_{n=1}^{N}  < y_iy_j >_{P(h|x^n;W)} - < y_iy_j >_{P(x,h;W)} 
$$

Bu terime basında eksi olduğu için negatif parçacıklar (negatıve partıcles)
ismi de veriliyor. 

Şimdi RBM'de gördüğümüz CD'yi hatırlayalım, CD bir tür ``tek adımlık Gibbs
örneklemesi'' yapıyordu; bu tek adım örnekleme sonrasında bir sonraki adım
öncesi, veri, tekrar başlangıç noktası olarak zincire veriliyordu. Yani her CD
adımının başlangıcı illa ki verinin kendisi olacaktır. Bu usul Gibbs'in veriden
uzaklaşma şansı çok azdır. Fakat çoğu ilginç yapay öğrenim verisi çok dorukludur
(multimodal), optimizasyon bağlamında düşünülürse birden fazla tepe (ya da
çukur) noktası içerir. Eğer eldeki veri, eğitimi bu noktalara yeterince kanalize
edemiyorsa o noktalar öğrenilmemiş olur. Bazen verinin (bile) söylediğinden
değişik yönleri gezebilen bir prosedür bu çokdoruklu alanı gezmesi açısından
daha başarılı olabilecektir.

PCD bu eksikleri düzeltmeye çabalar. PCD'ye göre modelden gelen ``negatif
parçacıkların'' örneklemesi arka planda, kendi başlarına ilerler, ve bu
zincir hiçbir zaman veriye, ya da başka bir şeye set edilmez (hatta
zincirin başlangıç noktası bile veriden alakasız olarak, rasgele
seçilir). Bu yönteme göre $h^0,x^0, h^1, x^1, ...$ üretimi neredeyse
tamamen ``kapalı devre'' kendi kendine ilerleyen bir süreç olacaktır. Diğer
yanda pozitif parçacıklar veriden geliyor (ve tabii ki her gradyan adımı
sonrası değişen $W$ hem pozitif hem negatif parçacıkları etkiler), ve bu
al/ver ilişkisi, hatta bir bakıma model ile verinin kapışmasının PCD'yi
daha avantajlı hale getirdiği iddia edilir, ki PCD, CD'den genellikle daha
iyi öğrenim sağlar [5].

CD'ye kıyasla PCD'nin Gibbs ya da genel olarak MCMC örneklemesinin
prensibine daha yakın durduğu iddia edilebilir, çünkü PCD ile bir örneklem
zinciri kesintisiz olarak devam ettirilir. 

\inputminted[fontsize=\footnotesize]{python}{rbmp.py}

Üstte görülen kod daha önce RBM için kullanılan kodla benzeşiyor, sadece
\verb!fit! değişik, ve \verb!_fit! eklendi. Bu kodda miniparça (minibatch)
kavramı da var, her gradyan adımı ufak verinin mini parçaları üzerinden
atılır. Bu parçalar hakikaten ufak, mesela 10 ila 100 satırlık veri
arasındadırlar ve bu ilginç bir durumu ortaya çıkartır, özellikle negatif
parçacıklar için, ki bu parçacıklar $W$ bağlantısı haricinde kendi başlarına
ilerler, çok az veri noktası ile işlem yapabilmektedirler.

Metot \verb!fit! içinde \verb!self.h_samples_! değişkenine dikkat, bu
değişken PCD'nin ``kalıcı'' olmasını sağlar, her \verb!_fit! çağrı sonrası
negatif parçacık örneklemesi \verb!self.h_samples_! 'in bıraktığı yerden
başlar.

RBM için kullandığımız aynı veri seti üzerine k-katlama ile test edelim,

\inputminted[fontsize=\footnotesize]{python}{test_rbmkfold.py}

\begin{minted}[fontsize=\footnotesize]{python}
! python test_rbmkfold.py
\end{minted}

\begin{verbatim}
0.989898989899
\end{verbatim}

Daha çetrefil bir veri seti MNIST veri setine [2] bakalım. Veri 28x28
boyutunda ikisel veri olarak kodlanmış rakamların el yazısından alınmış
resimlerini içerir. Veri seti ünlü çünkü Derin Öğrenim'in ilk büyük
başarıları bu veri seti üzerinde paylaşıldı. MNIST'i aldıktan sonra eğitim
/ test kısımlarının ilk 1000 tanesi üzerinde algoritmamızı kullanırsak, tek
komşulu KNN (yani 1-NN) yüzde 85.4 başarı sonucunu verir. Alttaki
parametreler üzerinden PCD ile RBM'in başarısı yüzde 86 olacaktır.

\inputminted[fontsize=\footnotesize]{python}{test_mnist.py}

Kaynaklar

[1] Tieleman, {\em Using Fast Weights to Improve Persistent Contrastive Divergence},\url{http://videolectures.net/icml09_tieleman_ufw/}

[2] Montreal Institute for Learning Algorithms, {\em MNIST Data}, \url{http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz}

[3] Bengio, Y., {\em Learning Deep Architectures for AI}

[4] Larochelle, H., {\em Neural networks [5.6] : Restricted Boltzmann machine - persistent CD},  \url{https://www.youtube.com/watch?v=S0kFFiHzR8M}

[5] Murphy, K. {\em Machine Learning A Probabilistic Perspective}

\end{document}
