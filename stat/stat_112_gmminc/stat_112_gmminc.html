<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Artımsal (Incremental) GMM</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="artımsal-incremental-gmm">Artımsal (Incremental) GMM</h1>
<p>Gaussian Karışım Modelleri (Mixtüre Models / GMM) [1]’de işlendi. O
yazı her biri <span class="math inline">\(d\)</span> boyutlu olabilecek
<span class="math inline">\(G\)</span> tane Gaussian’dan oluşan bir
karışım ile veriyi temsil edebilmekten bahsediyordu. Bu tür bir modelde
bilinmeyenler <span class="math inline">\(G\)</span> tane Gaussian’ın
parametreleriydi, yani her Gaussian için D x 1 boyutunda bir <span
class="math inline">\(\mu\)</span>, D x D boyutunda bir <span
class="math inline">\(\Sigma\)</span>, ve tüm karışım için karışım
seviyelerini gösteren D x 1 boyutunda bir <span
class="math inline">\(\pi\)</span> bulunması gerekiyordu. Eğer [2]
notasyonu ile gösterirsek, karışım olasılık dağılım fonksiyonu,</p>
<p><span class="math display">\[
p(x) = \sum _{g=1}^{G} \pi_g N(x | \mu_g, \Sigma_g)
\]</span></p>
<p>oluyordu, ki <span class="math inline">\(x \in R^{D}\)</span>.</p>
<p>Dikkat edersek [1] yazısı veriyi toptan (batch) işleyen bir yaklaşım
gösterdi, bu yaklaşım, ki Beklenti-Maksimizasyon
(Expectation-Maximization / EM) adıyla bilinir, döngüsünün her adımında
verinin tamamını işliyordu. Bu yazıda veriyi azar azar, artımsal şekilde
işleyerek yaklaşık EM yapmanın tekniğini göreceğiz. Eğer her veri
noktası aynı şekilde dağılmış, bağımsız şekilde geliyorsa uzun vadede bu
artımsal öğrenim tekniği normal toptan EM’in bulduğu sonuca
yaklaşacaktır.</p>
<p>Notasyon şöyle, veriler <span class="math inline">\(x_i\)</span>
olarak geliyor, <span class="math inline">\(i\)</span> bir indistir, ve
<span class="math inline">\(i = 1,..,m,m+1\)</span> olarak gider. Bizi
ilgilendiren önemli an <span class="math inline">\(m\)</span> anından
<span class="math inline">\(m+1\)</span> anına geçiş, yani <span
class="math inline">\(m\)</span>’ye kadar olan model parametrelerini
<span class="math inline">\(m+1\)</span>’inci veri gelince ona göre
güncellemek.</p>
<h3 id="tüm-veriyle-artımsal">Tüm Veriyle Artımsal</h3>
<p>Bu bağlamda ilk varsayımı, yaklaşıksallamayı şöyle yapalım, tek bir
veri noktası eklediğimizde GMM modelinin parametrelerinin çok fazla
değişmesini beklemeyiz, o zaman GMM karışımının içindeki <span
class="math inline">\(g\)</span>’inci Gaussian <span
class="math inline">\(x_{i+1}\)</span> verisi üzerinde hesapladığı
olasılık / sorumluluk (responsibility) şöyle gösterilebilir,</p>
<p><span class="math display">\[
p^{(m+1)}(C_g | \mathbf{x}_i) \approx p^{(m)}(C_g | \mathbf{x}_i)
\qquad (1)
\]</span></p>
<p>Karışım Ağırlıkları <span class="math inline">\(\pi_g\)</span></p>
<p>Her <span class="math inline">\(g\)</span> bileşeni için karışım
ağırlığı tüm <span class="math inline">\(m\)</span> verileri üzerinden
hesaplanan ortalama sorumluluktur. Bu hesap tabii ki toptan EM
mantığından geliyor, referans için [2],</p>
<p><span class="math display">\[
\pi_g^{(m)} = \frac{1}{m} \sum_{i=1}^{m} p^{(m)}(C_g | \mathbf{x}_i)
\]</span></p>
<p>Yeni <span class="math inline">\(x_{m+1}\)</span> verisi gelince
üstteki formül doğal olarak alttaki gibi olur,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} = \frac{1}{m+1} \sum_{i=1}^{m+1} p^{(m+1)}(C_g |
\mathbf{x}_i)
\]</span></p>
<p>Şimdi (1)’deki yaklaşıksallamayı kullanırız,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \frac{1}{m+1} \left( \sum_{i=1}^{m} p^{(m)}(C_g |
\mathbf{x}_i) + p^{(m)}(C_g | \mathbf{x}_{m+1}) \right)
\]</span></p>
<p>Hatırlarsak,</p>
<p><span class="math display">\[
\sum_{i=1}^{m} p^{(m)}(C_g | \mathbf{x}_i) = m \pi_g^{(m)}
\]</span></p>
<p>O zaman,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \frac{1}{m+1} \left( m \pi_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) \right)
\]</span></p>
<p>Burada bir tekrar düzenleme daha yapalım. Eşitliğin sağ tarafını
mevcut kestirme hesabı <span class="math inline">\(\pi_g^{(m)}\)</span>
artı bir düzeltme terimi olarak düşünelim. Bunu ortaya çıkartmak için
şöyle yazabiliriz,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \pi_g^{(m)} + \left[ \frac{1}{m+1} \left( m
\pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) \right) - \pi_g^{(m)}
\right]
\]</span></p>
<p>Üstte bir <span class="math inline">\(\pi_g^{(m)}\)</span> ekledik ve
bir tane de çıkarttık, böylece aslında hiçbir değişim yapmamış olduk.
Formül hala aynı şey söylüyor. Şimdi köşeli parantez içindekiler için
ortak bir bölen <span class="math inline">\((m+1)\)</span>
yaratalım,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \pi_g^{(m)} + \frac{1}{m+1} \left( m \pi_g^{(m)} +
p^{(m)}(C_g | \mathbf{x}_{m+1}) - (m+1)\pi_g^{(m)} \right)
\]</span></p>
<p>Parantez icindeki ifadeleri basitlestirelim,</p>
<p><span class="math display">\[
m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) - m\pi_g^{(m)} -
\pi_g^{(m)} = p^{(m)}(C_g | \mathbf{x}_{m+1}) - \pi_g^{(m)}
\]</span></p>
<p>Geri sokalım,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \pi_g^{(m)} + \frac{1}{m+1} \left( p^{(m)}(C_g |
\mathbf{x}_{m+1}) - \pi_g^{(m)} \right)
\]</span></p>
<p>Böylece karışım ağırlıkları için özyineli güncelleme formülüne
erişmiş olduk.</p>
<p>Ortalama <span class="math inline">\(\mu_g\)</span> Güncellemesi</p>
<p>Bileşen <span class="math inline">\(g\)</span>’nin ortalaması şöyle
tanımlıdır,</p>
<p><span class="math display">\[
\mu_g^{(m)} = \frac{1}{m \pi_g^{(m)}} \sum_{i=1}^{m} p^{(m)}(C_g |
\mathbf{x}_i) \mathbf{x}_i
\]</span></p>
<p>Bu ifade toptan EM matematiğinden geliyor [1].</p>
<p><span class="math inline">\(m \pi_g^{(m)}\)</span> ile bölüm
ifadesini eşitliğin sol tarafına geçirip ona <span
class="math inline">\(S_g^{(m)}\)</span> diyebiliriz,</p>
<p><span class="math display">\[
S_g^{(m)} = \sum_{i=1}^{m} p^{(m)}(C_g | \mathbf{x}_i) \mathbf{x}_i = m
\pi_g^{(m)} \mu_g^{(m)}
\qquad{(2)}
\]</span></p>
<p>Yeni veri noktası <span
class="math inline">\(\mathbf{x}_{m+1}\)</span> geldiğinde üstteki
toplama bir terim daha eklenmiş olacaktır, yani mevcut toplam <span
class="math inline">\(S_g^{(m)}\)</span> artı yeni veri için gereken
terim,</p>
<p><span class="math display">\[
S_g^{(m+1)} = S_g^{(m)} + p^{(m+1)}(C_g | \mathbf{x}_{m+1})
\mathbf{x}_{m+1}
\]</span></p>
<p>Bu noktada yine (1)’deki aynı yaklaşıksallamayı kullanabiliriz,</p>
<p><span class="math display">\[
S_g^{(m+1)} \approx S_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
\mathbf{x}_{m+1}
\]</span></p>
<p>Şimdi (2) formülündeki eşitliğin sol tarafını baz alırsak, ve bunu
<span class="math inline">\(m+1\)</span> için adapte edersek,</p>
<p><span class="math display">\[
S_g^{(m+1)} = (m+1) \pi_g^{(m+1)} \mu_g^{(m+1)}
\]</span></p>
<p>Ya da</p>
<p><span class="math display">\[
\mu_g^{(m+1)} = \frac{S_g^{(m+1)}}{(m+1) \pi_g^{(m+1)}}
\]</span></p>
<p>Üstteki formüle üç formül ustteki formülü sokalim,</p>
<p><span class="math display">\[
\mu_g^{(m+1)} \approx \frac{ S_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
\mathbf{x}_{m+1} }{ (m+1) \pi_g^{(m+1)} }
\]</span></p>
<p>Şimdi <span class="math inline">\(S_g^{(m)} = m \pi_g^{(m)}
\mu_g^{(m)}\)</span> ifadesini ve daha önceki <span
class="math inline">\(\pi_g^{(m+1)}\)</span> eşitliğini sok,</p>
<p><span class="math display">\[
\mu_g^{(m+1)} \approx \frac{ m \pi_g^{(m)} \mu_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) \mathbf{x}_{m+1} }{ (m+1) \left[ \pi_g^{(m)} +
\frac{1}{m+1} \left( p^{(m)}(C_g | \mathbf{x}_{m+1}) - \pi_g^{(m)}
\right) \right] }
\]</span></p>
<p>Böleni basitleştir,</p>
<p><span class="math display">\[
(m+1) \pi_g^{(m+1)} = m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
\]</span></p>
<p>O zaman</p>
<p><span class="math display">\[
\mu_g^{(m+1)} \approx \frac{ m \pi_g^{(m)} \mu_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) \mathbf{x}_{m+1} }{ m \pi_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) }
\]</span></p>
<p>Şöyle de yazılabilir,</p>
<p><span class="math display">\[
\mu_g^{(m+1)} = \mu_g^{(m)} + \frac{ p^{(m)}(C_g | \mathbf{x}_{m+1}) }{
m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) } \left(
\mathbf{x}_{m+1} - \mu_g^{(m)} \right)
\]</span></p>
<p>Şimdi, alttaki ifadenin doğru olduğunu bildiğimize göre</p>
<p><span class="math display">\[
m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) \approx (m+1)
\pi_g^{(m+1)}
\]</span></p>
<p>Büyük <span class="math inline">\(m\)</span> için alttaki de doğru
olmalıdır.</p>
<p><span class="math display">\[
\mu_g^{(m+1)} \approx \mu_g^{(m)} + \frac{1}{m+1} \cdot \frac{
p^{(m)}(C_g | \mathbf{x}_{m+1}) }{ \pi_g^{(m)} } \left( \mathbf{x}_{m+1}
- \mu_g^{(m)} \right)
\]</span></p>
<p>Böylece <span class="math inline">\(\mu_g\)</span> güncellemesini
elde etmiş oluyoruz.</p>
<p>Kovaryans <span class="math inline">\(\Sigma_g\)</span>
Güncellemesi</p>
<p>Bileşen <span class="math inline">\(g\)</span> için kovaryans</p>
<p><span class="math display">\[
\Sigma_g^{(m)} = \frac{1}{m \pi_g^{(m)}} \sum_{i=1}^{m} p^{(m)}(C_g |
\mathbf{x}_i) (\mathbf{x}_i - \mu_g^{(m)}) (\mathbf{x}_i -
\mu_g^{(m)})^T
\]</span></p>
<p>Şu tanımı öne sürelim,</p>
<p><span class="math display">\[
T_g^{(m)} = \sum_{i=1}^{m} p^{(m)}(C_g | \mathbf{x}_i) (\mathbf{x}_i -
\mu_g^{(m)}) (\mathbf{x}_i - \mu_g^{(m)})^T = m \pi_g^{(m)}
\Sigma_g^{(m)}
\]</span></p>
<p>Yeni veri noktası için</p>
<p><span class="math display">\[
T_g^{(m+1)} \approx T_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
(\mathbf{x}_{m+1} - \mu_g^{(m+1)}) (\mathbf{x}_{m+1} - \mu_g^{(m+1)})^T
\]</span></p>
<p>Şimdi <span class="math inline">\(\mu_g^{(m+1)}\)</span> güncellemesi
üzerinden de bir yaklaşıksallama tanımlayacağız,</p>
<p><span class="math display">\[
(\mathbf{x}_{m+1} - \mu_g^{(m+1)}) \approx (\mathbf{x}_{m+1} -
\mu_g^{(m)})
\]</span></p>
<p>O zaman</p>
<p><span class="math display">\[
T_g^{(m+1)} \approx T_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
(\mathbf{x}_{m+1} - \mu_g^{(m)}) (\mathbf{x}_{m+1} - \mu_g^{(m)})^T
\]</span></p>
<p>Yani</p>
<p><span class="math display">\[
\Sigma_g^{(m+1)} = \frac{T_g^{(m+1)}}{(m+1) \pi_g^{(m+1)}}
\]</span></p>
<p>Yerine geçir</p>
<p><span class="math display">\[
\Sigma_g^{(m+1)} \approx \frac{ T_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) (\mathbf{x}_{m+1} - \mu_g^{(m)}) (\mathbf{x}_{m+1} -
\mu_g^{(m)})^T }{ (m+1) \pi_g^{(m+1)} }
\]</span></p>
<p><span class="math inline">\(T_g^{(m)} = m \pi_g^{(m)}
\Sigma_g^{(m)}\)</span> eşitliğini kullanarak ve <span
class="math inline">\(\pi_g^{(m+1)}\)</span> formülü ile alttakini elde
ederiz,</p>
<p><span class="math display">\[
\Sigma_g^{(m+1)} \approx \frac{ m \pi_g^{(m)} \Sigma_g^{(m)} +
p^{(m)}(C_g | \mathbf{x}_{m+1}) (\mathbf{x}_{m+1} - \mu_g^{(m)})
(\mathbf{x}_{m+1} - \mu_g^{(m)})^T }{ m \pi_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) }
\]</span></p>
<p>Bu şöyle de yazılabilir</p>
<p><span class="math display">\[
\Sigma_g^{(m+1)} = \Sigma_g^{(m)} + \frac{ p^{(m)}(C_g |
\mathbf{x}_{m+1}) }{ m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) }
\left[ (\mathbf{x}_{m+1} - \mu_g^{(m)}) (\mathbf{x}_{m+1} -
\mu_g^{(m)})^T - \Sigma_g^{(m)} \right]
\]</span></p>
<p>Tekrar büyük <span class="math inline">\(m\)</span> için
yaklaşıksallama yapıyoruz</p>
<p><span class="math display">\[  
\Sigma_g^{(m+1)} \approx \Sigma_g^{(m)} + \frac{1}{m+1} \cdot \frac{
p^{(m)}(C_g | \mathbf{x}_{m+1}) }{ \pi_g^{(m)} } \left[
(\mathbf{x}_{m+1} - \mu_g^{(m)}) (\mathbf{x}_{m+1} - \mu_g^{(m)})^T -
\Sigma_g^{(m)} \right]
\]</span></p>
<p>Böylece kovaryans güncellemesini elde etmiş olduk.</p>
<p>Ornek</p>
<p>Alttaki kodda önce 2 boyutlu veri baz alındı, iki Gaussian tepesini
karışımından rasgele örneklem alıyoruz.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_gmm_data(weights, means, covs, n_samples):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    n_components <span class="op">=</span> <span class="bu">len</span>(weights)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.zeros((n_samples, <span class="dv">2</span>))    </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    component_choices <span class="op">=</span> np.random.choice(n_components, size<span class="op">=</span>n_samples, p<span class="op">=</span>weights)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.where(component_choices <span class="op">==</span> i)[<span class="dv">0</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        n_points_to_sample <span class="op">=</span> <span class="bu">len</span>(indices)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n_points_to_sample <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            data[indices, :] <span class="op">=</span> np.random.multivariate_normal(</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                mean<span class="op">=</span>means[i], </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                cov<span class="op">=</span>covs[i], </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                size<span class="op">=</span>n_points_to_sample</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    shuffle_indices <span class="op">=</span> np.arange(<span class="bu">len</span>(data))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(shuffle_indices)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    res_data <span class="op">=</span> data[shuffle_indices]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res_data</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gmm_pdf(x, y, weights, means, covs):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;(x,y) noktasinda GMM olasilik yogunlugunu hesapla&quot;&quot;&quot;</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        rv <span class="op">=</span> multivariate_normal(means[i], covs[i])</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        z <span class="op">+=</span> weights[i] <span class="op">*</span> rv.pdf([x, y])</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [<span class="fl">0.7</span>, <span class="fl">0.3</span>]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> [</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">5</span>],</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">5</span>, <span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>covs <span class="op">=</span> [</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">2</span>, <span class="fl">1.5</span>], [<span class="fl">1.5</span>, <span class="dv">3</span>]],</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">10</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">5</span>]]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>n_points <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data points</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>gmm_data <span class="op">=</span> generate_gmm_data(weights, means, covs, n_points)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plt.scatter(gmm_data[:, <span class="dv">0</span>], gmm_data[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;X-coordinate&quot;</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Y-coordinate&quot;</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_112_gmminc_01.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_112_gmminc_01.png" /></p>
<p>Grafikte iki tepenin aşağı yukarı nerede olduğu görülüyor.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> responsibilities(x, weights, means, covs):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.zeros(<span class="bu">len</span>(weights))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        p[k] <span class="op">=</span> weights[k] <span class="op">*</span> multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>covs[k]).pdf(x)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> p.<span class="bu">sum</span>()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> denom <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.ones_like(p) <span class="op">/</span> <span class="bu">len</span>(p)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p <span class="op">/</span> denom</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(X, weights, means, covs):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    ll <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> X:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        px <span class="op">=</span> <span class="bu">sum</span>(weights[k] <span class="op">*</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>                 multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>covs[k]).pdf(x)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        ll <span class="op">+=</span> np.log(px <span class="op">+</span> <span class="fl">1e-12</span>)  <span class="co"># guard against log(0)</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ll</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> generate_gmm_data(weights, means, covs, n_points)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>n_components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> data.shape[<span class="dv">1</span>]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>weights_tmp <span class="op">=</span> np.ones(n_components) <span class="op">/</span> n_components</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>means_tmp <span class="op">=</span> np.random.randn(n_components, d) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>covs_tmp <span class="op">=</span> np.array([np.eye(d) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_components)])</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>m_total <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>cumulative_r <span class="op">=</span> weights <span class="op">*</span> m_total</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data)):</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> data[idx]</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> responsibilities(x, weights_tmp, means_tmp, covs_tmp)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    m_total <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update cumulative responsibilities</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        cumulative_r[k] <span class="op">+=</span> r[k]</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        weights_tmp[k] <span class="op">=</span> cumulative_r[k] <span class="op">/</span> m_total</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        mu_old <span class="op">=</span> means_tmp[k].copy()</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        coef <span class="op">=</span> (<span class="fl">1.0</span> <span class="op">/</span> m_total) <span class="op">*</span> (r[k] <span class="op">/</span> (weights_tmp[k] <span class="op">+</span> <span class="fl">1e-12</span>))</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        means_tmp[k] <span class="op">=</span> mu_old <span class="op">+</span> coef <span class="op">*</span> (x <span class="op">-</span> mu_old)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        diff_var <span class="op">=</span> (x <span class="op">-</span> mu_old).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        covs_tmp[k] <span class="op">=</span> covs_tmp[k] <span class="op">+</span> coef <span class="op">*</span> (diff_var <span class="op">@</span> diff_var.T <span class="op">-</span> covs_tmp[k])</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> log_likelihood(data, weights_tmp, means_tmp, covs_tmp)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (ll)</span></code></pre></div>
<pre class="text"><code>-4373.507980537245</code></pre>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (weights_tmp)        </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (means_tmp)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (covs_tmp)</span></code></pre></div>
<pre class="text"><code>[0.29698269 0.70301731]
[[ 4.95850944 -1.96333327]
 [-0.09937333  4.98941925]]
[[[12.4988802  -3.52676769]
  [-3.52676769  4.98789128]]

 [[ 2.01466638  1.36292997]
  [ 1.36292997  3.1163339 ]]]</code></pre>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">120</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">120</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>Xg, Yg <span class="op">=</span> np.meshgrid(xs, ys)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Z_true <span class="op">=</span> np.zeros_like(Xg)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>Z_learned <span class="op">=</span> np.zeros_like(Xg)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Xg.shape[<span class="dv">0</span>]):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(Xg.shape[<span class="dv">1</span>]):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        pt <span class="op">=</span> np.array([Xg[i, j], Yg[i, j]])</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        Z_true[i, j] <span class="op">=</span> <span class="bu">sum</span>(weights[k] <span class="op">*</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                           multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>covs[k]).pdf(pt)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        Z_learned[i, j] <span class="op">=</span> <span class="bu">sum</span>(weights_tmp[k] <span class="op">*</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                              multivariate_normal(mean<span class="op">=</span>means_tmp[k], cov<span class="op">=</span>covs_tmp[k]).pdf(pt)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                              <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components))</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>), constrained_layout<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>ax1, ax2 <span class="op">=</span> axes</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>cf1 <span class="op">=</span> ax1.contourf(Xg, Yg, Z_true, levels<span class="op">=</span><span class="dv">40</span>, cmap<span class="op">=</span><span class="st">&#39;viridis&#39;</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>ax1.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">&#39;white&#39;</span>, s<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>            edgecolors<span class="op">=</span><span class="st">&#39;k&#39;</span>, linewidth<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">&quot;Veri&quot;</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">&quot;y&quot;</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>fig.colorbar(cf1, ax<span class="op">=</span>ax1, fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>cf2 <span class="op">=</span> ax2.contourf(Xg, Yg, Z_learned, levels<span class="op">=</span><span class="dv">40</span>, cmap<span class="op">=</span><span class="st">&#39;viridis&#39;</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>ax2.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">&#39;white&#39;</span>, s<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>            edgecolors<span class="op">=</span><span class="st">&#39;k&#39;</span>, linewidth<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">u&#39;Artımsal GMM ile Öğrenilmiş PDF&#39;</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">&quot;y&quot;</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>fig.colorbar(cf2, ax<span class="op">=</span>ax2, fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_112_gmminc_02.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_112_gmminc_02.png" /></p>
<h3 id="ewma-usulü-artımsal">EWMA Usulü Artımsal</h3>
<p>Şimdiye kadar gördüklerimiz artımsal güncelleme yapıyor olsa bile
görülen tüm verilere aynı ağırlığı / önemi veriyordu. Fakat son görülen
verilere daha fazla ağırlık veren ve eskileri (bir parametreye bağlı
olarak) yavaş yavaş “unutan” bir yaklaşım bazen daha faydalı olabilir.
EWMA yaklaşımı burada devreye girebilir, [5] yazısında gösterildiği gibi
EWMA’nın kabaca bir kaydırılan pencere içindeki verileri işleme mantığı
vardır (ve bu ‘etkili pencere’ büyüklüğü hesaplanabilir) böylece eski
verilerin yavaşça unutulması sağlanabilir.</p>
<p>EWMA yaklaşımında bir <span class="math inline">\(\lambda\)</span>
parametresi vardır, o zaman önceki güncelleme formüllerini bir <span
class="math inline">\(\lambda\)</span> içerecek şekilde tekrar
düzenlersek [2],</p>
<p><span class="math display">\[
\pi_g^{m+1} = \pi_g^{m} + \lambda [ p^{(m)} (C_g | x_{m+1}) - \pi_g^{m}
]
\]</span></p>
<p><span class="math display">\[
\mu_g^{(m+1)} = \mu_g^{(m)} + \lambda \frac{ p^{(m)}(C_g |
\mathbf{x}_{m+1}) }{ \pi_g^{(m)} } \left( \mathbf{x}_{m+1} - \mu_g^{(m)}
\right)
\]</span></p>
<p><span class="math display">\[  
\Sigma_g^{(m+1)} = \Sigma_g^{(m)} + \lambda \frac{ p^{(m)}(C_g |
\mathbf{x}_{m+1}) }{ \pi_g^{(m)} } \left[ (\mathbf{x}_{m+1} -
\mu_g^{(m)}) (\mathbf{x}_{m+1} - \mu_g^{(m)})^T - \Sigma_g^{(m)} \right]
\]</span></p>
<p>Üstteki formüller aslında daha önceki güncelleme formüllerinin
genelleşmiş halidir denebilir, eğer <span class="math inline">\(\lambda
= \frac{1}{m+1}\)</span> dersek, ve sürekli artan <span
class="math inline">\(m\)</span> bağlamında önceki formüllerin aynısını
elde ederiz. Fakat farklı bir unutma faktörü <span
class="math inline">\(\lambda\)</span> verirsek, mesela <span
class="math inline">\(\lambda=0.999\)</span>, ve onun tekabül ettiği
etkili pencere (örneklem) büyüklüğünü <span
class="math inline">\(m\)</span> için kullanırsak, o zaman EWMA usulü
bir GMM güncellemesi elde etmiş oluyoruz.</p>
<p>EWMA güncellemesi kodu,</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_gmm_data(weights, means, covs, n_samples):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> np.random.RandomState()</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    n_components <span class="op">=</span> <span class="bu">len</span>(weights)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> <span class="bu">len</span>(means[<span class="dv">0</span>])</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.zeros((n_samples, d))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    comps <span class="op">=</span> rng.choice(n_components, size<span class="op">=</span>n_samples, p<span class="op">=</span>weights)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.where(comps <span class="op">==</span> k)[<span class="dv">0</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(idx) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>            data[idx] <span class="op">=</span> rng.multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>covs[k], size<span class="op">=</span><span class="bu">len</span>(idx))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    rng.shuffle(data)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> data</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> responsibilities(x, weights, means, covs, eps<span class="op">=</span><span class="fl">1e-12</span>):</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.zeros(<span class="bu">len</span>(weights))</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># enforce SPD-ish covariance</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        cov <span class="op">=</span> (covs[k] <span class="op">+</span> covs[k].T) <span class="op">/</span> <span class="fl">2.0</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        cov <span class="op">=</span> cov <span class="op">+</span> np.eye(cov.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        rv <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>cov, allow_singular<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        p[k] <span class="op">=</span> weights[k] <span class="op">*</span> rv.pdf(x)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> p.<span class="bu">sum</span>()</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> denom <span class="op">&lt;</span> eps:</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.ones_like(p) <span class="op">/</span> <span class="bu">len</span>(p)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p <span class="op">/</span> denom</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(X, weights, means, covs, eps<span class="op">=</span><span class="fl">1e-12</span>):</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    ll <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> X:</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        px <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)):</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>            cov <span class="op">=</span> (covs[k] <span class="op">+</span> covs[k].T) <span class="op">/</span> <span class="fl">2.0</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>            cov <span class="op">=</span> cov <span class="op">+</span> np.eye(cov.shape[<span class="dv">0</span>]) <span class="op">*</span> <span class="fl">1e-6</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>            rv <span class="op">=</span> multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>cov, allow_singular<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>            px <span class="op">+=</span> weights[k] <span class="op">*</span> rv.pdf(x)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        ll <span class="op">+=</span> np.log(px <span class="op">+</span> eps)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ll</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    n_points <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> generate_gmm_data(weights, means, covs, n_points)</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>    d <span class="op">=</span> data.shape[<span class="dv">1</span>]</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    lambda_forget <span class="op">=</span> <span class="fl">0.1</span> </span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>    n_components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>    weights_tmp <span class="op">=</span> np.ones(n_components) <span class="op">/</span> n_components</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    means_tmp <span class="op">=</span> np.random.randn(n_components, d) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>    covs_tmp <span class="op">=</span> np.array([np.eye(d) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_components)])</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    min_covar <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> <span class="fl">1e-12</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, x <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>        r <span class="op">=</span> responsibilities(x, weights_tmp, means_tmp, covs_tmp)</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>        weights_tmp <span class="op">=</span> weights_tmp <span class="op">+</span> lambda_forget <span class="op">*</span> (r <span class="op">-</span> weights_tmp)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        weights_tmp <span class="op">=</span> np.maximum(weights_tmp, eps)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        weights_tmp <span class="op">/=</span> np.<span class="bu">sum</span>(weights_tmp)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>            pi_k <span class="op">=</span> <span class="bu">max</span>(weights_tmp[k], eps)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>            mu_old <span class="op">=</span> means_tmp[k].copy()</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>            means_tmp[k] <span class="op">=</span> mu_old <span class="op">+</span> lambda_forget <span class="op">*</span> (r[k] <span class="op">/</span> pi_k) <span class="op">*</span> (x <span class="op">-</span> mu_old)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>            diff <span class="op">=</span> (x <span class="op">-</span> mu_old).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>            outer <span class="op">=</span> diff <span class="op">@</span> diff.T</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>            covs_tmp[k] <span class="op">=</span> covs_tmp[k] <span class="op">+</span> lambda_forget <span class="op">*</span> (r[k] <span class="op">/</span> pi_k) <span class="op">*</span> (outer <span class="op">-</span> covs_tmp[k])</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>            covs_tmp[k] <span class="op">=</span> (covs_tmp[k] <span class="op">+</span> covs_tmp[k].T) <span class="op">/</span> <span class="fl">2.0</span></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>            diag <span class="op">=</span> np.diag(covs_tmp[k])</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>            diag <span class="op">=</span> np.maximum(diag, min_covar)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>            covs_tmp[k] <span class="op">=</span> covs_tmp[k] <span class="op">-</span> np.diag(np.diag(covs_tmp[k])) <span class="op">+</span> np.diag(diag)</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (idx <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">200</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>            ll <span class="op">=</span> log_likelihood(data[:<span class="dv">200</span>], weights_tmp, means_tmp, covs_tmp)</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;iter </span><span class="sc">{</span>idx<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: partial log-likelihood = </span><span class="sc">{</span>ll<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>    ll_final <span class="op">=</span> log_likelihood(data, weights_tmp, means_tmp, covs_tmp)</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Nihai log olurluk: </span><span class="sc">{</span>ll_final<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Hesaplanan Karisim Agirliklari:&quot;</span>, weights_tmp)</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Hesaplanan Ortalama:</span><span class="ch">\n</span><span class="st">&quot;</span>, means_tmp)</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Hesaplanan Kovaryans:</span><span class="ch">\n</span><span class="st">&quot;</span>, covs_tmp)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Unutma Faktoru:&quot;</span>, lambda_forget)</span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Orneklem:&quot;</span>, n_points)</span></code></pre></div>
<pre class="text"><code>iter 200: partial log-likelihood = -976.455
iter 400: partial log-likelihood = -937.786
iter 600: partial log-likelihood = -955.248
iter 800: partial log-likelihood = -999.386
iter 1000: partial log-likelihood = -936.467
Nihai log olurluk: -4604.283
Hesaplanan Karisim Agirliklari: [0.70080574 0.29919426]
Hesaplanan Ortalama:
 [[-0.35565126  4.74680112]
 [ 4.33770918 -1.21072193]]
Hesaplanan Kovaryans:
 [[[ 1.67484021  0.68862933]
  [ 0.68862933  1.41498905]]

 [[ 9.57519709 -2.98166738]
  [-2.98166738  4.51008999]]]
Unutma Faktoru: 0.1
Orneklem: 1000</code></pre>
<p>Kaynaklar</p>
<p>[1] Bayramli, <em>Istatistik, Gaussian Karışım Modeli (GMM) ile
Kümelemek</em></p>
<p>[2] Zheng, <em>Recursive Gaussian Mixture Models for Adaptive Process
Monitoring</em></p>
<p>[3] Titterington, <em>Recursive Parameter Estimation using Incomplete
Data</em></p>
<p>[4] Zivkovic, <em>Recursive unsupervised learning of finite mixture
models</em></p>
<p>[5] Bayramli, <em>Zaman Serileri - ARIMA, ARCH, GARCH, Periyotlar,
Yürüyen Ortalama</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
