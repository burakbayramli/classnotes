<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Artımsal (Incremental) GMM</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="artımsal-incremental-gmm">Artımsal (Incremental) GMM</h1>
<p>Gaussian Karışım Modelleri (Mixtüre Models / GMM) [1]’de işlendi. O
yazı her biri <span class="math inline">\(d\)</span> boyutlu olabilecek
<span class="math inline">\(G\)</span> tane Gaussian’dan oluşan bir
karışım ile veriyi temsil edebilmekten bahsediyordu. Bu tür bir modelde
bilinmeyenler <span class="math inline">\(G\)</span> tane Gaussian’ın
parametreleriydi, yani her Gaussian için D x 1 boyutunda bir <span
class="math inline">\(\mu\)</span>, D x D boyutunda bir <span
class="math inline">\(\Sigma\)</span>, ve tüm karışım için karışım
seviyelerini gösteren D x 1 boyutunda bir <span
class="math inline">\(\pi\)</span> bulunması gerekiyordu. Eğer [2]
notasyonu ile gösterirsek, karışım olasılık dağılım fonksiyonu,</p>
<p><span class="math display">\[
p(x) = \sum _{g=1}^{G} \pi_g N(x | \mu_g, \Sigma_g)
\]</span></p>
<p>oluyordu, ki <span class="math inline">\(x \in R^{D}\)</span>.</p>
<p>Dikkat edersek [1] yazısı veriyi toptan (batch) işleyen bir yaklaşım
gösterdi, bu yaklaşım, ki Beklenti-Maksimizasyon
(Expectation-Maximization / EM) adıyla bilinir, döngüsünün her adımında
verinin tamamını işliyordu. Bu yazıda veriyi azar azar, artımsal şekilde
işleyerek yaklaşık EM yapmanın tekniğini göreceğiz. Eğer her veri
noktası aynı şekilde dağılmış, bağımsız şekilde geliyorsa uzun vadede bu
artımsal öğrenim tekniği normal toptan EM’in bulduğu sonuca
yaklaşacaktır.</p>
<p>Notasyon şöyle, veriler <span class="math inline">\(x_i\)</span>
olarak geliyor, <span class="math inline">\(i\)</span> bir indistir, ve
<span class="math inline">\(i = 1,..,m,m+1\)</span> olarak gider. Bizi
ilgilendiren önemli an <span class="math inline">\(m\)</span> anından
<span class="math inline">\(m+1\)</span> anına geçiş, yani <span
class="math inline">\(m\)</span>’ye kadar olan model parametrelerini
<span class="math inline">\(m+1\)</span>’inci veri gelince ona göre
güncellemek.</p>
<p>Bu bağlamda ilk varsayımı, yaklaşıksallamayı şöyle yapalım, tek bir
veri noktası eklediğimizde GMM modelinin parametrelerinin çok fazla
değişmesini beklemeyiz, o zaman GMM karışımının içindeki <span
class="math inline">\(g\)</span>’inci Gaussian <span
class="math inline">\(x_{i+1}\)</span> verisi üzerinde hesapladığı
olasılık / sorumluluk (responsibility) şöyle gösterilebilir,</p>
<p><span class="math display">\[
p^{(m+1)}(C_g | \mathbf{x}_i) \approx p^{(m)}(C_g | \mathbf{x}_i)
\qquad (1)
\]</span></p>
<h3 id="karışım-ağırlıkları-pi_g">Karışım Ağırlıkları <span
class="math inline">\(\pi_g\)</span></h3>
<p>Her <span class="math inline">\(g\)</span> bileşeni için karışım
ağırlığı tüm <span class="math inline">\(m\)</span> verileri üzerinden
hesaplanan ortalama sorumluluktur. Bu hesap tabii ki toptan EM
mantığından geliyor, referans için [2],</p>
<p><span class="math display">\[
\pi_g^{(m)} = \frac{1}{m} \sum_{i=1}^{m} p^{(m)}(C_g | \mathbf{x}_i)
\]</span></p>
<p>Yeni <span class="math inline">\(x_{m+1}\)</span> verisi gelince
üstteki formül doğal olarak alttaki gibi olur,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} = \frac{1}{m+1} \sum_{i=1}^{m+1} p^{(m+1)}(C_g |
\mathbf{x}_i)
\]</span></p>
<p>Şimdi (1)’deki yaklaşıksallamayı kullanırız,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \frac{1}{m+1} \left( \sum_{i=1}^{m} p^{(m)}(C_g |
\mathbf{x}_i) + p^{(m)}(C_g | \mathbf{x}_{m+1}) \right)
\]</span></p>
<p>Hatırlarsak,</p>
<p><span class="math display">\[
\sum_{i=1}^{m} p^{(m)}(C_g | \mathbf{x}_i) = m \pi_g^{(m)}
\]</span></p>
<p>O zaman,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \frac{1}{m+1} \left( m \pi_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) \right)
\]</span></p>
<p>Burada bir tekrar düzenleme daha yapalım. Eşitliğin sağ tarafını
mevcut kestirme hesabı <span class="math inline">\(\pi_g^{(m)}\)</span>
artı bir düzeltme terimi olarak düşünelim. Bunu ortaya çıkartmak için
şöyle yazabiliriz,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \pi_g^{(m)} + \left[ \frac{1}{m+1} \left( m
\pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) \right) - \pi_g^{(m)}
\right]
\]</span></p>
<p>Üstte bir <span class="math inline">\(\pi_g^{(m)}\)</span> ekledik ve
bir tane de çıkarttık, böylece aslında hiçbir değişim yapmamış olduk.
Formül hala aynı şey söylüyor. Şimdi köşeli parantez içindekiler için
ortak bir bölen <span class="math inline">\((m+1)\)</span>
yaratalım,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \pi_g^{(m)} + \frac{1}{m+1} \left( m \pi_g^{(m)} +
p^{(m)}(C_g | \mathbf{x}_{m+1}) - (m+1)\pi_g^{(m)} \right)
\]</span></p>
<p>Parantez icindeki ifadeleri basitlestirelim,</p>
<p><span class="math display">\[
m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) - m\pi_g^{(m)} -
\pi_g^{(m)} = p^{(m)}(C_g | \mathbf{x}_{m+1}) - \pi_g^{(m)}
\]</span></p>
<p>Geri sokalım,</p>
<p><span class="math display">\[
\pi_g^{(m+1)} \approx \pi_g^{(m)} + \frac{1}{m+1} \left( p^{(m)}(C_g |
\mathbf{x}_{m+1}) - \pi_g^{(m)} \right)
\]</span></p>
<p>Böylece karışım ağırlıkları için özyineli güncelleme formülüne
erişmiş olduk.</p>
<h3 id="update-for-mean-mu_g">2. Update for Mean <span
class="math inline">\(\mu_g\)</span></h3>
<p><span class="math display">\[
\mu_g^{(m)} = \frac{1}{m \pi_g^{(m)}} \sum_{i=1}^{m} p^{(m)}(C_g |
\mathbf{x}_i) \mathbf{x}_i
\]</span></p>
<p><span class="math display">\[
S_g^{(m)} = \sum_{i=1}^{m} p^{(m)}(C_g | \mathbf{x}_i) \mathbf{x}_i = m
\pi_g^{(m)} \mu_g^{(m)}
\]</span></p>
<p><span class="math display">\[
S_g^{(m+1)} = S_g^{(m)} + p^{(m+1)}(C_g | \mathbf{x}_{m+1})
\mathbf{x}_{m+1}
\]</span></p>
<p><span class="math display">\[
S_g^{(m+1)} \approx S_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
\mathbf{x}_{m+1}
\]</span></p>
<p><span class="math display">\[
\mu_g^{(m+1)} = \frac{S_g^{(m+1)}}{(m+1) \pi_g^{(m+1)}}
\]</span></p>
<p><span class="math display">\[
\mu_g^{(m+1)} \approx \frac{ S_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
\mathbf{x}_{m+1} }{ (m+1) \pi_g^{(m+1)} }
\]</span></p>
<p>Now substitute <span class="math inline">\(S_g^{(m)} = m \pi_g^{(m)}
\mu_g^{(m)}\)</span> and the earlier expression for <span
class="math inline">\(\pi_g^{(m+1)}\)</span>:</p>
<p><span class="math display">\[
\mu_g^{(m+1)} \approx \frac{ m \pi_g^{(m)} \mu_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) \mathbf{x}_{m+1} }{ (m+1) \left[ \pi_g^{(m)} +
\frac{1}{m+1} \left( p^{(m)}(C_g | \mathbf{x}_{m+1}) - \pi_g^{(m)}
\right) \right] }
\]</span></p>
<p><span class="math display">\[
(m+1) \pi_g^{(m+1)} = m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
\]</span></p>
<p><span class="math display">\[
\mu_g^{(m+1)} \approx \frac{ m \pi_g^{(m)} \mu_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) \mathbf{x}_{m+1} }{ m \pi_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) }
\]</span></p>
<p><span class="math display">\[
\mu_g^{(m+1)} = \mu_g^{(m)} + \frac{ p^{(m)}(C_g | \mathbf{x}_{m+1}) }{
m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) } \left(
\mathbf{x}_{m+1} - \mu_g^{(m)} \right)
\]</span></p>
<p><span class="math display">\[
m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) \approx (m+1)
\pi_g^{(m+1)}
\]</span></p>
<p><span class="math display">\[
\mu_g^{(m+1)} \approx \mu_g^{(m)} + \frac{1}{m+1} \cdot \frac{
p^{(m)}(C_g | \mathbf{x}_{m+1}) }{ \pi_g^{(m)} } \left( \mathbf{x}_{m+1}
- \mu_g^{(m)} \right)
\]</span></p>
<h3 id="update-for-covariance-sigma_g">3. Update for Covariance <span
class="math inline">\(\Sigma_g\)</span></h3>
<p><span class="math display">\[
\Sigma_g^{(m)} = \frac{1}{m \pi_g^{(m)}} \sum_{i=1}^{m} p^{(m)}(C_g |
\mathbf{x}_i) (\mathbf{x}_i - \mu_g^{(m)}) (\mathbf{x}_i -
\mu_g^{(m)})^T
\]</span></p>
<p><span class="math display">\[
T_g^{(m)} = \sum_{i=1}^{m} p^{(m)}(C_g | \mathbf{x}_i) (\mathbf{x}_i -
\mu_g^{(m)}) (\mathbf{x}_i - \mu_g^{(m)})^T = m \pi_g^{(m)}
\Sigma_g^{(m)}
\]</span></p>
<p><span class="math display">\[
T_g^{(m+1)} \approx T_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
(\mathbf{x}_{m+1} - \mu_g^{(m+1)}) (\mathbf{x}_{m+1} - \mu_g^{(m+1)})^T
\]</span></p>
<p><span class="math display">\[
(\mathbf{x}_{m+1} - \mu_g^{(m+1)}) \approx (\mathbf{x}_{m+1} -
\mu_g^{(m)})
\]</span></p>
<p><span class="math display">\[
T_g^{(m+1)} \approx T_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1})
(\mathbf{x}_{m+1} - \mu_g^{(m)}) (\mathbf{x}_{m+1} - \mu_g^{(m)})^T
\]</span></p>
<p><span class="math display">\[
\Sigma_g^{(m+1)} = \frac{T_g^{(m+1)}}{(m+1) \pi_g^{(m+1)}}
\]</span></p>
<p><span class="math display">\[
\Sigma_g^{(m+1)} \approx \frac{ T_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) (\mathbf{x}_{m+1} - \mu_g^{(m)}) (\mathbf{x}_{m+1} -
\mu_g^{(m)})^T }{ (m+1) \pi_g^{(m+1)} }
\]</span></p>
<p>Using <span class="math inline">\(T_g^{(m)} = m \pi_g^{(m)}
\Sigma_g^{(m)}\)</span> and the expression for <span
class="math inline">\(\pi_g^{(m+1)}\)</span>, we get:</p>
<p><span class="math display">\[
\Sigma_g^{(m+1)} \approx \frac{ m \pi_g^{(m)} \Sigma_g^{(m)} +
p^{(m)}(C_g | \mathbf{x}_{m+1}) (\mathbf{x}_{m+1} - \mu_g^{(m)})
(\mathbf{x}_{m+1} - \mu_g^{(m)})^T }{ m \pi_g^{(m)} + p^{(m)}(C_g |
\mathbf{x}_{m+1}) }
\]</span></p>
<p><span class="math display">\[
\Sigma_g^{(m+1)} = \Sigma_g^{(m)} + \frac{ p^{(m)}(C_g |
\mathbf{x}_{m+1}) }{ m \pi_g^{(m)} + p^{(m)}(C_g | \mathbf{x}_{m+1}) }
\left[ (\mathbf{x}_{m+1} - \mu_g^{(m)}) (\mathbf{x}_{m+1} -
\mu_g^{(m)})^T - \Sigma_g^{(m)} \right]
\]</span></p>
<p><span class="math display">\[  
\Sigma_g^{(m+1)} \approx \Sigma_g^{(m)} + \frac{1}{m+1} \cdot \frac{
p^{(m)}(C_g | \mathbf{x}_{m+1}) }{ \pi_g^{(m)} } \left[
(\mathbf{x}_{m+1} - \mu_g^{(m)}) (\mathbf{x}_{m+1} - \mu_g^{(m)})^T -
\Sigma_g^{(m)} \right]
\]</span></p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_gmm_data(weights, means, covs, n_samples):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    n_components <span class="op">=</span> <span class="bu">len</span>(weights)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> np.zeros((n_samples, <span class="dv">2</span>))    </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    component_choices <span class="op">=</span> np.random.choice(n_components, size<span class="op">=</span>n_samples, p<span class="op">=</span>weights)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.where(component_choices <span class="op">==</span> i)[<span class="dv">0</span>]</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        n_points_to_sample <span class="op">=</span> <span class="bu">len</span>(indices)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n_points_to_sample <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            data[indices, :] <span class="op">=</span> np.random.multivariate_normal(</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                mean<span class="op">=</span>means[i], </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                cov<span class="op">=</span>covs[i], </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                size<span class="op">=</span>n_points_to_sample</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    shuffle_indices <span class="op">=</span> np.arange(<span class="bu">len</span>(data))</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(shuffle_indices)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    res_data <span class="op">=</span> data[shuffle_indices]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res_data</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gmm_pdf(x, y, weights, means, covs):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;(x,y) noktasinda GMM olasilik yogunlugunu hesapla&quot;&quot;&quot;</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        rv <span class="op">=</span> multivariate_normal(means[i], covs[i])</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        z <span class="op">+=</span> weights[i] <span class="op">*</span> rv.pdf([x, y])</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [<span class="fl">0.7</span>, <span class="fl">0.3</span>]</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> [</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">5</span>],</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">5</span>, <span class="op">-</span><span class="dv">2</span>]</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>covs <span class="op">=</span> [</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">2</span>, <span class="fl">1.5</span>], [<span class="fl">1.5</span>, <span class="dv">3</span>]],</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">10</span>, <span class="op">-</span><span class="dv">3</span>], [<span class="op">-</span><span class="dv">3</span>, <span class="dv">5</span>]]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>n_points <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate data points</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>gmm_data <span class="op">=</span> generate_gmm_data(weights, means, covs, n_points)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>plt.scatter(gmm_data[:, <span class="dv">0</span>], gmm_data[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.6</span>, s<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;X-coordinate&quot;</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Y-coordinate&quot;</span>)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_112_gmminc_01.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_112_gmminc_01.png" /></p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> responsibilities(x, weights, means, covs):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> np.zeros(<span class="bu">len</span>(weights))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        p[k] <span class="op">=</span> weights[k] <span class="op">*</span> multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>covs[k]).pdf(x)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> p.<span class="bu">sum</span>()</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> denom <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.ones_like(p) <span class="op">/</span> <span class="bu">len</span>(p)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p <span class="op">/</span> denom</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_likelihood(X, weights, means, covs):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    ll <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> X:</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        px <span class="op">=</span> <span class="bu">sum</span>(weights[k] <span class="op">*</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>                 multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>covs[k]).pdf(x)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)))</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        ll <span class="op">+=</span> np.log(px <span class="op">+</span> <span class="fl">1e-12</span>)  <span class="co"># guard against log(0)</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ll</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> generate_gmm_data(weights, means, covs, n_points)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>n_components <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> data.shape[<span class="dv">1</span>]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>weights_tmp <span class="op">=</span> np.ones(n_components) <span class="op">/</span> n_components</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>means_tmp <span class="op">=</span> np.random.randn(n_components, d) <span class="op">*</span> <span class="dv">5</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>covs_tmp <span class="op">=</span> np.array([np.eye(d) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_components)])</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>m_total <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>N_g <span class="op">=</span> weights <span class="op">*</span> m_total</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data)):</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> data[idx]</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> responsibilities(x, weights_tmp, means_tmp, covs_tmp)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    m_total <span class="op">+=</span> <span class="fl">1.0</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components):</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        r_k <span class="op">=</span> r[k]</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        N_old <span class="op">=</span> N_g[k]</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>        N_new <span class="op">=</span> N_old <span class="op">+</span> r_k</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        N_g[k] <span class="op">=</span> N_new</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update mixing weight</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>        weights_tmp[k] <span class="op">=</span> N_new <span class="op">/</span> m_total</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update mean</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>        mu_old <span class="op">=</span> means_tmp[k].copy()</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        means_tmp[k] <span class="op">=</span> mu_old <span class="op">+</span> (r_k <span class="op">/</span> N_new) <span class="op">*</span> (x <span class="op">-</span> mu_old)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update covariance</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>        diff_var <span class="op">=</span> (x <span class="op">-</span> mu_old).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        covs_tmp[k] <span class="op">=</span> covs_tmp[k] <span class="op">+</span> (r_k <span class="op">/</span> N_new) <span class="op">*</span> (diff_var <span class="op">@</span> diff_var.T <span class="op">-</span> covs_tmp[k])</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>ll <span class="op">=</span> log_likelihood(data, weights_tmp, means_tmp, covs_tmp)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (ll)</span></code></pre></div>
<pre class="text"><code>-4464.929418673403</code></pre>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (weights_tmp)        </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (means_tmp)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (covs_tmp)</span></code></pre></div>
<pre class="text"><code>[0.31203614 0.68796386]
[[ 5.2066373  -2.21923644]
 [ 0.02514121  4.96786565]]
[[[ 9.21310167 -2.07072758]
  [-2.07072758  4.41050016]]

 [[ 1.93204572  1.5493632 ]
  [ 1.5493632   3.63932262]]]</code></pre>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>xs <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">120</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>ys <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">120</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>Xg, Yg <span class="op">=</span> np.meshgrid(xs, ys)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>Z_true <span class="op">=</span> np.zeros_like(Xg)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>Z_learned <span class="op">=</span> np.zeros_like(Xg)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Xg.shape[<span class="dv">0</span>]):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(Xg.shape[<span class="dv">1</span>]):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        pt <span class="op">=</span> np.array([Xg[i, j], Yg[i, j]])</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        Z_true[i, j] <span class="op">=</span> <span class="bu">sum</span>(weights[k] <span class="op">*</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                           multivariate_normal(mean<span class="op">=</span>means[k], cov<span class="op">=</span>covs[k]).pdf(pt)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(weights)))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        Z_learned[i, j] <span class="op">=</span> <span class="bu">sum</span>(weights_tmp[k] <span class="op">*</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                              multivariate_normal(mean<span class="op">=</span>means_tmp[k], cov<span class="op">=</span>covs_tmp[k]).pdf(pt)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                              <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_components))</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>), constrained_layout<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>ax1, ax2 <span class="op">=</span> axes</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>cf1 <span class="op">=</span> ax1.contourf(Xg, Yg, Z_true, levels<span class="op">=</span><span class="dv">40</span>, cmap<span class="op">=</span><span class="st">&#39;viridis&#39;</span>)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>ax1.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">&#39;white&#39;</span>, s<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>            edgecolors<span class="op">=</span><span class="st">&#39;k&#39;</span>, linewidth<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>ax1.set_title(<span class="st">&quot;Veri&quot;</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">&quot;y&quot;</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>fig.colorbar(cf1, ax<span class="op">=</span>ax1, fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>cf2 <span class="op">=</span> ax2.contourf(Xg, Yg, Z_learned, levels<span class="op">=</span><span class="dv">40</span>, cmap<span class="op">=</span><span class="st">&#39;viridis&#39;</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>ax2.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">&#39;white&#39;</span>, s<span class="op">=</span><span class="dv">8</span>, alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>            edgecolors<span class="op">=</span><span class="st">&#39;k&#39;</span>, linewidth<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>ax2.set_title(<span class="st">u&#39;Artımsal GMM ile Öğrenilmiş PDF&#39;</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">&quot;x&quot;</span>)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">&quot;y&quot;</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>ax2.legend()</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>fig.colorbar(cf2, ax<span class="op">=</span>ax2, fraction<span class="op">=</span><span class="fl">0.046</span>, pad<span class="op">=</span><span class="fl">0.04</span>)</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_112_gmminc_02.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_112_gmminc_02.png" /></p>
<p>[devam edecek]</p>
<p>Kaynaklar</p>
<p>[1] Bayramli, <em>Istatistik, Gaussian Karışım Modeli (GMM) ile
Kümelemek</em></p>
<p>[2] Zheng, <em>Recursive Gaussian Mixture Models for Adaptive Process
Monitoring</em></p>
<p>[3] Titterington, <em>Recursive Parameter Estimation using Incomplete
Data</em></p>
<p>[4] Zivkovic, <em>Recursive unsupervised learning of finite mixture
models</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
