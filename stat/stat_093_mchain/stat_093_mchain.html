<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="markov-zincirleri-markov-chains">Markov Zincirleri (Markov Chains)</h1>
<p>Markov Zincirlerinde (MZ) <span class="math inline">\(i\)</span> konumundan <span class="math inline">\(j\)</span> konumuna geçiş olasılığını, <span class="math inline">\(P_{ij}\)</span> gösterir. Farklı şekile <span class="math inline">\(P(X_{n+1} = j | X_{n} = i)\)</span> olarak açılabilir. Açılımdan görüleceği üzere bir MZ sonraki adıma geçiş olasılığı için sadece bir önceki adıma bakar. Bu tür önce/sonra yapısındaki iki boyutlu hal, çok rahat bir şekilde matrise çevirilebilir. Önceki konum satırlar, sonraki konum kolonlar olarak temsil edilir mesela.</p>
<p>Örnek</p>
<p>Bir sonraki günde yağmur yağmayacağını bir MZ olarak tasarlayalım [1, sf 196]. Bir sonraki günde yağmur yağmayacağını sadece bugün etkiliyor olsun. Eğer bugün yağmur yağıyorsa yarın yağmur yağması 0.7, eğer bugün yağmıyor ise yarın yağması 0.4. MZ şöyle</p>
<p><span class="math display">\[ 
P =
\left[\begin{array}{cc}
0.7 &amp; 0.3 \\
0.4 &amp; 0.6
\end{array}\right]
\]</span></p>
<p>Geçiş olasılıklarından bahsettiğimize göre ve elimizde sınırlı / belli sayıda konum (state) olduğu için, bir MZ'nin her satırındaki olasılıkların toplamı tabii ki 1'e eşit olmalıdır.</p>
<p>MZ'lerin ilginç bir özelliği <span class="math inline">\(n\)</span> adım sonra <span class="math inline">\(i,j\)</span> geçişinin <span class="math inline">\(P_{i,j}^n\)</span> hesabıyla yapılabilmesidir. Yani <span class="math inline">\(P\)</span>'yi <span class="math inline">\(n\)</span> defa kendisiyle çarpıp <span class="math inline">\(i,j\)</span> indislerindeki öğeye bakarsak <span class="math inline">\(n\)</span> adım sonrasını görüyoruz. İspat altta [1, sf. 195].</p>
<p>Bulmak istediğimiz <span class="math inline">\(n\)</span> adım sonrası geçiş olasılıkları, yani <span class="math inline">\(i\)</span> adımında olan sürecin <span class="math inline">\(n\)</span> adım sonra <span class="math inline">\(j\)</span> adımında olma olasılığı. Aradığımız,</p>
<p><span class="math display">\[
P_{ij}^n = P ( X_{n+k} = j | X_k = i ), \quad n \ge 0, i,j \ge 0
\]</span></p>
<p>Tabii ki <span class="math inline">\(P_{ij}^1 = P_{ij}\)</span>. Chapman-Kolmogorov denklemleri bu n-adım geçişlerini hesaplamak için bize bir yöntem sağlıyoar. Bu denklemler,</p>
<p><span class="math display">\[
P_{ij}^{n+m} = \sum_{k=0}^{\infty} P_{ik}^n P_{kj}^m,
\quad \forall n,m \ge 0, \quad \forall i,j
\qquad (1)
\]</span></p>
<p><span class="math inline">\(P_{ij}^{n+m}\)</span> formülü şunu söylüyor, <span class="math inline">\(i\)</span>'de başlayan süreç <span class="math inline">\(n+m\)</span> geçiş sonrası <span class="math inline">\(j\)</span>'e varacak, ve geçtiği yol onu <span class="math inline">\(n\)</span> anında <span class="math inline">\(k\)</span>'den geçirecek. O zaman tüm bu geçiş noktaları <span class="math inline">\(k\)</span>'ler üzerinden bir toplam alırsak sürecin <span class="math inline">\(n+m\)</span> adım sonrası <span class="math inline">\(j\)</span>'de olma olasılığını elde etmiş oluyoruz.</p>
<p>Formel olarak</p>
<p><span class="math display">\[
P_{ij}^{n+m} = P(X_{n+m} = j | X_0 = i )
\]</span></p>
<p>söylenmiş oluyor. Üstteki olasılık hesabına / birleşik olasılığa <span class="math inline">\(k\)</span>'den geçme aksiyonunu ekleyip aynı anda tüm <span class="math inline">\(k\)</span>'ler üzerinden toplam alırsak (entegre edip çıkartma tekniği -integrate out-) hiçbir şey değiştirmemiş oluruz,</p>
<p><span class="math display">\[
= \sum_{k=0}^{\infty} P(X_{n+m} = j, X_n = k | X_0 = i )
\]</span></p>
<p><span class="math display">\[
= \sum_{k=0}^{\infty} P(X_{n+m} = j, X_n = k, X_0 = i )
P(X_n=k|X_0=i)
\]</span></p>
<p>Üstteki ifade diyor ki,</p>
<p><span class="math display">\[
P_{ij}^{n+m} = \sum_{k=0}^{\infty} P_{kj}^m P_{ik}^n 
\]</span></p>
<p>Ayrıca dikkat edersek (1)'deki tarif</p>
<p><span class="math display">\[
P^{n+m} = P^n \cdot P^m
\]</span></p>
<p>işlemini ima ediyor. Nokta işareti çarpım işlemi, çünkü hatırlarsak matris çarpımının tanımı şöyleydi; elimizde N x M boyutunda <span class="math inline">\(A\)</span> matrisi var, <span class="math inline">\(B\)</span> ise M x K boyutunda olsun, her ikisinin <span class="math inline">\(i\)</span> satırı <span class="math inline">\(j\)</span> kolonundaki öğesi <span class="math inline">\(a_{ij}\)</span>, <span class="math inline">\(b_{ij}\)</span> ise, <span class="math inline">\(A \cdot B\)</span> çarpımı bir N x K matrisidir, bu matrisin <span class="math inline">\(i,j\)</span> öğesi <span class="math inline">\(\sum_{k=1}^{M} a_{ik}b_{kj}\)</span> ile verilir. Toplamın üst sınırı sonsuz değil <span class="math inline">\(M\)</span> fakat sonsuzluk üst sınırı genel bir formül için tanımlanmış zaten.</p>
<p>İlk örneğe dönersek, eğer bugün yağmur yağıyorsa 4 gün sonra yağmur yağma olasılığı nedir?</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy.linalg <span class="im">as</span> lin
P <span class="op">=</span> np.array([[<span class="fl">0.7</span>,<span class="fl">0.3</span>],[<span class="fl">0.4</span>,<span class="fl">0.6</span>]])
P4 <span class="op">=</span> lin.matrix_power(P,<span class="dv">4</span>)
<span class="bu">print</span> P4</code></pre></div>
<pre><code>[[ 0.5749  0.4251]
 [ 0.5668  0.4332]]</code></pre>
<p>Aradığımız geçiş için kordinat 0,0'a bakıyoruz ve sonuç 0.5749. Numpy <code>matrix_power</code> bir matrisi istediğimiz kadar kendisiyle çarpmamızı sağlıyor.</p>
<p>Durağan Dağılım (Stationary Distribution)</p>
<p>Eğer yağmur örneğindeki matrisi çarpmaya devam edersek, mesela 8 kere kendisiyle çarpsak sonuç ne olurdu?</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy.linalg <span class="im">as</span> lin
P <span class="op">=</span> np.array([[<span class="fl">0.7</span>,<span class="fl">0.3</span>],[<span class="fl">0.4</span>,<span class="fl">0.6</span>]])
P8 <span class="op">=</span> lin.matrix_power(P,<span class="dv">8</span>)
<span class="bu">print</span> P8</code></pre></div>
<pre><code>[[ 0.57145669  0.42854331]
 [ 0.57139108  0.42860892]]</code></pre>
<p>Dikkat edilirse, her satır bir değere yaklaşmaya başladı. Bu değer MZ'nin durağan dağılımıdır, belli koşullara uyan her MZ'nin böyle bir durağan dağılımı vardır. Bu koşullar MZ'nin periyodik olmayan (aperiodic) ve tekrar eden (recurrent) olmasıdır. Bu şartlar çok &quot;özel'' şartlar değildir aslında, daha çok &quot;normal'' bir MZ'yi tarif ediyor diyebiliriz. Tüm konumları tekrar eden yapmak kolaydır, MZ tek bağlı (singly connected) hale getirilir, yani her konumdan her diğer konuma bir geçiş olur, ve periyodik olmaması için ise MZ'de olmadığı zamanlarda bir konumdan kendisine geçiş sağlanır (az bir gürültü üzerinden).</p>
<p>Neyse, matematiksel olarak durağanlık şu denklemi ortaya çıkartır,</p>
<p><span class="math display">\[ \pi = \pi P \]</span></p>
<p>Burada durağan dağılım <span class="math inline">\(\pi\)</span>'dir. Bu denklem tanıdık geliyor mu? Devriğini alarak şöyle gösterelim, belki daha iyi tanınır,</p>
<p><span class="math display">\[ P^T\pi^T = \pi^T \]</span></p>
<p>Bir şey daha ekleyelim,</p>
<p><span class="math display">\[ P^T\pi^T = 1 \cdot \pi^T \]</span></p>
<p>Özdeğer/vektör formuna benzemiyor mu? Evet. Bu form,</p>
<p><span class="math display">\[ Ax = \lambda x \]</span></p>
<p>MZ denklemi şunu söylüyor, 1 değerindeki özdeğere ait özvektör bir MZ'nin durağan dağılımıdır. Bu arada, MZ geçiş matrisi <span class="math inline">\(P\)</span>'nin en büyük özdeğerinin her zaman 1 olduğunu biliyoruz (çünkü üstteki tarif ettiğimiz özel şartlara sahip olan türden matrisler böyle özdeğerlere sahip olmalı). Bu durumda en büyük özdeğere ait özvektörü hesaplamak yeterli olacaktır. Bunu yapmayı zaten [2]'de öğrenmiştik, üst metot (power method) sayesinde bu hesap kolayca yapılabiliyor.</p>
<p>MZ kavramının ilginç bir uygulaması için [3] yazısına bakılabilir.</p>
<p>Çizitler ve Matrisler</p>
<p>Markov matrisleri kavramını biraz daha ilerletebiliriz. Üstteki Markov örneği için mesela alttaki çizit gösterilebilir,</p>
<div class="figure">
<img src="stat_093_mc_01.png" />

</div>
<p>Örnekteki durumda 1'den başlayınca hangi olasılıkla hangi diğer düğüme atlandığı görülebiliyor. Bu geçiş olasılıklarına göre zar atılıp geçiş yapılabilir. Markov matrisleri bu bağlamda kendi içindeki geçişleri gösteriyor, sürekli 1,2,3,.. düğümleri arasında gidip geliyoruz. 1'den 3'e geçiş için 1'inci satır 3'üncü kolona bakıyoruz, bir sonraki geçiş için <span class="math inline">\(P^2\)</span>'nin 3'üncü satırına bakıyoruz.</p>
<p>Bu kavramı daha da genişletebiliriz. Bir çizitin katman katman, farklı blokları arasındaki geçişleri de ayrı matris çarpımları olarak gösterebiliriz.</p>
<div class="figure">
<img src="stat_093_mc_02.png" />

</div>
<p>Mesela her X bölümündeki konumlardan Y bölümündeki konumlara geçişleri, oradan Z konumlarına geçişleri matris olarak göstermek mümkün, bu durumda matris çarpımı X ve Z arasındaki tüm geçişlerin bir toplamı haline gelir, tüm mümkün gidiş yollarının ağırlığını bu çarpımda görebiliriz. Üstteki örnekte mesela her geçiş bir olasılık hesabı taşıyabilirdi, o zaman <span class="math inline">\(M \cdot N\)</span> çarpımında her Z konumuna herhangi bir X konumundan varma olasılığını taşırdı. Ya da tüm bir Z konumuna varma olasılığı en fazla olan X başlangıcını bu matriste görebilirdik.</p>
<p>Bu tür bir yaklaşımın kullanma alanı geniştir. Mesela her katmanda farklı karar seçenekleri, olasılıkları olabilir, ve ara katmanlar binlerce, milyonlarca seçimi içerebilir. Fakat zincirleme bir matris çarpımı ile o tüm ara katmanların toplamını almış oluyoruz, ve elimizde üstteki başlangıç ve bitiş için 3 x 2 boyutunda bir matris kalıyor.</p>
<p>Kaynaklar</p>
<p>[1] Ross, <em>Introduction to Probability Models, 10th Ed</em></p>
<p>[2] Bayramlı, <em>Lineer Cebir, Ders 21</em></p>
<p>[3] Bayramlı, <em>Lineer Cebir, Google Nasıl İşler?</em></p>
<p>[4] Math3ma, <a href="https://www.math3ma.com/blog/matrices-probability-graphs" class="uri">https://www.math3ma.com/blog/matrices-probability-graphs</a></p>
</body>
</html>
