<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Lineer Regresyon</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="lineer-regresyon">Lineer Regresyon</h1>
<p>Bir hedef değişkeninin bir veya daha fazla kaynak değişkenine olan
bağlantısını bulmak için en basit yöntemlerden biri bu ilişkinin lineer
olduğunu kabul etmektir, yani eldeki değişkenlerin belli ağırlıklar ile
çarpımının toplamı olarak. İlk başta bilinmeyen bu ağırlıkları, ya da
katsayıları bulmak için En Az Kareler (Least Squares) en iyi bilinen
yöntemlerden biri; En Az Kareler daha önce pek çok değişik ders
notlarında, yazıda türetildi. Mesela [7], [8], ya da [9].</p>
<p>Lineer Regresyonun sadece iki değişken temelli işlemek gerekirse,</p>
<p><span class="math display">\[ Y = \beta_0 + \beta_1 x +
\epsilon\]</span></p>
<p>olabilir. Eğer iki değişkenden fazlası var ise bu bir düzlem
uydurulacak demektir. Değişken <span
class="math inline">\(\epsilon\)</span>, <span
class="math inline">\(N(0,\sigma^2)\)</span> dağılımından gelen hatadır
ve <span class="math inline">\(\sigma\)</span> bilinmez. Eğer veriyi
<span class="math inline">\((x_1,y_1),...(x_n,y_n)\)</span> ikili olarak
grafiklesek</p>
<p><img src="stat_linreg_02.png" /></p>
<p>gibi gözükebilirdi, lineer regresyon ile yapmaya çalıştığımız tüm
noktalara olabilecek en yakın düz çizgiyi (üstte görüldüğü gibi)
bulmaktır.</p>
<p>Bu düz çizgiyi (ki boyutlu ortamda bu çizgi bir hiper düzlem olurdu,
<span class="math inline">\(\beta_2,\beta_3,..\)</span> gibi daha fazla
katsayı gerekirdi), En Az Kareler ile bulduktan sonra elimize geçenler
katsayı değerlerinin tahminidir, ki bunlar bazı kaynaklarda <span
class="math inline">\(\hat{\beta}_0,\hat{\beta}_1\)</span> olarak
tanımlanır, bu notasyon istatistikteki “tahmin edici (estimator)’’
notasyon ile uyumlu. Bu tahmin ediciler ile elde edilen <span
class="math inline">\(y\)</span>’nin kendisi de bir tahmin edici haline
gelir ve bir düz çizgiyi tanımlar,</p>
<p><span class="math display">\[ \hat{y} = \hat{\beta}_0 +
\hat{\beta}_1x \]</span></p>
<p>Katsayıların tahmin edicilerinin de dağılımı vardır ve bu dağılım,
ideal şartlarda bir normal dağılımdır. İspat için bu yazının sonuna
bakınız.</p>
<p>Örnek olarak lineer regresyon için tarihte kullanılan neredeyse ilk
veri setini seçeceğim. Bu veri çocukların ve onların ebeveynlerinin boy
uzunluğunu içeren Galton’un 19. yüzyılda analiz ettiği veri setidir.
Hatta öyle ki regresyon kelimesinin bile bu problem ile alakası var,
İngilizce regress kelimesi baştaki (çoğunlukla daha iyi olmayan) bir
hale dönmek anlamında kullanılır, ve problemde çocukların boyunun
ebeveyn boyuna “geri döndüğü’’ ya da ondan ne kadar etkilendiği
incelenmektedir.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;galton.csv&#39;</span>,sep<span class="op">=</span><span class="st">&#39;,&#39;</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df.head(<span class="dv">4</span>))</span></code></pre></div>
<pre class="text"><code>   child  parent
0   61.7    70.5
1   61.7    68.5
2   61.7    65.5
3   61.7    64.5</code></pre>
<p>Şimdi regresyonu işletelim, sadece bağımsız tek değişken olacak,
ebeveyn boyu <code>parent</code>, hedef değişken ise çocuk
<code>child</code> içinde.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.ols(<span class="st">&#39;child ~ parent&#39;</span>, data<span class="op">=</span>df).fit()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (results.summary())</span></code></pre></div>
<pre class="text"><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  child   R-squared:                       0.210
Model:                            OLS   Adj. R-squared:                  0.210
Method:                 Least Squares   F-statistic:                     246.8
Date:                Thu, 03 Jul 2025   Prob (F-statistic):           1.73e-49
Time:                        12:11:57   Log-Likelihood:                -2063.6
No. Observations:                 928   AIC:                             4131.
Df Residuals:                     926   BIC:                             4141.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     23.9415      2.811      8.517      0.000      18.425      29.458
parent         0.6463      0.041     15.711      0.000       0.566       0.727
==============================================================================
Omnibus:                       11.057   Durbin-Watson:                   0.046
Prob(Omnibus):                  0.004   Jarque-Bera (JB):               10.944
Skew:                          -0.241   Prob(JB):                      0.00420
Kurtosis:                       2.775   Cond. No.                     2.61e+03
==============================================================================
</code></pre>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (pd.Series(results.resid).describe())</span></code></pre></div>
<pre class="text"><code>count    9.280000e+02
mean    -8.412581e-13
std      2.237339e+00
min     -7.805016e+00
25%     -1.366144e+00
50%      4.869321e-02
75%      1.633856e+00
max      5.926437e+00
dtype: float64</code></pre>
<p>Bu çıktıda gösterilenler ne anlama gelir?</p>
<ol type="1">
<li><code>coef</code> altında görülen değerler sırasıyla <span
class="math inline">\(\beta_o,\beta_1\)</span> tahminleridir, yani <span
class="math inline">\(\hat{\beta}_o,\hat{\beta}_1\)</span>. Bunlar
bulmak istediğimiz katsayılar. İki boyutta olduğumuz için düz bir
çizgiden bahsediyoruz, bu çizginin <span
class="math inline">\(y\)</span> eksenini kestiği yer kesi (intercept)
<span class="math inline">\(\hat{\beta}_0\)</span>’da ve ebeyne
(<code>parent</code>) tekabül eden katsayı <span
class="math inline">\(\hat{\beta}_1\)</span>.</li>
</ol>
<p>Teorik olarak eğer bir katsayı sıfır ise bu işe yaramaz bir
katsayıdır, çünkü modele hiçbir şey “eklemez’’. Fakat Basit En Az
Kareler (ordinary least squares -OLS-)’in hesapladığı bir tahmindir
nihayetinde ve hiçbir zaman sıfır olmayacaktır. O zaman soruyu biraz
daha değiştirmek gerekir: istatistiki olarak düşünürsek gerçek
katsayının sıfır olma <em>olasılığı</em> nedir? Katsayı yanında görülen
<span class="math inline">\(t\)</span> ve <span
class="math inline">\(P&gt;|t|\)</span> (diğer ismiyle p-değeri) bunun
için kullanılır.</p>
<p><span class="math inline">\(t\)</span> değeri bir katsayı için onun
tahminini ve standart hatasına bölerek elde edilir. Üstteki çıktıda
mesela 23.9415/2.811=8.517. Bu değer katsayı tahmininin veriden veriye
ne kadar değişik sonuçlar verebileceğini (variability) gösterir, ve bir
bakıma bu katsayı tahmininin kesinliği (precision) hakkında bir
rapordur. Eğer bir katsayı tahmini, standart hatasına göre büyük ise (ki
bölüm bunu gösterir) bu katsayının sıfır olmadığına dair güçlü bir
işaret olarak alınabilir.</p>
<p>Peki ne kadar büyük bir sayı büyük sayılmalıdır? Bunun için
p-değerine başvuruyoruz. P-değerini hesaplamak için t değeri ve standart
hatasının dağılımından bahsetmek lazım.</p>
<p>t değeri bir rasgele değişken olduğu için bir dağılımı vardır, ve bu
dağılım Öğrenci t (Student t) dağılımıdır. Sebep şu, t değerinin kendisi
de iki rasgele değişkeninin bölümüdür, bu değişkenlerden biri katsayının
kendisidir, ki bu değer nüfustaki “gerçek’’ katsayı etrafında normal
olarak dağılmış bir rasgele değişken olarak kabul edilir. Diğeri ise,
yani bölen, tahmin edici <span class="math inline">\(S\)</span>’tır ki
bir chi kare rasgele değişkenin kareköküdür. Bu bölümün Öğrenci t
dağılımına sahip olduğu daha önce gösterildi.</p>
<p>Standart hata ise, artık / kalıntı değerlerle (residuals) alakalıdır
(<code>results.resid</code> içinde), ve bu değerler model uydurulduktan
sonra o modeli kullanarak gerçek veriye ne kadar uzak düştüğümüzü
gösterir. Formül olarak her veri noktası <span
class="math inline">\(i\)</span> için <span class="math inline">\(r_i =
y_i - \beta_1x_i - \beta_o\)</span>. Her katsayı için de ayrı ayrı
kalıntı hesaplanabilir.</p>
<p>İdeal durumda, yani modelin doğru, veriye uyduğu durumda artıkların
mükemmel bir Normal dağılıma sahip olması gerekir, çünkü veri içindeki
tüm örüntü, kalıp model tarafından “bulunmuştur’’ ve geri kalanlar
gürültüdür (gürültü tabii ki Normal dağılımda). İdeal ortamda OLS
algoritmasının, matematiksel olarak, ortalaması (mean) sıfır olan
artıklar üretmesi garantidir. Bir diğer varsayım uyduralan değişkenlerin
katsayılarının onların”gerçek’’ değerleri etrafında merkezlenen bir
Normal dağılıma sahip olduğudur (ispat için [10] yazısının sonuna
bakılabilir). Bu normallik önemli çünkü katsayı tahmini ile standart
hatayı bölünce başka bir Öğrenci t dağılımı ortaya çıkacak.</p>
<p>Kalıntıların normalliği QQ grafiği ile kontrol edilebilir, bkz
[11],</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>sm.qqplot(results.resid)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_linreg_01.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_linreg_01.png" /></p>
<p>Oldukça düz bir çizgi, uyum başarılı demek ki..</p>
<p>Şimdi, katsayı için olan kalıntı değerlerinin karesini alıp toplarsak
ve karekökü alırsak, bu rasgele değişkenin Chi Kare (Chi Square) olarak
dağıldığı bilinir, ve yine bilinir ki standart normal rasgele değişken,
bolu, chi kare karekökü bize bir Öğrenci t dağılımını verir, mesela</p>
<p><span class="math display">\[ t = \frac{Z}{\sqrt{V / m}} =
t_m\]</span></p>
<p>serbestlik derecesi <span class="math inline">\(m\)</span> olan bir
Öğrenci t rasgele değişkenidir.</p>
<p>Öğrenci t’den p-değeri üretmek için t değerinin sıfırdan ne kadar
uzağa düştüğü bir Öğrenci t olasılık hesabına dönüştürülür. Önce
katsayının tam değeri (absolute value) alınır, eksileri artı yaparız,
çünkü sıfırdan uzaklık ile ilgileniyoruz sadece ve Öğrenci t dağılımı
simetriktir, sonra bu değer <span class="math inline">\(t_m\)</span>
dağılımı üzerinden bir olasılık hesabına dönüştürülür. Yani “katsayı /
standart hata bir <span class="math inline">\(t_m\)</span> ile dağılmış
ise, elde edilen bölümün o dağılımdan gelme olasılığı nedir?’’ gibi bir
soru. Olasılık hesabı yoğunluk fonksiyonu üzerinde bir alan hesabıdır, t
değeri 2 ise ve <span class="math inline">\(t_5\)</span> için bu alan
hesabı şöyle,</p>
<p><img src="stat_linreg_03.png" /></p>
<p>Ayrıca bu olasılık sonucu sıfır ile karşılaştırmak kolay olsun diye
1’den çıkartılır ve 2 ile çarpılır, istatistiğin böylece iki taraflı
(two-sided) olduğu belirtilir. <span class="math inline">\(m\)</span>,
veri nokta sayısı, eksi katsayı sayısı, artı bir olarak hesaplanıyor.
Eğer sonuç 0.05’ten küçük ise bu iyiye işarettir, 0.05’ten büyük olan
değerler iyi değildir. Galton örneğinde <span
class="math inline">\(\hat{\beta_0}\)</span> için,</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> t</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="dv">2</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>t(<span class="dv">927</span>).cdf(np.<span class="bu">abs</span>(<span class="fl">8.517</span>))))</span></code></pre></div>
<pre class="text"><code>0.0</code></pre>
<p>Üstteki sonuç 0.0 değeri çok iyi. Demek ki bu katsayı önemli
(significant).</p>
<ol start="2" type="1">
<li>Artıklarda sıfırdan sapma, herhangi bir yöne doğru yamukluk (skew)
OLS uyumsuzluğunun işareti olabilir, üstte artıklar üzerinde
<code>describe</code> çağrısı ile medyanı (%50 noktası) hesaplattık, bu
değerin 0.04 ile sıfırdan çok az sağa doğru saptığını görüyoruz. %25,
%75 bölgelerinin işaretlerine bakmadan tam (absolute) değerlerine
bakalım, 1.36 ve 1.63, çok az farklılar. İdealde hiç fark olmamasını
isteriz çünkü normal dağılım simetriktir, her iki tarafında da bu
bölgelerin yakın değerde olmasını bekleriz. Fakat bu değerler alarm
yaratacak nitelikte değil.</li>
</ol>
<p>Artıkların minimum, maksimum (<code>min,max</code>) değerleri
verideki ekstrem, aykırı değerlere (outlier) dair bir işaret
olabilir.</p>
<ol start="3" type="1">
<li><span class="math inline">\(R^2\)</span>, ya da
<code>R-squared</code>, modelin kalitesiyle alakalıdır, ne kadar büyükse
o kadar iyidir. Matematiksel olarak bu değer <span
class="math inline">\(y\)</span>’nin değişiminin / varyansının oran
olarak ne kadarının regresyon modeli tarafından “açıklanabildiğini’’
belirtir. Üstteki örnekte <span class="math inline">\(R^2=0.21\)</span>
ise model varyansın yüzde 21’ini açıklıyor. Ya da”bir çocuğun boyunun
yüzde 21’i ebeveyn boyu ile açıklanabilir’’ sözü de söylenebilir. Geri
kalan 0.75’lik yani yüzde 75’lik “açıklanamayan’’ kısmın değişik
sebepleri olabilir; belki hesaba katmadığımız değişkenler vardır, ya da
örnekleme prosedüründe hatalar yapılmıştır, ya da lineerlik bu probleme
uygun değildir, vs.</li>
</ol>
<p>Tavsiyemiz düz <span class="math inline">\(R^2\)</span> yerine OLS
çıktısında görülen “düzeltilmiş <span
class="math inline">\(R^2\)</span>’’ yani <code>Adj. R-squared</code>
bilgisinin kullanılmasıdır, çünkü bu bilgi modeldeki değişken sayısını
da hesaba katar ve daha iyi bir ölçüttür.</p>
<ol start="4" type="1">
<li>F istatistiği: Bu istatistik tüm modelin önemli mi önemsiz mi
olduğunu irdeler. Eğer modelde sıfır olmayan en az bir katsayı var ise
model önemlidir (herhangi bir <span class="math inline">\(i\)</span>
için <span class="math inline">\(\beta_i \ne 0\)</span>). Eğer tüm
katsayılar sıfır ise model önemsizdir (<span
class="math inline">\(\beta_0=\beta_1,\dots,\beta_n=0\)</span>).
Örnekte</li>
</ol>
<pre class="text"><code>... F-statistic:                     246.8
... Prob (F-statistic):           1.73e-49</code></pre>
<p><code>Prob (F-statistic)</code> bir p-değeri, ve bu değer 0.05’ten
küçük ise model büyük bir ihtimalle önemlidir, eğer 0.05’ten büyük ise
büyük ihtimalle önemli değildir. Üstteki p-değeri 1.73e-49 gösteriyor,
çok ufak bir değer, yani bu iyi.</p>
<p>Not: Çoğu kişi OLS çıktısında ilk önce <span
class="math inline">\(R^2\)</span>’ye bakar, fakat bilgili istatistikçi
F’e bakar, çünkü bir model önemli değilse, geri kalan hiçbir ölçütün
önemi yoktur.</p>
<p>Nihai analiz olarak bu veride <code>parent</code> katsayısının
pozitif olan değerine bakarak çocuk ve ebeveyn boyu arasında bir
bağlantı olduğunu söyleyebiliriz.</p>
<p>Basamaklı Regresyon (Stepwise Regression)</p>
<p>Eğer elimizde çok fazla değişken var ise, bu değişkenlerden
hangilerinin en iyi olduğunu seçmek oldukça zor olabilir. Önemlilik
sayıları burada biraz yardımcı olabilir, fakat değişkenlerin eklenip,
çıkartılması regresyonun tamamını etkilediği için deneme / yanılma ile
ekleme / çıkartma işleminin yapılması gerekebilir, ki bu işlemi elle
yapmak külfetli olur. Acaba bu yöntemi otomize edemez miyiz?</p>
<p>R dilindeki <code>lm</code>’in <code>step</code> adlı özelliği burada
yardımcı olabilir. Önce yapay bir veri üretelim,</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;x1&#39;</span>] <span class="op">=</span> np.random.normal(size<span class="op">=</span>n)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;x2&#39;</span>] <span class="op">=</span> np.random.normal(size<span class="op">=</span>n)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;x3&#39;</span>] <span class="op">=</span> np.random.normal(size<span class="op">=</span>n)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;x4&#39;</span>] <span class="op">=</span> np.random.normal(size<span class="op">=</span>n)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;y&#39;</span>] <span class="op">=</span> <span class="dv">10</span> <span class="op">+</span> <span class="op">-</span><span class="dv">100</span><span class="op">*</span>df[<span class="st">&#39;x1&#39;</span>] <span class="op">+</span>  <span class="dv">75</span><span class="op">*</span>df[<span class="st">&#39;x3&#39;</span>] <span class="op">+</span> np.random.normal(size<span class="op">=</span>n)</span></code></pre></div>
<p>Yapay veride farkedileceği üzere <code>x2,x4</code> modele eklenmedi
bile. Bu değişkenler önemsiz, ürettiğimiz için biz bunu biliyoruz.
Bakalım regresyon bunu keşfedecek mi? Şimdi tüm değişkenlerle bir OLS
yapalım,</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext rpy2.ipython</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>i df</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R fullmodel <span class="op">&lt;-</span> lm(y<span class="op">~</span>x1<span class="op">+</span>x2<span class="op">+</span>x3<span class="op">+</span>x4,data<span class="op">=</span>df)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o res res <span class="op">=</span> summary(fullmodel)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (res)</span></code></pre></div>
<pre><code>
Call:
lm(formula = y ~ x1 + x2 + x3 + x4, data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-3.15789 -0.63251 -0.01537  0.58051  2.30127 

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept)   9.94953    0.09378   106.098   &lt;2e-16 ***
x1          -99.95333    0.09686 -1031.975   &lt;2e-16 ***
x2           -0.04103    0.09500    -0.432    0.667    
x3           75.14720    0.10240   733.851   &lt;2e-16 ***
x4            0.04863    0.10015     0.486    0.628    
---

Residual standard error: 0.9292 on 95 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 4.23e+05 on 4 and 95 DF,  p-value: &lt; 2.2e-16

</code></pre>
<p>Görüldüğü gibi daha baştan <code>x2,x4</code> önemsiz bulundu. Ama
daha karmaşık bir modelde bu o kadar rahat bulunmayabilirdi. Şimdi
<code>step</code> ile tam modelden bu değişkenler çekip çıkartılabiliyor
mu ona bakacağız.</p>
<p>R dilinde basamaklı regresyon iki şekilde işler. Ya tam modelden
geriye gidersiniz yani tam modelden ise yaramayan değişkenleri
atarsınız, ya da en baz (boş) modelden başlayıp ileri gidersiniz yani
ekleye ekleye en iyi değişkenlere erişmeye uğraşırsınız. İlk önce
eliminasyonu görelim,</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R reducedmodel <span class="op">&lt;-</span> step(fullmodel, direction<span class="op">=</span><span class="st">&quot;backward&quot;</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o resred resred<span class="op">&lt;-</span>summary(reducedmodel)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (resred)</span></code></pre></div>
<pre><code>
Call:
lm(formula = y ~ x1 + x3, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1667 -0.6078 -0.0256  0.5732  2.3592 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   9.95039    0.09251   107.6   &lt;2e-16 ***
x1          -99.95181    0.09540 -1047.7   &lt;2e-16 ***
x3           75.14514    0.10101   744.0   &lt;2e-16 ***
---

Residual standard error: 0.9217 on 97 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 8.599e+05 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Doğru sonuçlar bulundu. Bu yöntem fena değildir, ama bazen o kadar
çok değişken vardır ki tam modelle başlamak iyi bir fikir olmayabilir, o
zaman boş başlayıp ileri gitmek daha mantıklı olabilir. Boş modelde
sadece <code>y ~ 1</code> olacak, biraz garip gelebilir, çünkü hiç
değişken yok (ki bu durumda uydurulan tüm değişkenler sadece <span
class="math inline">\(y\)</span>’nin ortalamasıdır). Neyse, ileri giden
modelde <code>step</code>’e hangi değişkenlerin aday / potansiyel
değişken olduğunu belirtmek gerekir, bunu <code>scope</code> ile
yaparız,</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R minmodel <span class="op">&lt;-</span> lm(y <span class="op">~</span> <span class="dv">1</span>,data<span class="op">=</span>df)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R fwd <span class="op">&lt;-</span> step(minmodel, direction<span class="op">=</span><span class="st">&quot;forward&quot;</span>, scope <span class="op">=</span> ( <span class="op">~</span> x1 <span class="op">+</span> x2 <span class="op">+</span> x3 <span class="op">+</span> x4))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R <span class="op">-</span>o fwdres fwdres <span class="op">&lt;-</span> summary(fwd)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (fwdres)</span></code></pre></div>
<pre><code>
Call:
lm(formula = y ~ x1 + x3, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.1667 -0.6078 -0.0256  0.5732  2.3592 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   9.95039    0.09251   107.6   &lt;2e-16 ***
x1          -99.95181    0.09540 -1047.7   &lt;2e-16 ***
x3           75.14514    0.10101   744.0   &lt;2e-16 ***
---

Residual standard error: 0.9217 on 97 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 8.599e+05 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Yine aynı sonuca geldik. Tabii bu çok basit bir yapay veri, o yüzden
aynı yere gelmiş olmamız şaşırtıcı değil. Gerçek problemlerde geriye ve
ileri giden modellerin ikisini de deneyip sonuçları karşılaştırmak iyi
oluyor. Sonuçlar şaşırtıcı olabilir.</p>
<p>Bir diğer tavsiye basamaklı regresyonu her derda deva bir yöntem
olarak görmemek, çünkü üstteki çıktılara göre sihirli bir şekilde en
kullanışlı alt kümeyi buluveriyor, vs, fakat bu metot, değişkenleri iyi
tanıyan birisi tarafından dikkatli bir şekilde alt kümenin elenip,
seçilerek bulunması yerine geçemez. Bunu özellikle belirtiyoruz, çünkü
bazılarının aklına şöyle bir şey gelebilir,</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R full.model <span class="op">&lt;-</span> lm(y <span class="op">~</span> (x1 <span class="op">+</span> x2 <span class="op">+</span> x3 <span class="op">+</span> x4)<span class="op">^</span><span class="dv">4</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>R reduced.model <span class="op">&lt;-</span> step(full.model, direction<span class="op">=</span><span class="st">&quot;backward&quot;</span>)</span></code></pre></div>
<p>Üstte görülen <code>^4</code> kullanımı dört değişken arasındaki
<em>tüm mümkün</em> etkileşimleri (interaction) ortaya çıkartır, yani
<code>x1:x2,x1:x2:x3:x4,x3:x4,..</code> gibi ve bunların tamamını
basamaklı regresyona sokar, çünkü bu cinliğe göre nasılsa eliminasyon
metotu ise yaramayan değişkenleri atacaktır (!). Bu metot iyi
işlemeyecektir, çoğu etkileşimin hiçbir anlamı yoktur, <code>step</code>
fonksiyonu herhalde çok fazla seçenek arasında boğulur, sonuçta elimizde
bir sürü ise yaramaz değişken kalacaktır.</p>
<p>Soru</p>
<p>Diyelim ki elimde bir veri seti var ve üzerinde OLS uyguladım,
sonuçlara baktım. Eğer bu veri setini alıp, kendisine eklersem, yani
veriyi iki katına çıkartırsam, ilk işlettiğim OLS’teki katsayılara, ve
standart hataya ne olur?</p>
<p>Cevap</p>
<p>Dikkat, bu soru bir mülakat sorusudur! :) Düşünelim, sezgisel bir
şekilde, 2 boyutta, uydurulan tek çizginin altında ve üstünde yine aynı
verilerin bir kez daha tekrarlanacağını farkederiz, ki bu çizginin
yerini değiştirmezdi. Yani katsayılar aynı kalırdı. Fakat standart
sapmaya ne olurdu? Artıklardan başlayalım,</p>
<p><span class="math display">\[ r_i = y_i - \beta_0 + \beta_1x_i
\]</span></p>
<p>Veriyi ikiye katlayınca,</p>
<p><span class="math display">\[ 2y_i - 2\beta_0 + 2\beta_1x_i
\Rightarrow 2r_i \]</span></p>
<p>Standart hata hesabı, kolaylık için <span
class="math inline">\(n-1\)</span> yerine <span
class="math inline">\(n\)</span>, ve <span class="math inline">\(C =
r_i^2\)</span>,</p>
<p><span class="math display">\[ \sqrt{\frac{\sum_i (2r_i)^2}{2n}} =
\sqrt{\frac{4 \sum_i r_i^2}{2n}}  =
\sqrt{\frac{4 C}{2n}}  
\]</span></p>
<p>Eski veri seti için aynı hesap <span
class="math inline">\(\sqrt{C/n}\)</span>. İki tarafta da karekök var,
sadece karekök içine bakalım,</p>
<p><span class="math display">\[  
\frac{C}{n} \quad ? \quad \frac{4 C}{2n}
\]</span></p>
<p>Aradaki ilişki nedir? Eğer veriyi ikiye katlarsak <span
class="math inline">\(C\)</span> 4 katına çıkıyor, ama herhangi bir
<span class="math inline">\(n &gt; 2\)</span> için, <span
class="math inline">\(2n\)</span> bu büyümeyi geçer, ve sağdaki büyüklük
soldakina nazaran küçülür.</p>
<p><span class="math display">\[  
\frac{C}{n} \quad &gt; \quad \frac{4 C}{2n}, \qquad n&gt;2 \textrm{ için
}
\]</span></p>
<p>Demek ki yeni veri setinde standard hata küçülür. Eğer bu değer
küçülürse, katsayılara ait olan standart hatalar da, ki onlar biraraya
gelerek standart hatayı oluşturacaklar, küçülecektir. Standart hatanın
küçülmesi aslında şaşırtıcı olmamalı, aynı yönde daha fazla veri alınca
elimizdeki katsayılarından daha “emin’’ hale geldik. Bu iyi bir şey
olarak görülebilirdi belki, ama bu durumun modelin geri kalanı
üzerindeki etkilerini şimdi düşünelim. Eğer katsayı aynı kalır, hata
küçülürse katsayı / hata olarak hesaplanan t değeri buyur. Daha büyüyen
t değeri daha küçülen p-değeri demektir! Yani veriyi ikiye katlayınca
birden bire önemsiz olan (<span class="math inline">\(&gt;0.05\)</span>)
bir değişken, önemli hale gelebilir. Altta örneğini görüyoruz,</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.ols(<span class="st">&#39;y~x1+x2+x3+x4&#39;</span>, data<span class="op">=</span>df).fit()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (results.summary())</span></code></pre></div>
<pre class="text"><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       1.000
Model:                            OLS   Adj. R-squared:                  1.000
Method:                 Least Squares   F-statistic:                 4.230e+05
Date:                Thu, 03 Jul 2025   Prob (F-statistic):          5.97e-201
Time:                        12:12:50   Log-Likelihood:                -131.99
No. Observations:                 100   AIC:                             274.0
Df Residuals:                      95   BIC:                             287.0
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      9.9495      0.094    106.098      0.000       9.763      10.136
x1           -99.9533      0.097  -1031.975      0.000    -100.146     -99.761
x2            -0.0410      0.095     -0.432      0.667      -0.230       0.148
x3            75.1472      0.102    733.851      0.000      74.944      75.350
x4             0.0486      0.100      0.486      0.628      -0.150       0.247
==============================================================================
Omnibus:                        3.126   Durbin-Watson:                   2.267
Prob(Omnibus):                  0.210   Jarque-Bera (JB):                2.795
Skew:                          -0.191   Prob(JB):                        0.247
Kurtosis:                       3.724   Cond. No.                         1.26
==============================================================================
</code></pre>
<p>Veriyi ikiye katlayıp bir daha OLS,</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> pd.concat((df,df))</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.ols(<span class="st">&#39;y~x1+x2+x3+x4&#39;</span>, data<span class="op">=</span>df2).fit()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (results.summary())</span></code></pre></div>
<pre class="text"><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       1.000
Model:                            OLS   Adj. R-squared:                  1.000
Method:                 Least Squares   F-statistic:                 8.683e+05
Date:                Thu, 03 Jul 2025   Prob (F-statistic):               0.00
Time:                        12:13:04   Log-Likelihood:                -263.98
No. Observations:                 200   AIC:                             538.0
Df Residuals:                     195   BIC:                             554.4
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      9.9495      0.065    152.006      0.000       9.820      10.079
x1           -99.9533      0.068  -1478.512      0.000    -100.087     -99.820
x2            -0.0410      0.066     -0.619      0.537      -0.172       0.090
x3            75.1472      0.071   1051.389      0.000      75.006      75.288
x4             0.0486      0.070      0.696      0.487      -0.089       0.186
==============================================================================
Omnibus:                        4.922   Durbin-Watson:                   2.274
Prob(Omnibus):                  0.085   Jarque-Bera (JB):                5.589
Skew:                          -0.191   Prob(JB):                       0.0611
Kurtosis:                       3.724   Cond. No.                         1.26
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<p>Görüldüğü gibi <code>x4</code> artık <span
class="math inline">\(&lt;0.05\)</span> altında!</p>
<p>OLS’in bu tür nüanslarını bilmek iyi olur. Eğer veride tekrar varsa,
herhangi bir sebeple, tekrarlayan verileri çıkartmak belki de mantıklı
olacaktır.</p>
<p>ABD Başkanlık Yarışını Tahmin Etmek</p>
<p>ABD başkanlık yarışlarının oldukça tahmin edilebilir olduğu uzunca
süredir iddia edilmektedir. Bu alanda pek çok model var, Andrew
Gelman’ın oldukça çetrefil, MCMC kullanan modelinden [4] (ki bunun için
başkasının yazdığı kod [5]’te bulunabilir), ya da daha öz, basit bir
metot [3] mevcuttur. En basit ve etkili yöntem <em>Değişim Zamanı</em>
(<em>Time for Change</em>) modeli, bu modele göre başkanlık yarışının
olduğu Haziran ayı itibariyle ekonomik büyüme yüzdesi
(<code>gdp_growth</code>), mevcut başkanın net destek oranı
(<code>net_approval</code>, ki bu rakam destek yüzdesinden desteklemeyen
yüzdesi çıkartılarak hesaplanır) ve o anki başkanının partisinin, 2
dönem ya da daha fazladır Beyaz Ev’de olup olmadığı bilgisi 1/0 değeri
ile kodlanarak (<code>two_terms)</code> lineer regresyona verilir ve
hedef değişken olarak, yönetimi elinde tutan partinin ülke genelinde tüm
oyların (popular vote) yüzde kaç alacağı tahmin edilmeye uğraşılır.</p>
<p>Örnek olarak Clinton ve Bush I arasındaki 1992 yarısında Cumhuriyetçi
adayın (çünkü o zamanki başkan Cumhuriyetçi) yüzde kaç oy alacağı tahmin
edilecek, <code>two_terms=1</code> çünkü iki dönem Cumhuriyetçi Reagan
ardından bir dönem Cumhuriyetçi Bush gelmiş, Cumhuriyetçiler uzun
süredir baştalar.</p>
<p>Gore / Bush arasındaki 2000 yılı yarısında Demokratların yüzdesini
tahmin etmeye uğraşıyoruz, çünkü başta Demokrat Clinton var, ve iki
dönemdir orada. Net popülarite ve büyüme hep o anki başkan ve onun
partisinin performansı ile alakalı. Bu regresyonu işlettiğimizde,
sonuçlar şöyle,</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;prez.csv&#39;</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df.head() , <span class="st">&#39;</span><span class="ch">\n</span><span class="st">&#39;</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>regr <span class="op">=</span> <span class="st">&#39;incumbent_vote ~ gdp_growth + net_approval + two_terms&#39;</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.ols(regr, data<span class="op">=</span>df).fit()</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (results.summary())</span></code></pre></div>
<pre class="text"><code>   year  gdp_growth  net_approval  two_terms  incumbent_vote
0  2016         1.2           0.0       48.6             NaN
1  2012         1.3          -0.8        0.0            52.0
2  2008         1.3         -37.0        1.0            46.3
3  2004         2.6          -0.5        0.0            51.2
4  2000         8.0          19.5        1.0            50.3 

                            OLS Regression Results                            
==============================================================================
Dep. Variable:         incumbent_vote   R-squared:                       0.901
Model:                            OLS   Adj. R-squared:                  0.878
Method:                 Least Squares   F-statistic:                     39.52
Date:                Thu, 03 Jul 2025   Prob (F-statistic):           8.50e-07
Time:                        12:13:14   Log-Likelihood:                -32.747
No. Observations:                  17   AIC:                             73.49
Df Residuals:                      13   BIC:                             76.83
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
================================================================================
                   coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------
Intercept       51.4363      0.811     63.409      0.000      49.684      53.189
gdp_growth       0.5799      0.118      4.903      0.000       0.324       0.835
net_approval     0.0987      0.021      4.764      0.000       0.054       0.143
two_terms       -4.2983      1.032     -4.164      0.001      -6.528      -2.069
==============================================================================
Omnibus:                        0.333   Durbin-Watson:                   1.545
Prob(Omnibus):                  0.847   Jarque-Bera (JB):                0.484
Skew:                          -0.169   Prob(JB):                        0.785
Kurtosis:                       2.246   Cond. No.                         71.4
==============================================================================
</code></pre>
<p>İnanılmaz bir başarı, <code>Prob (F-statistic)</code> değeri
neredeyse sıfır, <code>Adj. R-squared</code> değeri yüzde 80’den daha
fazla, tüm değişkenler istatistiki olarak önemli (<code>P&gt;|t|</code>
değerleri 0.05’ten küçük).</p>
<p>Acaba bu modeli kullanarak geçmişteki yarışları “tahmin etsek’’ sonuç
ne olurdu diye merak ediyoruz, bunun için tahmin edeceğimiz senenin veri
noktasını dışarıda bırakarak (out-of-sample) regresyon işletip o seneyi
bilmiyormuş gibi yapıp tahmin ediyoruz,</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> out_of_sample_pred(year):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    df2 <span class="op">=</span> df[df[<span class="st">&#39;year&#39;</span>] <span class="op">!=</span> year]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    results2 <span class="op">=</span> smf.ols(regr, data<span class="op">=</span>df2).fit()</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    conf <span class="op">=</span> results2.conf_int()</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> np.array(df[df[<span class="st">&#39;year&#39;</span>] <span class="op">==</span> year])[<span class="dv">0</span>][:<span class="op">-</span><span class="dv">1</span>]<span class="op">;</span> pred[<span class="dv">0</span>] <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(pred, conf)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># o senenin verisinin disarida birakarak gecmisi tahmin et</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;bush/clinton&#39;</span>)<span class="op">;</span> <span class="bu">print</span> (out_of_sample_pred(<span class="dv">1992</span>))</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;gore/bush&#39;</span>)<span class="op">;</span> <span class="bu">print</span> (out_of_sample_pred(<span class="dv">2000</span>))</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;bush/kerry&#39;</span>)<span class="op">;</span> <span class="bu">print</span> (out_of_sample_pred(<span class="dv">2004</span>))</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;mccain/obama&#39;</span>)<span class="op">;</span> <span class="bu">print</span> (out_of_sample_pred(<span class="dv">2008</span>))</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;obama/romney&#39;</span>)<span class="op">;</span> <span class="bu">print</span> (out_of_sample_pred(<span class="dv">2012</span>))</span></code></pre></div>
<pre class="text"><code>bush/clinton
[43.68758927 52.47911415]
gore/bush
[48.31291287 60.68132985]
bush/kerry
[50.66667848 55.79188333]
mccain/obama
[41.05409775 46.15966954]
obama/romney
[49.81182614 54.45584122]</code></pre>
<p>Tahmin hesabında değişken katsayılarının %95 güven aralıklarını veren
<code>conf_int()</code> çağrısını kullandık, değişkenlerin noktasal
değerlerini kullanmadık, bu şekilde tahmine olan güvenimizi aralığın
büyüklüğüne bakarak görebilmiş olacağız. Dikkat: aslında
<em>tahminin</em> güven aralığını hesaplamak biraz daha ek iş
gerektiriyor, türetilmesi [12] bölümünde.</p>
<p>Şimdi sonuçlara bakalım; Bush / Kerry yarışı için kesin Bush diyor
(çünkü güven aralığının iki ucu da yüzde 50 üstünde), Bush kazandı.
McCain / Obama için McCain kesin kaybedecek diyor, McCain kaybetti.
Obama / Romney yarışı için Obama (neredeyse) kesin kazanacak diyor,
Obama kazandı. Tahminler iyi!</p>
<p>Gore / Bush ilginç bir durum, Gore çok, çok daha şanslı, ama Gore
kaybetti. Fakat bu seçimin ne kadar yakın olduğunu o zaman yarısı takip
edenler hatırlar, ayrıca, Florida’da bir takım “şaibeli’’ işlerin (!)
olduğu biliniyor, <em>ve</em> model ülke genelinde oyu tahmin etmeye
uğraşıyor, ki ülke genelinde bakılınca Gore daha fazla oy almıştı.
Amerikan sistemine göre başkanlık seçimleri de eyalet bazında
hesaplanır, bir eyalette kazanan tüm oyları alır, bu sebeple ülke geneli
ile eyalet bazı arasında uyumsuzluk ortaya çıkabiliyor.</p>
<p>Gore / Bush olayına bir diğer bakış açısı şöyle: oy yüzdesi tahminini
yüzdenin kendisi için değil, kazanma / kazanmama için bir sinyal olarak
kabul etmek, yani popüler oyun kime gittiğine bakmamak, o zaman
modelimizin Göre / Bush seçimini başarısız tahmin ettiğini kabul etmek
lazım. Bu şaşırtıcı değil aslında çünkü 2000’de Bush kazandığına kendisi
bile şaşırmıştı.</p>
<p>2016 senesindeki yarışta kim kazanacak? Demokratların şansı şöyle
(dikkat belli bir adaydan bahsetmiyoruz bile); Haziran 2016 itibariyle
büyüme 2%, Obama’nın net popülaritesi sıfır olduğu durumda (bu
değişkenlerin ne olduğuna o tarihte tekrar bakılmalı),</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>conf <span class="op">=</span> results.conf_int()</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> [<span class="fl">1.</span>, <span class="fl">2.0</span>, <span class="fl">0.0</span>, <span class="dv">1</span>]</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (np.dot(pred, conf), np.dot(pred, results.params))</span></code></pre></div>
<pre class="text"><code>[43.80446415 52.79105137] 48.297757758277356</code></pre>
<p>Yani Demokrat adayın kaybetme şansı daha fazla, her ne kadar kesin
bir şey söylenemezse de, güven aralığının iki ucu da yüzde 50 altında
(ya da üstünde) değil, Hillary Clinton’un işi zor olacaktı, ki kaybetti.
Trump ülke genelinde oy çoğunluğunu kaybetti, ama eyalet bazında
kazandı. Demek ki model tahminini kazanma sinyali olarak almak daha
uygun.</p>
<p>Analiz</p>
<p>Model oldukça basit, 3 değişken ile tahmin yapılıyor, fakat bu
basitlik aldatıcı olabilir. Modele neyin dahil edildiği yanında neyin
dahil edilmediği de önemlidir, mesela.. ham petrol fiyatı, işsizlik,
seçim yılındaki suç oranı, iklim vs kullanılmamış, sadece bu 3 değişken
kullanılmış. Ya da model, Cumhuriyetçiler için ayrı, Demokratlar için
ayrı bir tahmin üretmiyor, {} onun başarısını tahmin etmeye uğraşıyor.
Yani bir bakıma iddiası şu, insanlar aslında başta olan partiye göre oy
verirler, bir süre sonra (2 dönem ardından) onu değiştirmeye meyilli
olurlar, ve o anda başta olan başkanın popülaritesi ve genel bir
ekonomik performansını kullanarak onun partisi hakkında bir tamam /
devam kararını verirler. Bu tür modelcilik yetenek ister. Basitlik zor
iş!</p>
<p>Tahmin edilirliğin yüksekliği ve değişkenlerin azlığı hakkında bir
diğer yorum; bu durum aslında o kadar da şaşırtıcı olmamalı belki de,
çünkü başkanlık seçimi son derece kaba hatlı bir karar, tek bir kişi /
parti hakkında karar veriliyor, ve doğal olarak seçim için kullanılan
parametreler de oldukça genel. Bir bakıma, bu tahmin edilirlik iyi
olarak ta görülebilir, stabilite, sakin ortamın işareti olarak
algılanabilir. “Vay o taraf ne dedi, bu taraf ne dedi’’ gibi faktörlerle
oylar haldır huldur inip çıkmıyor, belli genel parametreler ışığında
sonuç ta dört ay önceden oldukça belli (baz veri Haziran sonu itibariyle
alınır, seçim Kasım ayında).</p>
<p>Model Karşılaştırmak</p>
<p>Bu alanda, mesela gazetelerde, yorumlara rastlanıyor. Bunlardan biri
“mevcut başkanın (incumbent) ikinci dönem için yarışa girerse avantajlı
olduğu’’ söylemidir, ki üstteki modelin ilk halini keşfeden Abromitz de
bunu söylemektedir. Bizim referans aldığımız model [6] o söylemi biraz
değiştirmiş, avantajlı olan yerindeki başkan değil,
<em>dezavantajlı</em> olan 2 dönemden fazla başta kalan <em>parti</em>.
İnsanlar 2 veya daha dönemden fazla başta olan partiyi görevden almaya
meyilli oluyor. Tabii eğer parti yeni başa gelmişse, o zaman dezavantaj
olmadığı için bazı durumlarda”ilk dönem başkan avantajlıymış gibi’’
durmuş olabilir. Şimdi bu faraziyeyi test edelim, hangi model daha
doğru? Yeni bir veri setinde bu değişikliği test edebiliriz,</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;prez_incumb.csv&#39;</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>regr <span class="op">=</span> <span class="st">&#39;incumbent_vote ~ gdp_growth + net_approval + incumb_prez&#39;</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> smf.ols(regr, data<span class="op">=</span>df).fit()</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (results.aic)</span></code></pre></div>
<pre class="text"><code>91.79663654753915</code></pre>
<p>AIC sonucu arttı, bu modelin daha kötüleştiği anlamına gelir.</p>
<p>Not: Gayri-safi yurtiçi hasıla (GDP) 2. çeyrekteki artışına
bakılıyor. Bu artış bir sene önceye kıyasla değil (year-over-year) bir
önceki çeyreğe göre artıştır dikkat, ve sonra bu artış, yıl ölçeğine
çıkartılır, <span class="math inline">\(d\)</span> artışı diyelim <span
class="math inline">\((1+\d)^4 - 1\)</span> formülü üzerinden. Yani “her
çeyrekte artış <span class="math inline">\(d\)</span> olsaydı, tüm sene
artışı nereye gelirdi?’’ sorusunun cevabı.</p>
<p>Kaynaklar</p>
<p>[1] Teetor, <em>R Cookbook</em></p>
<p>[2] The Yhat Blog ,<em>Fitting &amp; Interpreting Linear Models in
R</em>, <a
href="http://blog.yhathq.com/posts/r-lm-summary.html">http://blog.yhathq.com/posts/r-lm-summary.html</a></p>
<p>[3] Abramowitz, {}, <a
href="http://www.centerforpolitics.org/crystalball/articles/abramowitzpolarizationmodel/">http://www.centerforpolitics.org/crystalball/articles/abramowitzpolarizationmodel/</a></p>
<p>[4] Gelman, A., <em>Bayesian Data Analysis</em></p>
<p>[5] Bayramlı, <em>Books Data</em>, <a
href="https://github.com/burakbayramli/books/tree/master/Gelman_BDA_ARM/bda/election">https://github.com/burakbayramli/books/tree/master/Gelman_BDA_ARM/bda/election</a></p>
<p>[6] Linzer, <em>R Code</em>, <a
href="https://github.com/dlinzer/BayesBARUG/blob/master/Linzer-BayesBARUG.R">https://github.com/dlinzer/BayesBARUG/blob/master/Linzer-BayesBARUG.R</a></p>
<p>[7] Bayramlı, Çok Değişkenli Calculus, <em>Ders 9</em></p>
<p>[8] Bayramlı, Lineer Cebir, <em>Ders 15</em></p>
<p>[9] Bayramlı, Bilgisayar Bilim, Yapay Zeka, <em>Regresyon, En Az
Kareler</em></p>
<p>[10] Bayramlı, Istatistik, <em>Tahmin Aralıkları</em></p>
<p>[11] Bayramlı, Istatistik, <em>Güven Aralıkları, Hipotez
Testleri</em></p>
<p>[12] Bayramlı, Istatistik, <em>Tahmin Aralıkları (Prediction
Interval)</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
