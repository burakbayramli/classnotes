<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Boltzman Makinaları (Rasgele Hopfield Ağları)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="boltzman-makinaları-rasgele-hopfield-ağları">Boltzman Makinaları
(Rasgele Hopfield Ağları)</h1>
<p>Alttaki ifade bir Boltmann dağılımını gösterir,</p>
<p><span class="math display">\[  
P(x;W) = \frac{1}{Z(W)}
\exp \bigg[ \frac{1}{2} x^T W x \bigg]
\qquad (3)
\]</span></p>
<p>ki <span class="math inline">\(x\)</span> çok boyutlu ve -1,+1
değerleri içeren bir vektör, <span class="math inline">\(W\)</span>
simetrik ve çaprazında (diagonal) sıfır içeren bir matristir, <span
class="math inline">\(n \times d\)</span> boyutlarındaki bir veri için
<span class="math inline">\(d \times d\)</span> boyutlarında olacaktır.
Boltzmann Makinaları (BM), Kısıtlı Boltzmann Makinaları (Restricted
Boltzmann Machines) kavramına geçiş yapmadan önce iyi bir durak
noktası.</p>
<p>BM <span class="math inline">\(W\)</span> içinde aslında tüm
değişkenlerin ikisel ilişkisini içerir. <span
class="math inline">\(W\)</span> çok değişkenli Gaussian dağılımındaki
<span class="math inline">\(\Sigma\)</span>’da olduğu gibi ikisel
bağlantıları saptar. Veriden <span class="math inline">\(W\)</span>’yu
öğrenmek için olurluğu hesaplamak lazım. Olurluk (likelihood)</p>
<p><span class="math display">\[  
\prod_{n=1}^{N} P(x^{(n)};W) = \frac{1}{Z(W)}
\exp \bigg[ \frac{1}{2} x^{(n)^T} W x^{(n)} \bigg]
\]</span></p>
<p>Log olurluk</p>
<p><span class="math display">\[  
\mathcal{L} = \ln \big( \prod_{n=1}^{N} P(x^{(n)};W) \big) =
\sum_{n=1}^{N} \bigg[ \frac{1}{2} x^{(n)^T} W x^{(n)} - \ln Z(W) \bigg]
\qquad (1)
\]</span></p>
<p>Birazdan <span class="math inline">\(\frac{\partial \mathcal
L}{\partial w_{ij}}\)</span> türevini alacağız, o sırada <span
class="math inline">\(\ln Z(W)\)</span>’nin türevi lazım, daha doğrusu
<span class="math inline">\(Z(W)\)</span>’yi nasıl türevi alınır hale
getiririz?</p>
<p><span class="math inline">\(Z(W)\)</span> normalizasyon sabiti
olduğuna göre, dağılımın geri kalanının sonsuzlar üzerinden entegrali
(ya da toplamı) normalizasyon sabitine eşittir,</p>
<p><span class="math display">\[
Z(W) = \sum_x  \exp \bigg[ \frac{1}{2} x^T W x \bigg]
\]</span></p>
<p><span class="math display">\[
\ln Z(W) = \ln \bigg[ \sum_x  \exp \big( \frac{1}{2} x^T W x \big)
\bigg]
\]</span></p>
<p>Log bazlı türev alınca log içindeki herşey olduğu gibi bölüme gider,
ve log içindekinin türevi alınırak bölüme koyulur. Fakat log içine
dikkatli bakarsak bu zaten <span class="math inline">\(Z(W)\)</span>’nin
tanımıdır, böylece denklemi temizleme şansı doğdu, bölüme hemen <span
class="math inline">\(Z(W)\)</span> deriz, ve türevi log’un içine
uygularız,</p>
<p><span class="math display">\[
\frac{\partial}{\partial w_{ij}} \ln Z(W) =
\frac{1}{Z(W)}
\bigg[
\sum_x \frac{\partial}{\partial w_{ij}} \exp \big( \frac{1}{2} x^T W x
\big)
\bigg]
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial w_{ij}} \exp \big( \frac{1}{2} x^T W x \big)  =
\frac{1}{2}  \exp \big( \frac{1}{2} x^T W x \big)
\frac{\partial}{\partial w_{ij}}x^T W x
\qquad (2)
\]</span></p>
<p>(2)’in içindeki bölümü açalım,</p>
<p><span class="math display">\[
\frac{\partial}{\partial w_{ij}}x^T W x = x_i x_j
\]</span></p>
<p>Şimdi (2)’ye geri koyalım,</p>
<p><span class="math display">\[
= \frac{1}{2} \exp \big( \frac{1}{2} x^T W x \big) x_i x_j
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial w_{ij}} \ln Z(W) =
\frac{1}{Z(W)}
\bigg[
\sum_x \frac{1}{2}  \exp \big( \frac{1}{2} x^T W x \big) x_i x_j
\bigg]
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2} \sum_x
\frac{1}{Z(W)} \exp \big( \frac{1}{2} x^T W x \big) x_i x_j
\]</span></p>
<p><span class="math display">\[
= \frac{1}{2}  \sum_x P(x;W) x_i x_j
\]</span></p>
<p>Üstteki son ifadede bir kısaltma kullanalım,</p>
<p><span class="math display">\[
\sum_x P(x;W) x_i x_j =  &lt; x_i,x_j &gt;_{P(x;W)}
\qquad (4)
\]</span></p>
<p>Artık <span class="math inline">\(\ln Z(W)\)</span>’nin türevini
biliyoruz. O zaman tüm log olurluğun türevine (1) dönebiliriz,</p>
<p><span class="math display">\[  
\frac{\partial \mathcal{L}}{\partial w_{ij}} =
\sum_{n=1}^{N} \bigg[
\frac{\partial}{\partial w_{ij}}  \frac{1}{2} x^{(n)^T} W x^{(n)} -
\frac{\partial}{\partial w_{ij}}  \ln Z(W) \bigg]
\]</span></p>
<p><span class="math display">\[  
= \sum_{n=1}^{N}
\bigg[
\frac{1}{2} x_i^{(n)^T}x_j^{(n)} -
\frac{\partial}{\partial w_{ij}}  \ln Z(W)
\bigg]
\]</span></p>
<p><span class="math display">\[  
= \sum_{n=1}^{N}
\bigg[
\frac{1}{2} x_i^{(n)^T}x_j^{(n)} -
\frac{1}{2}&lt; x_i x_j &gt;_{P(x;W)}
\bigg]
\]</span></p>
<p>1/2 sabitlerini atalım,</p>
<p><span class="math display">\[  
= \sum_{n=1}^{N}
\bigg[
x_i^{(n)^T}x_j^{(n)} - &lt; x_i x_j &gt;_{P(x;W)}
\bigg]
\]</span></p>
<p>Eğer</p>
<p><span class="math display">\[
&lt; x_i x_j &gt;_{Data} = \frac{1}{N}
\sum_{n=1}^{N}  x_i^{(n)^T}x_j^{(n)}
\]</span></p>
<p>olarak alırsak, eşitliğin sağ tarafı verisel kovaryansı (empirical
covariance) temsil eder. Düzenleyince,</p>
<p><span class="math display">\[
N \cdot &lt; x_i x_j &gt;_{Data} = \sum_{n=1}^{N}  x_i^{(n)^T}x_j^{(n)}
\]</span></p>
<p>şimdi eşitliğin sağ tarafı üç üstteki formüle geri koyulabilir,</p>
<p><span class="math display">\[
\frac{\partial \mathcal{L}}{\partial w_{ij}}  =
N \big[ &lt; x_i x_j &gt;_{Data}  - &lt; x_ix_j &gt;_{P(x;W)} \big]
\]</span></p>
<p>Her ne kadar <span class="math inline">\(N\)</span> veri noktası
sayısını gösteriyor olsa da, üstteki ifade bir gradyan güncelleme
formülü olarak ta görülebilir, ve <span class="math inline">\(N\)</span>
yerine bir güncelleme sabiti alınabilir. Gradyan güncelleme olarak
görülebilir çünkü <span class="math inline">\(w_{ij}\)</span>’ye göre
türev aldık, o zaman bizi <span
class="math inline">\(\mathcal{L}\)</span>’in minimumuna götürecek <span
class="math inline">\(w\)</span> adımları üstte görüldüğü gibidir.</p>
<p>(4)‘te görülen <span class="math inline">\(&lt; x_ix_j
&gt;_{P(x;W)}\)</span>’in anlamı nedir? Bu ifade mümkün tüm <span
class="math inline">\(x\)</span> değerleri üzerinden alınıyor ve ikisel
ilişkilerin olasılığını “mevcut modele’’ göre hesaplıyor. Yani bu ifade
de bir korelasyon hesabıdır, sadece veriye göre değil, tüm mümkün
değerler ve model üzerinden alınır. Bu hesabı yapmak oldukça zordur,
fakat yaklaşıksal olarak Monte Carlo yöntemi ile hesaplanabilir. Nihayet
MC ve MCMC metotlarının kullanılma sebebini görmeye başlıyoruz; bu
metotlar zaten aşırı yüksek boyutlu, analitik çözümü olmayan,
hesaplanamaz (intractable) entegraller (ya da toplamlar) için
keşfedilmiştir.</p>
<p>Yani bu ifadeyi hesaplamak için Monte Carlo simulasyonu kullanacağız.
Tüm değerleri teker teker ziyaret etmek yerine (ki bu çok uzun zaman
alırdı) mevcut modele en olası <span class="math inline">\(x\)</span>
değerleri “ürettireceğiz’‘, ve bu değerleri alıp sanki gerçek veriymiş
gibi sayısal korelasyonlarını hesaplayacağız. Eğer veriler dağılımın en
olası noktalarından geliyorlarsa, elimizde veri dağılımı “iyi’’ temsil
eden bir veri setidir. Daha sonra bu korelasyon hesabını değeri gerçek
veri korelasyonunundan çıkartıp bir sabit üzerinden gradyan adımı
atmamız mümkün olacak.</p>
<p>Gibbs Örneklemesi (Sampling)</p>
<p>Gibbs örneklemesinin detayları için [5]. Bolzmann dağılımından
örneklem almak için bize tek bir değişken (hücre) haricinde diğer
hepsinin bilindiği durumun olasılık hesabı lazım, yani koşulsal olasılık
<span class="math inline">\(P(x_i = 1 | x_j, j \ne i)\)</span>. Yani
<span class="math inline">\(x\)</span> üzerinde, biri hariç tüm öğelerin
bilindiği durumda bilinmeyen tek hücre <span
class="math inline">\(i\)</span>’nin 1 olma olasılık değeri,</p>
<p><span class="math display">\[
P(x_i = 1 | x_j, j \ne i) = \frac{1}{1 + e^{-a_i}}
\]</span></p>
<p>ve,</p>
<p><span class="math display">\[
a_i = \sum_j  w_{ij}x_j
\]</span></p>
<p>Bu koşulsal olasılığın temiz / basit bir formül olması önemli,
üstteki görülen bir sigmoid fonksiyonu bu türden bir fonksiyondur… Bu
fonksiyonlar hakkında daha fazla bilgi [6] yazısında bulunabilir.</p>
<p>Ama, ana formül (3)‘ten bu noktaya nasıl eriştik? Bu noktada biraz
türetme yapmak lazım. <span class="math inline">\(x\)</span> vektörü
içinde sadece <span class="math inline">\(x_i\)</span> öğesinin <span
class="math inline">\(b\)</span> olmasını <span
class="math inline">\(x^b\)</span> olarak alalım. Önce koşulsal
dağılımda “verili’’ olan kısmı elde etmek lazım. O uzaman</p>
<p><span class="math display">\[
P(x_j,j \ne i) = P(x^0) + P(x^1)
\]</span></p>
<p>Bu bir marjinalizasyon ifadesi, tüm olası <span
class="math inline">\(i\)</span> değerleri üzerinde bir toplam alınca
geri kalan <span class="math inline">\(j\)</span> değerlerinin
dağılımını elde etmiş oluruz.</p>
<p><span class="math display">\[  
P(x_i = 1 | x_j,j \ne i)  = \frac{P(x^1)}{P(x^0) + P(x^1)}
\]</span></p>
<p>çünkü <span class="math inline">\(P(A|B) = P(A,B) / P(B)\)</span>
bilindiği gibi, ve <span class="math inline">\(P(x^1)\)</span> içinde
<span class="math inline">\(x_1=1\)</span> setini içeren tüm veriler
üzerinden.</p>
<p>Eşitliğin sağ tarafında <span class="math inline">\(P(x^1)\)</span>’i
bölen olarak görmek daha iyi, ayrıca ulaşmak istediğimiz <span
class="math inline">\(1/1 + e^{-a_i}\)</span> ifadesinde <span
class="math inline">\(+1\)</span>’den kurtulmak iyi olur, böylece sadece
<span class="math inline">\(e^{-a_i}\)</span> olan eşitliği ispatlarız.
Bunun her iki denklemde ters çevirip 1 çıkartabiliriz,</p>
<p><span class="math display">\[  
1 / P(x_i = 1 | x_j,j \ne i) = \frac{P(x^0) + P(x^1)}{P(x^1)}
\]</span></p>
<p><span class="math display">\[
= 1 + \frac{ P(x^0)}{P(x^1)}
\]</span></p>
<p>Bir çıkartırsak, <span class="math inline">\(\frac{
P(x^0)}{P(x^1)}\)</span> kalır. Bu bize ulaşmak istediğimiz denklemde
<span class="math inline">\(e^{-a_i}\)</span> ibaresini bırakır. Artık
sadece <span class="math inline">\(\frac{P(x^0)}{P(x^1)}\)</span>’in
<span class="math inline">\(e^{-a_i}\)</span>’e eşit olduğunu göstermek
yeterli.</p>
<p><span class="math display">\[
\frac{ P(x^0)}{P(x^1)} = \exp( x^{0^T}Wx^0 -   x^{1^T}Wx^1 )
\]</span></p>
<p>Şimdi <span class="math inline">\(x^TWx\)</span> gibi bir ifadeyi
indisler bazında açmak için şunları yapalım,</p>
<p><span class="math display">\[
x^TWx = \sum_{k,j} x_kx_jw_{kj}
\]</span></p>
<p>Üstteki çok iyi bilinen bir açılım. Eğer</p>
<p><span class="math display">\[
\sum_{k,j} \underbrace{x_kx_jw_{ij}}_{Y_{kj}} = \sum_{k,j}Y_{kj}
\]</span></p>
<p>alırsak birazdan yapacağımız işlemler daha iyi görülebilir. Mesela
<span class="math inline">\(k=i\)</span> olan durumu dış toplamdan
dışarı çekebiliriz</p>
<p><span class="math display">\[
= \sum_{k \ne i}\sum_j Y_{kj} + \sum_{j} Y_{ij}
\]</span></p>
<p>Daha sonra <span class="math inline">\(j = i\)</span> olan durumu iç
toplamdan dışarı çekebiliriz,</p>
<p><span class="math display">\[
= \sum_{k \ne i}( \sum_{j \ne i} Y_{kj} + Y_{ki}) + \sum_{j} Y_{ij}
\]</span></p>
<p>İç dış toplamları birleştirelim,</p>
<p><span class="math display">\[
= \sum_{k \ne i,j \ne i} Y_{kj} + \sum_{k \ne i}  Y_{ki} + \sum_{j}
Y_{ij}
\]</span></p>
<p><span class="math display">\[
= \sum_{k \ne i,j \ne i} Y_{kj} + \sum_{k}  Y_{ki} + \sum_{j} Y_{ij} +
Y_{ii}
\]</span></p>
<p>Üstteki ifadeyi <span class="math inline">\(\exp( x^{0^T}Wx^0 -
x^{1^T}Wx^1 )\)</span> için kullanırsak,</p>
<p><span class="math display">\[
\exp
\big(
\sum_{k}  Y_{ki}^0 + \sum_{j} Y_{ij}^0 + Y_{ii}^0 -
( \sum_{k}  Y_{ki}^1 + \sum_{j} Y_{ij}^1 + Y_{ii}^1  )
\big)
\]</span></p>
<p><span class="math inline">\(\sum_{k \ne i,j \ne i} Y_{kj}\)</span>
teriminin nereye gittiği merak edilirse, bu ifade <span
class="math inline">\(i\)</span>’ye dayanmadığı için bir eksi bir artı
olarak iki defa dahil edilip iptal olacaktı.</p>
<p><span class="math display">\[
= \exp \big(
0 - ( \sum_{k}  Y_{ki}^1 + \sum_{j} Y_{ij}^1 + Y_{ii}^1  )
\big)
\]</span></p>
<p><span class="math inline">\(W\)</span>’nin simetrik matris olduğunu
düşünürsek, <span class="math inline">\(\sum_{k} Y_{ki}^1\)</span> ile
<span class="math inline">\(\sum_{j}Y_{ij}^1\)</span> aynı ifadedir,</p>
<p><span class="math display">\[
= \exp \big(
- ( 2 \sum_{j} Y_{ij}^1 + Y_{ii}^1  )
\big)
\]</span></p>
<p><span class="math inline">\(W\)</span> sıfır çaprazlı bir matristir,
o zaman <span class="math inline">\(Y_{ii}^1=0\)</span>,</p>
<p><span class="math display">\[
= \exp \big( 2 \sum_{j} Y_{ij}^1 \big) = \exp (- 2 a_i )
\]</span></p>
<p>Orijinal dağılım denkleminde <span class="math inline">\(1/2\)</span>
ifadesi vardı, onu başta işlemlere dahil etmemiştik, edilseydi sonuç
<span class="math inline">\(\exp (- a_i)\)</span> olacaktı.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Boltzmann:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,n_iter<span class="op">=</span><span class="dv">100</span>,eta<span class="op">=</span><span class="fl">0.1</span>,sample_size<span class="op">=</span><span class="dv">100</span>,init_sample_size<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_iter <span class="op">=</span> n_iter</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eta <span class="op">=</span> eta</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sample_size <span class="op">=</span> sample_size</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_sample_size <span class="op">=</span> init_sample_size</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid(<span class="va">self</span>, u):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.</span><span class="op">/</span>(<span class="fl">1.</span><span class="op">+</span>np.exp(<span class="op">-</span>u))<span class="op">;</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> draw(<span class="va">self</span>, Sin,T):</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">        Bir Gibbs gecisi yaparak dagilimdan bir orneklem al</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        D<span class="op">=</span>Sin.shape[<span class="dv">0</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        S<span class="op">=</span>Sin.copy()</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        rand <span class="op">=</span> np.random.rand(D,<span class="dv">1</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">xrange</span>(D):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>            h<span class="op">=</span>np.dot(T[i,:],S)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>            S[i]<span class="op">=</span>rand[i]<span class="op">&lt;</span><span class="va">self</span>.sigmoid(h)<span class="op">;</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> S</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>, T):</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        N<span class="op">=</span>T.shape[<span class="dv">0</span>]        </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sigmoid(0) her zaman 0.5 olacak</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        s<span class="op">=</span>np.random.rand(N)<span class="op">&lt;</span><span class="va">self</span>.sigmoid(<span class="dv">0</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># alttaki dongu atlama / gozonune alinmayacak degerler icin        </span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">xrange</span>(<span class="va">self</span>.init_sample_size):</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            s<span class="op">=</span><span class="va">self</span>.draw(s,T)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        S<span class="op">=</span>np.zeros((N,<span class="va">self</span>.sample_size))</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        S[:,<span class="dv">0</span>]<span class="op">=</span>s</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># simdi degerleri toplamaya basla</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">xrange</span>(<span class="dv">1</span>,<span class="va">self</span>.sample_size):</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>            S[:,i]<span class="op">=</span><span class="va">self</span>.draw(S[:,i<span class="op">-</span><span class="dv">1</span>],T)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> S.T</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> normc(<span class="va">self</span>, X):</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="co">        normalizasyon sabitini dondur</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> f(x): <span class="cf">return</span> np.exp(<span class="fl">0.5</span> <span class="op">*</span> np.dot(np.dot(x,<span class="va">self</span>.W), x))</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        S <span class="op">=</span> <span class="dv">2</span><span class="op">*</span><span class="va">self</span>.sample(<span class="va">self</span>.W)<span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sozluk icinde anahtar tek x degeri boylece bir</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># olasilik degeri sadece bir kere toplanir</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> <span class="bu">dict</span>((<span class="bu">tuple</span>(s),f(s)) <span class="cf">for</span> s <span class="kw">in</span> S)</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.<span class="bu">sum</span>(res.values())</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        W<span class="op">=</span>np.zeros((X.shape[<span class="dv">1</span>],X.shape[<span class="dv">1</span>]))</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>        W_data<span class="op">=</span>np.dot(X.T,X)<span class="op">/</span>X.shape[<span class="dv">1</span>]<span class="op">;</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_iter):</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span> <span class="st">&#39;Iteration&#39;</span>, i</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>            S <span class="op">=</span> <span class="va">self</span>.sample(W)</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>            S <span class="op">=</span> (S<span class="op">*</span><span class="dv">2</span>)<span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>            W_guess<span class="op">=</span>np.dot(S.T,S)<span class="op">/</span>S.shape[<span class="dv">1</span>]<span class="op">;</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>            W <span class="op">+=</span> <span class="va">self</span>.eta <span class="op">*</span> (W_data <span class="op">-</span> W_guess)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>            np.fill_diagonal(W, <span class="dv">0</span>)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W <span class="op">=</span> W</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.C <span class="op">=</span> <span class="va">self</span>.normc(X)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_proba(<span class="va">self</span>, X):</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.diag(np.exp(<span class="fl">0.5</span> <span class="op">*</span> np.dot(np.dot(X, <span class="va">self</span>.W), X.T))) <span class="op">/</span> <span class="va">self</span>.C</span></code></pre></div>
<p>Fonksiyon <code>draw</code> içinde, tek bir veri satırı için ve
sırayla her değişken (hücre) için, diğer değişkenleri baz alıp diğerinin
koşulsal olasılığını hesaplıyoruz, ve sonra bu olasılığı kullanarak bir
sayı üretimi yapıyoruz. Üretimin yapılması için
<code>np.random.rand</code>’dan gelen 0 ve 1 arasındaki birörnek
(uniform) dağılımdan bir rasgele sayıyı geçip geçmeme irdelemesi
yeterli. Bir Bernoulli olasılık hesabını üretilen bir rasgele değişkene
bu şekilde çevirebilirsiniz. Bu niye işler? Üstte belirttiğimiz
irdelemeyi rasgele değişken olarak kodlarsak (ki bu da bir Bernoulli
rasgele değişkeni olur), ve birörnek rasgele değişken <span
class="math inline">\(U\)</span> olsun,</p>
<p><span class="math display">\[ Y =
\left\{ \begin{array}{ll}
1 &amp; U &lt; p \\
0 &amp; U \ge p \\
\end{array} \right.
\]</span></p>
<p>Bu durumda <span class="math inline">\(P(X=1) = P(U&lt;p) =
p\)</span> olurdu. Neden? Çünkü üstte bir sürekli (continuous) bir
birörnek değişken yarattık, ve <span class="math inline">\(P(U&lt;p) =
F_u(p) = p\)</span>.</p>
<p>Devam edelim; Çağrı <code>sample</code> ise <code>draw</code>‘u
kullanarak pek çok veri satırını içeren ve dağılımı temsil eden bir
örneklem yaratmakla sorumlu. Bunu her örneklem satırını baz alarak bir
sonrakini ürettirerek yapıyor, böylelikle MCMC’nin dağılımı “gezmesi’’
sağlanmış oluyor.</p>
<p>Normalizasyon Sabiti</p>
<p>Birazdan göreceğimiz örnek için normalizasyon sabitini de
hesaplamamız gerekecek. Niye? Mesela iki farklı BM dağılımını farklı
etiketli verilerden öğreniyoruz, sonra test veri noktasını her iki ayrı
dağılıma “soruyoruz’’? Olasılığı nedir? Bu noktada kesin bir olasılık
hesabı istediğimiz için artık <span class="math inline">\(Z\)</span>
bilinmek zorunda. Bu sabitin hesaplanması için ise <span
class="math inline">\(&lt; x_ix_j &gt;_{P(x;W)}\)</span> için olduğu
gibi, tüm mümkün <span class="math inline">\(x\)</span>’ler üzerinden
bir toplam gerekir, bu toplam <span class="math inline">\(\sum_x \exp
1/2 x^T W x\)</span> toplamı. Bu toplamın hesaplanması çok zor olduğu
için, yine MCMC’ye başvuracağız. Tek fark alınan örneklemi (3) formülüne
geceğiz, ve bir olasılık hesabı yapacağız, ve bu olasılıkları
toplayacağız. Tabii aynı <span class="math inline">\(x\)</span>’i (eğer
tekrar tekrar üretilirse -ufak bir ihtimal ama mümkün-) tekrar tekrar
toplamamak için hangi <span class="math inline">\(x\)</span>’lerin
üretildiğini bir sözlük içinde hatırlayacağız, yani bir <span
class="math inline">\(x\)</span> olasılığı sadece bir kere
toplanacak.</p>
<p>Şimdi ufak bir örnek üzerinde BM’i işletelim.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> boltz</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([<span class="op">\</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>[<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">1</span>],</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>[<span class="fl">1.</span>,<span class="fl">0.</span>,<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>[<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>],</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>],</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">1.</span>,<span class="dv">0</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>A[A<span class="op">==</span><span class="dv">0</span>]<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> boltz.Boltzmann(n_iter<span class="op">=</span><span class="dv">50</span>,eta<span class="op">=</span><span class="fl">0.01</span>,sample_size<span class="op">=</span><span class="dv">200</span>,init_sample_size<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>clf.fit(A)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;W&#39;</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> clf.W</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;normalizasyon sabiti&#39;</span>, clf.C</span></code></pre></div>
<pre><code>Iteration 0
Iteration 10
Iteration 20
Iteration 30
Iteration 40
W
[[ 0.    -0.065 -0.06  -0.055]
 [-0.065  0.     0.17   0.105]
 [-0.06   0.17   0.    -0.09 ]
 [-0.055  0.105 -0.09   0.   ]]
normalizasyon sabiti 16.4620358997</code></pre>
<p>Sonuç <span class="math inline">\(W\)</span> üstte görüldüğü gibi.
Örnek veriye bakarsak 2. satır 3. kolonda artı bir değer var, 1. satır
4. kolonda eksi değer var. Bu beklediğimiz bir şey çünkü 2. ve 3.
değişkenlerin arasında bir korelasyon var, <span
class="math inline">\(x_2\)</span> ne zaman 1/0 ise <span
class="math inline">\(x_3\)</span> te 1/0. Fakat <span
class="math inline">\(x_1\)</span> ile <span
class="math inline">\(x_4\)</span> ters bir korelasyon var,
birbirlerinin zıttı değerlere sahipler.</p>
<p>Şimdi yeni test verisini dağılıma “soralım’’,</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> np.array([<span class="op">\</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>[<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">1</span>],</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>[<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>,<span class="dv">0</span>],</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>[<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">1</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>])    </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> clf.predict_proba(test)</span></code></pre></div>
<pre><code>[ 0.0730905   0.05692294  0.0730905 ]</code></pre>
<p>Görüntü Tanıma</p>
<p>Elimizde el yazısı tanıma algoritmaları için kullanılan bir veri seti
var. Veride 0,5,7 harflerinin görüntüleri var. Mesela 5 için bazı örnek
görüntüler,</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/binarydigits.txt&#39;</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>label <span class="op">=</span> np.ravel(np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/bindigitlabels.txt&#39;</span>))</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>Y5 <span class="op">=</span> Y[label<span class="op">==</span><span class="dv">5</span>]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>plt.imshow(Y5[<span class="dv">0</span>,:].reshape((<span class="dv">8</span>,<span class="dv">8</span>),order<span class="op">=</span><span class="st">&#39;C&#39;</span>), cmap<span class="op">=</span>plt.cm.gray)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;boltzmann_01.png&#39;</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(Y5[<span class="dv">1</span>,:].reshape((<span class="dv">8</span>,<span class="dv">8</span>),order<span class="op">=</span><span class="st">&#39;C&#39;</span>), cmap<span class="op">=</span>plt.cm.gray)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;boltzmann_02.png&#39;</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>plt.imshow(Y5[<span class="dv">2</span>,:].reshape((<span class="dv">8</span>,<span class="dv">8</span>),order<span class="op">=</span><span class="st">&#39;C&#39;</span>), cmap<span class="op">=</span>plt.cm.gray)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;boltzmann_03.png&#39;</span>)</span></code></pre></div>
<p><img src="boltzmann_01.png" /> <img src="boltzmann_02.png" /> <img
src="boltzmann_03.png" /></p>
<p>Bu görüntüleri tanımak için BM kullanalım. Eğitim ve test olarak
veriyi ikiye ayıracağız, ve eğitim seti her etiketin <span
class="math inline">\(W\)</span>’sini öğrenmek için kullanılacak. Daha
sonra test setinde her veri noktalarını her üç BM’ye ayrı ayrı “sorup’’
o test verisinin o BM’e göre olasılığını alacağız, ve hangi BM daha
yüksek olasılık döndürüyorsa etiket olarak onu kabul edeceğiz. Hangi BM
daha yüksek olasılık döndürüyorsa, o BM”bu verinin benden gelme
olasılığı yüksek’’ diyor demektir, ve etiket o olmalıdır.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> neighbors</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, boltz</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cross_validation <span class="im">import</span> train_test_split</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/binarydigits.txt&#39;</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> np.ravel(np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/bindigitlabels.txt&#39;</span>))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(Y, labels, test_size<span class="op">=</span><span class="fl">0.4</span>,random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>X_train[X_train<span class="op">==</span><span class="dv">0</span>]<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>X_test[X_test<span class="op">==</span><span class="dv">0</span>]<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>clfs <span class="op">=</span> {}</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> [<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">7</span>]:</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> X_train[y_train<span class="op">==</span>label]</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    clf <span class="op">=</span> boltz.Boltzmann(n_iter<span class="op">=</span><span class="dv">30</span>,eta<span class="op">=</span><span class="fl">0.05</span>,sample_size<span class="op">=</span><span class="dv">500</span>,init_sample_size<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    clf.fit(x)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    clfs[label] <span class="op">=</span> clf</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> []</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> [<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">7</span>]:</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    res.append(clfs[label].predict_proba(X_test))</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>res3 <span class="op">=</span> np.argmax(np.array(res).T,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>res3[res3<span class="op">==</span><span class="dv">1</span>] <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>res3[res3<span class="op">==</span><span class="dv">2</span>] <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;Boltzmann Makinasi&#39;</span>, np.<span class="bu">sum</span>(res3<span class="op">==</span>y_test) <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(y_test))</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> neighbors.KNeighborsClassifier()</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>clf.fit(X_train,y_train)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>res3 <span class="op">=</span> clf.predict(X_test)    </span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;KNN&#39;</span>, np.<span class="bu">sum</span>(res3<span class="op">==</span>y_test) <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(y_test))</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>python testbm.py</span></code></pre></div>
<pre><code>Iteration 0
Iteration 10
Iteration 20
Iteration 0
Iteration 10
Iteration 20
Iteration 0
Iteration 10
Iteration 20
Boltzmann Makinasi 0.975
KNN 0.975</code></pre>
<p>Sonuç yüzde 97.5, oldukça yüksek, ve KNN metotu ile aynı sonucu
aldık, ki bu aslında oldukça temiz / basit bir veri seti için fena
değil.</p>
<p>Biraz Hikaye</p>
<p>Boltzman Makinalarıyla ilgilenmemizin ilginç bir hikayesi var.
Aslında bu metottan haberimiz yoktu, ayrıca mevcut işimizde 0/1 içeren
ikisel verilerle çok hasır neşirdik, ve bu tür verilerde ikisel
ilişkiler (coöccürence) hesabı iyi sonuçlar verir, ki bu hesap basit bir
matris çarpımı ile elde edilir.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([<span class="op">\</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>[<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>],</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>[<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>[<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>],</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>],</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>[<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">1.</span>,<span class="dv">0</span>]</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> A.T.dot(A).astype(<span class="bu">float</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> c </span></code></pre></div>
<pre><code>[[ 2.  2.  1.  0.]
 [ 2.  4.  3.  1.]
 [ 1.  3.  4.  1.]
 [ 0.  1.  1.  1.]]</code></pre>
<p>Burada bakılırsa 2. satır 3. kolon 3 değerini taşıyor çünkü 2. ve 3.
değişkenlerin aynı anda 1 olma sayısı tam olarak 3. Sonra acaba bu
bilgiyi veri üzerinde hesaplayıp bir kenara koysak bir dağılım gibi
kullanamaz mıyız, sonra yeni veri noktasını bu “dağılıma sorabiliriz’’
diye düşündük. Biraz matris çarpım cambazlığı sonrası, yeni veri noktası
için</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>])</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> np.dot(np.dot(x.T,c), x) <span class="op">/</span> <span class="dv">2</span></span></code></pre></div>
<pre><code>7.0</code></pre>
<p>gibi sonuçlar alabildiğimizi gördük; Bu değerin ilişki matrisinin tam
ortasındaki 4,3,3,4 sayılarının toplamının yarısı olduğuna dikkat
edelim. Yani <span class="math inline">\(x\)</span> çarpımı ilişki
matrisinin sadece kendini ilgilendiren kısmını çekip çıkarttı, yani 2.
ve 3. değişenleri arasındaki ilişkiyi toplayıp aldı.</p>
<p>Buradan sonra, “acaba bu bir dağılım olsa normalizasyon sabiti ne
olurdu?’’ sorusuna geldik, ki [4] sorusu buradan çıktı ve bu soruya
bomba bir cevap geldi. Sonra diğer okumalarımız sırasında Boltzmann
Dağılımına ulaştık, bu dağılımın ek olarak bir <span
class="math inline">\(\exp\)</span> tanımı var (ki türev alımı sırasında
bu faydalı), ve tabii öğrenim için daha net bir matematiği var. Biz de
maksimum olurluk ile [4]’teki fikrin sayısal kovaryansa ulaştırıp
ulaştırmayacağını merak ediyorduk, BM formunda verisel kovaryans direk
elde ediliyor. Böylece BM konusuna girmiş olduk.</p>
<p>Bitirmeden önce ufak not, BM’ler Kısıtlı BM (RBM) için bir zıplama
tahtası, ve RBM’ler Derin Öğrenimin (Deep Learning) bir türü için
kullanılabilir, bu yapay sinir ağlarını birden fazla RBM’leri üst üste
koyarak elde etmek mümkün (gerçi son zamanlarda moda yaklaşım evrişimsel
ağ -convolutional network- kullanmak).</p>
<p>[1] D. MacKay, <em>Information Theory, Inference and Learning
Algorithms</em>, sf. 523</p>
<p>[2] Flaxman, <em>Notebook</em>, <a
href="http://nbviewer.ipython.org/gist/aflaxman/7d946762ee99daf739f1">http://nbviewer.ipython.org/gist/aflaxman/7d946762ee99daf739f1</a></p>
<p>[3] Stack Exchange, <a
href="http://math.stackexchange.com/questions/1095491/from-pxw-frac1zw-exp-bigl-frac12-xt-w-x-bigr-to-sigmoid/">http://math.stackexchange.com/questions/1095491/from-pxw-frac1zw-exp-bigl-frac12-xt-w-x-bigr-to-sigmoid/</a></p>
<p>[4] Stack Exchange, <a
href="http://math.stackexchange.com/questions/1080504/calculating-the-sum-frac12-sum-xt-sigma-x-for-all-x-in-0-1-n">http://math.stackexchange.com/questions/1080504/calculating-the-sum-frac12-sum-xt-sigma-x-for-all-x-in-0-1-n</a></p>
<p>[5] Bayramlı, Istatistik, <em>Monte Carlo, Entegraller, MCMC</em></p>
<p>[6] Bayramlı, Istatistik, <em>Lojistik Regresyon</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
