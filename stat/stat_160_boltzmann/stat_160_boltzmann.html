<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Boltzman Makinaları (Rasgele Hopfield Ağları)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h1 id="boltzman-makinaları-rasgele-hopfield-ağları">Boltzman Makinaları (Rasgele Hopfield Ağları)</h1>
<p>Alttaki ifade bir Boltmann dağılımını gösterir,</p>
<p><span class="math display">\[  
P(x;W) = \frac{1}{Z(W)} 
\exp \bigg[ \frac{1}{2} x^T W x \bigg]
\qquad (3)
\]</span></p>
<p>ki <span class="math inline">\(x\)</span> çok boyutlu ve -1,+1 değerleri içeren bir vektör, <span class="math inline">\(W\)</span> simetrik ve çaprazında (diagonal) sıfır içeren bir matristir, <span class="math inline">\(n \times d\)</span> boyutlarındaki bir veri için <span class="math inline">\(d \times d\)</span> boyutlarında olacaktır. Boltzmann Makinaları (BM), Kısıtlı Boltzmann Makinaları (Restricted Boltzmann Machines) kavramına geçiş yapmadan önce iyi bir durak noktası.</p>
<p>BM <span class="math inline">\(W\)</span> içinde aslında tüm değişkenlerin ikisel ilişkisini içerir. <span class="math inline">\(W\)</span> çok değişkenli Gaussian dağılımındaki <span class="math inline">\(\Sigma\)</span>'da olduğu gibi ikisel bağlantıları saptar. Veriden <span class="math inline">\(W\)</span>'yu öğrenmek için olurluğu hesaplamak lazım. Olurluk (likelihood)</p>
<p><span class="math display">\[  
\prod_{n=1}^{N} P(x^{(n)};W) = \frac{1}{Z(W)} 
\exp \bigg[ \frac{1}{2} x^{(n)^T} W x^{(n)} \bigg]
\]</span></p>
<p>Log olurluk</p>
<p><span class="math display">\[  
\mathcal{L} = \ln \big( \prod_{n=1}^{N} P(x^{(n)};W) \big) = 
\sum_{n=1}^{N} \bigg[ \frac{1}{2} x^{(n)^T} W x^{(n)} - \ln Z(W) \bigg]
\qquad (1)
\]</span></p>
<p>Birazdan <span class="math inline">\(\frac{\partial \mathcal L}{\partial w_{ij}}\)</span> türevini alacağız, o sırada <span class="math inline">\(\ln Z(W)\)</span>'nin türevi lazım, daha doğrusu <span class="math inline">\(Z(W)\)</span>'yi nasıl türevi alınır hale getiririz?</p>
<p><span class="math inline">\(Z(W)\)</span> normalizasyon sabiti olduğuna göre, dağılımın geri kalanının sonsuzlar üzerinden entegrali (ya da toplamı) normalizasyon sabitine eşittir,</p>
<p><span class="math display">\[ 
Z(W) = \sum_x  \exp \bigg[ \frac{1}{2} x^T W x \bigg]
\]</span></p>
<p><span class="math display">\[ 
\ln Z(W) = \ln \bigg[ \sum_x  \exp \big( \frac{1}{2} x^T W x \big) \bigg]
\]</span></p>
<p>Log bazlı türev alınca log içindeki herşey olduğu gibi bölüme gider, ve log içindekinin türevi alınırak bölüme koyulur. Fakat log içine dikkatli bakarsak bu zaten <span class="math inline">\(Z(W)\)</span>'nin tanımıdır, böylece denklemi temizleme şansı doğdu, bölüme hemen <span class="math inline">\(Z(W)\)</span> deriz, ve türevi log'un içine uygularız,</p>
<p><span class="math display">\[ 
\frac{\partial}{\partial w_{ij}} \ln Z(W) = 
\frac{1}{Z(W)}
\bigg[ 
\sum_x \frac{\partial}{\partial w_{ij}} \exp \big( \frac{1}{2} x^T W x \big) 
\bigg]
\]</span></p>
<p><span class="math display">\[ 
\frac{\partial}{\partial w_{ij}} \exp \big( \frac{1}{2} x^T W x \big)  = 
\frac{1}{2}  \exp \big( \frac{1}{2} x^T W x \big) 
\frac{\partial}{\partial w_{ij}}x^T W x
\qquad (2)
\]</span></p>
<p>(2)'in içindeki bölümü açalım,</p>
<p><span class="math display">\[ 
\frac{\partial}{\partial w_{ij}}x^T W x = x_i x_j 
\]</span></p>
<p>Şimdi (2)'ye geri koyalım,</p>
<p><span class="math display">\[ 
= \frac{1}{2} \exp \big( \frac{1}{2} x^T W x \big) x_i x_j
\]</span></p>
<p><span class="math display">\[ 
\frac{\partial}{\partial w_{ij}} \ln Z(W) = 
\frac{1}{Z(W)}
\bigg[ 
\sum_x \frac{1}{2}  \exp \big( \frac{1}{2} x^T W x \big) x_i x_j
\bigg]
\]</span></p>
<p><span class="math display">\[ 
= \frac{1}{2} \sum_x 
\frac{1}{Z(W)} \exp \big( \frac{1}{2} x^T W x \big) x_i x_j 
\]</span></p>
<p><span class="math display">\[ 
= \frac{1}{2}  \sum_x P(x;W) x_i x_j
\]</span></p>
<p>Üstteki son ifadede bir kısaltma kullanalım,</p>
<p><span class="math display">\[ 
\sum_x P(x;W) x_i x_j =  &lt; x_i,x_j &gt;_{P(x;W)}
\qquad (4)
\]</span></p>
<p>Artık <span class="math inline">\(\ln Z(W)\)</span>'nin türevini biliyoruz. O zaman tüm log olurluğun türevine (1) dönebiliriz,</p>
<p><span class="math display">\[  
\frac{\partial \mathcal{L}}{\partial w_{ij}} = 
\sum_{n=1}^{N} \bigg[ 
\frac{\partial}{\partial w_{ij}}  \frac{1}{2} x^{(n)^T} W x^{(n)} - 
\frac{\partial}{\partial w_{ij}}  \ln Z(W) \bigg]
\]</span></p>
<p><span class="math display">\[  
= \sum_{n=1}^{N} 
\bigg[ 
\frac{1}{2} x_i^{(n)^T}x_j^{(n)} - 
\frac{\partial}{\partial w_{ij}}  \ln Z(W) 
\bigg]
\]</span></p>
<p><span class="math display">\[  
= \sum_{n=1}^{N} 
\bigg[ 
\frac{1}{2} x_i^{(n)^T}x_j^{(n)} - 
\frac{1}{2}&lt; x_i x_j &gt;_{P(x;W)}
\bigg]
\]</span></p>
<p>1/2 sabitlerini atalım,</p>
<p><span class="math display">\[  
= \sum_{n=1}^{N} 
\bigg[ 
 x_i^{(n)^T}x_j^{(n)} - &lt; x_i x_j &gt;_{P(x;W)}
\bigg]
\]</span></p>
<p>Eğer</p>
<p><span class="math display">\[
&lt; x_i x_j &gt;_{Data} = \frac{1}{N} \sum_{n=1}^{N}  x_i^{(n)^T}x_j^{(n)}
\]</span></p>
<p>olarak alırsak, eşitliğin sağ tarafı verisel kovaryansı (empirical covariance) temsil eder. Düzenleyince,</p>
<p><span class="math display">\[ 
N \cdot &lt; x_i x_j &gt;_{Data} = \sum_{n=1}^{N}  x_i^{(n)^T}x_j^{(n)}
\]</span></p>
<p>şimdi eşitliğin sağ tarafı üç üstteki formüle geri koyulabilir,</p>
<p><span class="math display">\[ 
\frac{\partial \mathcal{L}}{\partial w_{ij}}  = 
N \big[ &lt; x_i x_j &gt;_{Data}  - &lt; x_ix_j &gt;_{P(x;W)} \big] 
\]</span></p>
<p>Her ne kadar <span class="math inline">\(N\)</span> veri noktası sayısını gösteriyor olsa da, üstteki ifade bir gradyan güncelleme formülü olarak ta görülebilir, ve <span class="math inline">\(N\)</span> yerine bir güncelleme sabiti alınabilir. Gradyan güncelleme olarak görülebilir çünkü <span class="math inline">\(w_{ij}\)</span>'ye göre türev aldık, o zaman bizi <span class="math inline">\(\mathcal{L}\)</span>'in minimumuna götürecek <span class="math inline">\(w\)</span> adımları üstte görüldüğü gibidir.</p>
<p>(4)'te görülen <span class="math inline">\(&lt; x_ix_j &gt;_{P(x;W)}\)</span>'in anlamı nedir? Bu ifade mümkün tüm <span class="math inline">\(x\)</span> değerleri üzerinden alınıyor ve ikisel ilişkilerin olasılığını &quot;mevcut modele'' göre hesaplıyor. Yani bu ifade de bir korelasyon hesabıdır, sadece veriye göre değil, tüm mümkün değerler ve model üzerinden alınır. Bu hesabı yapmak oldukça zordur, fakat yaklaşıksal olarak Monte Carlo yöntemi ile hesaplanabilir. Nihayet MC ve MCMC metotlarının kullanılma sebebini görmeye başlıyoruz; bu metotlar zaten aşırı yüksek boyutlu, analitik çözümü olmayan, hesaplanamaz (intractable) entegraller (ya da toplamlar) için keşfedilmiştir.</p>
<p>Yani bu ifadeyi hesaplamak için Monte Carlo simulasyonu kullanacağız. Tüm değerleri teker teker ziyaret etmek yerine (ki bu çok uzun zaman alırdı) mevcut modele en olası <span class="math inline">\(x\)</span> değerleri &quot;ürettireceğiz'', ve bu değerleri alıp sanki gerçek veriymiş gibi sayısal korelasyonlarını hesaplayacağız. Eğer veriler dağılımın en olası noktalarından geliyorlarsa, elimizde veri dağılımı &quot;iyi'' temsil eden bir veri setidir. Daha sonra bu korelasyon hesabını değeri gerçek veri korelasyonunundan çıkartıp bir sabit üzerinden gradyan adımı atmamız mümkün olacak.</p>
<p>Gibbs Örneklemesi (Sampling)</p>
<p>Gibbs örneklemesinin detayları için [5]. Bolzmann dağılımından örneklem almak için bize tek bir değişken (hücre) haricinde diğer hepsinin bilindiği durumun olasılık hesabı lazım, yani koşulsal olasılık <span class="math inline">\(P(x_i = 1 | x_j, j \ne i)\)</span>. Yani <span class="math inline">\(x\)</span> üzerinde, biri hariç tüm öğelerin bilindiği durumda bilinmeyen tek hücre <span class="math inline">\(i\)</span>'nin 1 olma olasılık değeri,</p>
<p><span class="math display">\[ 
P(x_i = 1 | x_j, j \ne i) = \frac{1}{1 + e^{-a_i}} 
\]</span></p>
<p>ve,</p>
<p><span class="math display">\[ 
a_i = \sum_j  w_{ij}x_j 
\]</span></p>
<p>Bu koşulsal olasılığın temiz / basit bir formül olması önemli, üstteki görülen bir sigmoid fonksiyonu bu türden bir fonksiyondur... Bu fonksiyonlar hakkında daha fazla bilgi [6] yazısında bulunabilir.</p>
<p>Ama, ana formül (3)'ten bu noktaya nasıl eriştik? Bu noktada biraz türetme yapmak lazım. <span class="math inline">\(x\)</span> vektörü içinde sadece <span class="math inline">\(x_i\)</span> öğesinin <span class="math inline">\(b\)</span> olmasını <span class="math inline">\(x^b\)</span> olarak alalım. Önce koşulsal dağılımda &quot;verili'' olan kısmı elde etmek lazım. O uzaman</p>
<p><span class="math display">\[ 
P(x_j,j \ne i) = P(x^0) + P(x^1) 
\]</span></p>
<p>Bu bir marjinalizasyon ifadesi, tüm olası <span class="math inline">\(i\)</span> değerleri üzerinde bir toplam alınca geri kalan <span class="math inline">\(j\)</span> değerlerinin dağılımını elde etmiş oluruz.</p>
<p><span class="math display">\[  
P(x_i = 1 | x_j,j \ne i)  = \frac{P(x^1)}{P(x^0) + P(x^1)} 
\]</span></p>
<p>çünkü <span class="math inline">\(P(A|B) = P(A,B) / P(B)\)</span> bilindiği gibi, ve <span class="math inline">\(P(x^1)\)</span> içinde <span class="math inline">\(x_1=1\)</span> setini içeren tüm veriler üzerinden.</p>
<p>Eşitliğin sağ tarafında <span class="math inline">\(P(x^1)\)</span>'i bölen olarak görmek daha iyi, ayrıca ulaşmak istediğimiz <span class="math inline">\(1/1 + e^{-a_i}\)</span> ifadesinde <span class="math inline">\(+1\)</span>'den kurtulmak iyi olur, böylece sadece <span class="math inline">\(e^{-a_i}\)</span> olan eşitliği ispatlarız. Bunun her iki denklemde ters çevirip 1 çıkartabiliriz,</p>
<p><span class="math display">\[  
1 / P(x_i = 1 | x_j,j \ne i) = \frac{P(x^0) + P(x^1)}{P(x^1)} 
\]</span></p>
<p><span class="math display">\[
= 1 + \frac{ P(x^0)}{P(x^1)}
\]</span></p>
<p>Bir çıkartırsak, <span class="math inline">\(\frac{ P(x^0)}{P(x^1)}\)</span> kalır. Bu bize ulaşmak istediğimiz denklemde <span class="math inline">\(e^{-a_i}\)</span> ibaresini bırakır. Artık sadece <span class="math inline">\(\frac{P(x^0)}{P(x^1)}\)</span>'in <span class="math inline">\(e^{-a_i}\)</span>'e eşit olduğunu göstermek yeterli.</p>
<p><span class="math display">\[ 
\frac{ P(x^0)}{P(x^1)} = \exp( x^{0^T}Wx^0 -   x^{1^T}Wx^1 )
\]</span></p>
<p>Şimdi <span class="math inline">\(x^TWx\)</span> gibi bir ifadeyi indisler bazında açmak için şunları yapalım,</p>
<p><span class="math display">\[ 
x^TWx = \sum_{k,j} x_kx_jw_{kj} 
\]</span></p>
<p>Üstteki çok iyi bilinen bir açılım. Eğer</p>
<p><span class="math display">\[ 
\sum_{k,j} \underbrace{x_kx_jw_{ij}}_{Y_{kj}} = \sum_{k,j}Y_{kj} 
\]</span></p>
<p>alırsak birazdan yapacağımız işlemler daha iyi görülebilir. Mesela <span class="math inline">\(k=i\)</span> olan durumu dış toplamdan dışarı çekebiliriz</p>
<p><span class="math display">\[ 
= \sum_{k \ne i}\sum_j Y_{kj} + \sum_{j} Y_{ij}
\]</span></p>
<p>Daha sonra <span class="math inline">\(j = i\)</span> olan durumu iç toplamdan dışarı çekebiliriz,</p>
<p><span class="math display">\[ 
= \sum_{k \ne i}( \sum_{j \ne i} Y_{kj} + Y_{ki}) + \sum_{j} Y_{ij}
\]</span></p>
<p>İç dış toplamları birleştirelim,</p>
<p><span class="math display">\[ 
= \sum_{k \ne i,j \ne i} Y_{kj} + \sum_{k \ne i}  Y_{ki} + \sum_{j} Y_{ij}
\]</span></p>
<p><span class="math display">\[ 
= \sum_{k \ne i,j \ne i} Y_{kj} + \sum_{k}  Y_{ki} + \sum_{j} Y_{ij} + Y_{ii}
\]</span></p>
<p>Üstteki ifadeyi <span class="math inline">\(\exp( x^{0^T}Wx^0 - x^{1^T}Wx^1 )\)</span> için kullanırsak,</p>
<p><span class="math display">\[ 
\exp 
\big( 
\sum_{k}  Y_{ki}^0 + \sum_{j} Y_{ij}^0 + Y_{ii}^0 - 
( \sum_{k}  Y_{ki}^1 + \sum_{j} Y_{ij}^1 + Y_{ii}^1  )
\big)
\]</span></p>
<p><span class="math inline">\(\sum_{k \ne i,j \ne i} Y_{kj}\)</span> teriminin nereye gittiği merak edilirse, bu ifade <span class="math inline">\(i\)</span>'ye dayanmadığı için bir eksi bir artı olarak iki defa dahil edilip iptal olacaktı.</p>
<p><span class="math display">\[ 
= \exp \big( 
0 - ( \sum_{k}  Y_{ki}^1 + \sum_{j} Y_{ij}^1 + Y_{ii}^1  ) 
\big)
\]</span></p>
<p><span class="math inline">\(W\)</span>'nin simetrik matris olduğunu düşünürsek, <span class="math inline">\(\sum_{k} Y_{ki}^1\)</span> ile <span class="math inline">\(\sum_{j}Y_{ij}^1\)</span> aynı ifadedir,</p>
<p><span class="math display">\[ 
= \exp \big( 
- ( 2 \sum_{j} Y_{ij}^1 + Y_{ii}^1  ) 
\big)
\]</span></p>
<p><span class="math inline">\(W\)</span> sıfır çaprazlı bir matristir, o zaman <span class="math inline">\(Y_{ii}^1=0\)</span>,</p>
<p><span class="math display">\[ 
= \exp \big( 2 \sum_{j} Y_{ij}^1 \big) = \exp (- 2 a_i )
\]</span></p>
<p>Orijinal dağılım denkleminde <span class="math inline">\(1/2\)</span> ifadesi vardı, onu başta işlemlere dahil etmemiştik, edilseydi sonuç <span class="math inline">\(\exp (- a_i)\)</span> olacaktı.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np

<span class="kw">class</span> Boltzmann:

    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,n_iter<span class="op">=</span><span class="dv">100</span>,eta<span class="op">=</span><span class="fl">0.1</span>,sample_size<span class="op">=</span><span class="dv">100</span>,init_sample_size<span class="op">=</span><span class="dv">10</span>):
        <span class="va">self</span>.n_iter <span class="op">=</span> n_iter
        <span class="va">self</span>.eta <span class="op">=</span> eta
        <span class="va">self</span>.sample_size <span class="op">=</span> sample_size
        <span class="va">self</span>.init_sample_size <span class="op">=</span> init_sample_size
    
    <span class="kw">def</span> sigmoid(<span class="va">self</span>, u):
        <span class="cf">return</span> <span class="fl">1.</span><span class="op">/</span>(<span class="fl">1.</span><span class="op">+</span>np.exp(<span class="op">-</span>u))<span class="op">;</span>

    <span class="kw">def</span> draw(<span class="va">self</span>, Sin,T):
        <span class="co">&quot;&quot;&quot;</span>
<span class="co">        Bir Gibbs gecisi yaparak dagilimdan bir orneklem al</span>
<span class="co">        &quot;&quot;&quot;</span>
        D<span class="op">=</span>Sin.shape[<span class="dv">0</span>]
        S<span class="op">=</span>Sin.copy()
        rand <span class="op">=</span> np.random.rand(D,<span class="dv">1</span>)
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">xrange</span>(D):
            h<span class="op">=</span>np.dot(T[i,:],S)
            S[i]<span class="op">=</span>rand[i]<span class="op">&lt;</span><span class="va">self</span>.sigmoid(h)<span class="op">;</span>
        <span class="cf">return</span> S

    <span class="kw">def</span> sample(<span class="va">self</span>, T):
        N<span class="op">=</span>T.shape[<span class="dv">0</span>]        
        <span class="co"># sigmoid(0) her zaman 0.5 olacak</span>
        s<span class="op">=</span>np.random.rand(N)<span class="op">&lt;</span><span class="va">self</span>.sigmoid(<span class="dv">0</span>)
        <span class="co"># alttaki dongu atlama / gozonune alinmayacak degerler icin        </span>
        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">xrange</span>(<span class="va">self</span>.init_sample_size):
            s<span class="op">=</span><span class="va">self</span>.draw(s,T)
        S<span class="op">=</span>np.zeros((N,<span class="va">self</span>.sample_size))
        S[:,<span class="dv">0</span>]<span class="op">=</span>s
        <span class="co"># simdi degerleri toplamaya basla</span>
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">xrange</span>(<span class="dv">1</span>,<span class="va">self</span>.sample_size):
            S[:,i]<span class="op">=</span><span class="va">self</span>.draw(S[:,i<span class="dv">-1</span>],T)
        <span class="cf">return</span> S.T

    <span class="kw">def</span> normc(<span class="va">self</span>, X):
        <span class="co">&quot;&quot;&quot;</span>
<span class="co">        normalizasyon sabitini dondur</span>
<span class="co">        &quot;&quot;&quot;</span>
        <span class="kw">def</span> f(x): <span class="cf">return</span> np.exp(<span class="fl">0.5</span> <span class="op">*</span> np.dot(np.dot(x,<span class="va">self</span>.W), x))
        S <span class="op">=</span> <span class="dv">2</span><span class="op">*</span><span class="va">self</span>.sample(<span class="va">self</span>.W)<span class="op">-</span><span class="dv">1</span>
        <span class="co"># sozluk icinde anahtar tek x degeri boylece bir</span>
        <span class="co"># olasilik degeri sadece bir kere toplanir</span>
        res <span class="op">=</span> <span class="bu">dict</span>((<span class="bu">tuple</span>(s),f(s)) <span class="cf">for</span> s <span class="kw">in</span> S)
        <span class="cf">return</span> np.<span class="bu">sum</span>(res.values())
    
    <span class="kw">def</span> fit(<span class="va">self</span>, X):
        W<span class="op">=</span>np.zeros((X.shape[<span class="dv">1</span>],X.shape[<span class="dv">1</span>]))
        W_data<span class="op">=</span>np.dot(X.T,X)<span class="op">/</span>X.shape[<span class="dv">1</span>]<span class="op">;</span>
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_iter):
            <span class="cf">if</span> i <span class="op">%</span> <span class="dv">10</span> <span class="op">==</span> <span class="dv">0</span>: <span class="bu">print</span> <span class="st">&#39;Iteration&#39;</span>, i
            S <span class="op">=</span> <span class="va">self</span>.sample(W)
            S <span class="op">=</span> (S<span class="op">*</span><span class="dv">2</span>)<span class="op">-</span><span class="dv">1</span>
            W_guess<span class="op">=</span>np.dot(S.T,S)<span class="op">/</span>S.shape[<span class="dv">1</span>]<span class="op">;</span>
            W <span class="op">+=</span> <span class="va">self</span>.eta <span class="op">*</span> (W_data <span class="op">-</span> W_guess)
            np.fill_diagonal(W, <span class="dv">0</span>)
        <span class="va">self</span>.W <span class="op">=</span> W
        <span class="va">self</span>.C <span class="op">=</span> <span class="va">self</span>.normc(X)

    <span class="kw">def</span> predict_proba(<span class="va">self</span>, X):
        <span class="cf">return</span> np.diag(np.exp(<span class="fl">0.5</span> <span class="op">*</span> np.dot(np.dot(X, <span class="va">self</span>.W), X.T))) <span class="op">/</span> <span class="va">self</span>.C</code></pre></div>
<p>Fonksiyon <code>draw</code> içinde, tek bir veri satırı için ve sırayla her değişken (hücre) için, diğer değişkenleri baz alıp diğerinin koşulsal olasılığını hesaplıyoruz, ve sonra bu olasılığı kullanarak bir sayı üretimi yapıyoruz. Üretimin yapılması için <code>np.random.rand</code>'dan gelen 0 ve 1 arasındaki birörnek (uniform) dağılımdan bir rasgele sayıyı geçip geçmeme irdelemesi yeterli. Bir Bernoulli olasılık hesabını üretilen bir rasgele değişkene bu şekilde çevirebilirsiniz. Bu niye işler? Üstte belirttiğimiz irdelemeyi rasgele değişken olarak kodlarsak (ki bu da bir Bernoulli rasgele değişkeni olur), ve birörnek rasgele değişken <span class="math inline">\(U\)</span> olsun,</p>
<p><span class="math display">\[ Y = 
\left\{ \begin{array}{ll}
1 &amp; U &lt; p \\
0 &amp; U \ge p \\
\end{array} \right.
\]</span></p>
<p>Bu durumda <span class="math inline">\(P(X=1) = P(U&lt;p) = p\)</span> olurdu. Neden? Çünkü üstte bir sürekli (continuous) bir birörnek değişken yarattık, ve <span class="math inline">\(P(U&lt;p) = F_u(p) = p\)</span>.</p>
<p>Devam edelim; Çağrı <code>sample</code> ise <code>draw</code>'u kullanarak pek çok veri satırını içeren ve dağılımı temsil eden bir örneklem yaratmakla sorumlu. Bunu her örneklem satırını baz alarak bir sonrakini ürettirerek yapıyor, böylelikle MCMC'nin dağılımı &quot;gezmesi'' sağlanmış oluyor.</p>
<p>Normalizasyon Sabiti</p>
<p>Birazdan göreceğimiz örnek için normalizasyon sabitini de hesaplamamız gerekecek. Niye? Mesela iki farklı BM dağılımını farklı etiketli verilerden öğreniyoruz, sonra test veri noktasını her iki ayrı dağılıma &quot;soruyoruz''? Olasılığı nedir? Bu noktada kesin bir olasılık hesabı istediğimiz için artık <span class="math inline">\(Z\)</span> bilinmek zorunda. Bu sabitin hesaplanması için ise <span class="math inline">\(&lt; x_ix_j &gt;_{P(x;W)}\)</span> için olduğu gibi, tüm mümkün <span class="math inline">\(x\)</span>'ler üzerinden bir toplam gerekir, bu toplam <span class="math inline">\(\sum_x \exp 1/2 x^T W x\)</span> toplamı. Bu toplamın hesaplanması çok zor olduğu için, yine MCMC'ye başvuracağız. Tek fark alınan örneklemi (3) formülüne geceğiz, ve bir olasılık hesabı yapacağız, ve bu olasılıkları toplayacağız. Tabii aynı <span class="math inline">\(x\)</span>'i (eğer tekrar tekrar üretilirse -ufak bir ihtimal ama mümkün-) tekrar tekrar toplamamak için hangi <span class="math inline">\(x\)</span>'lerin üretildiğini bir sözlük içinde hatırlayacağız, yani bir <span class="math inline">\(x\)</span> olasılığı sadece bir kere toplanacak.</p>
<p>Şimdi ufak bir örnek üzerinde BM'i işletelim.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> boltz
A <span class="op">=</span> np.array([<span class="op">\</span>
[<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">1</span>],
[<span class="fl">1.</span>,<span class="fl">0.</span>,<span class="dv">0</span>,<span class="dv">0</span>],
[<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>],
[<span class="dv">0</span>, <span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>],
[<span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">1.</span>,<span class="dv">0</span>]
])
A[A<span class="op">==</span><span class="dv">0</span>]<span class="op">=-</span><span class="dv">1</span>

clf <span class="op">=</span> boltz.Boltzmann(n_iter<span class="op">=</span><span class="dv">50</span>,eta<span class="op">=</span><span class="fl">0.01</span>,sample_size<span class="op">=</span><span class="dv">200</span>,init_sample_size<span class="op">=</span><span class="dv">50</span>)
clf.fit(A)
<span class="bu">print</span> <span class="st">&#39;W&#39;</span>
<span class="bu">print</span> clf.W
<span class="bu">print</span> <span class="st">&#39;normalizasyon sabiti&#39;</span>, clf.C</code></pre></div>
<pre><code>Iteration 0
Iteration 10
Iteration 20
Iteration 30
Iteration 40
W
[[ 0.    -0.065 -0.06  -0.055]
 [-0.065  0.     0.17   0.105]
 [-0.06   0.17   0.    -0.09 ]
 [-0.055  0.105 -0.09   0.   ]]
normalizasyon sabiti 16.4620358997</code></pre>
<p>Sonuç <span class="math inline">\(W\)</span> üstte görüldüğü gibi. Örnek veriye bakarsak 2. satır 3. kolonda artı bir değer var, 1. satır 4. kolonda eksi değer var. Bu beklediğimiz bir şey çünkü 2. ve 3. değişkenlerin arasında bir korelasyon var, <span class="math inline">\(x_2\)</span> ne zaman 1/0 ise <span class="math inline">\(x_3\)</span> te 1/0. Fakat <span class="math inline">\(x_1\)</span> ile <span class="math inline">\(x_4\)</span> ters bir korelasyon var, birbirlerinin zıttı değerlere sahipler.</p>
<p>Şimdi yeni test verisini dağılıma &quot;soralım'',</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">test <span class="op">=</span> np.array([<span class="op">\</span>
[<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">1</span>],
[<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>,<span class="dv">0</span>],
[<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">1</span>]
])    
<span class="bu">print</span> clf.predict_proba(test)</code></pre></div>
<pre><code>[ 0.0730905   0.05692294  0.0730905 ]</code></pre>
<p>Görüntü Tanıma</p>
<p>Elimizde el yazısı tanıma algoritmaları için kullanılan bir veri seti var. Veride 0,5,7 harflerinin görüntüleri var. Mesela 5 için bazı örnek görüntüler,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">Y <span class="op">=</span> np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/binarydigits.txt&#39;</span>)
label <span class="op">=</span> np.ravel(np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/bindigitlabels.txt&#39;</span>))
Y5 <span class="op">=</span> Y[label<span class="op">==</span><span class="dv">5</span>]
plt.imshow(Y5[<span class="dv">0</span>,:].reshape((<span class="dv">8</span>,<span class="dv">8</span>),order<span class="op">=</span><span class="st">&#39;C&#39;</span>), cmap<span class="op">=</span>plt.cm.gray)
plt.savefig(<span class="st">&#39;boltzmann_01.png&#39;</span>)

plt.imshow(Y5[<span class="dv">1</span>,:].reshape((<span class="dv">8</span>,<span class="dv">8</span>),order<span class="op">=</span><span class="st">&#39;C&#39;</span>), cmap<span class="op">=</span>plt.cm.gray)
plt.savefig(<span class="st">&#39;boltzmann_02.png&#39;</span>)

plt.imshow(Y5[<span class="dv">2</span>,:].reshape((<span class="dv">8</span>,<span class="dv">8</span>),order<span class="op">=</span><span class="st">&#39;C&#39;</span>), cmap<span class="op">=</span>plt.cm.gray)
plt.savefig(<span class="st">&#39;boltzmann_03.png&#39;</span>)</code></pre></div>
<p><img src="boltzmann_01.png" /> <img src="boltzmann_02.png" /> <img src="boltzmann_03.png" /></p>
<p>Bu görüntüleri tanımak için BM kullanalım. Eğitim ve test olarak veriyi ikiye ayıracağız, ve eğitim seti her etiketin <span class="math inline">\(W\)</span>'sini öğrenmek için kullanılacak. Daha sonra test setinde her veri noktalarını her üç BM'ye ayrı ayrı &quot;sorup'' o test verisinin o BM'e göre olasılığını alacağız, ve hangi BM daha yüksek olasılık döndürüyorsa etiket olarak onu kabul edeceğiz. Hangi BM daha yüksek olasılık döndürüyorsa, o BM &quot;bu verinin benden gelme olasılığı yüksek'' diyor demektir, ve etiket o olmalıdır.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn <span class="im">import</span> neighbors
<span class="im">import</span> numpy <span class="im">as</span> np, boltz
<span class="im">from</span> sklearn.cross_validation <span class="im">import</span> train_test_split

Y <span class="op">=</span> np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/binarydigits.txt&#39;</span>)
labels <span class="op">=</span> np.ravel(np.loadtxt(<span class="st">&#39;../../stat/stat_mixbern/bindigitlabels.txt&#39;</span>))
X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(Y, labels, test_size<span class="op">=</span><span class="fl">0.4</span>,random_state<span class="op">=</span><span class="dv">0</span>)
X_train[X_train<span class="op">==</span><span class="dv">0</span>]<span class="op">=-</span><span class="dv">1</span>
X_test[X_test<span class="op">==</span><span class="dv">0</span>]<span class="op">=-</span><span class="dv">1</span>

clfs <span class="op">=</span> {}
<span class="cf">for</span> label <span class="kw">in</span> [<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">7</span>]:
    x <span class="op">=</span> X_train[y_train<span class="op">==</span>label]
    clf <span class="op">=</span> boltz.Boltzmann(n_iter<span class="op">=</span><span class="dv">30</span>,eta<span class="op">=</span><span class="fl">0.05</span>,sample_size<span class="op">=</span><span class="dv">500</span>,init_sample_size<span class="op">=</span><span class="dv">100</span>)
    clf.fit(x)
    clfs[label] <span class="op">=</span> clf
    
res <span class="op">=</span> []
<span class="cf">for</span> label <span class="kw">in</span> [<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">7</span>]:
    res.append(clfs[label].predict_proba(X_test))
    
res3 <span class="op">=</span> np.argmax(np.array(res).T,axis<span class="op">=</span><span class="dv">1</span>)
res3[res3<span class="op">==</span><span class="dv">1</span>] <span class="op">=</span> <span class="dv">5</span>
res3[res3<span class="op">==</span><span class="dv">2</span>] <span class="op">=</span> <span class="dv">7</span>
<span class="bu">print</span> <span class="st">&#39;Boltzmann Makinasi&#39;</span>, np.<span class="bu">sum</span>(res3<span class="op">==</span>y_test) <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(y_test))

clf <span class="op">=</span> neighbors.KNeighborsClassifier()
clf.fit(X_train,y_train)
res3 <span class="op">=</span> clf.predict(X_test)    
<span class="bu">print</span> <span class="st">&#39;KNN&#39;</span>, np.<span class="bu">sum</span>(res3<span class="op">==</span>y_test) <span class="op">/</span> <span class="bu">float</span>(<span class="bu">len</span>(y_test))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="op">!</span>python testbm.py</code></pre></div>
<pre><code>Iteration 0
Iteration 10
Iteration 20
Iteration 0
Iteration 10
Iteration 20
Iteration 0
Iteration 10
Iteration 20
Boltzmann Makinasi 0.975
KNN 0.975</code></pre>
<p>Sonuç yüzde 97.5, oldukça yüksek, ve KNN metotu ile aynı sonucu aldık, ki bu aslında oldukça temiz / basit bir veri seti için fena değil.</p>
<p>Biraz Hikaye</p>
<p>Boltzman Makinalarıyla ilgilenmemizin ilginç bir hikayesi var. Aslında bu metottan haberimiz yoktu, ayrıca mevcut işimizde 0/1 içeren ikisel verilerle çok hasır neşirdik, ve bu tür verilerde ikisel ilişkiler (coöccürence) hesabı iyi sonuçlar verir, ki bu hesap basit bir matris çarpımı ile elde edilir.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np
A <span class="op">=</span> np.array([<span class="op">\</span>
[<span class="fl">0.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>],
[<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>, <span class="dv">0</span>],
[<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="dv">0</span>],
[<span class="dv">0</span>, <span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>],
[<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">1.</span>,<span class="dv">0</span>]
])
c <span class="op">=</span> A.T.dot(A).astype(<span class="bu">float</span>)
<span class="bu">print</span> c </code></pre></div>
<pre><code>[[ 2.  2.  1.  0.]
 [ 2.  4.  3.  1.]
 [ 1.  3.  4.  1.]
 [ 0.  1.  1.  1.]]</code></pre>
<p>Burada bakılırsa 2. satır 3. kolon 3 değerini taşıyor çünkü 2. ve 3. değişkenlerin aynı anda 1 olma sayısı tam olarak 3. Sonra acaba bu bilgiyi veri üzerinde hesaplayıp bir kenara koysak bir dağılım gibi kullanamaz mıyız, sonra yeni veri noktasını bu &quot;dağılıma sorabiliriz'' diye düşündük. Biraz matris çarpım cambazlığı sonrası, yeni veri noktası için</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">x <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>])
<span class="bu">print</span> np.dot(np.dot(x.T,c), x) <span class="op">/</span> <span class="dv">2</span></code></pre></div>
<pre><code>7.0</code></pre>
<p>gibi sonuçlar alabildiğimizi gördük; Bu değerin ilişki matrisinin tam ortasındaki 4,3,3,4 sayılarının toplamının yarısı olduğuna dikkat edelim. Yani <span class="math inline">\(x\)</span> çarpımı ilişki matrisinin sadece kendini ilgilendiren kısmını çekip çıkarttı, yani 2. ve 3. değişenleri arasındaki ilişkiyi toplayıp aldı.</p>
<p>Buradan sonra, &quot;acaba bu bir dağılım olsa normalizasyon sabiti ne olurdu?'' sorusuna geldik, ki [4] sorusu buradan çıktı ve bu soruya bomba bir cevap geldi. Sonra diğer okumalarımız sırasında Boltzmann Dağılımına ulaştık, bu dağılımın ek olarak bir <span class="math inline">\(\exp\)</span> tanımı var (ki türev alımı sırasında bu faydalı), ve tabii öğrenim için daha net bir matematiği var. Biz de maksimum olurluk ile [4]'teki fikrin sayısal kovaryansa ulaştırıp ulaştırmayacağını merak ediyorduk, BM formunda verisel kovaryans direk elde ediliyor. Böylece BM konusuna girmiş olduk.</p>
<p>Bitirmeden önce ufak not, BM'ler Kısıtlı BM (RBM) için bir zıplama tahtası, ve RBM'ler Derin Öğrenimin (Deep Learning) bir türü için kullanılabilir, bu yapay sinir ağlarını birden fazla RBM'leri üst üste koyarak elde etmek mümkün (gerçi son zamanlarda moda yaklaşım evrişimsel ağ -convolutional network- kullanmak).</p>
<p>[1] D. MacKay, <em>Information Theory, Inference and Learning Algorithms</em>, sf. 523</p>
<p>[2] Flaxman, <em>Notebook</em>, <a href="http://nbviewer.ipython.org/gist/aflaxman/7d946762ee99daf739f1" class="uri">http://nbviewer.ipython.org/gist/aflaxman/7d946762ee99daf739f1</a></p>
<p>[3] Stack Exchange, <a href="http://math.stackexchange.com/questions/1095491/from-pxw-frac1zw-exp-bigl-frac12-xt-w-x-bigr-to-sigmoid/" class="uri">http://math.stackexchange.com/questions/1095491/from-pxw-frac1zw-exp-bigl-frac12-xt-w-x-bigr-to-sigmoid/</a></p>
<p>[4] Stack Exchange, <a href="http://math.stackexchange.com/questions/1080504/calculating-the-sum-frac12-sum-xt-sigma-x-for-all-x-in-0-1-n" class="uri">http://math.stackexchange.com/questions/1080504/calculating-the-sum-frac12-sum-xt-sigma-x-for-all-x-in-0-1-n</a></p>
<p>[5] Bayramlı, Istatistik, <em>Monte Carlo, Entegraller, MCMC</em></p>
<p>[6] Bayramlı, Istatistik, <em>Lojistik Regresyon</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
