<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Naive Bayes</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h1 id="naive-bayes">Naive Bayes</h1>
<p>Reel sayılar arasında bağlantı kurmak için istatistikte regresyon kullanılır. Eğer reel değerleri, (mesela) iki kategorik grup arasında seçmek için kullanmak istenirse, bunun için lojistik regresyon gibi teknikler de vardır.</p>
<p>Fakat kategoriler / gruplar ile başka kategorik gruplar arasında bağlantılar kurulmak istenirse, standart istatistik yöntemleri faydalı olamıyor. Bu gibi ihtiyaçlar için yapay öğrenim (machine learning) dünyasından Naive Bayes gibi tekniklere bakmamız lazım.</p>
<p>Not: Daha ilerlemeden belirtelim, bu tekniğin ismi Naive Bayes ama bu tanım tam doğru değil, çünkü NB Olasılık Teorisi'nden bilinen Bayes Teorisini kullanmıyor.</p>
<p>Öncelikle kategorik değerler ile ne demek istediğimizi belirtelim. Reel sayılar <span class="math inline">\(0.3423, 2.4334\)</span> gibi değerlerdir, kategorik değerler ile ise mesela bir belge içinde 'a','x' gibi harflerin mevcut olmasıdır. Ya da, bir evin 'beyaz', 'gri' renkli olması.. Burada öyle kategorilerden bahsediyoruz ki istesek te onları sayısal bir değere çeviremiyoruz; kıyasla mesela bir günün 'az sıcak', 'orta', 'çok sıcak' olduğu verisini kategorik bile olsa regresyon amacıyla sayıya çevirip kullanabilirdik. Az sıcak = 0, orta = 1, çok sıcak = 2 değerlerini kullanabilirdik, regresyon hala anlamlı olurdu (çünkü arka planda bu kategoriler aslında sayısal sıcaklık değerlerine tekabül ediyor olurlardı). Fakat 'beyaz', 'gri' değerlere sayı atamanın regresyon açısından bir anlamı olmazdı, hatta bunu yapmak yanlış olurdu. Eğer elimizde fazla sayıda 'gri' ev verisi olsa, bu durum regresyon sırasında beyaz evlerin beyazlığını mı azaltacaktır?</p>
<p>İşte bu gibi durumlarda kategorileri olduğu gibi işleyebilen bir teknik gerekiyor. Bu yazıda kullanacağımız örnek, bir belgenin içindeki kelimelere göre kategorize edilmesi. Elimizde iki türlü doküman olacak. Bir tanesi Stephen Hawking adlı bilim adamının bir kitabından 3 sayfa, diğeri başkan Barack Obama'nın bir kitabından 3 sayfa. Bu sayfalar ve içindeki kelimeler NB yöntemini &quot;eğitmek&quot; için kullanılacak, sonra NB tarafından hiç görülmemiş yeni sayfaları yöntemimize kategorize ettireceğiz.</p>
<p>Çok Boyutlu Bernoulli ve Kelimeler</p>
<div class="figure">
<img src="dims.png" />

</div>
<p>Bir doküman ile içindeki kelimeler arasında nasıl bağlantı kuracağız? Burada olasılık teorisinden Çok Boyutlu Bernoulli (Multivariate Bernoulli) dağılımını kullanacağız. Üstteki resimde görüldüğü gibi her doküman bir <span class="math inline">\(x^i\)</span> rasgele değişkeniyle temsil edilecek. Tek boyutlu Bernoulli değişkeni '1' ya da '0' değerine sahip olabilir, çok boyutlu olanı ise bir vektör içinde '1' ve '0' değerlerini taşıyabilir. İşte bu vektörün her hücresi, önceden tanımlı bir kelimeye tekabül edecek, ve bu kelimeden bir doküman içinde en az bir tane var ise, o hücre '1' değerini taşıyacak, yoksa '0' değerini taşıyacak. Üstteki örnekte 2. kelime &quot;hello&quot; ve 4. doküman içinde bu kelimeden en az bir tane var, o zaman <span class="math inline">\(x_2^4 = 1\)</span>. Tek bir dokümanı temsil eden dağılımı matematiksel olarak şöyle yazabiliriz:</p>
<p><span class="math display">\[ p(x_1,...,x_{D}) = \prod_{d=1}^{D} p(x_d)=\prod_{d=1}^{D}
\alpha_d^{x_d}(1-\alpha_d)^{1-x_d} 
\]</span></p>
<p>Bu formülde her <span class="math inline">\(d\)</span> boyutu bir tek boyutlu Bernoulli, ve bir doküman için tüm bu boyutların ortak (joint) dağılımı gerekiyor, çarpımın sebebi bu. Formüldeki <span class="math inline">\(\alpha_d\)</span> bir dağılımı &quot;tanımlayan&quot; değer, <span class="math inline">\(\alpha\)</span> bir vektör, ve unutmayalım, her &quot;sınıf&quot; için NB ayrı ayrı eğitilecek, ve her sınıf için farklı <span class="math inline">\(\alpha\)</span> vektörü olacak. Yani Obama'nın kitapları için <span class="math inline">\(\alpha_2 = 0.8\)</span> olabilir, Hawking kitabı için <span class="math inline">\(\alpha_2 = 0.3\)</span> olabilir. Birinin kitabında &quot;hello&quot; kelimesi olma şansı fazla, diğerinde pek yok. O zaman NB'yi &quot;eğitmek&quot; ne demektir? Eğitmek her sınıf için yukarıdaki <span class="math inline">\(\alpha\)</span> değerlerini bulmak demektir.</p>
<p>Bunun için istatistikteki &quot;olurluk (likelihood)&quot; kavramını kullanmak yeterli. Olurluk, bir dağılımdan geldiği farzedilen bir veri setini alır, tüm veri noktalarını teker teker olasılığa geçerek olasılık değerlerini birbirine çarpar. Sonuç ne kadar yüksek çıkarsa, bu verinin o dağılımdan gelme olasılığı o kadar yüksek demektir. Bizim problemimiz için tek bir sınıfın olurluğu, o sınıf içindeki tüm (N tane) belgeyi kapsamalıdır, tek bir &quot;veri noktası&quot; tek bir belgedir, o zaman:</p>
<p><span class="math display">\[ L(\theta) = \prod_{i=1}^N \prod_{d=1}^{D} p(x_d^i) = 
\prod_{i=1}^N \prod_{d=1}^{D} \alpha_d^{x_d^i}(1-\alpha_d)^{1-x_d^i}
 \]</span></p>
<p><span class="math inline">\(\theta\)</span> bir dağılımı tanımlayan her türlü değişken anlamında kullanıldı, bu örnekte içinde sadece <span class="math inline">\(\alpha\)</span> var.</p>
<p>Devam edelim: Eğer <span class="math inline">\(\alpha\)</span>'nin ne olduğunu bilmiyorsak (ki bilmiyoruz -eğitmek zaten bu demek-) o zaman maksimum olurluk (maximum likelihood) kavramını resme dahil etmek gerekli. Bunun için üstteki olurluk formülünün <span class="math inline">\(\alpha\)</span>'ya göre türevini alıp sıfıra eşitlersek, bu formülden bir maksimum noktasındaki <span class="math inline">\(\alpha\)</span> elimize geçecektir. İşte bu <span class="math inline">\(\alpha\)</span> bizim aradığımız değer. Veriyi en iyi temsil eden <span class="math inline">\(\alpha\)</span> değeri bu demektir. Onu bulunca eğitim tamamlanır.</p>
<p>Türev almadan önce iki tarafın log'unu alalım, böylece çarpımlar toplamlara dönüşecek ve türevin formülün içine nüfuz etmesi daha kolay olacak.</p>
<p><span class="math display">\[ \log(L) = \sum_{i=1}^N \sum_{d=1}^{D} {x_d^i}\ log (\alpha_d) + 
(1-x_d^i)\ log (1-\alpha_d) \]</span></p>
<p>Türevi alalım:</p>
<p><span class="math display">\[ \frac{dlog(L)}{d\alpha_d} = \sum_{i=1}^N \bigg( \frac{x_d^i}{\alpha_d} -
\frac{1-x_d^i}{1-\alpha_d} \bigg) = 0
 \]</span></p>
<p>1- <span class="math inline">\(\alpha_d\)</span>'ye göre türev alırken <span class="math inline">\(x_d^i\)</span>'ler sabit sayı gibi muamele görürler. 2- log'un türevi alırken log içindeki değerlerin türev alınmış hali bölümün üstüne, kendisini olduğu gibi bölüm altına alınır, örnek <span class="math inline">\(dlog(-x)/dx = -1/x\)</span> olur üstteki eksi işaretinin sebebi bu.</p>
<p>Peki <span class="math inline">\(\sum_{d=1}^{D}\)</span> nereye gitti? Türevi <span class="math inline">\(\alpha_d\)</span>'ye göre alıyoruz ve o türevi alırken tek bir <span class="math inline">\(\alpha_d\)</span> ile ilgileniyoruz, mesela <span class="math inline">\(\alpha_{22}\)</span>, bunun haricindeki diğer tüm <span class="math inline">\(\alpha_?\)</span> değerleri türev alma işlemi sırasında sabit kabul edilirler, türev sırasında sıfırlanırlar. Bu sebeple <span class="math inline">\(\sum_{d=1}^{D}\)</span> içinde sadece bizim ilgilendiğimiz <span class="math inline">\(\alpha_d\)</span> geriye kalır. Tabii ki bu aynı zamanda her <span class="math inline">\(d=1,2,..D\)</span>, <span class="math inline">\(\alpha_d\)</span> için ayrı bir türev var demektir, ama bu türevlerin hepsi birbirine benzerler, yani tek bir <span class="math inline">\(\alpha_d\)</span>'yi çözmek, hepsini çözmek anlamına gelir.</p>
<p>Devam edelim:</p>
<p><span class="math display">\[ \sum_{i=1}^N \bigg( \frac{x_d^i}{\alpha_d} - \frac{1-x_d^i}{1-\alpha_d} \bigg) =
\frac{N_d}{\alpha_d} - \frac{N-N_d}{1-\alpha_d} = 0
 \]</span></p>
<p><span class="math inline">\(\sum_{i=1}^N x_d^i = N_d\)</span> olarak kabul ediyoruz, <span class="math inline">\(N_d\)</span> tüm veri içinde <span class="math inline">\(d\)</span> boyutu (kelimesi) '1' kaç tane hücre olduğunu bize söyler. <span class="math inline">\(x_d^i\)</span> ya '1' ya '0' olabildiğine göre bir <span class="math inline">\(d\)</span> için, tüm <span class="math inline">\(N\)</span> hücrenin toplamı otomatik olarak bize kaç tane '1' olduğunu söyler. Sonra:</p>
<p><span class="math display">\[ \frac{N_d}{\alpha_d} - \frac{N-N_d}{1-\alpha_d} = 0  \]</span></p>
<p><span class="math display">\[ \frac{1-\alpha_d}{\alpha_d} = \frac{N-N_d}{N_d}   \]</span></p>
<p><span class="math display">\[ \frac{1}{\alpha_d} - 1 = \frac{N}{N_d} - 1  \]</span></p>
<p><span class="math display">\[ \frac{1}{\alpha_d} = \frac{N}{N_d}  \]</span></p>
<p><span class="math display">\[ \alpha_d = \frac{N_d}{N}  \]</span></p>
<p>Python Kodu</p>
<p><span class="math inline">\(\alpha_d\)</span>'nin formülünü buldumuza göre artık kodu yazabiliriz. İlk önce bir dokümanı temsil eden çok boyutlu Bernoulli vektörünü ortaya çıkartmamız lazım. Bu vektörün her hücresi belli bir kelime olacak, ve o kelimelerin ne olduğunu önceden kararlaştırmamız lazım. Bunun için her sınıftaki tüm dokümanlardaki tüm kelimeleri içeren bir sözlük yaratırız:</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> re
<span class="im">import</span> math

words <span class="op">=</span> {}

<span class="co"># find all words in all files, creating a </span>
<span class="co"># global dictionary.</span>
base <span class="op">=</span> <span class="st">&#39;./data/&#39;</span>
<span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> [<span class="st">&#39;a1.txt&#39;</span>,<span class="st">&#39;a2.txt&#39;</span>,<span class="st">&#39;a3.txt&#39;</span>,
             <span class="st">&#39;b1.txt&#39;</span>,<span class="st">&#39;b2.txt&#39;</span>,<span class="st">&#39;b3.txt&#39;</span>]:
    f <span class="op">=</span> <span class="bu">open</span> (base <span class="op">+</span> <span class="bu">file</span>)
    s <span class="op">=</span> f.read()
    tokens <span class="op">=</span> re.split(<span class="st">&#39;\W+&#39;</span>, s)
    <span class="cf">for</span> x <span class="kw">in</span> tokens: words[x] <span class="op">=</span> <span class="fl">0.</span>
    
hawking_alphas <span class="op">=</span> words.copy()   
<span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> [<span class="st">&#39;a1.txt&#39;</span>,<span class="st">&#39;a2.txt&#39;</span>,<span class="st">&#39;a3.txt&#39;</span>]:
    words_hawking <span class="op">=</span> <span class="bu">set</span>()
    f <span class="op">=</span> <span class="bu">open</span> (base <span class="op">+</span> <span class="bu">file</span>)
    s <span class="op">=</span> f.read()
    tokens <span class="op">=</span> re.split(<span class="st">&#39;\W+&#39;</span>, s)
    <span class="cf">for</span> x <span class="kw">in</span> tokens: 
        words_hawking.add(x)
    <span class="cf">for</span> x <span class="kw">in</span> words_hawking:
        hawking_alphas[x] <span class="op">+=</span> <span class="fl">1.</span>
        
obama_alphas <span class="op">=</span> words.copy()   
<span class="cf">for</span> <span class="bu">file</span> <span class="kw">in</span> [<span class="st">&#39;b1.txt&#39;</span>,<span class="st">&#39;b2.txt&#39;</span>,<span class="st">&#39;b3.txt&#39;</span>]:
    words_obama <span class="op">=</span> <span class="bu">set</span>()
    f <span class="op">=</span> <span class="bu">open</span> (base <span class="op">+</span> <span class="bu">file</span>)
    s <span class="op">=</span> f.read()
    tokens <span class="op">=</span> re.split(<span class="st">&#39;\W+&#39;</span>, s)
    <span class="cf">for</span> x <span class="kw">in</span> tokens: 
        words_obama.add(x)
    <span class="cf">for</span> x <span class="kw">in</span> words_obama:
        obama_alphas[x] <span class="op">+=</span> <span class="fl">1.</span>

<span class="cf">for</span> x <span class="kw">in</span> hawking_alphas.keys():
    hawking_alphas[x] <span class="op">=</span> hawking_alphas[x] <span class="op">/</span> <span class="fl">3.</span>        
<span class="cf">for</span> x <span class="kw">in</span> obama_alphas.keys():
    obama_alphas[x] <span class="op">=</span> obama_alphas[x] <span class="op">/</span> <span class="fl">3.</span>        

<span class="kw">def</span> prob(xd, alpha):
    <span class="cf">return</span> math.log(alpha<span class="op">*</span>xd <span class="op">+</span> <span class="fl">1e-10</span>) <span class="op">+</span> <span class="op">\</span>
        math.log((<span class="fl">1.</span><span class="op">-</span>alpha)<span class="op">*</span>(<span class="fl">1.</span><span class="op">-</span>xd) <span class="op">+</span> <span class="fl">1e-10</span>)
        
<span class="kw">def</span> test(<span class="bu">file</span>):
    test_vector <span class="op">=</span> words.copy()   
    words_test <span class="op">=</span> <span class="bu">set</span>()
    f <span class="op">=</span> <span class="bu">open</span> (base <span class="op">+</span> <span class="bu">file</span>)
    s <span class="op">=</span> f.read()
    tokens <span class="op">=</span> re.split(<span class="st">&#39;\W+&#39;</span>, s)
    <span class="cf">for</span> x <span class="kw">in</span> tokens: 
        words_test.add(x)
    <span class="cf">for</span> x <span class="kw">in</span> words_test:  
        test_vector[x] <span class="op">=</span> <span class="fl">1.</span>
    ob <span class="op">=</span> <span class="fl">0.</span>
    ha <span class="op">=</span> <span class="fl">0.</span>
    <span class="cf">for</span> x <span class="kw">in</span> test_vector.keys(): 
        <span class="cf">if</span> x <span class="kw">in</span> obama_alphas: 
            ob <span class="op">+=</span> prob(test_vector[x], obama_alphas[x])
        <span class="cf">if</span> x <span class="kw">in</span> hawking_alphas: 
            ha <span class="op">+=</span> prob(test_vector[x], hawking_alphas[x])
                
    <span class="bu">print</span> <span class="st">&quot;obama&quot;</span>, ob, <span class="st">&quot;hawking&quot;</span>, ha, <span class="op">\</span>
    <span class="co">&quot;obama&quot;</span>, ob <span class="op">&gt;</span> ha, <span class="st">&quot;hawking&quot;</span>, ha <span class="op">&gt;</span> ob


<span class="bu">print</span> <span class="st">&quot;hawking test&quot;</span>    
test(<span class="st">&#39;a4.txt&#39;</span>)
<span class="bu">print</span> <span class="st">&quot;hawking test&quot;</span>    
test(<span class="st">&#39;a5.txt&#39;</span>)
<span class="bu">print</span> <span class="st">&quot;obama test&quot;</span>    
test(<span class="st">&#39;b4.txt&#39;</span>)
<span class="bu">print</span> <span class="st">&quot;obama test&quot;</span>    
test(<span class="st">&#39;b5.txt&#39;</span>)</code></pre></div>
<pre><code>hawking test
obama -34048.7734496 hawking -32192.3692113 obama False hawking True
hawking test
obama -33027.3182425 hawking -32295.7149639 obama False hawking True
obama test
obama -32531.9918709 hawking -32925.037558 obama True hawking False
obama test
obama -32205.4710748 hawking -32549.6924713 obama True hawking False</code></pre>
<p>Test için yeni dokümanı kelimelerine ayırıyoruz, ve her kelimeye tekabül eden alpha vektörlerini kullanarak bir yazar için toplam olasılığı hesaplıyoruz. Nasıl? Her kelimeyi <span class="math inline">\(\alpha_d^{x_d}(1-\alpha_d)^{1-x_d}\)</span> formülüne soruyoruz, yeni dokümanı temsilen elimizde bir <span class="math inline">\([1,0,0,1,0,0,...,1]\)</span> şeklinde bir vektör olduğunu farz ediyoruz, buna göre mesela <span class="math inline">\(x_1=1\)</span>, <span class="math inline">\(x_2=0\)</span>. Eğer bir <span class="math inline">\(d\)</span> kelimesi yeni belgede &quot;var&quot; ise o kelime için <span class="math inline">\(x_d = 1\)</span> ve bu durumda <span class="math inline">\(\alpha_d^{x_d} = \alpha_d^{1} = \alpha_d\)</span> haline gelir, ama formülün öteki tarafı yokolur, <span class="math inline">\((1-\alpha_d)^{1-x_d} = (1-\alpha_d)^0 = 1\)</span>, o zaman <span class="math inline">\(\alpha_d \cdot 1 = \alpha_d\)</span>.</p>
<p>Çarpım diyoruz ama biz aslında sınıflama sırasında <span class="math inline">\(\alpha_d^{x_d}(1-\alpha_d)^{1-x_d}\)</span> çarpımı yerine yine log() numarasını kullandık; çünkü olasılık değerleri hep 1'e eşit ya da ondan küçük sayılardır, ve bu küçük değerlerin birbiriyle sürekli çarpımı nihai sonucu aşırı fazla küçültür. Aşırı ufak değerlerle uğraşmamak için olasılıkların log'unu alıp birbirleri ile toplamayı seçtik, yani hesapladığımız değer <span class="math inline">\(x_d \cdot log(\alpha_d) + (1-x_d) \cdot \log(1-\alpha_d)\)</span></p>
<p>Fonksiyon <code>prob</code> içindeki <code>1e-7</code> kullanımı neden? Bu kullanım log numarasını yapabilmek için -- sıfır değerinin log değeri tanımsızdır, bir kelime olmadığı zaman log'a sıfır geleceği için hata olmaması için log içindeki değerlere her seferinde yeterince küçük bir sayı ekliyoruz, böylece pür sıfırla uğraşmak zorunda kalmıyoruz. Sıfır olmadığı zamanlarda çok eklenen çok küçük bir sayı sonuçta büyük farklar (hatalar) yaratmıyor.</p>
<p>Toparlarsak, yeni belge <code>a4.txt</code> için iki tür alpha değerleri kullanarak iki farklı log toplamını hesaplatıyoruz. Bu iki toplamı birbiri ile karşılaştırıyoruz, hangi toplam daha büyükse, dokümanın o yazardan gelmesi daha olasıdır, ve o seçimimiz o yazar olur.</p>
<p>Kaynaklar</p>
<p>[1] Jebara, T., <em>Columbia U. COMS 4771 Machine Learning Lecture Notes, Lecture 7</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
