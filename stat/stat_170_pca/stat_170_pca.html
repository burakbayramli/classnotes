<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Asal Bileşen Analizi (Principal Component Analysis -PCA-)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="asal-bileşen-analizi-principal-component-analysis--pca-">Asal
Bileşen Analizi (Principal Component Analysis -PCA-)</h1>
<p>PCA yöntemi boyut azaltan yöntemlerden biri, denetimsiz
(unsupervised) işleyebilir. Ana fikir veri noktalarının izdüşümünün
yapılacağı yönler bulmaktır ki bu yönler bağlamında (izdüşüm sonrası)
noktaların arasındaki sayısal varyans (empirical variance) en fazla
olsun, yani noktalar grafik bağlamında düşünürsek en “yayılmış” şekilde
bulunsunlar. Böylece birbirinden daha uzaklaşan noktaların mesela daha
rahat kümelenebileceğini umabiliriz. Bir diğer amaç, hangi değişkenlerin
varyansının daha fazla olduğunun görülmesi üzerine, o değişkenlerin daha
önemli olabileceğinin anlaşılması. Örnek olarak alttaki grafiğe
bakalım,</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> read_csv(<span class="st">&quot;testSet.txt&quot;</span>,sep<span class="op">=</span><span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>,header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (data[:<span class="dv">10</span>])</span></code></pre></div>
<pre class="text"><code>           0          1
0  10.235186  11.321997
1  10.122339  11.810993
2   9.190236   8.904943
3   9.306371   9.847394
4   8.330131   8.340352
5  10.152785  10.123532
6  10.408540  10.821986
7   9.003615  10.039206
8   9.534872  10.096991
9   9.498181  10.825446</code></pre>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(data.iloc[:,<span class="dv">0</span>],data.iloc[:,<span class="dv">1</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.plot(data.iloc[<span class="dv">1</span>,<span class="dv">0</span>],data.iloc[<span class="dv">1</span>,<span class="dv">1</span>],<span class="st">&#39;rd&#39;</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plt.plot(data.iloc[<span class="dv">4</span>,<span class="dv">0</span>],data.iloc[<span class="dv">4</span>,<span class="dv">1</span>],<span class="st">&#39;rd&#39;</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;pca_1.png&#39;</span>)</span></code></pre></div>
<p><img src="pca_1.png" /></p>
<p>PCA ile yapmaya çalıştığımız öyle bir yön bulmak ki, <span
class="math inline">\(x\)</span> veri noktalarının tamamının o yöne
izdüşümü yapılınca sonuç olacak, “izdüşümü yapılmış” <span
class="math inline">\(z\)</span>’nin varyansı en büyük olsun. Bu bir
maksimizasyon problemidir. Fakat ondan önce <span
class="math inline">\(x\)</span> nedir, <span
class="math inline">\(z\)</span> nedir bunlara yakından bakalım.</p>
<p>Veri <span class="math inline">\(x\)</span> ile tüm veri noktaları
kastedilir, fakat PCA probleminde genellikle bir “vektörün diğeri
üzerine” yapılan izdüşümü, “daha optimal bir <span
class="math inline">\(w\)</span> yönü bulma”, ve “o yöne doğru izdüşüm
yapmak” kelimeleri kullanılır. Demek ki veri noktalarını bir vektör
olarak görmeliyiz. Eğer üstte kırmızı ile işaretlenen iki noktayı
alırsak (bu noktalar verideki 1. ve 4. sıradaki noktalar),</p>
<p><img src="proj1.png" /></p>
<p>gibi bir görüntüden bahsediyoruz. Hayali bir <span
class="math inline">\(w\)</span> kullandık, ve noktalardan biri veri
noktası, <span class="math inline">\(w\)</span> üzerine izdüşüm
yapılarak yeni bir vektörü / noktayı ortaya çıkartılıyor. Genel olarak
ifade edersek, bir nokta için</p>
<p><span class="math display">\[ z_i =  x_i^Tw = x_i \cdot
w\]</span></p>
<p>Yapmaya çalıştığımız sayısal varyansı maksimize etmek demiştik. Bu
arada verinin hangi dağılımdan geldiğini söylemedik, “her veri noktası
birbirinden ayrı, bağımsız ama aynı bir dağılımdandır’’ bile demedik,
<span class="math inline">\(x\)</span> bir rasgele değişkendir beyanı
yapmadık (<span class="math inline">\(x\)</span> veri noktalarını tutan
bir şey sadece). Sadece sayısal varyans ile iş yapacağız. Sayısal
varyans,</p>
<p><span class="math display">\[ \frac{1}{n}\sum_i  (x_i \cdot w)^2
\]</span></p>
<p>Toplama işlemi yerine şöyle düşünelim, tüm <span
class="math inline">\(x_i\)</span> noktalarını istifleyip bir <span
class="math inline">\(x\)</span> matrisi haline getirelim, o zaman <span
class="math inline">\(xw\)</span> ile bir yansıtma yapabiliriz, bu
yansıtma sonucu bir vektördür. Bu tek vektörün karesini almak demek onun
devriğini alıp kendisi ile çarpmak demektir, yani</p>
<p><span class="math display">\[ = \frac{1}{n}(xw)^T(xw) = \frac{1}{n}
w^Tx^Txw\]</span></p>
<p><span class="math display">\[ =  w^T\frac{x^Tx}{n}w\]</span></p>
<p><span class="math inline">\(x^Tx / n\)</span> sayısal kovaryanstır
(empirical covariance). Ona <span class="math inline">\(\Sigma\)</span>
diyelim.</p>
<p><span class="math display">\[ =  w^T\Sigma w\]</span></p>
<p>Üstteki sonuçların boyutları <span class="math inline">\(1 \times N
\cdot N \times N \cdot N \times 1 = 1 \times 1\)</span>. Tek boyutlu
skalar degerler elde ettik. Yani <span class="math inline">\(w\)</span>
yönündeki izdüşüm bize tek boyutlu bir çizgi verecektir. Bu sonuç
aslında çok şaşırtıcı olmasa gerek, tüm veri noktalarını alıp,
başlangıcı başnokta 0,0 (origin) noktasında olan vektörlere çevirip aynı
yöne işaret edecek şekilde düzenliyoruz, bu vektörleri tekrar nokta
olarak düşünürsek, tabii ki aynı yönü gösteriyorlar, bilahere aynı çizgi
üzerindeki noktalara dönüşüyorlar. Aynı çizgi üzerinde olmak ne demek?
Tek boyuta inmiş olmak demek.</p>
<p>Ufak bir sorun <span class="math inline">\(w^T\Sigma w\)</span>’i
sürekli daha büyük <span class="math inline">\(w\)</span>’lerle sonsuz
kadar büyütebilirsiniz. Bize ek bir kısıtlama şartı daha lazım, bu şart
<span class="math inline">\(||w|| = 1\)</span> olabilir, yani <span
class="math inline">\(w\)</span>’nin norm’u 1’den daha büyük olmasın.
Böylece optimizasyon <span class="math inline">\(w\)</span>’yi sürekli
büyüte büyüte maksimizasyon yapmayacak, sadece yön bulmak ile
ilgilenecek, iyi, zaten biz <span class="math inline">\(w\)</span>’nin
yönü ile ilgileniyoruz. Aradığımız ifadeyi yazalım, ve ek sınırı
Lagrange ifadesi olarak ekleyelim, ve yeni bir <span
class="math inline">\(L\)</span> ortaya çıkartalım,</p>
<p><span class="math display">\[ L(w,\lambda) =  w^T \Sigma w  -
\lambda(w^T w - 1) \]</span></p>
<p>Niye eksiden sonraki terim o şekilde eklendi? O terim öyle şekilde
seçildi ki, <span class="math inline">\(\partial L / \partial \lambda =
0\)</span> alınınca <span class="math inline">\(w^Tw = 1\)</span> geri
gelsin / ortaya çıksın [2, sf 340]. Bu Lagrange’in dahice buluşu. Bu
kontrol edilebilir, <span class="math inline">\(\lambda\)</span> ’ya
göre türev alırken <span class="math inline">\(w_1\)</span> sabit olarak
yokolur, parantez içindeki ifadeler kalır ve sıfıra eşitlenince orijinal
kısıtlama ifadesi geri gelir. Şimdi</p>
<p><span class="math display">\[ \max\limits_{w} L(w,\lambda)
\]</span></p>
<p>için türevi <span class="math inline">\(w\)</span>’e göre alırsak, ve
sıfıra eşitlersek,</p>
<p><span class="math display">\[ \frac{\partial L}{\partial w} = 2w
\Sigma - 2 \lambda w = 0 \]</span></p>
<p><span class="math display">\[ 2w \Sigma = 2 \lambda w \]</span></p>
<p><span class="math display">\[ \Sigma w  = \lambda w \]</span></p>
<p>Üstteki ifade özdeğer, özvektör ana formülüne benzemiyor mu? Evet.
Eğer <span class="math inline">\(w\)</span>, <span
class="math inline">\(\Sigma\)</span>’nin özvektörü ise ve eşitliğin
sağındaki <span class="math inline">\(\lambda\)</span> ona tekabül eden
özdeğer ise, bu eşitlik doğru olacaktır.</p>
<p>Peki hangi özdeğer / özvektör maksimal değeri verir? Unutmayalım,
maksimize etmeye çalıştığımız şey <span class="math inline">\(w^T \Sigma
w\)</span> idi</p>
<p>Eger <span class="math inline">\(\Sigma w = \lambda w\)</span> yerine
koyarsak</p>
<p><span class="math display">\[ w^T \lambda w =  \lambda w^T  w =
\lambda \]</span></p>
<p>Çünkü <span class="math inline">\(w_1^T w\)</span>’nin 1 olacağı
şartını koymuştuk. Neyse, maksimize etmeye çalıştığımız değer <span
class="math inline">\(\lambda\)</span> çıktı, o zaman en büyük <span
class="math inline">\(\lambda\)</span> kullanırsak, en maksimal varyansı
elde ederiz, bu da en büyük özdeğerin ta kendisidir. Demek ki izdüşüm
yapılacak “yön” kovaryans <span
class="math inline">\(\Sigma\)</span>’nin en büyük özdeğerine tekabül
eden özvektör olarak seçilirse, temel bileşenlerden en önemlisini hemen
bulmuş olacağız. İkinci, üçüncü en büyük özdeğerin özvektörleri ise
diğer daha az önemli yönleri bulacaklar.</p>
<p><span class="math inline">\(\Sigma\)</span> matrisi <span
class="math inline">\(n \times n\)</span> boyutunda bir matris, bu
sebeple <span class="math inline">\(n\)</span> tane özvektörü olacak.
Her kovaryans matrisi simetriktir, o zaman lineer cebir bize der ki
özvektörler birbirine dikgen (orthogonal) olmalı. Yıne <span
class="math inline">\(\Sigma\)</span> bir kovaryans matrisi olduğu için
pozitif bir matris olmalı, yani herhangi bir <span
class="math inline">\(x\)</span> için <span class="math inline">\(x
\Sigma x \ge 0\)</span>. Bu bize tüm özvektörlerin <span
class="math inline">\(\ge 0\)</span> olması gerektiğini söylüyor.</p>
<p>Kovaryansın özvektörleri verinin asal bileşenleridir (principal
components), ki metotun ismi burada geliyor.</p>
<p>Örnek</p>
<p>Şimdi tüm bunları bir örnek üzerinde görelim. İki boyutlu örnek
veriyi üstte yüklemiştik. Şimdi veriyi “sıfırda ortalayacağız” yani her
kolon için o kolonun ortalama değerini tüm kolondan çıkartacağız. PCA
ile işlem yaparken tüm değerlerin sıfır merkezli olması gerekiyor, çünkü
bu sayısal kovaryans için gerekli. Daha sonra özdeğer / vektör hesabı
için kovaryansı bulacağız.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.linalg <span class="im">as</span> lin</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> <span class="op">*</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> read_csv(<span class="st">&quot;testSet.txt&quot;</span>,sep<span class="op">=</span><span class="st">&quot;</span><span class="ch">\t</span><span class="st">&quot;</span>,header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (data.shape)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (data[:<span class="dv">10</span>])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>means <span class="op">=</span> data.mean()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>meanless_data <span class="op">=</span> data <span class="op">-</span> means</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>cov_mat <span class="op">=</span> np.cov(meanless_data, rowvar<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (cov_mat.shape)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>eigs,eigv <span class="op">=</span> lin.eig(cov_mat)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>eig_ind <span class="op">=</span> np.argsort(eigs)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (eig_ind)</span></code></pre></div>
<pre class="text"><code>(1000, 2)
           0          1
0  10.235186  11.321997
1  10.122339  11.810993
2   9.190236   8.904943
3   9.306371   9.847394
4   8.330131   8.340352
5  10.152785  10.123532
6  10.408540  10.821986
7   9.003615  10.039206
8   9.534872  10.096991
9   9.498181  10.825446
(2, 2)
[0 1]</code></pre>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (eigs[<span class="dv">1</span>],eigv[:,<span class="dv">1</span>].T)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (eigs[<span class="dv">0</span>],eigv[:,<span class="dv">0</span>].T)</span></code></pre></div>
<pre class="text"><code>2.8971349561751865 [-0.52045195 -0.85389096]
0.366513708669308 [-0.85389096  0.52045195]</code></pre>
<p>En büyük olan yönü quiver komutunu kullanarak orijinal veri seti
üzerinde gösterelim,</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(data.iloc[:,<span class="dv">0</span>],data.iloc[:,<span class="dv">1</span>]) </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># merkez 9,9, tahminen secildi</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span class="dv">9</span>,<span class="dv">9</span>,eigv[<span class="dv">1</span>,<span class="dv">1</span>],eigv[<span class="dv">0</span>,<span class="dv">1</span>],scale<span class="op">=</span><span class="dv">10</span>,color<span class="op">=</span><span class="st">&#39;r&#39;</span>) </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;pca_2.png&#39;</span>)</span></code></pre></div>
<p><img src="pca_2.png" /></p>
<p>Görüldüğü gibi bu yön hakikaten dağılımın, veri noktalarının en çok
yayılmış olduğu yön. Demek ki PCA yöntemi doğru sonucu buldu. Her iki
yönü de çizersek,</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(data.iloc[:,<span class="dv">0</span>],data.iloc[:,<span class="dv">1</span>]) </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span class="dv">9</span>,<span class="dv">9</span>,eigv[<span class="dv">1</span>,<span class="dv">0</span>],eigv[<span class="dv">0</span>,<span class="dv">0</span>],scale<span class="op">=</span><span class="dv">10</span>,color<span class="op">=</span><span class="st">&#39;r&#39;</span>) </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.quiver(<span class="dv">9</span>,<span class="dv">9</span>,eigv[<span class="dv">1</span>,<span class="dv">1</span>],eigv[<span class="dv">0</span>,<span class="dv">1</span>],scale<span class="op">=</span><span class="dv">10</span>,color<span class="op">=</span><span class="st">&#39;r&#39;</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;pca_3.png&#39;</span>)</span></code></pre></div>
<p><img src="pca_3.png" /></p>
<p>Bu ikinci yön birinciye dik olmalıydı, ve o da bulundu. Aslında iki
boyut olunca başka seçenek kalmıyor, 1. yön sonrası ikincisi başka bir
şey olamazdı, fakat çok daha yüksek boyutlarda en çok yayılımın olduğu
ikinci yön de doğru şekilde geri getirilecekti.</p>
<p>Artımsal PCA (Incremental PCA)</p>
<p>Toptan işlem yapmak yerine ufak parçalar üzerinde PCA işletebilmek
için [9]’deki fikir kullanılabilir. Böylece elimize yeni bir veri
geçince tüm önceki veriler + yeni veriyi birarada işlememize gerek
kalmıyor. Eldeki son PCA durumunu yeni veriyi kullanarak güncelliyoruz.
Bu sekilde isleyen bir PCA teknigi CCIPCA.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># https://github.com/kevinhughes27/pyIPCA baz alinmistir</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># online PCA using CCIPCA method which can process sparse</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co"># rows (minibatches of 1). </span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sp</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> linalg <span class="im">as</span> la</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sps</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CCIPCA:    </span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_components, n_features, amnesic<span class="op">=</span><span class="fl">2.0</span>, copy<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_components <span class="op">=</span> n_components</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_features <span class="op">=</span> n_features</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.copy <span class="op">=</span> copy</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.amnesic <span class="op">=</span> amnesic</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.iteration <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.components_ <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean_ <span class="op">=</span> np.zeros([<span class="va">self</span>.n_features], <span class="bu">float</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.components_ <span class="op">=</span> np.ones((<span class="va">self</span>.n_components,<span class="va">self</span>.n_features)) <span class="op">/</span> <span class="op">\</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>                           (<span class="va">self</span>.n_features<span class="op">*</span><span class="va">self</span>.n_components)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> partial_fit(<span class="va">self</span>, u):</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        n <span class="op">=</span> <span class="bu">float</span>(<span class="va">self</span>.iteration)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.components_</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># amnesic learning params</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n <span class="op">&lt;=</span> <span class="bu">int</span>(<span class="va">self</span>.amnesic):</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>            w1 <span class="op">=</span> <span class="bu">float</span>(n<span class="op">+</span><span class="dv">2</span><span class="op">-</span><span class="dv">1</span>)<span class="op">/</span><span class="bu">float</span>(n<span class="op">+</span><span class="dv">2</span>)    </span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>            w2 <span class="op">=</span> <span class="bu">float</span>(<span class="dv">1</span>)<span class="op">/</span><span class="bu">float</span>(n<span class="op">+</span><span class="dv">2</span>)    </span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>            w1 <span class="op">=</span> <span class="bu">float</span>(n<span class="op">+</span><span class="dv">2</span><span class="op">-</span><span class="va">self</span>.amnesic)<span class="op">/</span><span class="bu">float</span>(n<span class="op">+</span><span class="dv">2</span>)    </span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>            w2 <span class="op">=</span> <span class="bu">float</span>(<span class="dv">1</span><span class="op">+</span><span class="va">self</span>.amnesic)<span class="op">/</span><span class="bu">float</span>(n<span class="op">+</span><span class="dv">2</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update mean</span></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mean_ <span class="op">=</span> w1<span class="op">*</span><span class="va">self</span>.mean_ <span class="op">+</span> w2<span class="op">*</span>u</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mean center u        </span></span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> u <span class="op">-</span> <span class="va">self</span>.mean_</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># update components</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="va">self</span>.n_components):</span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> j <span class="op">&gt;</span> n: <span class="cf">pass</span>            </span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> j <span class="op">==</span> n: V[j,:] <span class="op">=</span> u</span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:       </span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>                <span class="co"># update the components</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a>                V[j,:] <span class="op">=</span> w1<span class="op">*</span>V[j,:] <span class="op">+</span> w2<span class="op">*</span>np.dot(u,V[j,:])<span class="op">*</span>u <span class="op">/</span> la.norm(V[j,:])</span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>                normedV <span class="op">=</span> V[j,:] <span class="op">/</span> la.norm(V[j,:])</span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>                normedV <span class="op">=</span> normedV.reshape((<span class="va">self</span>.n_features, <span class="dv">1</span>))</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>                u <span class="op">=</span> u <span class="op">-</span> np.dot(np.dot(u,normedV),normedV.T)</span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.iteration <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.components_ <span class="op">=</span> V <span class="op">/</span> la.norm(V)</span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span></span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> post_process(<span class="va">self</span>):        </span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.explained_variance_ratio_ <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>(<span class="va">self</span>.components_<span class="op">**</span><span class="dv">2</span>,axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> np.argsort(<span class="op">-</span><span class="va">self</span>.explained_variance_ratio_)</span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.explained_variance_ratio_ <span class="op">=</span> <span class="va">self</span>.explained_variance_ratio_[idx]</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.components_ <span class="op">=</span> <span class="va">self</span>.components_[idx,:]</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.explained_variance_ratio_ <span class="op">=</span> (<span class="va">self</span>.explained_variance_ratio_ <span class="op">/</span> <span class="op">\</span></span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>                                          <span class="va">self</span>.explained_variance_ratio_.<span class="bu">sum</span>())</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="va">self</span>.components_.shape[<span class="dv">0</span>]):</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>            d <span class="op">=</span> np.sqrt(np.dot(<span class="va">self</span>.components_[r,:],<span class="va">self</span>.components_[r,:]))</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.components_[r,:] <span class="op">/=</span> d</span></code></pre></div>
<p>Örnek için Iris veri setinde görelim,</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd, ccipca</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;../stat_010_cov_corr/iris.csv&#39;</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> np.array(df)[:,:<span class="dv">4</span>].astype(<span class="bu">float</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> ccipca.CCIPCA(n_components<span class="op">=</span><span class="dv">2</span>,n_features<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>S  <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df[<span class="dv">0</span>, :])</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">150</span>): pca.partial_fit(df[i, :])</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>pca.post_process()</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;varyans orani&#39;</span>,pca.explained_variance_ratio_)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;sonuc&#39;</span>, pca.components_.T)</span></code></pre></div>
<pre class="text"><code>[5.1 3.5 1.4 0.2]
varyans orani [0.99758595 0.00241405]
sonuc [[ 0.80321426  0.21317031]
 [ 0.38265982  0.38577571]
 [ 0.44985225 -0.8021981 ]
 [ 0.07778993 -0.40275764]]</code></pre>
<p>Paylaşılan CCIPCA kodu satırları seyrek matris formatında da
işleyebiliyor.</p>
<p>SVD ile PCA Hesaplamak</p>
<p>PCA bölümünde anlatılan yöntem temel bileşenlerin hesabında
özdeğerler ve özvektörler kullandı. Alternatif bir yöntem Eşsiz Değer
Ayrıştırma (Singular Value Decomposition -SVD-) üzerinden bu hesabı
yapmaktır. SVD için [10]’a bakabiliriz. Peki ne zaman klasik PCA ne
zaman SVD üzerinden PCA kullanmalı? Bir cevap belki mevcut
kütüphanelerde SVD kodlamasının daha iyi olması, ayrıştırmanın özvektör
/ değer hesabından daha hızlı işleyebilmesi [6].</p>
<p>Ayrıca birazdan göreceğimiz gibi SVD, kovaryans matrisi üzerinde
değil, <span class="math inline">\(A\)</span>’nin kendisi üzerinde
işletilir, bu hem kovaryans hesaplama aşamasını atlamamızı, hem de
kovaryans hesabı sırasında ortaya çıkabilecek sayısal (numeric)
pürüzlerden korunmamızı sağlar (çok ufak değerlerin kovaryans hesabını
bozabileceği literatürde bahsedilmektedir).</p>
<p>PCA ve SVD bağlantısına gelelim:</p>
<p>Biliyoruz ki SVD bir matrisi şu şekilde ayrıştırır</p>
<p><span class="math display">\[A = USV^T\]</span></p>
<p><span class="math inline">\(U\)</span> matrisi <span
class="math inline">\(n \times n\)</span> dikgen (orthogonal), <span
class="math inline">\(V\)</span> ise <span class="math inline">\(m
\times m\)</span> dikgen. <span class="math inline">\(S\)</span>’in
sadece köşegeni üzerinde değerler var ve bu <span
class="math inline">\(\sigma_j\)</span> değerleri <span
class="math inline">\(A\)</span>’nin eşsiz değerleri (singular values)
olarak biliniyor.</p>
<p>Şimdi <span class="math inline">\(A\)</span> yerine <span
class="math inline">\(AA^T\)</span> koyalım, ve bu matrisin SVD
ayrıştırmasını yapalım, acaba elimize ne geçecek?</p>
<p><span class="math display">\[ AA^T = (USV^T)(USV^T)^T \]</span></p>
<p><span class="math display">\[ = (USV^T)(V S^T U^T) \]</span></p>
<p><span class="math display">\[ = U S S^T U^T \]</span></p>
<p><span class="math inline">\(S\)</span> bir köşegen matrisi, o zaman
<span class="math inline">\(SS^T\)</span> matrisi de köşegen, tek farkla
köşegen üzerinde artık <span class="math inline">\(\sigma_j^2\)</span>
değerleri var. Bu normal.</p>
<p><span class="math inline">\(SS^T\)</span> yerine <span
class="math inline">\(\Lambda\)</span> sembolünü kullanalım, ve denklemi
iki taraftan (ve sağdan) <span class="math inline">\(U\)</span> ile
çarparsak (unutmayalım <span class="math inline">\(U\)</span> ortanormal
bir matris ve <span class="math inline">\(U^T U = I\)</span>),</p>
<p><span class="math display">\[ AA^TU = U \Lambda U^TU \]</span></p>
<p><span class="math display">\[ AA^TU = U \Lambda   \]</span></p>
<p>Son ifadeye yakından bakalım, <span
class="math inline">\(U\)</span>’nun tek bir kolonuna, <span
class="math inline">\(u_k\)</span> diyelim, odaklanacak olursak, üstteki
ifadeden bu sadece kolona yönelik nasıl bir eşitlik çıkartabilirdik?
Şöyle çıkartabilirdik,</p>
<p><span class="math display">\[ (AA^T)u_k = \sigma^2
u_k   \]</span></p>
<p>Bu ifade tanıdık geliyor mu? Özdeğer / özvektör klasik yapısına
eriştik. Üstteki eşitlik sadece ve sadece eğer <span
class="math inline">\(u_k\)</span>, <span
class="math inline">\(AA^T\)</span>’nin özvektörü ve <span
class="math inline">\(\sigma^2\)</span> onun özdeğeri ise geçerlidir. Bu
eşitliği tüm <span class="math inline">\(U\)</span> kolonları için
uygulayabileceğimize göre demek ki <span
class="math inline">\(U\)</span>’nun kolonlarında <span
class="math inline">\(AA^T\)</span>’nin özvektörleri vardır, ve <span
class="math inline">\(AA^T\)</span>’nin özdeğerleri <span
class="math inline">\(A\)</span>’nin eşsiz değerlerinin karesidir.</p>
<p>Bu müthiş bir buluş. Demek ki <span
class="math inline">\(AA^T\)</span>’nin özektörlerini hesaplamak için
<span class="math inline">\(A\)</span> üzerinde SVD uygulayarak <span
class="math inline">\(U\)</span>’yu bulmak ise yarar, kovaryans
matrisini hesaplamak gerekli değil. <span
class="math inline">\(AA^T\)</span> özdeğerleri üzerinde büyüklük
karşılaştırması için ise <span class="math inline">\(A\)</span>’nin
eşsiz değerlerine bakmak yeterli!</p>
<p>Dikkat, daha önce kovaryansı <span
class="math inline">\(A^TA\)</span> olarak tanımlamıştık, şimdi <span
class="math inline">\(AA^T\)</span> ifadesi görüyoruz, bir devrik
uyuşmazlığı var, bu sebeple, aslında <span
class="math inline">\(A^T\)</span>’nin SVD’si alınmalı (altta
görüyoruz).</p>
<p>Örnek</p>
<p>İlk bölümdeki örneğe dönelim, ve özvektörleri SVD üzerinden
hesaplatalım.</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.linalg <span class="im">as</span> lin</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>U,s,Vt <span class="op">=</span> lin.svd(meanless_data.T,full_matrices<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (U)</span></code></pre></div>
<pre class="text"><code>[[-0.52045195 -0.85389096]
 [-0.85389096  0.52045195]]</code></pre>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (np.dot(U.T,U))</span></code></pre></div>
<pre class="text"><code>[[ 1.00000000e+00 -2.72770824e-19]
 [-2.72770824e-19  1.00000000e+00]]</code></pre>
<p>Görüldüğü gibi aynı özvektörleri bulduk.</p>
<p>New York Times Yazıları Analizi</p>
<p>Şimdi daha ilginç bir örneğe bakalım. Bir araştırmacı belli yıllar
arasındaki NY Times makalelerinde her yazıda hangi kelimenin kaç kere
çıktığının verisini toplamış [1,2,3], bu veri 4000 küsur kelime, her
satır (yazı) için bir boyut (kolon) olarak kaydedilmiş. Bu veri
<code>nytimes.csv</code> üzerinde ek bir normalize işleminden sonra,
onun üzerinde boyut indirgeme yapabiliriz.</p>
<p>Veri setinde her yazı ayrıca ek olarak sanat (arts) ve müzik (music)
olarak etiketlenmiş, ama biz PCA kullanarak bu etiketlere hiç bakmadan,
verinin boyutlarını azaltarak acaba verinin “ayrılabilir” hale
indirgenip indirgenemediğine bakacağız. Sonra etiketleri veri üstüne
koyup sonucun doğruluğunu kontrol edeceğiz.</p>
<p>Bakmak derken veriyi (en önemli) iki boyuta indirgeyip sonucu
grafikleyeceğiz. İlla 2 olması gerekmez tabii, 10 boyuta indirgeyip (ki
4000 küsur boyuttan sonra bu hala müthiş bir kazanım) geri kalanlar
üzerinde mesela bir kümeleme algoritması kullanabilirdik.</p>
<p>Ana veriyi yükleyip birkaç satırını ve kolonlarını gösterelim.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pandas <span class="im">import</span> <span class="op">*</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy.linalg <span class="im">as</span> lin</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>nyt <span class="op">=</span> read_csv (<span class="st">&quot;nytimes.csv&quot;</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> nyt[<span class="st">&#39;class.labels&#39;</span>]</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (nyt.iloc[:<span class="dv">8</span>,<span class="dv">102</span>:<span class="dv">107</span>])</span></code></pre></div>
<pre class="text"><code>   after  afternoon  afterward  again  against
0      1          0          0      0        0
1      1          1          0      0        0
2      1          0          0      1        2
3      3          0          0      0        0
4      0          1          0      0        0
5      0          0          0      1        2
6      7          0          0      0        1
7      0          0          0      0        0</code></pre>
<p>Yüklemeyi yapıp sadece etiketleri aldık ve onları bir kenara koyduk.
Şimdi önemli bir normalizasyon işlemi gerekiyor - ki bu işleme ters
doküman-frekans ağırlıklandırması (inverse document-frequency weighting
-IDF-) ismi veriliyor - her dokümanda aşırı fazla ortaya çıkan
kelimelerin önemi özellikle azaltılıyor, ki diğer kelimelerin etkisi
artabilsin.</p>
<p>IDF kodlaması alttaki gibidir. Önce <code>class.labels</code>
kolonunu atarız. Sonra “herhangi bir değer içeren” her hücrenin 1
diğerlerinin 0 olması için kullanılan DataFrame üzerinde
<code>astype(bools)</code> işletme numarasını kullanırız, böylece aşırı
büyük değerler bile sadece 1 olacaktır. Bazı diğer işlemler sonrası her
satırı kendi içinde tekrar normalize etmek için o satırdaki tüm
değerlerin karesinin toplamının karekökünü alırız ve satırdaki tüm
değerler bu karekök ile bölünür. Buna Öklitsel (Euclidian) normalizasyon
denebilir.</p>
<p>Not: Öklitsel norm alırken toplamın hemen ardından çok ufak bir 1e-16
değeri eklememize dikkat çekelim, bunu toplamın sıfır olma durumu için
yapıyoruz, ki sonra sıfırla bölerken NaN sonucundan kaçınalım.</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>nyt2 <span class="op">=</span> nyt.drop(<span class="st">&#39;class.labels&#39;</span>,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>freq <span class="op">=</span> nyt2.astype(<span class="bu">bool</span>).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>freq <span class="op">=</span> freq.replace(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.log(<span class="bu">float</span>(nyt2.shape[<span class="dv">0</span>])<span class="op">/</span>freq)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>nyt2 <span class="op">=</span> nyt2.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x<span class="op">*</span>w,axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>nyt2 <span class="op">=</span> nyt2.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x <span class="op">/</span> np.sqrt(np.<span class="bu">sum</span>(np.square(x))<span class="op">+</span><span class="fl">1e-16</span>), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>nyt2<span class="op">=</span>nyt2.iloc[:,<span class="dv">1</span>:] <span class="co"># ilk kolonu atladik</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (nyt2.iloc[:<span class="dv">8</span>,<span class="dv">102</span>:<span class="dv">107</span>])</span></code></pre></div>
<pre class="text"><code>   afterward     again   against       age  agent
0        0.0  0.000000  0.000000  0.051085    0.0
1        0.0  0.000000  0.000000  0.000000    0.0
2        0.0  0.021393  0.045869  0.000000    0.0
3        0.0  0.000000  0.000000  0.000000    0.0
4        0.0  0.000000  0.000000  0.000000    0.0
5        0.0  0.024476  0.052480  0.000000    0.0
6        0.0  0.000000  0.008536  0.000000    0.0
7        0.0  0.000000  0.000000  0.000000    0.0</code></pre>
<p>Not: Bir diğer normalizasyon metotu</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame([[<span class="fl">1.</span>,<span class="fl">1.</span>,np.nan],</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>                   [<span class="fl">1.</span>,<span class="fl">2.</span>,<span class="fl">0.</span>],</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>                   [<span class="fl">1.</span>,<span class="fl">3.</span>,np.nan]])</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df.div(df.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>), axis<span class="op">=</span><span class="dv">1</span>))</span></code></pre></div>
<pre class="text"><code>     0    1    2
0  1.0  1.0  NaN
1  1.0  2.0  0.0
2  1.0  3.0  NaN
          0         1   2
0  0.333333  0.166667 NaN
1  0.333333  0.333333 NaN
2  0.333333  0.500000 NaN</code></pre>
<p>SVD yapalım</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>nyt3 <span class="op">=</span> nyt2 <span class="op">-</span> nyt2.mean(<span class="dv">0</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>u,s,v <span class="op">=</span> lin.svd(nyt3.T,full_matrices<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (s[:<span class="dv">10</span>])</span></code></pre></div>
<pre class="text"><code>[1.41676764 1.37161893 1.31840061 1.24567955 1.20596873 1.18624932
 1.15118771 1.13820504 1.1138296  1.10424634]</code></pre>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (u.shape)</span></code></pre></div>
<pre class="text"><code>(4430, 102)</code></pre>
<p>SVD’nin verdiği <span class="math inline">\(u\)</span> içinden iki
özvektörü seçiyoruz (en baştakiler, çünkü Numpy SVD kodu bu özvektörleri
zaten sıralanmış halde döndürür), ve veriyi bu yeni kordinata
izdüşümlüyoruz.</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>proj <span class="op">=</span> np.dot(nyt3, u[:,:<span class="dv">2</span>])</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>proj.shape</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>plt.plot(proj[:,<span class="dv">0</span>],proj[:,<span class="dv">1</span>],<span class="st">&#39;.&#39;</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;pca_4.png&#39;</span>)</span></code></pre></div>
<p><img src="pca_4.png" /></p>
<p>Şimdi aynı veriyi bir de etiket bilgisini devreye sokarak çizdirelim.
Sanat kırmızı müzik mavi olacak.</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>arts <span class="op">=</span>proj[labels <span class="op">==</span> <span class="st">&#39;art&#39;</span>]</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>music <span class="op">=</span>proj[labels <span class="op">==</span> <span class="st">&#39;music&#39;</span>]</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>plt.plot(arts[:,<span class="dv">0</span>],arts[:,<span class="dv">1</span>],<span class="st">&#39;r.&#39;</span>)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>plt.plot(music[:,<span class="dv">0</span>],music[:,<span class="dv">1</span>],<span class="st">&#39;b.&#39;</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;pca_5.png&#39;</span>)</span></code></pre></div>
<p><img src="pca_5.png" /></p>
<p>Görüldüğü gibi veride ortaya çıkan / özvektörlerin keşfettiği doğal
ayırım, hakikaten doğruymuş.</p>
<p>Metotun ne yaptığına dikkat, bir sürü boyutu bir kenara atmamıza
rağmen geri kalan en önemli 2 boyut üzerinden net bir ayırım ortaya
çıkartabiliyoruz. Bu PCA yönteminin iyi bir iş becerdiğini gösteriyor,
ve kelime sayılarının makalelerin içeriği hakkında ipucu içerdiğini
ispatlıyor.</p>
<p>Not: Lineer Cebir notlarımızda SVD türetilmesine bakınca
özdeğer/vektör mantığına atıf yapıldığını görebiliriz ve akla şu
gelebilir; “özdeğer / vektör rutini işletmekten kurtulalım dedik, SVD
yapıyoruz, ama onun içinde de özdeğer/vektör hesabı var”. Fakat şunu
belirtmek gerekir ki SVD sayısal hesabını yapmanın tek yöntemi
özdeğer/vektör yöntemi değildir. Mesela Numpy Linalg kütüphanesi
içindeki SVD, LAPACK <code>dgesdd</code> rutinini kullanır ve bu rutin
iç kodlamasında QR, ve bir tür böl / istila et (divide and conquer)
algoritması işletmektedir.</p>
<p>Kaynaklar</p>
<p>[1] Alpaydın, E., <em>Introduction to Machine Learning, 2nd
Edition</em></p>
<p>[2] Strang, G., <em>Linear Algebra and Its Applications, 4th
Edition</em></p>
<p>[3] Wood, <em>Principal Component Analysis</em>, Lecture,<a
href="http://www.robots.ox.ac.uk/~fwood/teaching/index.html">http://www.robots.ox.ac.uk/~fwood/teaching/index.html</a></p>
<p>[4] Cosma Shalizi, <em>Advanced Data Analysis from an Elementary
Point of View</em></p>
<p>[5] <em>The New York Times Annotated Corpus</em>, <a
href="http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T19">http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2008T19</a></p>
<p>[6] Shalizi, <em>Statistics 36-350: Data Mining Lecture</em>,<a
href="http://www.stat.cmu.edu/~cshalizi/350/">http://www.stat.cmu.edu/~cshalizi/350/</a></p>
<p>[7] Goodman, <em>Risk and Portfolio Management with
Econometrics</em>, <a
href="http://www.math.nyu.edu/faculty/goodman/teaching/RPME/notes/Section3.pdf">http://www.math.nyu.edu/faculty/goodman/teaching/RPME/notes/Section3.pdf</a></p>
<p>[8] Collins, <em>Introduction to Computer Vision</em>, <a
href="http://www.cse.psu.edu/~rtc12/CSE486/">http://www.cse.psu.edu/~rtc12/CSE486/</a></p>
<p>[9] Weng, <em>Candid Covariance-free Incremental Principal Component
Analysis</em>, <a
href="https://pdfs.semanticscholar.org/4e22/d6b9650a4ff9ccc8c9b860442d162d559025.pdf">https://pdfs.semanticscholar.org/4e22/d6b9650a4ff9ccc8c9b860442d162d559025.pdf</a></p>
<p>[10] Bayramlı, Lineer Cebir <em>Ders 29</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
