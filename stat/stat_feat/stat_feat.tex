\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Özellik Ýþleme, Seçme, Veri Ýncelemesi

Medyan ve Yüzdelikler (Percentile)

Üstteki hesaplarýn çoðu sayýlarý toplayýp, bölmek üzerinden yapýldý. Medyan
ve diðer yüzdeliklerin hesabý (ki medyan 50. yüzdeliðe tekabül eder) için
eldeki tüm deðerleri "sýraya dizmemiz" ve sonra 50. yüzdelik için
ortadakine bakmamýz gerekiyor. Mesela eðer ilk 5. yüzdeliði arýyorsak ve
elimizde 80 tane deðer var ise, baþtan 4. sayýya / vektör hücresine / öðeye
bakmamýz gerekiyor. Eðer 100 eleman var ise, 5. sayýya bakmamýz gerekiyor,
vs.

Bu sýraya dizme iþlemi kritik. Kýyasla ortalama hesabý hangi sýrada olursa
olsun, sayýlarý birbirine topluyor ve sonra bölüyor. Zaten ortalama ve
sapmanýn istatistikte daha çok kullanýlmasýnýn tarihi sebebi de aslýnda bu;
bilgisayar öncesi çaðda sayýlarý sýralamak (sorting) zor bir iþti. Bu
sebeple hangi sýrada olursa olsun, toplayýp, bölerek hesaplanabilecek
özetler daha makbuldü. Fakat artýk sýralama iþlemi kolay, ve veri setleri
her zaman tek tepeli, simetrik olmayabiliyor. Örnek veri seti olarak ünlü
\verb!dellstore2! tabanýndaki satýþ miktarlarý kullanýrsak,

\begin{minted}[fontsize=\footnotesize]{python}
print np.mean(data)
\end{minted}

\begin{verbatim}
213.948899167
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.median(data)
\end{minted}

\begin{verbatim}
214.06
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.std(data)
\end{minted}

\begin{verbatim}
125.118481954
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.mean(data)+2*np.std(data)
\end{minted}

\begin{verbatim}
464.185863074
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print np.percentile(data, 95)
\end{minted}

\begin{verbatim}
410.4115
\end{verbatim}

Görüldüðü gibi üç nokta hesabý için ortalamadan iki sapma ötesini
kullanýrsak, 464.18, fakat 95. yüzdeliði kullanýrsak 410.41 elde
ediyoruz. Niye? Sebep ortalamanýn kendisi hesaplanýrken çok üç
deðerlerin toplama dahil edilmiþ olmasý ve bu durum, ortalamanýn
kendisini daha büyük seviyeye doðru itiyor. Yüzdelik hesabý ise sadece
sayýlarý sýralayýp belli bazý elemanlarý otomatik olarak üç nokta
olarak addediyor.

Box Whisker Grafikleri

Tek boyutlu bir verinin daðýlýmýný görmek için Box ve Whisker grafikleri
faydalý araçlardýr; medyan (median), daðýlýmýn geniþliðini ve sýradýþý
noktalarý (outliers) açýk þekilde gösterirler. Ýsim nereden geliyor? Box
yani kutu, daðýlýmýn aðýrlýðýnýn nerede olduðunu gösterir, medyanýn
saðýndada ve solunda olmak üzere iki çeyreðin arasýndaki kýsýmdýr, kutu
olarak resmedilir. Whiskers kedilerin býyýklarýna verilen isimdir, zaten
grafikte birazcýk býyýk gibi duruyorlar. Bu uzantýlar medyan noktasýndan
her iki yana kutunun iki katý kadar uzatýlýr sonra verideki "ondan az olan
en büyük" noktaya kadar geri çekilir. Tüm bunlarýn dýþýnda kalan veri ise
teker teker nokta olarak grafikte basýlýr. Bunlar sýradýþý (outlier)
olduklarý için daha az olacaklarý tahmin edilir.

BW grafikleri iki veriyi daðýlýmsal olarak karþýlaþtýrmak için
birebirdir. Mesela Larsen and Marx adlý araþtýrmacýlar çok az veri
içeren Quintus Curtius Snodgrass veri setinin deðiþik olduðunu
ispatlamak için bir sürü hesap yapmýþlardýr, bir sürü matematiksel
iþleme girmiþlerdir, fakat basit bir BW grafiði iki setin farklýlýðýný
hemen gösterir.

BW grafikleri iki veriyi daðýlýmsal olarak karþýlaþtýrmak için
birebirdir. Mesela Larsen and Marx adlý araþtýrmacýlar çok az veri
içeren Quintus Curtius Snodgrass veri setinin deðiþik olduðunu
ispatlamak için bir sürü hesap yapmýþlardýr, bir sürü matematiksel
iþleme girmiþlerdir, fakat basit bir BW grafiði iki setin farklýlýðýný
hemen gösterir.

Python üzerinde basit bir BW grafiði 

\begin{minted}[fontsize=\footnotesize]{python}
spread= rand(50) * 100
center = ones(25) * 50
flier_high = rand(10) * 100 + 100
flier_low = rand(10) * -100
data =concatenate((spread, center, flier_high, flier_low), 0)
plt.boxplot(data)
plt.savefig('stat_feat_01.png')
\end{minted}

\includegraphics[height=6cm]{stat_feat_01.png}

Bir diðer örnek Glass veri seti üzerinde

\begin{minted}[fontsize=\footnotesize]{python}
data = loadtxt("glass.data",delimiter=",")
head = data[data[:,10]==7]
tableware = data[data[:,10]==6]
containers = data[data[:,10]==5]

print head[:,1]

data =(containers[:,1], tableware[:,1], head[:,1])

plt.yticks([1, 2, 3], ['containers', 'tableware', 'head'])

plt.boxplot(data,0,'rs',0,0.75)
plt.savefig('stat_feat_02.png')
\end{minted}

\begin{verbatim}
[ 1.51131  1.51838  1.52315  1.52247  1.52365  1.51613  1.51602  1.51623
  1.51719  1.51683  1.51545  1.51556  1.51727  1.51531  1.51609  1.51508
  1.51653  1.51514  1.51658  1.51617  1.51732  1.51645  1.51831  1.5164
  1.51623  1.51685  1.52065  1.51651  1.51711]
\end{verbatim}

\includegraphics[height=6cm]{stat_feat_02.png}

Zaman Kolonlarýný Zenginleþtirmek

Veri madenciliðinde "veriden veri yaratma" tekniði çok kullanýlýyor; mesela
bir sipariþ veri satýrýnda o sipariþin hangi zamanda (timestamp) olduðunu
belirten bir kolon varsa (ki çoðu zaman vardýr), bu kolonu "parçalayarak"
ek, daha genel, özetsel bilgi kolonlarý yaratýlabilir. Zaman kolonlarý çoðu
zaman saniyeye kadar kaydedilir, bu bilgiyi alýp mesela ay, mevsim,
haftanýn günü, saat, iþ saati mi (9-5 arasý), akþam mý, sabah mý, öðlen mi,
vs. gibi ek bilgiler çýkartýlabilir. Tüm kolonlar veri madenciliði
algoritmasýna verilir, ve algoritma belki öðlen saati ile sipariþ verilmiþ
olmasý arasýnda genel bir baðlantý bulacaktýr.

Python + Pandas ile bir zaman kolonu þöyle parçalanabilir, örnek veri
üzerinde görelim, sadece iki kolon var, müþteri no, ve sipariþ zamaný,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
from StringIO import StringIO
s = """customer_id;order_date
299;2012-07-20 19:44:55.661000+01:00
421;2012-02-17 21:54:15.013000+01:00
437;2012-02-20 22:18:12.021000+01:00
463;2012-02-20 23:46:21.587000+01:00
482;2012-05-21 09:50:02.739000+01:00
607;2012-02-21 11:57:12.462000+01:00
641;2012-02-21 13:40:28.088000+01:00
674;2012-08-21 14:53:15.851000+01:00
780;2012-02-23 10:31:05.571000+01:00
"""
df = pd.read_csv(StringIO(s),sep=';', parse_dates=True)

def f(x):
   tmp = pd.to_datetime(x['order_date'])
   tpl = tmp.timetuple(); yymm = int(tmp.strftime('%m%d'))
   spring = int(yymm >= 321 and yymm < 621)
   summer = int(yymm >= 621 and yymm < 921)
   fall = int(yymm >= 921 and yymm < 1221)
   winter = int( spring==0 and summer==0 and fall==0 )
   warm_season = float(tpl.tm_mon >= 4 and tpl.tm_mon <= 9)
   work_hours = float(tpl.tm_hour > 9 and tpl.tm_hour < 17)
   morning = float(tpl.tm_hour >= 7 and tpl.tm_hour <= 11)
   noon = float(tpl.tm_hour >= 12 and tpl.tm_hour <= 14)
   afternoon = float(tpl.tm_hour >= 15 and tpl.tm_hour <= 19)
   night = int (morning==0 and noon==0 and afternoon==0)

   return pd.Series([tpl.tm_hour, tpl.tm_mon,
                     tpl.tm_wday, warm_season,
                     work_hours, morning, noon, afternoon, night,
                     spring, summer, fall, winter])
cols = ['ts_hour','ts_mon','ts_wday','ts_warm_season',
        'ts_work_hours','ts_morning','ts_noon','ts_afternoon',
        'ts_night', 'ts_spring', 'ts_summer', 'ts_fall', 'ts_winter']
df[cols] = df.apply(f, axis=1)
print df[cols]
\end{minted}

\begin{verbatim}
   ts_hour  ts_mon  ts_wday  ts_warm_season  ts_work_hours  ts_morning  \
0       18       7        4               1              0           0   
1       20       2        4               0              0           0   
2       21       2        0               0              0           0   
3       22       2        0               0              0           0   
4        8       5        0               1              0           1   
5       10       2        1               0              1           1   
6       12       2        1               0              1           0   
7       13       8        1               1              1           0   
8        9       2        3               0              0           1   

   ts_noon  ts_afternoon  ts_night  ts_spring  ts_summer  ts_fall  ts_winter  
0        0             1         0          0          1        0          0  
1        0             0         1          0          0        0          1  
2        0             0         1          0          0        0          1  
3        0             0         1          0          0        0          1  
4        0             0         0          1          0        0          0  
5        0             0         0          0          0        0          1  
6        1             0         0          0          0        0          1  
7        1             0         0          0          1        0          0  
8        0             0         0          0          0        0          1  
\end{verbatim}

Sýcak mevsim (warm season) Mart-Eylül aylarýný kapsar, bu ikisel bir
deðiþken hale getirildi. Belki sipariþin, ya da diðer baþka bir verinin
bununla bir alakasý vardýr. Genel 4 sezon tek baþýna yeterli deðil midir?
Olabilir, fakat bazý kalýplar / örüntüler (patterns) belki sýcak / soðuk
mevsim bilgisiyle daha çok baðlantýlýdýr. 

Ayný þekilde saat 1-24 arasýnda bir sayý olarak var, fakat "iþ saatini"
ayrý bir ikisel deðiþken olarak kodlamak yine bir "kalýp yakalama"
þansýmýzý arttýrabilir. Bu kolonun ayrý bir þekilde kodlanmýþ olmasý veri
tasarýmý açýsýndan ona önem verildiðini gösterir, ve madencilik
algoritmalarý bu kolonu, eðer ona baðlý bir kalýp var ise,
yakalayabilirler.

Not: Burada ufak bir pürüz sabah, öðlen, akþamüstü gibi zamanlarý kodlarken
çýktý. Gece 19'dan sonra ve 7'den önce bir sayý olacaktý, fakat bu durumda
$x>19$ ve $x<7$ hiçbir sonuç getirmeyecekti. Burada saatlerin 24 sonrasý baþa
dönmesi durumu problem çýkartýyordu, tabii ki karþýlaþtýrma ifadelerini
çetrefilleþtirerek bu iþ çözülebilir, ama o zaman kod temiz olmaz (mesela
($x>19$ ve $x<24$) ya da ($x>0$ ve $x<7$) yapabilirdik). Temiz kod için gece
haricinde diðer tüm seçenekleri kontrol ediyoruz, ve gece "sabah, öðlen,
akþamüstü olmayan þey" haline geliyor. Ayný durum mevsimler için de
geçerli. Onun için 

\begin{minted}[fontsize=\footnotesize]{python}
night = int (morning==0 and noon==0 and afternoon==0)
\end{minted}

kullanýldý.

Kategörleri Ýkileþtirme, Anahtarlama Numarasý (Öne-Hot Encoding, Hashing Trick)

Üstteki kodda bir problem var, dokümaný temsil eden ve içinde 1 ya da
0 hücreli özellik vektörünü (feature vector) oluþturmak için tüm
kelimelerin ne olduðunu bilmeliyiz. Yani veriyi bir kere baþtan sonra
tarayarak bir sözlük oluþturmalýyýz (ki öyle yapmaya mecbur kaldýk) ve
ancak ondan sonra her doküman için hangi kelimenin olup olmadýðýný
saptamaya ve onu kodlamaya baþlayabiliriz. Halbuki belgelere bakar
bakmaz, teker teker giderken bile hemen bir özellik vektörü
oluþturabilseydik daha iyi olmaz mýydý?

Bunu baþarmak için anahtarlama numarasýný kullanmamýz lazým. Bilindiði
gibi temel yazýlým bilime göre bir kelimeyi temsil eden bir anahtar
(hash) üretebiliriz, ki bu hash deðeri bir sayýdýr. Elimizde bir
"sayý" olmasý bize faydalý olur yarar, bu sayýnýn en fazla kaç
olabileceðinden hareketle (hatta bu sayýya bir limit koyarak) özellik
vektörümüzün boyutunu önceden saptamýþ oluruz.  Sonra kelimeye
bakarýz, hash üretiriz, sonuç mesela 230 geldi, o zaman özellik
vektöründeki 230'uncu kolonun deðerini 1 yaparýz. 

\begin{minted}[fontsize=\footnotesize]{python}
d_input = dict()

def add_word(word):
    hashed_token = hash(word) % 127
    d_input[hashed_token] = d_input.setdefault(hashed_token, 0) + 1

add_word("obama")
print d_input
\end{minted}

\begin{verbatim}
{48: 1}
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
add_word("politics")
print d_input
\end{minted}

\begin{verbatim}
{48: 1, 91: 1}
\end{verbatim}

Üstteki kodda bunun örneðini görüyoruz. Hash sonrasý mod uyguladýk
(yüzde iþareti ile) ve hash sonucunu en fazla 127 olacak þekilde
sýnýrladýk. Sözlük (dictionary) yavaþ yavaþ büyüyebiliyor.
Potansiyel problemler ne olabilir? Hashing mükemmel deðildir, çarpýþma
(collision) olmasý mümkündür yani nadiren farklý kelimelerin ayný
numaraya eþlenebilmesi durumu. Bu problemleri iyi bir anahtarlama
algoritmasý kullanarak, mod edilen sayýyý büyük tutarak çözmek
mümkündür, ya da bu tür nadir çarpýþmalar "kabul edilir hata" olarak
addedilebilir.

Pandas kullanarak bir Dataframe'i otomatik olarak anahtarlamak istersek,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}

data = pd.DataFrame(data)
print data
\end{minted}

\begin{verbatim}
   pop   state  year
0  1.5    Ohio  2000
1  1.7    Ohio  2001
2  3.6    Ohio  2002
3  2.4  Nevada  2001
4  2.9  Nevada  2002
\end{verbatim}

Þimdi bu veri üzerinde sadece eyalet (state) için bir anahtarlama numarasý
yapalým

\begin{minted}[fontsize=\footnotesize]{python}
def hash_col(df,col,N):
    for i in range(N): df[col + '_' + str(i)] = 0.0
    df[col + '_hash'] = df.apply(lambda x: hash(x[col]) % N,axis=1)    
    for i in range(N):
        idx = df[df[col + '_hash'] == i].index
        df.ix[idx,'%s_%d' % (col,i)] = 1.0
    df = df.drop([col, col + '_hash'], axis=1)
    return df

print hash_col(data,'state',4)
\end{minted}

\begin{verbatim}
   pop  year  state_0  state_1  state_2  state_3
0  1.5  2000        0        1        0        0
1  1.7  2001        0        1        0        0
2  3.6  2002        0        1        0        0
3  2.4  2001        0        0        0        1
4  2.9  2002        0        0        0        1
\end{verbatim}

Azar Azar Ýþlemek (Incremental, Minibatch Processing)

Üstteki yöntemler eðer tüm veri hafýzada ise iyi. Fakat çoðu zaman onlarca
kategori, birkaç milyonluk satýr içeren bir veriye bakmamýz gerekiyor;
biliyoruz ki bu kadar veri için Büyük Veri teknolojilerine (mesela Spark,
Hadoop gibi) geçmek gereðinden fazla külfet getirecek, elimizdeki dizüstü,
masaüstü bilgisayarý bu iþlemler için yeterli olmalý, fakat çoðu kütüphane
tek makinada azar azar iþlem yapmak için yazýlmamýþ.

Bu durumda kendimiz çok basit Python kavramlarýný, iyi bir anahtarlama
kodunu, ve lineer cebir hesaplarýnda seyreklik (sparsýty) tekniklerini
kullanarak ufak veri parçalarý iþleyen bir ortamý yaratabiliriz.

Örnek veri olarak {\em Lineer Regresyon} yazýsýnda görülen oy kalýplarý
verisini biraz deðiþtirerek yeni bir analiz için kullanalým. Veri oy
verenlerin ýrk, cinsiyet, meslek, hangi partiye oy verdikleri ve
kazançlarýný kaydetmiþ, biz analizimizde bahsedilen kategorilerin bu
kiþilerin kazancýyla baðlantýlý olup olmadýðýna bakacaðýz. Veriyi oluþturalým,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('../stat_logit/nes.dat',sep=r'\s+')
df = df[['presvote','year','gender','income','race','occup1']]
df = df.dropna()
df.to_csv('nes2.csv',index=None)
\end{minted}

Önce kategorilerden ne kadar var, sayalým. Basit toplam yani,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('nes2.csv')
print u'tüm veri', len(df)
print 'cinsiyet', np.array(df['gender'].value_counts())
print u'ýrk', np.array(df['race'].value_counts())
print 'parti', np.array(df['presvote'].value_counts())
\end{minted}

\begin{verbatim}
tüm veri 13804
cinsiyet [7461 6343]
ýrk [12075  1148   299   180    85    17]
parti [6998 6535  271]
\end{verbatim}

Mesela son sonuçtaki her hücre belli bir partiye verilen oylarýn sayýsý;
veriye göre üç farklý kategori varmýþ demek ki, veri ABD için olduðuna göre
bunlardan ilk ikisi bilinen iki büyük parti, üçüncü hücre de herhalde
baðýmsýz adaylar.

Kategorik verileri ikileþtirmeye gelelim. Burada üç nokta önemli, azar azar
iþleyeceðiz, sonucu seyrek matris almak istiyoruz, ve hangi kategorik
deðerin hangi kolona eþleneceðini elle tanýmlamak istemiyoruz (eþleme
otomatik olmalý). Seyreklik önemli çünkü eðer 1000 farklý kategorik deðere
sahip olan 10 tane kolon varsa, bu 10000 tane yeni kolon yaratýlmasý
demektir - her farklý kategori için o deðere tekabül eden kolon 1 olacak
gerisi 0 olacak. Bu rakamlar orta ölçekte bile rahatlýkla milyonlara
ulaþabilir. Eðer ikileþtirme için seyrek matris kullanýrsak çoðu sýfýr olan
deðerler hafýzada bile tutulmaz. Eþleme otomatik olmalý, zaten onun için
anahtarlama yapacaðýz. 

Anahtarlama icin \verb!sklearn.feature_extraction.text.HashingVectorizer!
var,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.feature_extraction.text import HashingVectorizer
import numpy as np
vect = HashingVectorizer(n_features=20)
a = ['aa','bb','cc']
res = vect.transform(a)
print res
\end{minted}

\begin{verbatim}
  (0, 5)	1.0
  (1, 19)	1.0
  (2, 18)	-1.0
\end{verbatim}

Sonuçlar seyrek matris olarak, ve üç deðer için üç ayrý satýr olarak
geldi. Anahtarlama niye-1 ya da +1 veriyor? Aslýnda bu bizim için çok faydalý,
çünkü birazdan PCA iþleteceðiz mesela ve bu yöntem her veri kolonunun
sýfýrda ortalanmýþ olmasýný ister. Üstteki teknikte bizim tahminimiz -1,+1
arasýnda rasgele seçim yapýlýyor böylece üretilen anahtar kolonlarýnda
-1 / +1 doðal olarak dengelenmiþ olacaktýr, o zaman otomatik olarak
ortalamalarý sýfýra iner. Akýllýca bir teknik.

Devam edelim, sonucu tek satýr olacak þekilde kendimiz tekrar
düzenleyebiliriz. O zaman Python \verb!yield! kavramýný [3] kullanarak
(azar azar satýr okumak için), ve anahtarlama, seyrek matrisler ile þu
þekilde bir kod olabilir,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.feature_extraction.text import HashingVectorizer
import numpy as np
import pandas as pd, csv
import scipy.sparse as sps

HASH = 30
vect = HashingVectorizer(decode_error='ignore',n_features=HASH)

def get_row(cols):
    with open("nes2.csv", 'r') as csvfile:
        rd = csv.reader(csvfile)
        headers = {k: v for v, k in enumerate(next(rd))}
        for row in rd:
            label = float(row[headers['income']])
            rrow = [x + str(row[headers[x]]) for x in headers if x in cols]
            X_train = vect.transform(rrow)
            yield X_train.tocoo(), label            

def get_minibatch(row_getter,size=10):
    X_train = sps.lil_matrix((size,HASH))
    y_train = []
    for i in range(size):
        cx,y = row_getter.next()
        for dummy,j,val in zip(cx.row, cx.col, cx.data): X_train[i,j] = val
        y_train.append(y)
    return X_train, y_train
    
cols = ['gender','income','race','occup1']
row_getter = get_row(cols)
X,y = get_minibatch(row_getter,size=1)
print y, X.todense()
\end{minted}

\begin{verbatim}
[4.0] [[ 0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.
   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.]]
\end{verbatim}

Ýlgilendiðimiz kolon listesini \verb!get_row!'a verip bir gezici fonksiyon
yarattýk. Bu geziciyi \verb!get_minibatch!'e verdik, kaç tane satýr
istediðimizi ona söylüyoruz, o bize istenen kadar satýrý seyrek matris
olarak veriyor. 10 tane daha isteyelim,

\begin{minted}[fontsize=\footnotesize]{python}
X,y = get_minibatch(row_getter,size=10)
print len(y), X.shape, type(X)
\end{minted}

\begin{verbatim}
10 (10, 30) <class 'scipy.sparse.lil.lil_matrix'>
\end{verbatim}

PCA

Lineer Cebir'in temel bileþen analizi (PCA) tekniðini kullanarak boyut
azaltmasý yapabiliriz. Bu yazýda veriyi satýr satýr iþleyerek PCA hesabý
yapan bir kod var, ayrýca bu kod satýrlarý seyrek formatta alabiliyor. Bu
iyi haber, çünkü bu ufak örnekte onlarca kolon var fakat milyonlarca kolon
olsa bu kod yine iþleyecektir. 

\begin{minted}[fontsize=\footnotesize]{python}
import sys; sys.path.append('../../linear/linear_app05pca')
import ccipca
cols = ['gender','income','race','occup1']
row_getter = get_row(cols)
pca = ccipca.CCIPCA(n_components=10,n_features=30)
for i in range(10000): 
    X,y = get_minibatch(row_getter,size=1)
    pca.partial_fit(X)
pca.post_process()

print 'varyans orani'
print pca.explained_variance_ratio_
\end{minted}

\begin{verbatim}
varyans orani
[ 0.36086926  0.16186391  0.13377998  0.09440711  0.0702763   0.05113956
  0.04768294  0.0343724   0.02336052  0.02224802]
\end{verbatim}

Her bileþenin verideki varyansýn ne kadarýný açýkladýðý görülüyor. 30
kolonu 10 kolona indirdik, acaba veri temsilinde ilerleme elde ettik mi?
Veriyi PCA'nýn bulduðu uzaya yansýtýp bu boyutu azaltýlmýþ veriyi
regresyonda kullansak ne olur acaba? Yansýtma ve regresyon,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.linear_model import SGDRegressor
clf = SGDRegressor(random_state=1, n_iter=1)
row_getter = get_row(cols)
P = pca.components_.T
for i in range(10000):
    X_train, y_train = get_minibatch(row_getter,1)
    Xp = np.dot((X_train-pca.mean_),P)
    clf.partial_fit(Xp, y_train)
\end{minted}

Þimdi sonraki 1000 satýrý test için kullanalým,

\begin{minted}[fontsize=\footnotesize]{python}
y_predict = []
y_real = []
for i in range(1000):
    X_test,y_test = get_minibatch(row_getter,1)
    Xp = np.dot((X_test-pca.mean_),P)
    y_predict.append(clf.predict(Xp)[0])
    y_real.append(y_test[0])
y_predict = np.array(y_predict)
y_real = np.array(y_real)
print 'ortalama tahmin hatasi', np.sqrt(((y_predict-y_real)**2).sum()) / len(y_predict)
print 'maksimum deger', np.max(y_real)
\end{minted}

\begin{verbatim}
ortalama tahmin hatasi 0.0105872845541
maksimum deger 5.0
\end{verbatim}

1 ile 5 arasinda gidip gelen degerlerin tahmininde 0.01 civari ortalama
hata var. Fena degil. Peki verinin kendisini oldugu gibi alip regresyonda
kullansaydik? Regresyonun hedef verisi kazanç, kaynak verisi geri kalan
kategoriler. Her parça 1000 satýr, 10 parça alacaðýz, 10,000 satýr modeli
eðitmek için kullanýlacak. Geri kalanlar test verisi
olacak. \verb!sklearn.linear_model.SGDRegressor! ufak seyrek matris
parçalarý ile eðitilebiliyor,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.linear_model import SGDRegressor
clf = SGDRegressor(random_state=1, n_iter=1)
row_getter = get_row(cols)

y_predict = []; y_real = []

for i in range(10):
    X_train, y_train = get_minibatch(row_getter,1000)
    clf.partial_fit(X_train, y_train)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
X_test,y_test = get_minibatch(row_getter,1000)
print 'Skor', clf.score(X_test, y_test)    
y_predict = clf.predict(X_test) 
print 'ortalama tahmin hatasi', np.sqrt(((y_predict-y_test)**2).sum()) / len(y_predict)
\end{minted}

\begin{verbatim}
Skor 0.609560912327
ortalama tahmin hatasi 0.0208096951078
\end{verbatim}


Bu sonuç ta hiç fena deðil.  Sonuç olarak veri içinde bazý kalýplar
olduðunu gördük, tahmin yapabiliyoruz. Hangi kolonlarýn daha önemli
olduðunu bulmak için her kolonu teker teker atýp skorun yukarý mý aþaðý mý
indiðine bakabilirdik.

Kaynaklar 

[1] Teetor, {\em R Cookbook}

[2] Scikit-Learn Documentation, {\em 4.2. Feature extraction}, \url{http://scikit-learn.org/dev/modules/feature_extraction.html}

[3] Bayramli, {\em Fonksiyon Gezmek ve Yield}, \url{http://sayilarvekuramlar.blogspot.com/2011/02/fonksiyon-gezmek-ve-yield.html}

\end{document}
	

