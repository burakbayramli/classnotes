<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Lojistik Regresyon (Logistic Regression)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="lojistik-regresyon-logistic-regression">Lojistik Regresyon (Logistic Regression)</h1>
<p>Lojistik regresyon normal regresyonun <span class="math inline">\(\theta^T x\)</span> olarak kullandığı ağırlıklar (katsayılar) ile verinin çarpımını alır ve ek bir filtre fonksiyonundan geçirerek onları 0/1 değerleri bağlamında bir olasılığa eşler. Yani elimizdeki veri çok boyutta veri noktaları ve o noktaların 0 ya da 1 olarak bir &quot;etiketi&quot; olacaktır. Mesela</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> pandas <span class="im">import</span> <span class="op">*</span>
df <span class="op">=</span> read_csv(<span class="st">&quot;testSet.txt&quot;</span>,sep<span class="op">=</span><span class="st">&#39;</span><span class="ch">\t</span><span class="st">&#39;</span>,names<span class="op">=</span>[<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;labels&#39;</span>],header<span class="op">=</span><span class="va">None</span>)
df[<span class="st">&#39;intercept&#39;</span>]<span class="op">=</span><span class="fl">1.0</span>
data <span class="op">=</span> df[[<span class="st">&#39;intercept&#39;</span>,<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>]]
labels <span class="op">=</span> df[<span class="st">&#39;labels&#39;</span>]
<span class="bu">print</span> df[[<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>,<span class="st">&#39;labels&#39;</span>]][:<span class="dv">10</span>]</code></pre></div>
<pre><code>          x          y  labels
0 -0.017612  14.053064       0
1 -1.395634   4.662541       1
2 -0.752157   6.538620       0
3 -1.322371   7.152853       0
4  0.423363  11.054677       0
5  0.406704   7.067335       1
6  0.667394  12.741452       0
7 -2.460150   6.866805       1
8  0.569411   9.548755       0
9 -0.026632  10.427743       0</code></pre>
<p>Görüldüğü gibi veride <span class="math inline">\(x,y\)</span> boyutları için etiketler (labels) verilmiş. Lojistik regresyon bu veriyi kullanarak eğitim sonrası <span class="math inline">\(\theta\)</span>'ları elde eder, bunlar katsayılarımızdır, artık bu katsayıları hiç görmediğimiz yeni bir veri üzerinde 0/1 etiketlerinin tahminini yapmak için kullanabiliriz.</p>
<p>Filtre fonksiyonu için kullanılan bir fonksiyon sigmoid fonksiyonudur, <span class="math inline">\(g(x)\)</span> ismini verelim,</p>
<p><span class="math display">\[ g(x) = \frac{e^{x}}{1+e^{x}} \]</span></p>
<p>Bu nasıl bir fonksiyondur, kabaca davranışını nasıl tarif ederiz? Cebirsel olarak bakarsak, fonksiyon öyle bir durumda ki ne zaman bir <span class="math inline">\(x\)</span> değeri geçersek, bu değer ne kadar büyük olursa olsun, bölendeki değer her zaman bölünenden 1 daha fazla olacaktır bu da fonksiyonun sonucunun 1'den her zaman küçük olmasını garantiler. Çok küçük <span class="math inline">\(x\)</span> değerleri için bölüm sonucu biraz daha büyük olacaktır tabii, vs.</p>
<p>Daha temiz bir ifade için bölen ve bölüneni <span class="math inline">\(e^{-x}\)</span> ile çarpalım,</p>
<p><span class="math display">\[ g(x) = \frac{e^{x}e^{-x}}{e^{-x}+e^{x}e^{-x}} \]</span></p>
<p><span class="math display">\[ g(x) = \frac{1}{1+e^{-x}} \]</span></p>
<p>Sigmoid fonksiyonun &quot;-sonsuzluk ile +sonsuzluk arasındaki değerleri 0 ve 1 arasına eşlediği / indirgediği (map)&quot; ifadesi de litaratürde mevcuttur.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> sigmoid(arr):
    <span class="cf">return</span> <span class="fl">1.0</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>exp(<span class="op">-</span>arr))

x <span class="op">=</span> np.array(arange(<span class="op">-</span><span class="fl">10.0</span>, <span class="fl">10.0</span>, <span class="fl">0.1</span>))
plt.plot(x,sigmoid(x))
plt.savefig(<span class="st">&#39;stat_logit_02.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_02.png" />

</div>
<p>Üstteki grafiğe bakınca katsayılarla çarpım, toplam ardından sonucun niye bu fonksiyona verildiğini anlamak mümkün. Sigmoid'in 0 seviyesinden 1 seviyesine zıplayısı oldukça hızlı ve <span class="math inline">\(x\)</span> kordinatı bağlamında (ve 0.5'ten küçük <span class="math inline">\(y\)</span>'ye eşlenen) sıfır öncesi bölgesi, aynı şekilde sıfır sonrası (ve 0.5'ten büyük <span class="math inline">\(y\)</span>'ye eşlenen) bölgesi oldukça büyük. Yani bu fonksiyonu seçmekle veriye katsayılarla çarpılıp 0 ya da 1 bölgesi altına düşmesi için oldukça geniş bir şans veriyoruz. Böylece veriyi iki parçaya ayırmak için şansımızı arttırmış oluyoruz.</p>
<p>Peki sigmoid fonksiyonu bir olasılık fonksiyonu (dağılımı) olarak kullanılabilir mi? Entegralini alalım, ve -/+ sonsuzluklar üzerinden alan hesabı yapalım, sonucun 1 çıkması gerekli,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> sympy
x <span class="op">=</span> sympy.Symbol(<span class="st">&#39;x&#39;</span>)
<span class="bu">print</span> sympy.integrate(<span class="st">&#39;1/(1+exp(-x))&#39;</span>)</code></pre></div>
<pre><code>x + log(1 + exp(-x))</code></pre>
<p>Daha temizlemek için</p>
<p><span class="math display">\[ x + \ln(1 + e^{-x}) \]</span></p>
<p><span class="math inline">\(x\)</span> ifadesi aynı zamanda suna eşittir <span class="math inline">\(x=ln( e^{x} )\)</span>. Bu ifade bize kolaylık sağlayacak böylece,</p>
<p><span class="math display">\[ \ln e^{x} + \ln(1+e^{-x})  \]</span></p>
<p>diyebiliriz. Doğal log'un (ln) çarpımları toplamlara dönüştürdüğünü biliyoruz, bunu tersinden uygulayalım,</p>
<p><span class="math display">\[ \ln (e^{x}\cdot 1 + e^{x}e^{-x})  \]</span></p>
<p><span class="math display">\[ \ln (e^{x} + 1)  = \ln (1 + e^{x} )  \]</span></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> log (<span class="dv">1</span><span class="op">+</span>exp(<span class="op">-</span>inf))
<span class="bu">print</span> log(<span class="dv">1</span><span class="op">+</span>exp(inf))</code></pre></div>
<pre><code>0.0
inf</code></pre>
<p>Demek ki fonksiyon bir olasılık dağılımı olamaz, çünkü eğri altındaki alan sonsuz büyüklüğünde. Aslında bu fonksiyonun kümülatif dağılım fonksiyonu (cumulative distribution function -CDF-) özellikleri vardır, yani kendisi değil ama türevi bir olasılık fonksiyonu olarak kullanılabilir (bu konumuz dışında). Her neyse, sigmoid'in bir CDF gibi hareket ettiğini <span class="math inline">\(g\)</span>'nin 0 ile 1 arasında olmasından da anlıyoruz, sonuçta CDF alan demektir (yoğunluğun entegrali) ve en üst değeri 1 demektir, ki bu CDF tanımına uygundur.</p>
<p>Şimdi elimizde olabilecek <span class="math inline">\(k\)</span> tane değişken ve bu değişkenlerin bilinmeyen katsayıları için 0 ve 1'e eşlenecek bir regresyon oluşturalım. Diyelim ki katsayılar <span class="math inline">\(\theta_0,..,\theta_k\)</span>. Bu katsayıları değişkenler ile çarpıp toplayarak <span class="math inline">\(h(x)\)</span>'e verelim, (0/1) çıkıp çıkmayacağı katsayılara bağlı olacak, verideki etiketler ile <span class="math inline">\(h(x)\)</span> sonucu arasında bir bağlantı kurabilirsek, bu bize katsayıları verebilir. Bu modele göre eğer <span class="math inline">\(\theta\)</span>'yi ne kadar iyi seçersek, eldeki veri etiketlerine o kadar yaklaşmış olacağız. Şimdi sigmoid'i katsayılarla beraber yazalım,</p>
<p><span class="math display">\[ h_\theta(x) = g(\theta^T x) = \frac{1}{1+e^{-\theta^T x}} \]</span></p>
<p>&quot;Veriye olabildiğince yaklaşmak için en iyi <span class="math inline">\(\alpha\)</span>'yi bulmak&quot; sözü bize maksimum olurluk (maximum likelihood) hesabını hatırlatmalı. Bu hesaba göre içinde bilinmeyen <span class="math inline">\(\alpha\)</span>'yi barındıran formülün üzerinden tüm verinin sonuçlarının teker teker birbiri ile çarpımı olabildiğince büyük olmalıdır. Bu ifadeyi maksimize edecek <span class="math inline">\(\alpha\)</span> veriye en uygun <span class="math inline">\(\alpha\)</span> olacaktır.</p>
<p>Şimdi her iki etiket için ve sigmoid'i kullanarak olasılık hesaplarını yapalım,</p>
<p><span class="math display">\[ P(y=1 | x;\theta) = h_\theta(x) \]</span></p>
<p><span class="math display">\[ P(y=0 | x;\theta) = 1 - h_\theta(x) \]</span></p>
<p>Not: Olasılık değerleri (büyük <span class="math inline">\(P(\cdot)\)</span> ile) ve CDF fonksiyonları olurluk hesabında kullanılabilir. <span class="math inline">\(P(\cdot)\)</span> ile CDF bağlantısı var, <span class="math inline">\(P(X&lt;x)\)</span> gibi kümülatif alansal hesapların CDF üzerinden gerçekleştirilebildiğini hatırlayalım.</p>
<p>Devam edelim, hepsi bir arada olacak şekilde yanyana koyarsak ve sonuca, <span class="math inline">\(y\)</span>'yi doğru tahmin edip etmediğimizin ölçümünü de eklersek,</p>
<p><span class="math display">\[p(y | x;\theta) = (h_\theta(x))^y (1-h_\theta(x))^{1-y}\]</span></p>
<p>Olurluk için tüm veri noktalarını teker teker bu fonksiyona geçip sonuçlarını çarpacağız (ve verilerin birinden bağımsız olarak üretildiğini farzediyoruz), eğer <span class="math inline">\(m\)</span> tane veri noktası var ise</p>
<p><span class="math display">\[ L(\theta) = \prod_{i=1}^{m} (h_\theta(x^i))^{y^i}
(1-h_\theta(x^i))^{1-{y^i}}\]</span></p>
<p>Eğer log'unu alırsak çarpımlar toplama dönüşür, işimiz daha rahatlaşır,</p>
<p><span class="math display">\[ l(\theta) = \log L(\theta) \]</span></p>
<p><span class="math display">\[ = \sum_{i=1}^{m}
     y^i \log( (h_\theta(x^i)) ) +
     (1-{y^i}) \log( (1-h_\theta(x^i)) )
\]</span></p>
<p>İşte bu ifadenin maksimize edilmesi gerekiyor.</p>
<p>Ama daha fazla ilerlemeden önce bir eşitlik ve bir türev göstermemiz gerekiyor. Önce eşitlik</p>
<p><span class="math display">\[ 1-g(z) = g(-z) \]</span></p>
<p>İspat</p>
<p><span class="math display">\[ 1-\frac{1}{1+e^{-z}}  = \frac{1+e^{-z}-1}{1+e^{-z}}\]</span></p>
<p><span class="math display">\[ \frac{e^{-z}}{1+e^{-z}} = \frac{1}{1+e^{z}}\]</span></p>
<p>Hakikaten son eşitliğin sağ tarafına bakarsak, <span class="math inline">\(g(-z)\)</span>'yi elde ettiğimizi görüyoruz. Şimdi türeve gelelim,</p>
<p><span class="math display">\[
g&#39;(z) = \frac{d}{dz} \frac{ 1}{1+ e^{ -z}} 
\]</span></p>
<p>Ispat</p>
<p><span class="math display">\[
= \frac{1}{(1+ e^{-z})^2} (e^{-z}) 
\]</span></p>
<p><span class="math inline">\(e^{ -z}\)</span> türevinden bir eksi işareti geleceğini beklemiş olabiliriz, fakat hatırlanacağı üzere</p>
<p><span class="math display">\[\frac{d}{dx} \frac{ 1}{1+x}  = \frac{-1}{(1+x)^2}\]</span></p>
<p>Yani eksiler birbirini yoketti. Şimdi iki üstteki denklemin sağ tarafını açalım</p>
<p><span class="math display">\[
 = \frac{1}{1+e^{-z}} \frac{e^{-z}}{1+e^{-z}}
\]</span></p>
<p><span class="math display">\[
 = \frac{1}{1+e^{-z}} \frac{1}{1+e^{z}}
\]</span></p>
<p>Çarpımda iki bölüm var, bölümler <span class="math inline">\(g(z)\)</span> ve <span class="math inline">\(g(-z)\)</span> olarak temsil edilebilir, ya da <span class="math inline">\(g(z)\)</span> ve <span class="math inline">\(1-g(z)\)</span>,</p>
<p><span class="math display">\[
 = g(z)(1-g(z))
\]</span></p>
<p>Bu bağlamda ilginç bir diğer denklem log şansı (log odds) denklemidir. Eğer ilk baştaki denklemi düşünürsek,</p>
<p><span class="math display">\[ p = P(y=1|x;\theta) = g(z) = \frac{e^{z}}{1+e^{z}}  \]</span></p>
<p>Bu denklem 1 olma olasılığını hesaplıyor. Temiz bir denklem log şansı olabilir ki bu denklem olma olasılığını olmama olasılığına böler ve log alır.</p>
<p><span class="math display">\[ \log \bigg( \frac{p}{1-p} \bigg)\]</span></p>
<p>olarak gösterilir. Şimdi biraz daha cambazlık, <span class="math inline">\(1 - g(z) = g(-z)\)</span> demiştik, ve <span class="math inline">\(g(-z)\)</span>'nin de ne olduğunu biliyoruz <span class="math inline">\(\frac{1}{1+e^{z}}\)</span>, log şansını bu şekilde yazalım, <span class="math inline">\(\frac{1}{1+e^{z}}\)</span> ile bölelim daha doğrusu <span class="math inline">\(1+e^{z}\)</span> ile çarpalım ve log alalım,</p>
<p><span class="math display">\[ 
\log \bigg( \frac{e^{z}}{1+e^{z}} (1+e^{z})  \bigg) 
= \log(e^{z}) 
= z = \theta^Tx
\]</span></p>
<p>Artık olurluk denklemine dönebiliriz. Olurluğu nasıl maksimize ederiz? Gradyan çıkışı (gradient ascent) kullanılabilir. Eğer olurluk <span class="math inline">\(l(\theta)\)</span>'nin en maksimal olduğu noktadaki <span class="math inline">\(\theta\)</span>'yi bulmak istiyorsak (dikkat sadece olurluğun en maksimal noktasını aramıyoruz, o noktadaki <span class="math inline">\(\theta\)</span>'yi arıyoruz), o zaman bir <span class="math inline">\(\theta\)</span> ile başlarız, ve adım adım <span class="math inline">\(\theta\)</span>'yi maksimal olana doğru yaklaştırırız. Formül</p>
<p><span class="math display">\[ \theta_{yeni} = \theta_{eski} + \alpha \nabla_\theta l(\theta)\]</span></p>
<p>Üstteki formül niye işler? Çünkü gradyan <span class="math inline">\(\nabla_\theta l(\theta)\)</span>, yani <span class="math inline">\(l(\theta)\)</span>'nin gradyanı her zaman fonksiyon artışının en fazla olduğu yönü gösterir. Demek ki o yöne adım atmak, yani <span class="math inline">\(l(\theta)\)</span>'a verilen <span class="math inline">\(\theta\)</span>'yi o yönde değiştirmek (değişim tabii ki <span class="math inline">\(\theta\)</span> bazında, <span class="math inline">\(\theta\)</span>'nin değişimi), bizi fonksiyonun bir sonraki noktasına yaklaştıracaktır. Sabit <span class="math inline">\(\alpha\)</span> bir tek sayı sadece, atılan adımın (hangi yönde olursa olsun) ölçeğini azaltıp / arttırabilmek için dışarıdan eklenir. Adım yönü vektör, bu sabit bir tek sayı. Çarpımları vektörü azaltır ya da çoğaltır.</p>
<p>Not: Bu şekilde azar azar sonuca yaklaşmaya uğraşmak tabii ki her fonksiyon için geçerli değildir, çünkü eğer fonksiyonda &quot;yerel maksimumlar&quot; var ise, gradyan çıkışı bu noktalarda takılıp kalabilir (o yerel tepelerde de birinci türev sıfırlanır, gradyanın kafası karışır). Gradyan metotunun kullanmadan önce fonksiyonumuzun tek (global) bir maksimumu olup olmadığını düşünmemiz gerekir. Fakat şanlıyız ki olurluk fonksiyonu tam da böyle bir fonksiyondur (şans değil tabii, bu özelliği sebebiyle seçildi). Fonksiyon içbükeydir (concave), yani tek bir tepe noktası vardır. Bir soru daha: olurluğun içbükey olduğunu nasıl anladık? Fonksiyona bakarak pat diye bunu söylemek mümkün, değişkenlerde polinom bağlamında küpsel ve daha üstü seviyesinde üstellik yok, ayrıca <span class="math inline">\(\log\)</span>, <span class="math inline">\(\exp\)</span> içbükeyliği bozmuyor.</p>
<p>Simdi <span class="math inline">\(\nabla_\theta l(\theta)\)</span> turetmemiz gerekiyor.</p>
<p>Eğer tek bir <span class="math inline">\(\frac{\partial l(\theta)}{\partial \theta_j}\)</span>'yi hesaplarsak ve bunu her <span class="math inline">\(j\)</span> için yaparsak, bu sonuçları bir vektörde üstüste koyunca <span class="math inline">\(\nabla_\theta l(\theta)\)</span>'yi elde ederiz.</p>
<p><span class="math display">\[ 
\frac{\partial l(\theta)}{\partial \theta_j} =
y \frac{\frac{\partial }{\partial \theta_j}g(\theta^Tx) }{g(\theta^Tx)}  -
(1-y) \frac{\frac{\partial }{\partial \theta_j}g(\theta^Tx) }{1-g(\theta^Tx)} 
\]</span></p>
<p><span class="math display">\[ 
= \big(
y  \frac{1}{g(\theta^Tx)}  -
(1-y) 
\frac{1}{1-g(\theta^Tx)} 
\big)
\frac{\partial }{\partial \theta_j}g(\theta^Tx)
\]</span></p>
<p>Şimdi en sağdaki kısmı açalım,</p>
<p><span class="math display">\[ 
\frac{\partial }{\partial \theta_j}g(\theta^Tx) 
= g&#39;(\theta^Tx) \frac{\partial }{\partial \theta_j} \theta^Tx 
= g&#39;(\theta^Tx) x_j 
\]</span></p>
<p><span class="math inline">\(\frac{\partial }{\partial \theta_j} \theta^Tx\)</span> nasıl <span class="math inline">\(x_j\)</span> haline geldi? Çünkü tüm <span class="math inline">\(\theta\)</span> vektörünün kısmi türevini alıyoruz fakat o kısmi türev sadece tek bir <span class="math inline">\(\theta_j\)</span> için, o zaman vektördeki diğer tüm öğeler sıfır olacaktır, sadece <span class="math inline">\(\theta_j\)</span> 1 olacak, ona tekabül eden <span class="math inline">\(x\)</span> öğesi, yani <span class="math inline">\(x_j\)</span> ayakta kalabilecek, diğer <span class="math inline">\(x\)</span> öğelerinin hepsi sıfırla çarpılmış olacak.</p>
<p>Türevin kendisinden de kurtulabiliriz şimdi, daha önce gösterdiğimiz eşitliği devreye sokalım,</p>
<p><span class="math display">\[ 
= g(\theta^Tx)(1-g(\theta^Tx)) x_j 
\]</span></p>
<p>Bu son formülü 3 üstteki formülün sağ tarafına geri koyarsak, ve basitleştirirsek,</p>
<p><span class="math display">\[
\big(
y(1-g(\theta^Tx)) - (1-y)g(\theta^T x)
\big) x_j
\]</span></p>
<p>Çarpımı daha temiz görmek için sadece <span class="math inline">\(y,g\)</span> harflerini kullanırsak,</p>
<p><span class="math display">\[
\big(y(1-g) - (1-y)g \big) x_j =
(y - yg - g + yg)x_j = (y - g)x_j
\]</span></p>
<p>yani</p>
<p><span class="math display">\[
= (y - g(\theta^Tx))x_j
\]</span></p>
<p><span class="math display">\[
= (y - h_\theta(x))x_j
\]</span></p>
<p>İşte <span class="math inline">\(\nabla_\theta l(\theta)\)</span> için ne kullanacağımızı bulduk. O zaman</p>
<p><span class="math display">\[ \theta_{yeni} = \theta_{eski} + \alpha (y - h_\theta(x))x_j \]</span></p>
<p>Her <span class="math inline">\(i\)</span> veri noktası için</p>
<p><span class="math display">\[ \theta_{yeni} = \theta_{eski} + \alpha (y^{i} - h_\theta(x^{i}))x^{i}_j \]</span></p>
<p>Kodu işletelim,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> grad_ascent(data_mat, label_mat):
    m,n <span class="op">=</span> data_mat.shape
    label_mat<span class="op">=</span>label_mat.reshape((m,<span class="dv">1</span>))
    alpha <span class="op">=</span> <span class="fl">0.001</span>
    <span class="bu">iter</span> <span class="op">=</span> <span class="dv">500</span>
    theta <span class="op">=</span> ones((n,<span class="dv">1</span>))
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">iter</span>):   
        h <span class="op">=</span> sigmoid(dot(data_mat,theta))
        error <span class="op">=</span> label_mat <span class="op">-</span> h
        theta <span class="op">=</span> theta <span class="op">+</span> alpha <span class="op">*</span> dot(data_mat.T,error) 
    <span class="cf">return</span> theta

theta <span class="op">=</span> np.array(grad_ascent(array(data),array(labels).T ))
<span class="bu">print</span> theta.T</code></pre></div>
<pre><code>[[ 4.12414349  0.48007329 -0.6168482 ]]</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> plot_theta(theta):
     x <span class="op">=</span> np.array(arange(<span class="op">-</span><span class="fl">3.0</span>, <span class="fl">3.0</span>, <span class="fl">0.1</span>))
     y <span class="op">=</span> np.array((<span class="op">-</span>theta[<span class="dv">0</span>]<span class="op">-</span>theta[<span class="dv">1</span>]<span class="op">*</span>x)<span class="op">/</span>theta[<span class="dv">2</span>])
     plt.plot(x, y)
     plt.hold(<span class="va">True</span>)
     class0 <span class="op">=</span> data[labels<span class="op">==</span><span class="dv">0</span>]
     class1 <span class="op">=</span> data[labels<span class="op">==</span><span class="dv">1</span>]
     plt.plot(class0[<span class="st">&#39;x&#39;</span>],class0[<span class="st">&#39;y&#39;</span>],<span class="st">&#39;b.&#39;</span>)
     plt.hold(<span class="va">True</span>)
     plt.plot(class1[<span class="st">&#39;x&#39;</span>],class1[<span class="st">&#39;y&#39;</span>],<span class="st">&#39;r.&#39;</span>)
     plt.hold(<span class="va">True</span>)

plot_theta(theta)
plt.savefig(<span class="st">&#39;stat_logit_03.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_03.png" />

</div>
<p>Üstteki kod bir döngü içinde belli bir <span class="math inline">\(x\)</span> noktasından başlayarak gradyan inişi yaptı ve optimal <span class="math inline">\(\theta\)</span> değerlerini, yani regresyon ağırlıklarını (weights) hesapladı. Sonra bu ağırlıkları bir ayraç olarak üstte grafikledi. Ayracın oldukça iyi değerler bulduğu belli oluyor.</p>
<p>Rasgele Gradyan Çıkışı (Stochastic Gradient Ascent)</p>
<p>Acaba <span class="math inline">\(\theta\)</span>'yi güncellerken daha az veri kullanmak mümkün mü? Yani yön hesabı için sürekli tüm veriyi kullanmasak olmaz mı?</p>
<p>Olabilir. Güncellemeyi sadece tek bir veri noktası kullanarak yapabiliriz. Yine gradyanı değiştirmiş oluruz, sadece azar azar değişim olur, fakat belki de bu şekilde sonuca daha çabuk ulaşmak mümkün olacaktır.</p>
<p>Kodlama açısından, <span class="math inline">\(\theta\)</span> güncellemesi için bulduğumuz formülü tek nokta bazında da vermiştik. O zaman o tek noktayı sırayla alıp güncellersek, otomatik olarak yeni bir şekilde gradyan çıkışı yapmış oluruz.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> stoc_grad_ascent0(data_mat, label_mat):
    m,n <span class="op">=</span> data_mat.shape
    label_mat<span class="op">=</span>label_mat.reshape((m,<span class="dv">1</span>))
    alpha <span class="op">=</span> <span class="fl">0.01</span>
    theta <span class="op">=</span> ones((n,<span class="dv">1</span>))
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):
        h <span class="op">=</span> sigmoid(<span class="bu">sum</span>(dot(data_mat[i],theta)))
        error <span class="op">=</span> label_mat[i] <span class="op">-</span> h
        theta <span class="op">=</span> theta <span class="op">+</span> alpha <span class="op">*</span> data_mat[i].reshape((n,<span class="dv">1</span>)) <span class="op">*</span> error
        theta <span class="op">=</span> theta.reshape((n,<span class="dv">1</span>))
    <span class="cf">return</span> theta

theta <span class="op">=</span> np.array(stoc_grad_ascent0(array(data),array(labels).T ))
<span class="bu">print</span> theta.T</code></pre></div>
<pre><code>[[ 1.01702007  0.85914348 -0.36579921]]</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plot_theta(theta)
plt.savefig(<span class="st">&#39;stat_logit_04.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_04.png" />

</div>
<p>Neredeyse işimiz tamamlandı. Üstteki grafik pek iyi bir ayraç göstermedi. Niye? Problem çok fazla salınım (oscillation) var, yani değerler çok fazla uç noktalar arasında gidip geliyor. Ayrıca veri noktalarını sırayla işliyoruz, veri tabii ki rasgele bir şekilde sıralanmış olabilir, ama sıralanmamışsa, o zaman algoritmaya raslantısal noktaları vermek için kod içinde zar atmamız lazım. Metotun ismi &quot;rasgele (stochastic)&quot; gradyan çıkışı, bu rasgelelik önemli. 2. problemi düzeltmek için yapılacak belli, 1. problem için <span class="math inline">\(\alpha\)</span> değeri her döngüde belli oranda küçültülerek (yani <span class="math inline">\(\alpha\)</span> artık sabit değil) sonuca yaklaşırken oradan buraya savrulmasını engellemiş olacağız. Yeni kod altta,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> stoc_grad_ascent1(data_mat, label_mat):
    m,n <span class="op">=</span> data_mat.shape
    <span class="bu">iter</span> <span class="op">=</span> <span class="dv">150</span>
    label_mat<span class="op">=</span>label_mat.reshape((m,<span class="dv">1</span>))
    alpha <span class="op">=</span> <span class="fl">0.01</span>
    theta <span class="op">=</span> ones((n,<span class="dv">1</span>))
    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">iter</span>):
        data_index <span class="op">=</span> <span class="bu">range</span>(m)
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):
            alpha <span class="op">=</span> <span class="dv">4</span><span class="op">/</span>(<span class="fl">1.0</span><span class="op">+</span>j<span class="op">+</span>i)<span class="op">+</span><span class="fl">0.0001</span>  
            rand_index <span class="op">=</span> <span class="bu">int</span>(random.uniform(<span class="dv">0</span>,<span class="bu">len</span>(data_index)))
        h <span class="op">=</span> sigmoid(<span class="bu">sum</span>(dot(data_mat[rand_index],theta)))
            error <span class="op">=</span> label_mat[rand_index] <span class="op">-</span> h
            theta <span class="op">=</span> theta <span class="op">+</span> alpha <span class="op">*</span> data_mat[rand_index].reshape((n,<span class="dv">1</span>)) <span class="op">*</span> error
            theta <span class="op">=</span> theta.reshape((n,<span class="dv">1</span>))
    <span class="cf">return</span> theta

theta <span class="op">=</span> np.array(stoc_grad_ascent1(array(data),array(labels).T ))
<span class="bu">print</span> theta.T</code></pre></div>
<pre><code>[[ 14.67440542   1.30317067  -2.08702677]]</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plot_theta(theta)
plt.savefig(<span class="st">&#39;stat_logit_05.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_05.png" />

</div>
<p>Sonuç çok iyi, ayrıca daha az işlemle bu noktaya eriştik, yani daha az işlem ve daha hızlı bir şekilde sonuca ulaşmış olduk.</p>
<p>Tahmin (Prediction)</p>
<p>Elde edilen ağırlıkları tahmin için nasıl kullanırız? Bu ağırlıkları alıp, yeni veri noktası ile çarpıp sonuçları sigmoid'den geçirdiğimiz zaman bu noktanın &quot;1 etiketi olma olasılığını&quot; hesaplamış olacağız. Örnek (diyelim ki mevcut veri noktası içinden bir veriyi, -mesela 15. nokta- sanki yeniymiş gibi seçtik)</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">pt <span class="op">=</span> df.ix[<span class="dv">15</span>,[<span class="st">&#39;intercept&#39;</span>,<span class="st">&#39;x&#39;</span>,<span class="st">&#39;y&#39;</span>]]
<span class="bu">print</span> sigmoid(dot(array(pt), theta)), 
<span class="bu">print</span> <span class="st">&#39;label =&#39;</span>,labels[<span class="dv">15</span>]</code></pre></div>
<pre><code>[ 0.99999653] label = 1</code></pre>
<p>Oldukça yüksek bir olasılık çıktı, ve hakikaten de o noktanın gerçek değeri 1 imiş.</p>
<p>Logit</p>
<p>İstatistik kaynaklarında genellikle &quot;logit'' adlı bir regresyon türünden bahsedildiğini görebilirsiniz, burada aslında lojistik regresyondan bahsediliyor, ki bu konuyu [10] yazısında bulabiliriz. Ama istatistik literatüründe (yapay öğrenim literatüründen farklı olarak), terminoloji biraz kafa karıştırıcı olabiliyor. Lojistik regresyon yazısında odağımız sigmoid fonksiyonuydu, peki logit nereden geliyor? Logit,</p>
<p><span class="math display">\[ logit(x) = \log ( x/(1-x) ) \]</span></p>
<p>fonksiyonudur, ve bu fonksiyon (0,1) arasındaki bir sayıyı <span class="math inline">\(-\infty,\infty\)</span> arasına eşler (map). Fonksiyona verilen <span class="math inline">\(x\)</span> bir olasılık, ve bu olasılık, bir olayın (event) olma / olmama <em>oranlarının</em> <span class="math inline">\(\log\)</span>'una dönüşüyor. Ki bu fonksiyona &quot;log ihtimali (log odds)'' ismi de veriliyor. Hatırlamanın kolay bir yolu belki de logit, &quot;bir şeyi logla'' çağrışımı yapıyor, sonra &quot;neyi logluyoruz?'' diye düşünürüz, cevap bir olasılığı, daha detaylı olarak olma / olmama oranını.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> logit(p): <span class="cf">return</span> np.log(p<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>p))
p <span class="op">=</span> <span class="fl">0.1</span><span class="op">;</span> <span class="bu">print</span> logit(p)
p <span class="op">=</span> <span class="fl">0.5</span><span class="op">;</span> <span class="bu">print</span> logit(p)
p <span class="op">=</span> <span class="fl">0.7</span><span class="op">;</span> <span class="bu">print</span> logit(p)
p <span class="op">=</span> <span class="fl">0.99999</span><span class="op">;</span> <span class="bu">print</span> logit(p)</code></pre></div>
<pre><code>-2.19722457734
0.0
0.847297860387
11.5129154649</code></pre>
<p>Sigmoid bunun tam tersidir, <span class="math inline">\(-\infty,\infty\)</span> arasındaki bir değeri (0,1) arasına eşler, ki lojistik regresyon katsayılarından bir olasılık üretmek istiyorsak, sigmoid lazım. Bu ters gidişi ispatlamak kolay, ki bu &quot;ters yönde'' harekete <span class="math inline">\(logit^{-1}\)</span> ismi de veriliyor, ters yöne doğru gidelim,</p>
<p><span class="math display">\[ 
\qquad (1)
logit(p) = \log ( \frac{p}{1-p} ) = x
\]</span></p>
<p><span class="math display">\[ 
\Rightarrow  \frac{p}{1-p} = \exp(x) 
\Rightarrow \frac{1}{e^x} = \frac{1}{p}-1
\]</span></p>
<p><span class="math display">\[ 
\Rightarrow \frac{1}{e^x} + 1 = \frac{1}{p}
\Rightarrow \frac{1+e^x}{e^x} =  \frac{1}{p}
\Rightarrow p = \frac{e^x}{1+e^x}
\]</span></p>
<p>Lojistik regresyon modeli</p>
<p><span class="math display">\[ Pr(y_i=1) = logit^{-1}(X_i\beta) \]</span></p>
<p>ki her <span class="math inline">\(X_i\)</span> bir vektördür, veri noktalarımız <span class="math inline">\(X_i,y_i\)</span> olarak eşli olarak gelir. Diyelim ki elimizde 1992'de ABD seçimlerinde oy vermiş insanların gelir seviyesi (income) ve kime oy (vote) verdikleri var. Bush'a verilmiş oyu 1 verilmemişi 0 olarak işaretlersek, bu problemi lojistik regresyon problemine çevirebiliriz,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
df <span class="op">=</span> pd.read_csv(<span class="st">&#39;nes.dat&#39;</span>,sep<span class="op">=</span><span class="vs">r&#39;\s+&#39;</span>)
df <span class="op">=</span> df[[<span class="st">&#39;presvote&#39;</span>,<span class="st">&#39;year&#39;</span>,<span class="st">&#39;female&#39;</span>,<span class="st">&#39;income&#39;</span>,<span class="st">&#39;black&#39;</span>]]
df <span class="op">=</span> df[df[<span class="st">&#39;presvote&#39;</span>] <span class="op">&lt;</span> <span class="dv">3</span>] <span class="co"># sadece 2 partinin oylarini al</span>
df <span class="op">=</span> df.dropna()
<span class="co"># 1,2 oylari 1,0 yap, Cumhuriyetciye verildi mi evet/hayir </span>
<span class="co"># haline getir</span>
df[<span class="st">&#39;presvote2&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;presvote&#39;</span>].<span class="bu">map</span>(<span class="kw">lambda</span> x: x<span class="dv">-1</span>) 
df <span class="op">=</span> df.drop(<span class="st">&#39;presvote&#39;</span>,axis<span class="op">=</span><span class="dv">1</span>)
df2 <span class="op">=</span> df[df[<span class="st">&#39;year&#39;</span>] <span class="op">==</span> <span class="dv">1992</span>]
<span class="bu">print</span> df2[:<span class="dv">4</span>]</code></pre></div>
<pre><code>       year  female  income  black  presvote2
32093  1992       1       4      0          1
32094  1992       1       2      0          1
32096  1992       1       1      1          0
32097  1992       0       2      0          1</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> statsmodels.api <span class="im">as</span> sm
<span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf
mdlm <span class="op">=</span> smf.logit(<span class="st">&quot;presvote2 ~ income&quot;</span>, df2)
mdlmf <span class="op">=</span> mdlm.fit()
<span class="bu">print</span>(mdlmf.summary())</code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 0.661553
         Iterations 5
                           Logit Regression Results                           
==============================================================================
Dep. Variable:              presvote2   No. Observations:                 1207
Model:                          Logit   Df Residuals:                     1205
Method:                           MLE   Df Model:                            1
Date:                Mon, 23 Feb 2015   Pseudo R-squ.:                 0.02134
Time:                        09:01:08   Log-Likelihood:                -798.49
converged:                       True   LL-Null:                       -815.91
                                        LLR p-value:                 3.598e-09
==============================================================================
                 coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept     -1.3863      0.187     -7.400      0.000        -1.754    -1.019
income         0.3245      0.056      5.775      0.000         0.214     0.435
==============================================================================</code></pre>
<p>En basit haliyle bu denklem,</p>
<p><span class="math display">\[ 
\qquad (2)
Pr(y_i=1) = logit^{-1}(\alpha + \beta x)  \]</span></p>
<p>Üstteki katsayı değerlerini baz alarak,</p>
<p><span class="math display">\[ Pr(y_i = 1) = logit^{-1}(-1.38 + 0.32 \cdot income) \]</span></p>
<p>Katsayıları irdelemenin iyi bir yolu şudur. Logit tersinden kurtulmak istiyorsak, (1)'e deki formülü (2)'ye uygularız, iki tarafın logit'ini alırız, sağ taraftaki ters logit kaybolur,</p>
<p><span class="math display">\[ \log \bigg( \frac{Pr(y=1)}{Pr(y=0)} \bigg) = \alpha + \beta x\]</span></p>
<p>Bu formülün sağ tarafına göre, <span class="math inline">\(x\)</span>'e 1 eklemek demek, <span class="math inline">\(\beta (1+x) = \beta x + \beta \cdot 1 = \beta x + \beta\)</span>, formüle bir bakıma <span class="math inline">\(\beta\)</span> eklemek demektir. Bunu bir tarafa koyalım, şimdi üstteki formülün iki tarafının <span class="math inline">\(\exp\)</span>'sini alalım,</p>
<p><span class="math display">\[ \frac{Pr(y=1)}{Pr(y=0)} = \exp(\alpha + \beta x)\]</span></p>
<p>Şimdi <span class="math inline">\(\exp\)</span> içindeki <span class="math inline">\(x\)</span>'e 1 eklersek, <span class="math inline">\(\exp(\alpha + \beta x + \beta)\)</span> olur, bu da <span class="math inline">\(\exp(\alpha)\exp(\beta x ) \exp (\beta)\)</span> demektir, o zaman iki tarafı da dengelemek için her iki tarafı da <span class="math inline">\(\exp(\beta)\)</span> ile çarpmak lazım,</p>
<p><span class="math display">\[ 
\frac{Pr(y=1)}{Pr(y=0)}\exp(\beta) = \exp(\alpha + \beta x + \beta) = 
\exp(\alpha)\exp(\beta x ) \exp (\beta)
\]</span></p>
<p>O zaman, <span class="math inline">\(x\)</span>'deki bir birimlik değişimin, olma / olmama oranı olan ihtimal (odds) üzerindeki etkisini hesaplamak istiyorsak, incelediğimiz değişkenin katsayısını alıp mesela <span class="math inline">\(\beta = 0.3\)</span>, onun <span class="math inline">\(\exp\)</span>'sini hesaplarız, <span class="math inline">\(\exp(0.32) = 1.37\)</span>, ve bu değerin üstteki eşitliğin sol tarafını da çarpacağı bilgisinden hareketle, ihtimalin o kadar artacak olduğunu rapor edebiliriz. Yani <span class="math inline">\(\beta = 0.3\)</span> için bu artış 1.37 katıdır. Yani gelirde 1 birimlik bir artış (ki gelir veride 1,2,3,4 gibi sayısal aralıklar olarak gösterilmiş) Bush'a oy verme şansının 1.37 kat arttırıyor. Bu aslında mantıklı, ABD'de Cumhuriyetçiler &quot;zenginlerin partisi'' olarak biliniyor.</p>
<p>İhtimal oranları (odds ratio) ile düşünmek için bazı örnekler, ihtimal oranı 1, olasılıkların 0.5 olması demektir, yani her iki ihtimal de eşit ağırlıktadır. İhtimal oranı 0.5, ya da 2.0 (1/3,2/3) olasılıklarına tekabül eder, formülü hatırlayalım, <span class="math inline">\(p / (1-p)\)</span>.</p>
<p>Katsayıları İrdelemek</p>
<p>Eğer bir katsayı değerinin sıfırdan uzaklığı Std. Hatanın (Error) iki katından fazla ise katsayı istatistiki olarak anlamlı / değerli (significant) demektir ve kullanılabilir. Tabii burada biraz daha ek irdeleme gerekebilir; mesela kişilerin arabalarının beygir gücünü kazandıkları maaşa bağlayan bir regresyon, beygir gücü katsayısı için beygir başına 10 Eur ve std. hata 2 Eur vermişse bu istatistiki olarak önemli, ama pratikte önemsizdir. Benzer şekilde eğer beygir katsayısı için 10,000 Eur ve std. hata 10,000 Eur bulmuşsak, bu istatistiki olarak önemsiz, ama pratikte önemlidir.</p>
<p>İlginç Durum</p>
<p>Siyahi oyların bazı yıllara göre analizini yapalım,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> <span class="st">&#39;coefs&#39;</span>,<span class="st">&#39;error&#39;</span>

df2 <span class="op">=</span> df[df[<span class="st">&#39;year&#39;</span>] <span class="op">==</span> <span class="dv">1960</span>]
mdlm <span class="op">=</span> smf.logit(<span class="st">&quot;presvote2 ~ female + black + income&quot;</span>, df2)
mdlmf <span class="op">=</span> mdlm.fit()
<span class="bu">print</span> np.vstack((mdlmf.params.index, mdlmf.params,mdlmf.bse)).T

df2 <span class="op">=</span> df[df[<span class="st">&#39;year&#39;</span>] <span class="op">==</span> <span class="dv">1964</span>]
mdlm <span class="op">=</span> smf.logit(<span class="st">&quot;presvote2 ~ female + black + income&quot;</span>, df2)
mdlmf <span class="op">=</span> mdlm.fit()
<span class="bu">print</span> np.vstack((mdlmf.params.index, mdlmf.params,mdlmf.bse)).T

df2 <span class="op">=</span> df[df[<span class="st">&#39;year&#39;</span>] <span class="op">==</span> <span class="dv">1968</span>]
mdlm <span class="op">=</span> smf.logit(<span class="st">&quot;presvote2 ~ female + black + income&quot;</span>, df2)
mdlmf <span class="op">=</span> mdlm.fit()
<span class="bu">print</span> np.vstack((mdlmf.params.index, mdlmf.params,mdlmf.bse)).T</code></pre></div>
<pre><code>coefs error
Optimization terminated successfully.
         Current function value: 0.685646
         Iterations 5
[[&#39;Intercept&#39; -0.15937090803207216 0.22525976274228318]
 [&#39;female&#39; 0.23863850517270727 0.1365775569712597]
 [&#39;black&#39; -1.0585625868981525 0.3621668012097297]
 [&#39;income&#39; 0.03122275696614234 0.06237925936065817]]
Warning: Maximum number of iterations has been exceeded.
         Current function value: 0.590399
         Iterations: 35
[[&#39;Intercept&#39; -1.1551333142403977 0.21592167898447412]
 [&#39;female&#39; -0.07918311120690241 0.1361066805886836]
 [&#39;black&#39; -26.62869325566435 93069.88953763059]
 [&#39;income&#39; 0.190103316020662 0.05839253555236441]]
Optimization terminated successfully.
         Current function value: 0.626797
         Iterations 7
[[&#39;Intercept&#39; 0.47889431087596257 0.24427421556953816]
 [&#39;female&#39; -0.03135633331713884 0.1481293019619361]
 [&#39;black&#39; -3.6417024852622455 0.5946042228547078]
 [&#39;income&#39; -0.02613777851523365 0.06740777911367761]]</code></pre>
<p>1964 yılında siyahi (black) seçmenlerin oylarına ne oldu? Üstteki analizde katsayı müthiş alçak (büyük negatif değer), ve standard hata çok büyük. Eğer o sene için veriye bakarsak neler olduğunu anlıyoruz; elimizdeki anket verisindeki kişilerden siyahi olan kimse 1964 yılında Cumhuriyetçilere oy vermemiş. Bu durumda katsayı tabii ki büyük negatif değer (çünkü regresyon hedefimiz Cumhuriyetçilere oy verilip verilmeyeceği), siyahi olmak ile Cumhuriyetçilere oy vermek arasında negatif bir korelasyon ortaya çıkmış oluyor. Büyük standart hata iyi durmuyor tabii, ama bunun sebebi özyineli (iteratif) model uyduran (fitting) algoritmanın bir nüansıdır. Daha ufak bir değer elde etmek için [3]'de görülen numaralar yapılabilir, fakat pratikte bu büyük değeri görünce analizi yapan kişi veriye bakacak, ve neler olduğunu anlayacaktır.</p>
<p>Bangladeş'te su kuyusu değişiminin lojistik modeli</p>
<p>Verimizde 3,000 haneye gidilerek anketle toplanmış veri var. Veride hanelerin yakınlarındaki kuyudaki arsenik seviyesi toplanmış, ve paylaşılan verideki tüm hanelerin kuyular sağlıksız seviyede arsenik içeriyor. Verideki diğer bilgiler en yakındaki &quot;sağlıklı&quot; bir kuyuya yakınlık, ve o hanenin bu sağlıklı su kuyusuna (bir sene sonra yapılan kontrole göre) geçip geçmediği. Ayrıca hanede fikri sorulan kişinin eğitim seviyesi ve bu hanedeki kişilerin herhangi bir sosyal topluluğa (community assocation) ait olup olmadıkları.</p>
<p>Amacımız su kuyusunun değişimini modellemek. Bu eylem olup / olmama bağlamında evet / hayır şeklinde bir değişken olduğu için ikili (binary) olarak temsil edilebilir ve ikili cevaplar / sonuçlar lojistik regresyon ile modellenebilirler.</p>
<p>Veriye bakalım.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> pandas <span class="im">import</span> <span class="op">*</span>
<span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit
df <span class="op">=</span> read_csv(<span class="st">&#39;wells.dat&#39;</span>, sep <span class="op">=</span> <span class="st">&#39; &#39;</span>, header <span class="op">=</span> <span class="dv">0</span>, index_col <span class="op">=</span> <span class="dv">0</span>)
<span class="bu">print</span> df.head()</code></pre></div>
<pre><code>   switch  arsenic       dist  assoc  educ
1       1     2.36  16.826000      0     0
2       1     0.71  47.321999      0     0
3       0     2.07  20.966999      0    10
4       1     1.15  21.486000      0    12
5       1     1.10  40.874001      1    14</code></pre>
<p>Model 1: Güvenli su kuyusuna uzaklık</p>
<p>İlk önce modelde kuyu uzaklığını kullanalım.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">model1 <span class="op">=</span> logit(<span class="st">&quot;switch ~ dist&quot;</span>, df).fit()
<span class="bu">print</span> model1.summary()</code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 0.674874
         Iterations 4
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                 switch   No. Observations:                 3020
Model:                          Logit   Df Residuals:                     3018
Method:                           MLE   Df Model:                            1
Date:                Wed, 20 May 2015   Pseudo R-squ.:                 0.01017
Time:                        21:25:34   Log-Likelihood:                -2038.1
converged:                       True   LL-Null:                       -2059.0
                                        LLR p-value:                 9.798e-11
==============================================================================
                 coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept      0.6060      0.060     10.047      0.000         0.488     0.724
dist          -0.0062      0.001     -6.383      0.000        -0.008    -0.004
==============================================================================</code></pre>
<p>Uzaklık (dist) için elde edilen katsayı -0.0062, fakat bu sayı kafa karıştırıcı olabilir çünkü uzaklık metre olarak ölçülür, o zaman bu katsayı mesela 90 metre ile 91 metre uzaklığın değişime olan etkisini ölçmektedir, kısacası pek faydalı değildir. Yani uzaklık metre ile ölçüldüğü için 1 metrenin modeldeki etkisi ufak, o yüzden bu ölçütü ölçeklersek (scale) belki regresyon katsayılarımız daha net çıkar.</p>
<p>Bunu nasıl yapacağız? Ölçeklenmiş yeni bir değişken yaratmak yerine, onu formülün içinde tanımlayabiliriz. Burada bir ara not: eğer formül içinde +,- gibi operasyonları aritmetik işlem olarak kullanmak istiyorsak, o zaman 'I()' çağrısını yapmak lazım, çünkü + operasyonu mesela statsmodels formüllerinde başka amaçlar için kullanılıyor. 'I' harfi birim (identity) kelimesinden geliyor, yani hiçbir şeyin değişmediğini anlatmaya uğraşıyoruz, &quot;içinde ne varsa onu ver&quot; diyoruz .</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">model1 <span class="op">=</span> logit(<span class="st">&#39;switch ~I(dist/100.)&#39;</span>, df).fit()
<span class="bu">print</span> model1.summary()</code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 0.674874
         Iterations 4
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                 switch   No. Observations:                 3020
Model:                          Logit   Df Residuals:                     3018
Method:                           MLE   Df Model:                            1
Date:                Wed, 20 May 2015   Pseudo R-squ.:                 0.01017
Time:                        21:25:40   Log-Likelihood:                -2038.1
converged:                       True   LL-Null:                       -2059.0
                                        LLR p-value:                 9.798e-11
==================================================================================
                     coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------
Intercept          0.6060      0.060     10.047      0.000         0.488     0.724
I(dist / 100.)    -0.6219      0.097     -6.383      0.000        -0.813    -0.431
==================================================================================</code></pre>
<p>Şimdi modelimizi grafikleyelim. Yanlız değişim (switch) verisini suni olarak kaydırmamız / seğirtmemiz (jitter) gerekiyor, çünkü değişim 0 ve 1'den başka bir şey olamaz ve grafik sürekli aynı iki bölgeye nokta basıp duracak.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> binary_jitter(x, jitter_amount <span class="op">=</span> <span class="fl">.05</span>):
    <span class="co">&#39;&#39;&#39;</span>
<span class="co">    0/1 vektoru iceren veriye segirtme ekle</span>
<span class="co">    &#39;&#39;&#39;</span>
    jitters <span class="op">=</span> np.random.rand(<span class="op">*</span>x.shape) <span class="op">*</span> jitter_amount
    x_jittered <span class="op">=</span> x <span class="op">+</span> np.where(x <span class="op">==</span> <span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">1</span>) <span class="op">*</span> jitters
    <span class="cf">return</span> x_jittered

plt.plot(df[<span class="st">&#39;dist&#39;</span>], binary_jitter(df[<span class="st">&#39;switch&#39;</span>], <span class="fl">.1</span>), <span class="st">&#39;.&#39;</span>, alpha <span class="op">=</span> <span class="fl">.1</span>)
plt.plot(np.sort(df[<span class="st">&#39;dist&#39;</span>]), model1.predict()[np.argsort(df[<span class="st">&#39;dist&#39;</span>])], lw <span class="op">=</span> <span class="dv">2</span>)
plt.ylabel(<span class="st">&#39;Switched Wells&#39;</span>)
plt.xlabel(<span class="st">&#39;Distance from safe well (meters)&#39;</span>)
plt.savefig(<span class="st">&#39;stat_logit_06.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_06.png" />

</div>
<p>Mavi noktalar gerçek veri, yeşil çizgi ise uzaklık geçilerek modelin oluşturduğu &quot;tahmin&quot;. Modelin gerçek veriye ne kadar uyduğunu görüyoruz böylece, yeşil çizginin yüksek olasılık verdiği bölgelerde üst kısmın daha mavi olmasını bekleriz mesela. Üstteki resimde aşağı yukarı bunu gösteriyor.</p>
<p>Bir problemin grafiklemesine başka bir yönden yaklaşalım, kuyu değiştirenlerin değişim uzaklığının yoğunluğu, bir de kuyu değiştirmeyenlerin değişim uzaklığının yoğunluğu. Değişimi yapanların dağılımına bakınca, kısa mesafelerde daha fazla yoğunluk görmeyi bekliyoruz, değiştirmeyenlerin ise uzun mesafelerde daha fazla yoğunluğu olur herhalde.</p>
<p>Yoğunluğu göstermek için çekirdek yoğunluk hesabı (kernel density estimation) tekniğini kullanıyoruz. Bu teknik her veri noktasına Gaussian, kutu (box), ya da diğer türden bir &quot;çekirdek&quot; fonksiyonunu koyar (ve veriyi o fonksiyona geçer, sonucu kaydeder), ve bu iş bitince tüm çekirdekler üst üste toplanarak genel dağılım ortaya çıkartılır. Teknik histogram tekniğiyle aynı işi yapmaya uğraşır, bir anlamda verinin dağılımını daha pürüzsüz (smooth) hale getirir.</p>
<p>Bu teknik istatistikte oldukça yeni bir teknik sayılır, kullanılması için bilgisayar hesabı gerekiyor (kıyasla histogram elle de yapılabilir), yeni hesapsal tekniklerde olan ilerlemelerin veri analizine getirdiği bir yenilik yani!</p>
<p>[KDE bölümü atlandı]</p>
<p>Model 2: Güvenli kuyuya olan uzaklık ve kendi kuyusunun arsenik seviyesi</p>
<p>Şimdi arsenik seviyesini modelimize ekleyelim. Bekleriz ki kuyusunda yüksek arsenik miktarı olan kimselerin kuyu değiştirmesi daha çok beklenen bir şeydir.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">model2 <span class="op">=</span> logit(<span class="st">&#39;switch ~ I(dist / 100.) + arsenic&#39;</span>, df).fit()
<span class="bu">print</span> model2.summary()</code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 0.650773
         Iterations 5
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                 switch   No. Observations:                 3020
Model:                          Logit   Df Residuals:                     3017
Method:                           MLE   Df Model:                            2
Date:                Wed, 20 May 2015   Pseudo R-squ.:                 0.04551
Time:                        21:25:48   Log-Likelihood:                -1965.3
converged:                       True   LL-Null:                       -2059.0
                                        LLR p-value:                 1.995e-41
==================================================================================
                     coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------
Intercept          0.0027      0.079      0.035      0.972        -0.153     0.158
I(dist / 100.)    -0.8966      0.104     -8.593      0.000        -1.101    -0.692
arsenic            0.4608      0.041     11.134      0.000         0.380     0.542
==================================================================================</code></pre>
<p>Ki katsayılar da aynen bunu gösteriyor. Güvenli kuyuya olan uzaklık büyüdükçe değişime negatif etki yapıyor ama kendi kuyusundaki arsenik seviyesinin artması değişimde pozitif etki yapıyor.</p>
<p>Kısmi (marginal) etkiler</p>
<p>Tüm bu değişkenlerin değişim olasılığı üzerindeki etkilerini görmek için verinin ortalama noktasında bir kısmi olasılık hesabı yapalım.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> model2.get_margeff(at <span class="op">=</span> <span class="st">&#39;mean&#39;</span>).summary()</code></pre></div>
<pre><code>        Logit Marginal Effects       
=====================================
Dep. Variable:                 switch
Method:                          dydx
At:                              mean
==================================================================================
                    dy/dx    std err          z      P&gt;|z|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------
I(dist / 100.)    -0.2181      0.025     -8.598      0.000        -0.268    -0.168
arsenic            0.1121      0.010     11.217      0.000         0.092     0.132
==================================================================================</code></pre>
<p>Bu sonuca göre, ankette soru sorulan ortalama kişi için en yakın kuyuya olan uzaklıkta 100 metrelik bir değişim olasılığında %22 düşüş anlamına gelmektedir. Fakat kendi kuyusundaki arsenikte 1 seviyesinde bir artış değişim olasılığını %11 oranında arttırmaktadır.</p>
<p>Sınıfların ayırılabilirliği</p>
<p>Bu modelin kuyu değiştirenler ile değiştirmeyenleri ne kadar iyi sınıflayabildiğini anlamak için her sınıftaki kişiyi uzaklık-arsenik uzayında grafikleyebiliriz.</p>
<p>Biz pek bir iyi bir ayırım göremedik, o sebeple modelin oldukça yüksek bir hata oranının olmasını bekliyoruz. Fakat başka bir şey farkediyoruz, grafiğin &quot;kısa mesafe-yüksek arsenik&quot; bölgesinde çoğunlukla değişimciler var, ve &quot;uzun mesafe-düşük arsenik&quot; bölgesinde çoğunlukla değiştirmeyenler var.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">logit_pars <span class="op">=</span> model2.params
intercept <span class="op">=</span> <span class="op">-</span>logit_pars[<span class="dv">0</span>] <span class="op">/</span> logit_pars[<span class="dv">2</span>]
slope <span class="op">=</span> <span class="op">-</span>logit_pars[<span class="dv">1</span>] <span class="op">/</span> logit_pars[<span class="dv">2</span>]

dist_sw <span class="op">=</span> df[<span class="st">&#39;dist&#39;</span>][df[<span class="st">&#39;switch&#39;</span>] <span class="op">==</span> <span class="dv">1</span>]
dist_nosw <span class="op">=</span> df[<span class="st">&#39;dist&#39;</span>][df[<span class="st">&#39;switch&#39;</span>] <span class="op">==</span> <span class="dv">0</span>]
arsenic_sw <span class="op">=</span> df[<span class="st">&#39;arsenic&#39;</span>][df[<span class="st">&#39;switch&#39;</span>] <span class="op">==</span> <span class="dv">1</span>]
arsenic_nosw <span class="op">=</span> df[<span class="st">&#39;arsenic&#39;</span>][df[<span class="st">&#39;switch&#39;</span>] <span class="op">==</span> <span class="dv">0</span>]
plt.figure(figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">8</span>))
plt.plot(dist_sw, arsenic_sw, <span class="st">&#39;.&#39;</span>, mec <span class="op">=</span> <span class="st">&#39;purple&#39;</span>, mfc <span class="op">=</span> <span class="st">&#39;None&#39;</span>, 
         label <span class="op">=</span> <span class="st">&#39;Switch&#39;</span>)
plt.plot(dist_nosw, arsenic_nosw, <span class="st">&#39;.&#39;</span>, mec <span class="op">=</span> <span class="st">&#39;orange&#39;</span>, mfc <span class="op">=</span> <span class="st">&#39;None&#39;</span>, 
         label <span class="op">=</span> <span class="st">&#39;No switch&#39;</span>)
plt.plot(np.arange(<span class="dv">0</span>, <span class="dv">350</span>, <span class="dv">1</span>), intercept <span class="op">+</span> slope <span class="op">*</span> np.arange(<span class="dv">0</span>, <span class="dv">350</span>, <span class="dv">1</span>) <span class="op">/</span> <span class="fl">100.</span>,
         <span class="st">&#39;-k&#39;</span>, label <span class="op">=</span> <span class="st">&#39;Separating line&#39;</span>)
plt.ylim(<span class="dv">0</span>, <span class="dv">10</span>)
plt.xlabel(<span class="st">&#39;Distance to safe well (meters)&#39;</span>)
plt.ylabel(<span class="st">&#39;Arsenic level&#39;</span>)
plt.legend(loc <span class="op">=</span> <span class="st">&#39;best&#39;</span>)
plt.savefig(<span class="st">&#39;stat_logit_07.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_07.png" />

</div>
<p>Model 3: Etkileşim eklemek</p>
<p>Arsenik seviyesi ve uzaklık değişkenlerinin modele ayrı ayrı yaptığı etkiler yanında, beraber olarak ta bazı etkiler yapacağını düşünebiliriz. 100 metrelik mesafenin değişim kararına olan etkisi kuyunuzdaki arsenik seviyesiyle bağlantılı olabilmesi.. İnsanların böyle düşünmesini bekleyebiliriz, yani, bu problem bağlamında, tipik kişi durup ta &quot;önce arsenik yokmuş gibi düşüneyim, sadece mesafeye bakayım&quot;, sonra &quot;şimdi arseniği düşüneyim, mesafe yokmuş gibi yapayım&quot;, ve bunlardan sonra &quot;şimdi bu iki ayrı kararı üst üste koyayım&quot; şeklinde düşünmez.</p>
<p>Statsmodels'de formül arayüzü ile modele etkileşim eklemenin yolu değişkenler arasında <code>:</code> operatörünü kullanmak ile olur.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">model3 <span class="op">=</span> logit(<span class="st">&#39;switch ~ I(dist / 100.) + arsenic + I(dist / 100.):arsenic&#39;</span>, df).fit()
<span class="bu">print</span> model3.summary()</code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 0.650270
         Iterations 5
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                 switch   No. Observations:                 3020
Model:                          Logit   Df Residuals:                     3016
Method:                           MLE   Df Model:                            3
Date:                Wed, 20 May 2015   Pseudo R-squ.:                 0.04625
Time:                        21:26:26   Log-Likelihood:                -1963.8
converged:                       True   LL-Null:                       -2059.0
                                        LLR p-value:                 4.830e-41
==========================================================================================
                             coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------------
Intercept                 -0.1479      0.118     -1.258      0.208        -0.378     0.083
I(dist / 100.)            -0.5772      0.209     -2.759      0.006        -0.987    -0.167
arsenic                    0.5560      0.069      8.021      0.000         0.420     0.692
I(dist / 100.):arsenic    -0.1789      0.102     -1.748      0.080        -0.379     0.022
==========================================================================================</code></pre>
<p>Sonuca göre etkileşimin katsayısı negatif ve istatistiki olarak anlamlı (significant). Bu katsayının değişim üzerindeki etkisini nicesel olarak hemen bakar bakmaz anlayamıyor olsak bile, niteliksel olarak etkisi sezgilerimiz ile uyuşuyor. Uzaklık değişimde negatif etkili, ama bu negatif etki yüksek arsenik seviyesi devreye girince azalıyor. Diğer yandan arsenik seviyesinin değişimde pozitif etkisi var, ama o etki en yakın kuyu mesafesi arttıkça azalıyor.</p>
<p>Model 4: Eğitim seviyesi ve ek bazı etkileşimler, ve değişkenleri ortalamak</p>
<p>Eğitim seviyesi kişilerin arseniğin kötü etkilerini anlamasında pozitif etki yapması beklenir, ve bu sebeple eğitim seviyesi değişim kararına pozitif etki yapmalıdır. Elimizdeki veride eğitim yıl bazında kayıtlanmış, biz bu veri noktasını ölçekleyeceğiz (aynen uzaklığa yaptımız gibi, çünkü eğitimde 1 senelik değişimin pek bir anlamı yok), bunu için 4'e böleceğiz. Ayrıca bu yeni değişkenin diğer değişkenler ile etkileşimini devreye sokacağız.</p>
<p>Ek olarak tüm değişkenleri ortalayacağız ki böylece onları yorumlamamız rahatlaşacak. Bir kez daha bu işi tamamen statsmodels sayesinde formül içinde halledeceğiz, dışarıdan on hesap yapıp formüle geçmemiz gerekmeyecek.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">model_form <span class="op">=</span> (<span class="st">&#39;switch ~ center(I(dist / 100.)) + center(arsenic) + &#39;</span> <span class="op">+</span>
              <span class="st">&#39;center(I(educ / 4.)) + &#39;</span> <span class="op">+</span>
              <span class="st">&#39;center(I(dist / 100.)) : center(arsenic) + &#39;</span> <span class="op">+</span> 
              <span class="st">&#39;center(I(dist / 100.)) : center(I(educ / 4.)) + &#39;</span> <span class="op">+</span> 
              <span class="st">&#39;center(arsenic) : center(I(educ / 4.))&#39;</span>
             )
model4 <span class="op">=</span> logit(model_form, df).fit()
<span class="bu">print</span> model4.summary()</code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 0.644328
         Iterations 5
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                 switch   No. Observations:                 3020
Model:                          Logit   Df Residuals:                     3013
Method:                           MLE   Df Model:                            6
Date:                Wed, 20 May 2015   Pseudo R-squ.:                 0.05497
Time:                        21:27:19   Log-Likelihood:                -1945.9
converged:                       True   LL-Null:                       -2059.0
                                        LLR p-value:                 4.588e-46
===============================================================================================================
                                                  coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
---------------------------------------------------------------------------------------------------------------
Intercept                                       0.3563      0.040      8.844      0.000         0.277     0.435
center(I(dist / 100.))                         -0.9029      0.107     -8.414      0.000        -1.113    -0.693
center(arsenic)                                 0.4950      0.043     11.497      0.000         0.411     0.579
center(I(educ / 4.))                            0.1850      0.039      4.720      0.000         0.108     0.262
center(I(dist / 100.)):center(arsenic)         -0.1177      0.104     -1.137      0.256        -0.321     0.085
center(I(dist / 100.)):center(I(educ / 4.))     0.3227      0.107      3.026      0.002         0.114     0.532
center(arsenic):center(I(educ / 4.))            0.0722      0.044      1.647      0.100        -0.014     0.158
===============================================================================================================</code></pre>
<p>Modelin başarısını irdelemek: Kutulanmış Kalıntı grafikleri (Binned Residual plots)</p>
<p>Model kalıntısının (yani model ile gerçek veri arasındaki hatalar -residual-) ile ayrı ayrı her değişken ile grafikleri, uzaklık-kalıntı, arsenik-kalıntı gibi, bizi modelde gayrı lineerlik olup olmadığı hakkında uyarabilir. Çünkü kalıntının Gaussian bir dağılımda olmasını bekleriz, model hatası tam anlamıyla bir &quot;gürültü&quot; halinde olmalıdır, ki doğada gürültünün tanımı Gaussian dağılımına sahip olmaktır. Eğer bu grafikte kabaca her yere eşit şekilde dağılmış bir görüntü görmüyorsak, o zaman modelimizde yakalayamadığımız bir gayrı lineerlik (nonlinearity) vardır, ya da, birbirinden farklı olan kalıntı grafikleri kalıntıları dağılımlarının birbirinden farklı olduğunun işaretidir (heteroskedaştıcıty).</p>
<p>İkili bir modelde kalıntıları ham şekilde grafiklemenin pek anlamı yoktur, o sebeple biraz pürüzsüzleştirme uygulayacağız. Altta değişkenler için oluşturduğumuz kutucuklar (bins) içine kalıntıların ortalamasını koyacağız ve bunları grafikleyeceğiz (lowess ya da hareketli ortalama -moving average- tekniği de burada ise yarayabilirdi).</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> bin_residuals(resid, var, bins):
    <span class="co">&#39;&#39;&#39;</span>
<span class="co">    Compute average residuals within bins of a variable.</span>
<span class="co">    </span>
<span class="co">    Returns a dataframe indexed by the bins, with the bin midpoint,</span>
<span class="co">    the residual average within the bin, and the confidence interval </span>
<span class="co">    bounds.</span>
<span class="co">    &#39;&#39;&#39;</span>
    resid_df <span class="op">=</span> DataFrame({<span class="st">&#39;var&#39;</span>: var, <span class="st">&#39;resid&#39;</span>: resid})
    resid_df[<span class="st">&#39;bins&#39;</span>] <span class="op">=</span> qcut(var, bins)
    bin_group <span class="op">=</span> resid_df.groupby(<span class="st">&#39;bins&#39;</span>)
    bin_df <span class="op">=</span> bin_group[<span class="st">&#39;var&#39;</span>, <span class="st">&#39;resid&#39;</span>].mean()
    bin_df[<span class="st">&#39;count&#39;</span>] <span class="op">=</span> bin_group[<span class="st">&#39;resid&#39;</span>].count()
    bin_df[<span class="st">&#39;lower_ci&#39;</span>] <span class="op">=</span> <span class="dv">-2</span> <span class="op">*</span> (bin_group[<span class="st">&#39;resid&#39;</span>].std() <span class="op">/</span> 
                               np.sqrt(bin_group[<span class="st">&#39;resid&#39;</span>].count()))
    bin_df[<span class="st">&#39;upper_ci&#39;</span>] <span class="op">=</span>  <span class="dv">2</span> <span class="op">*</span> (bin_group[<span class="st">&#39;resid&#39;</span>].std() <span class="op">/</span> 
                               np.sqrt(bin_df[<span class="st">&#39;count&#39;</span>]))
    bin_df <span class="op">=</span> bin_df.sort(<span class="st">&#39;var&#39;</span>)
    <span class="cf">return</span>(bin_df)

<span class="kw">def</span> plot_binned_residuals(bin_df):
    <span class="co">&#39;&#39;&#39;</span>
<span class="co">    Plotted binned residual averages and confidence intervals.</span>
<span class="co">    &#39;&#39;&#39;</span>
    plt.plot(bin_df[<span class="st">&#39;var&#39;</span>], bin_df[<span class="st">&#39;resid&#39;</span>], <span class="st">&#39;.&#39;</span>)
    plt.plot(bin_df[<span class="st">&#39;var&#39;</span>], bin_df[<span class="st">&#39;lower_ci&#39;</span>], <span class="st">&#39;-r&#39;</span>)
    plt.plot(bin_df[<span class="st">&#39;var&#39;</span>], bin_df[<span class="st">&#39;upper_ci&#39;</span>], <span class="st">&#39;-r&#39;</span>)
    plt.axhline(<span class="dv">0</span>, color <span class="op">=</span> <span class="st">&#39;gray&#39;</span>, lw <span class="op">=</span> <span class="fl">.5</span>)
    
arsenic_resids <span class="op">=</span> bin_residuals(model4.resid, df[<span class="st">&#39;arsenic&#39;</span>], <span class="dv">40</span>)
dist_resids <span class="op">=</span> bin_residuals(model4.resid, df[<span class="st">&#39;dist&#39;</span>], <span class="dv">40</span>)
plt.figure(figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">5</span>))
plt.subplot(<span class="dv">121</span>)
plt.ylabel(<span class="st">&#39;Residual (bin avg.)&#39;</span>)
plt.xlabel(<span class="st">&#39;Arsenic (bin avg.)&#39;</span>)
plot_binned_residuals(arsenic_resids)
plt.subplot(<span class="dv">122</span>)
plot_binned_residuals(dist_resids)
plt.ylabel(<span class="st">&#39;Residual (bin avg.)&#39;</span>)
plt.xlabel(<span class="st">&#39;Distance (bin avg.)&#39;</span>)
plt.savefig(<span class="st">&#39;stat_logit_08.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_08.png" />

</div>
<p>Üstteki kutulama sırasında kullanılan <code>qcut</code> işlemlerin için en altta ek bölümüne bakın</p>
<p>Model 5: arseniği log ölçeklemek</p>
<p>Kutulanmış artık grafiklerine bakınca arsenik değişkeninde biraz gayrı lineerlik görüyoruz, çünkü noktaların dağılımı çok fazla belli bir bölgede. Dikkat edelim, model nasıl düşük arseniği gerçekte olduğundan daha fazla olacağını tahmin etmiş (overestimate), ayrıca yüksek arseniği gerçekte olduğundan daha az olacağını tahmin etmiş (underestimate). Bu bize arsenik değişkeni üzerinde belki de log transformasyonu gibi bir şeyler yapmamızın gerektiğinin işareti.</p>
<p>Bu değişimi de direk formül içinde yapabiliriz.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">model_form <span class="op">=</span> (<span class="st">&#39;switch ~ center(I(dist / 100.)) + center(np.log(arsenic)) + &#39;</span> <span class="op">+</span>
              <span class="st">&#39;center(I(educ / 4.)) + &#39;</span> <span class="op">+</span>
              <span class="st">&#39;center(I(dist / 100.)) : center(np.log(arsenic)) + &#39;</span> <span class="op">+</span> 
              <span class="st">&#39;center(I(dist / 100.)) : center(I(educ / 4.)) + &#39;</span> <span class="op">+</span> 
              <span class="st">&#39;center(np.log(arsenic)) : center(I(educ / 4.))&#39;</span>
             )

model5 <span class="op">=</span> logit(model_form, df).fit()
<span class="bu">print</span> model5.summary()</code></pre></div>
<pre><code>Optimization terminated successfully.
         Current function value: 0.639587
         Iterations 5
                           Logit Regression Results                           
==============================================================================
Dep. Variable:                 switch   No. Observations:                 3020
Model:                          Logit   Df Residuals:                     3013
Method:                           MLE   Df Model:                            6
Date:                Wed, 20 May 2015   Pseudo R-squ.:                 0.06192
Time:                        21:26:41   Log-Likelihood:                -1931.6
converged:                       True   LL-Null:                       -2059.0
                                        LLR p-value:                 3.517e-52
==================================================================================================================
                                                     coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------------------------------------
Intercept                                          0.3452      0.040      8.528      0.000         0.266     0.425
center(I(dist / 100.))                            -0.9796      0.111     -8.809      0.000        -1.197    -0.762
center(np.log(arsenic))                            0.9036      0.070     12.999      0.000         0.767     1.040
center(I(educ / 4.))                               0.1785      0.039      4.577      0.000         0.102     0.255
center(I(dist / 100.)):center(np.log(arsenic))    -0.1567      0.185     -0.846      0.397        -0.520     0.206
center(I(dist / 100.)):center(I(educ / 4.))        0.3384      0.108      3.141      0.002         0.127     0.550
center(np.log(arsenic)):center(I(educ / 4.))       0.0601      0.070      0.855      0.393        -0.078     0.198
==================================================================================================================</code></pre>
<p>Şimdi arsenik için kutulanmış kalıntı grafikleri daha iyi gözüküyor.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">arsenic_resids <span class="op">=</span> bin_residuals(model5.resid, df[<span class="st">&#39;arsenic&#39;</span>], <span class="dv">40</span>)
dist_resids <span class="op">=</span> bin_residuals(model5.resid, df[<span class="st">&#39;dist&#39;</span>], <span class="dv">40</span>)
plt.figure(figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">5</span>))
plt.subplot(<span class="dv">121</span>)
plot_binned_residuals(arsenic_resids)
plt.ylabel(<span class="st">&#39;Residual (bin avg.)&#39;</span>)
plt.xlabel(<span class="st">&#39;Arsenic (bin avg.)&#39;</span>)
plt.subplot(<span class="dv">122</span>)
plot_binned_residuals(dist_resids)
plt.ylabel(<span class="st">&#39;Residual (bin avg.)&#39;</span>)
plt.xlabel(<span class="st">&#39;Distance (bin avg.)&#39;</span>)
plt.savefig(<span class="st">&#39;stat_logit_09.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_09.png" />

</div>
<p>Model hata oranları</p>
<p><code>pred_table()</code> çağrısı bize bu modelin &quot;kafa karışıklığı matrisini (confusion matrix)&quot; veriyor. Bu matrisi kullanarak modelimizin hata oranını hesaplayabiliriz.</p>
<p>Not: Kafa karışıklığı matrisi sınıflandırma hatalarını verir, ve her türlü hata kombinasyonunu içerir, mesela iki sınıf için, gerçekte 0 ama 1 tahmin hataları, gerçekte 1 ama 0 hataları vs. Bu matrisin satırlar gerçek veri, kolonları tahminleri içerir. Tabii ki köşegendeki sayılar doğru tahmin oranlarıdır.</p>
<p>Sonra bu sonucu, en fazla verilen cevabı herkesin cevabıymış gibi farzeden daha basit bir &quot;sıfır (null) modelinin&quot; hata oranı ile karşılaştırmalıyız. Mesela burada kişilerin %58'i kuyu değiştirmiş, bu durumda sıfır modeli &quot;herkes kuyu değiştiriyor&quot; diye modeller, ve bu basit modelin hata payı 42% olur. Bizim model bu modelden daha iyi bir sonuç verecek midir? Sonuç altta.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> model5.pred_table()
<span class="bu">print</span> <span class="st">&#39;Model Error rate: </span><span class="sc">{0: 3.0%}</span><span class="st">&#39;</span>.<span class="bu">format</span>(
    <span class="dv">1</span> <span class="op">-</span> np.diag(model5.pred_table()).<span class="bu">sum</span>() <span class="op">/</span> model5.pred_table().<span class="bu">sum</span>())
<span class="bu">print</span> <span class="st">&#39;Null Error Rate: </span><span class="sc">{0: 3.0%}</span><span class="st">&#39;</span>.<span class="bu">format</span>(
    <span class="dv">1</span> <span class="op">-</span> df[<span class="st">&#39;switch&#39;</span>].mean())</code></pre></div>
<pre><code>[[  568.   715.]
 [  387.  1350.]]
Model Error rate:  36%
Null Error Rate:  42%</code></pre>
<p>Ek: qcut</p>
<p>Yukarıdaki <code>qcut</code> kullanımını özetlemek gerekirse; arsenik değişkeni için mesela dağılım bölgeleri (n-tile) üzerinden bir atama yapacağız, önce DataFrame yaratalım,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">resid_df <span class="op">=</span> DataFrame({<span class="st">&#39;var&#39;</span>: df[<span class="st">&#39;arsenic&#39;</span>], <span class="st">&#39;resid&#39;</span>: model4.df_resid})
<span class="bu">print</span> resid_df[:<span class="dv">10</span>]</code></pre></div>
<pre><code>    resid   var
1    3013  2.36
2    3013  0.71
3    3013  2.07
4    3013  1.15
5    3013  1.10
6    3013  3.90
7    3013  2.97
8    3013  3.24
9    3013  3.28
10   3013  2.52</code></pre>
<p>Şimdi 40 tane dağılım bölgesi yaratalım</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> qcut(df[<span class="st">&#39;arsenic&#39;</span>], <span class="dv">40</span>)</code></pre></div>
<pre><code>1      (2.327, 2.47]
2       (0.68, 0.71]
3      (1.953, 2.07]
4        (1.1, 1.15]
5      (1.0513, 1.1]
6     (3.791, 4.475]
7       (2.81, 2.98]
8       (3.21, 3.42]
9       (3.21, 3.42]
10      (2.47, 2.61]
11      (2.98, 3.21]
12      (2.98, 3.21]
13      (2.81, 2.98]
14      (2.98, 3.21]
15      (1.66, 1.76]
...
3006      (0.64, 0.68]
3007     (2.327, 2.47]
3008      (0.71, 0.75]
3009       (1.25, 1.3]
3010      (0.71, 0.75]
3011      (0.56, 0.59]
3012    (0.95, 1.0065]
3013       (0.86, 0.9]
3014      [0.51, 0.53]
3015    (0.95, 1.0065]
3016      [0.51, 0.53]
3017     (1.0513, 1.1]
3018      [0.51, 0.53]
3019      (0.62, 0.64]
3020      (0.64, 0.68]
Name: arsenic, Length: 3020, dtype: category
Categories (40, object): [[0.51, 0.53] &lt; (0.53, 0.56] &lt; (0.56, 0.59] &lt;
(0.59, 0.62] ... (3.21, 3.42] 
                          &lt; (3.42, 3.791] &lt; (3.791, 4.475] &lt; (4.475, 9.65]] </code></pre>
<p>Görüldüğü gibi bölgeler bir obje aslında ve içinde levels diye bir değişkeni var. Ayrıca labels diye bir değişken de var,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> qcut(df[<span class="st">&#39;arsenic&#39;</span>], <span class="dv">40</span>).labels</code></pre></div>
<pre><code>[31  6 28 ...,  0  4  5]</code></pre>
<p>ki bu değişken içinde hangi noktanın hangi olasılık bölgesine ait olduğunun ataması var. Mesela 2. nokta 6. bölgeye aitmiş, bu bölge hangisi?</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> qcut(df[<span class="st">&#39;arsenic&#39;</span>], <span class="dv">40</span>).levels[<span class="dv">6</span>]</code></pre></div>
<pre><code>(0.68, 0.71]</code></pre>
<p>Şimdi şöyle bir atama yaparsak, yani qcut sonucunu direk olduğu gibi <code>resid_df</code> içine atarsak, <code>qcut</code> içindeki <code>levels</code>, <code>resid_df</code> üzerindeki index (sıra) ile uyumlandırılacaktır, ve her var için doğru olan <code>qcut</code> sonucu atanmış olacaktır!</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">resid_df[<span class="st">&#39;bins&#39;</span>] <span class="op">=</span> qcut(df[<span class="st">&#39;arsenic&#39;</span>], <span class="dv">40</span>)
<span class="bu">print</span> resid_df[:<span class="dv">10</span>]</code></pre></div>
<pre><code>       resid   var            bins
1   0.842596  2.36   (2.327, 2.47]
2   1.281417  0.71    (0.68, 0.71]
3  -1.613751  2.07   (1.953, 2.07]
4   0.996195  1.15     (1.1, 1.15]
5   1.005102  1.10   (1.0513, 1.1]
6   0.592056  3.90  (3.791, 4.475]
7   0.941372  2.97    (2.81, 2.98]
8   0.640139  3.24    (3.21, 3.42]
9   0.886626  3.28    (3.21, 3.42]
10  1.130149  2.52    (2.47, 2.61]</code></pre>
<p>Üstte hakikaten bakıyoruz ki 2. nokta var=<code>0.71</code> doğru aralık olan <code>(0.68, 0.71]</code> ile eşleşmiş.</p>
<p>Kredi Kart Analizi ve Lojistik Regresyon</p>
<p>Kredi Kart başvurularının kabul edilip edilmediğinin kayıtları üzerinde başvurunun kabul edilip edilmeyeceğini tahmin etmek için bir model kullanabiliriz. Örnek [1, sf. 390]'dan alındı. Veri</p>
<p><code>card</code> = Başvuru kabul edilmiş mi?</p>
<p><code>reports</code> = Kişi hakkında kötü bir olay rapor edilmiş mi?</p>
<p><code>income</code> = Yıllık gelir, birim $10000</p>
<p><code>age</code> = Yaş</p>
<p><code>owner</code> = Kişi kendi evinin sahibi mi?</p>
<p><code>dependents</code> = Bakılan / bağımlı kaç kişi var (çocuk, yaşlı kişi, vs)</p>
<p><code>months</code> = Mevcut adreste kaç aydır yaşanıyor</p>
<p><code>share</code> = Aylık kredi kart harcamalarının yıllık kazanca olan oranı</p>
<p><code>selfemp</code> = Kişi kendi işinin sahibi mi?</p>
<p><code>majorcards</code> = Büyük kredi kart şirketlerinden kaç tane kartı var</p>
<p><code>active</code> = Kaç tane aktif kredi kart hesabı var</p>
<p><code>expenditure</code> = Aylık kredi kartı harcaması</p>
<p>Tahmin edeceğimiz ilk değişken <code>card</code> olacak, ki bu değişken evet/hayır bazında; ona bağlı olarak modellenecek değişkenler geri kalanları, bu değişkenlerin kredi kart kabulünde ne kadar etkili olacağını analiz edeceğiz.</p>
<p>Biraz veri önişlemesi yapalım; 1'den küçük bazı yaş verileri var, onları silelim ve değişkenlerin histogramını basalım.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
df <span class="op">=</span> pd.read_csv(<span class="st">&#39;CreditCard.csv&#39;</span>,index_col<span class="op">=</span><span class="dv">0</span>)

<span class="co"># etiketi 1/0 degerine cevir</span>
df[<span class="st">&#39;card&#39;</span>] <span class="op">=</span> (df[<span class="st">&#39;card&#39;</span>]<span class="op">==</span><span class="st">&#39;yes&#39;</span>).astype(<span class="bu">int</span>)
df[<span class="st">&#39;owner&#39;</span>] <span class="op">=</span> (df[<span class="st">&#39;owner&#39;</span>]<span class="op">==</span><span class="st">&#39;yes&#39;</span>).astype(<span class="bu">int</span>)

<span class="co"># 1&#39;den kucuk yaslari sil</span>
df <span class="op">=</span> df[df[<span class="st">&#39;age&#39;</span>] <span class="op">&gt;</span> <span class="dv">1</span>]
df[<span class="st">&#39;log_reports1&#39;</span>] <span class="op">=</span> np.log(df[<span class="st">&#39;reports&#39;</span>]<span class="op">+</span><span class="dv">1</span>)
df[<span class="st">&#39;log_share&#39;</span>] <span class="op">=</span> np.log(df[<span class="st">&#39;share&#39;</span>])</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">fig, axes <span class="op">=</span> plt.subplots(<span class="dv">3</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">10</span>))

col<span class="op">=</span><span class="st">&#39;reports&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">0</span>,<span class="dv">0</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)
col<span class="op">=</span><span class="st">&#39;income&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">0</span>,<span class="dv">1</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)
col<span class="op">=</span><span class="st">&#39;share&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">0</span>,<span class="dv">2</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)

col<span class="op">=</span><span class="st">&#39;age&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">1</span>,<span class="dv">0</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)
col<span class="op">=</span><span class="st">&#39;owner&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)
col<span class="op">=</span><span class="st">&#39;dependents&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)

col<span class="op">=</span><span class="st">&#39;months&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">2</span>,<span class="dv">0</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)
col<span class="op">=</span><span class="st">&#39;log(share)&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)
col<span class="op">=</span><span class="st">&#39;log(reports+1)&#39;</span><span class="op">;</span>ax<span class="op">=</span>axes[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">;</span>ax.set_title(col)
df[col].hist(ax<span class="op">=</span>ax)

plt.savefig(<span class="st">&#39;stat_logit_01.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_logit_01.png" />

</div>
<p>Görüldüğü gibi <code>share</code> sola doğru çok yamuk (highly skewed) duruyor, bu değişkenin bu sebeple log'unu aldık. Değişken <code>reports</code> aynı durumda, ayrıca bu değişkenin çoğu değeri 0 ya da 1, ama maksimum değeri 14. Bu sebeple <code>log(reports+1)</code> kullandık ki 0'in logunu almak zorunda olmayalım, ki zaten bu tanımsızdır. Bu transformasyonu yapıyoruz çünkü belli noktalarda aşırı yoğun olan değişkenler (ki yamukluklarının sebebi bu) çok yüksek katsayı değerlerinin çıkmasını tetikleyebiliyor, bu yüzden transformasyon ile onları biraz daha yaymaya uğraşıyoruz. Alttaki regresyon transformasyon yapılmış hali, onun altında yapılmamış hali de var.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf
<span class="im">import</span> statsmodels.api <span class="im">as</span> sm
model <span class="op">=</span> <span class="st">&quot;card ~ log_reports1 + income + log_share + age + &quot;</span> <span class="op">+</span> <span class="op">\</span>
        <span class="co">&quot;owner + dependents + months &quot;</span>
model<span class="op">=</span>smf.glm(model, data<span class="op">=</span>df, family<span class="op">=</span>sm.families.Binomial()).fit()
<span class="bu">print</span>(model.summary())
<span class="bu">print</span> <span class="st">&#39;BIC&#39;</span>, model.bic
<span class="bu">print</span> <span class="st">&#39;AIC&#39;</span>, model.aic</code></pre></div>
<pre><code>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                   card   No. Observations:                 1312
Model:                            GLM   Df Residuals:                     1304
Model Family:                Binomial   Df Model:                            7
Link Function:                  logit   Scale:                             1.0
Method:                          IRLS   Log-Likelihood:                -69.895
Date:                Thu, 21 May 2015   Deviance:                       139.79
Time:                        10:14:51   Pearson chi2:                     247.
No. Iterations:                    13                                         
================================================================================
                   coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
--------------------------------------------------------------------------------
Intercept       21.4739      3.674      5.844      0.000        14.272    28.675
log_reports1    -2.9086      1.098     -2.650      0.008        -5.060    -0.757
income           0.9033      0.190      4.760      0.000         0.531     1.275
log_share        3.4230      0.530      6.452      0.000         2.383     4.463
age              0.0227      0.022      1.036      0.300        -0.020     0.066
owner            0.7052      0.533      1.323      0.186        -0.340     1.750
dependents      -0.6649      0.267     -2.487      0.013        -1.189    -0.141
months          -0.0057      0.004     -1.435      0.151        -0.014     0.002
================================================================================
BIC -9222.02662057
AIC 155.790971664</code></pre>
<p>Değişken <code>reports</code> ve <code>share</code> transforme edilmemiş hali,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf

reg2 <span class="op">=</span> <span class="st">&quot;card ~ reports + income + share + age + &quot;</span> <span class="op">+</span> <span class="op">\</span>
       <span class="co">&quot;owner + dependents + months &quot;</span>
model2<span class="op">=</span>smf.glm(reg2, data<span class="op">=</span>df, family<span class="op">=</span>sm.families.Binomial()).fit()
<span class="bu">print</span>(model2.summary())
<span class="bu">print</span> <span class="st">&#39;BIC&#39;</span>, model2.bic
<span class="bu">print</span> <span class="st">&#39;AIC&#39;</span>, model2.aic</code></pre></div>
<pre><code>                 Generalized Linear Model Regression Results                  
==============================================================================
Dep. Variable:                   card   No. Observations:                 1312
Model:                            GLM   Df Residuals:                     1304
Model Family:                Binomial   Df Model:                            7
Link Function:                  logit   Scale:                             1.0
Method:                          IRLS   Log-Likelihood:                    nan
Date:                Thu, 21 May 2015   Deviance:                       142.88
Time:                        10:14:59   Pearson chi2:                     229.
No. Iterations:                    18                                         
==============================================================================
                 coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept     -4.5817      0.859     -5.334      0.000        -6.265    -2.898
reports       -2.0253      0.900     -2.249      0.024        -3.790    -0.261
income         0.3850      0.133      2.884      0.004         0.123     0.647
share       2966.3111    587.349      5.050      0.000      1815.128  4117.495
age            0.0174      0.022      0.801      0.423        -0.025     0.060
owner          0.5966      0.526      1.135      0.256        -0.434     1.627
dependents    -0.6130      0.249     -2.457      0.014        -1.102    -0.124
months        -0.0046      0.004     -1.201      0.230        -0.012     0.003
==============================================================================
BIC -9218.93777391
AIC nan</code></pre>
<p>Görüldüğü gibi <code>share</code> katsayısı oldukça büyük, ve bu modelde modelin veriye uyum kalitesini ölçen BIC değeri aşağı yukarı 3 civarında büyüdü (daha küçük BIC daha iyi).</p>
<p>Bir önceki regresyona bakarsak (ki doğru olarak onu kabul ediyoruz artık) değişkenlerin p-değerine göre <code>log_reports1</code>, <code>share</code>, <code>income</code>, ve <code>dependents</code> değişkenlerinin önemli (significant) olduğunu görüyoruz. Demek ki bu değişkenlerin kredi kartı başvurusunun kabulü bağlamında en önemli değişkenler. Sayısal olarak <code>income</code> ve <code>share</code> arttıkça kabul şansı artıyor, <code>reports</code> ve <code>dependents</code> arttıkça şans azalıyor.</p>
<p>Not: Üstte AIC değeri niye <code>nan</code> oldu? Tabii ondan önce AIC ve BIC nedir tanımlayalım.</p>
<p><span class="math display">\[ AIC = -2\log \{ L(\hat{\theta}_{ML}) \} + 2p \]</span></p>
<p><span class="math display">\[ BIC = -2\log \{ L(\hat{\theta}_{ML}) \} + \log(n)p \]</span></p>
<p>AIC ve BIC bir modelin veriye ne kadar iyi uyduğunu gösteren ölçütlerdir. Log olurluk, yani verinin bir modele göre kadar ne kadar olası olduğu AIC'in önemli bir parçası. Fakat ek olarak modelin parametre sayısı <span class="math inline">\(p\)</span> e <span class="math inline">\(2p\)</span> olarak AIC'e dahil edilmiş, yani olurluğu aynı olan iki modelden daha basit olanı tercih edilecektir [8].</p>
<p>Ayrıca AIC ölçütünün, standart varsayımlar altında, verinin bir parçasını dışarıda bırakarak yapılan çapraz sağlamaya (cross-validation) eşdeğer olduğu da ispatlanmıştır [9, sf. 90]. Bu çok ciddi bir avantajdır! Düşünürsek, yeni veri üzerinde elde edilecek başarının ölçümü çoğunlukla ana &quot;eğitim'' verisinin bir kısmı dışarıda bırakılarak, yani hakikaten &quot;yeni veri'' yaratarak hesaplanmaya uğraşılır - AIC bu işi verinin kendisine bakarak doğal olarak yapıyor.</p>
<p>Bozukluğa dönelim: sebep nedir? Burada tahmin yürütmek gerekirse, AIC hesabındaki log olurluk (likelihood) hesabı bozukluğa yol açmış olabilir, çünkü problem log transform edilmemiş veride ortaya çıktı. Hatırlarsak transformasyonda sıfırlara 1 eklenmişti çünkü sıfırın log'u tanımsızdır. Bu tanımsızlık AIC hesabındaki log olurluk hesabını da bozmuş olabilir. BIC için de aynı şey geçerli olabilirdi, fakat bu modül değişik bir şekilde bu hesabı yapıyor herhalde (ayrı kişiler ayrı tekniklerle yazılmış olabilirler).</p>
<p>Kaynaklar</p>
<p>[1] Ruppert, <em>Statistics and Data Analysis for Financial Engineering</em></p>
<p>[2] Vogel, <em>Logistic models of well switching in Bangladesh</em>, <a href="http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/ARM/ch5/arsenic_wells_switching.ipynb" class="uri">http://nbviewer.ipython.org/urls/raw.github.com/carljv/Will_it_Python/master/ARM/ch5/arsenic_wells_switching.ipynb</a></p>
<p>[3] <em>A Weakly Informative Default Prior Distribution for Logistic and Other Regression models</em>, <a href="http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf" class="uri">http://www.stat.columbia.edu/~gelman/research/published/priors11.pdf</a></p>
<p>[5] Harrington, P. <em>Machine Learning in Action</em></p>
<p>[7] Gelman, Hill, <em>Data Analysis Using Regression and Multilevel/Hierarchical Models</em></p>
<p>[8] Burnham, <em>Model Selection and Inference</em></p>
<p>[9] Shalizi, <em>Advanced Data Analysis from an Elementary Point of View</em></p>
<p>[10] Bayramlı, Istatistik, <em>Lojistik Regresyon</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
