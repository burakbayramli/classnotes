\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Monte Carlo, Entegraller, MCMC

Fizik, biyoloji ve özellikle makina öðrenimi problemlerinde bazen çok
boyutlu bir fonksiyon üzerinden entegral almak gerekebiliyor. En basit
örnek, mesela bir daðýlýmýn baþka bir fonksiyon ile çarpýmýnýn beklentisini
(expectation) hesaplamak gerektiðinde, ki bu

$$ E(f) = \int p(x)h(x) \ud x $$

entegralidir, $x \in \mathbb{R}^n$, $p(x)$ daðýlým fonksiyonu, $h(x)$
herhangi bir baþka fonksiyon olmak üzere, o zaman tüm $x$ deðerlerini göz
önüne alarak (ayrýksal baðlamda ya teker teker geçerek, ya da analitik
olarak) entegral hesabýný yapmak gerekecekti.

Fakat $p(x)$ bir daðýlým olduðuna göre, ve bizim geçtiðimiz her $x$ için
bir olasýlýk deðeri varsa, bu iþi tersine çevirerek, $p(x)$'teki
olasýlýklara göre belli (az) sayýda $x$ ürettirirsek, ve sadece bu $x$'leri
entegral hesabýnda kullanýrsak yaklaþýksal açýdan gerçek entegral hesabýna
yaklaþmýþ oluruz. 

Bu mantýklý deðil mi? Düþünürsek, mesela 10 deðeri 0.4 olasýlýðýnda ise, 5
deðeri 0.1 olasýlýðýnda ise, hem sayý, hem olasýlýðý ile çarpmak yerine
``daha fazla 10 deðeri üretmek'' ve bu deðerleri $h$'e geçmek, toplamak,
sonra bölmek, vs. yaklaþýksal olarak ayný kapýya çýkar. Yani

$$ E_N = \frac{1}{N}\sum_{1=1}^N h(x^{(i)}) $$

üstteki entegralin yaklaþýksal temsilidir, $x^{(i)}$ $p(x)$ olasýlýðýna
göre üretilen sayýlarý temsil ediyor. Üstteki baðlantýnýn teorik olarak
ispatý da var, bu ispatý burada vermeyeceðiz. 

Ýþte Monte Carlo entegral hesabýnýn artasýnda yatan numara budur. 

Demek ki Monte Carlo entegralýnýn iþlemesi için $p(x)$'den örnekleme yapmak
gerekiyor. Þimdi ikinci numaraya gelelim.  Bazen ne yazýk ki $p(x)$'den
örnekleme yapmak kolay olmuyor. Mesela alttaki bölümdeki entegralýn hesabý
zorlaþýyor,

$$ I = \frac{\int h(x) p(x) \ud x}{\int p(x) \ud x} $$

ki bölünende yine $h(x)$'e göre beklenti almýþ oluyoruz. Bu durumda $q(x)$
adýnda kolay örneklenebilen baþka bir yoðunluk fonksiyonu buluyoruz. Ve formülü
þu hale getirerek bir þey deðiþtirmiþ olmayýz [3],

$$ I = \frac
{\int h(x) \frac{p(x)}{q(x)} q(x) \ud x}
{\int \frac{p(x)}{q(x)} \ud x}
$$

Bu formül bir nevi $q(x)$'e göre alýnmýþ beklentilerin oraný. Ayný numarayý
kullanýp entegralý toplam haline getirebiliriz, $x_i$ deðerleri $i=1,..,N$ i.i.d
olarak $q(x)$'ten örneklenir, o zaman $I$'yi yaklaþýk olarak $\hat{I}$ olarak
hesaplarýz,

$$ \hat{I} = \frac
{\frac{1}{N}\sum _{=1}^{N} h(x) \frac{p(x)}{q(x)} \ud x}
{\frac{1}{N}\sum \frac{p(x)}{q(x)} \ud x}
$$

Bölünendeki bir $q(x)$'in yokolduðuna dikkat.

Yeni bir deðiþken $\tilde{w}_i = p(x)/q(x)$ oraný tanýmlayabiliriz, normalize
edilmiþ halde,

$$ w_i = \frac{\tilde{w}_i}{\sum \tilde{w}_j} =
= \frac{p(x_i)/q(x_i)}{\sum p(x_i)/q(x_i)}
$$

Son formülü iki üstteki formül içine koyarsak,

$$ \hat{I} = \sum _{i=1}^{N} w_i h(x^i)$$

Bu metota Önemsel Örnekleme metotu adý veriliyor çünkü üstteki aðýrlýklar bir
nevi ``önemi'' temsil ediyorlar.  Bir örnek [1]'den görelim; $q(x)$ için
birörnek bir daðýlým seçilmiþ, belli deðerler arasýnda hep ayný deðeri
donduruyor.

\begin{minted}[fontsize=\footnotesize]{python}
def qsample(): return np.random.rand()*4.

def p(x): return 0.3*np.exp(-(x-0.3)**2) + 0.7* np.exp(-(x-2.)**2/0.3) 

def q(x): return 4.0

def importance(nsamples):    
    samples = np.zeros(nsamples,dtype=float)
    w = np.zeros(nsamples,dtype=float)    
    for i in range(nsamples):
            samples[i] = qsample()
            w[i] = p(samples[i])/q(samples[i])                
    return samples, w

x = np.arange(0,4,0.01)
x2 = np.arange(-0.5,4.5,0.1)
realdata = 0.3*np.exp(-(x-0.3)**2) + 0.7* np.exp(-(x-2.)**2/0.3) 
box = np.ones(len(x2))*0.8
box[:5] = 0; box[-5:] = 0
plt.plot(x,realdata,'k',lw=6)
plt.plot(x2,box,'k--',lw=6)
samples,w = importance(5000)
plt.hist(samples,normed=1,fc='k')
plt.savefig('stat_mcmc_02.png')
\end{minted}

\includegraphics[height=6cm]{stat_mcmc_02.png}

Altta Örnekleme ve Öneme Göre Tekrar Örnekleme (Sampling Ýmportance Resampling)
metotu için örnek kod,

\begin{minted}[fontsize=\footnotesize]{python}
def p(x): return 0.3*exp(-(x-0.3)**2) + 0.7* exp(-(x-2.)**2/0.3) 

def q(x): return 4.0

def sir(n):    
    sample1 = np.zeros(n)
    w = np.zeros(n)
    sample2 = np.zeros(n)    
    sample1 = np.random.rand(n)*4
    w = p(sample1)/q(sample1)
    w /= sum(w)
    cumw = zeros(len(w))
    cumw[0] = w[0]
    for i in range(1,len(w)): cumw[i] = cumw[i-1]+w[i]
    u = np.random.rand(n)
    index = 0
    for i in range(n):
        indices = where(u<cumw[i])
        sample2[index:index+size(indices)] = sample1[i]
        index += size(indices)
        u[indices]=2
    return sample2

x = np.arange(0,4,0.01)
x2 = np.arange(-0.5,4.5,0.1)
realdata = 0.3*np.exp(-(x-0.3)**2) + 0.7* np.exp(-(x-2.)**2/0.3) 
box = np.ones(len(x2))*0.8
box[:5] = 0
box[-5:] = 0
plt.plot(x,realdata,'k',lw=6)
plt.plot(x2,box,'k--',lw=6)
plt.savefig('stat_mcmc_03.png')
\end{minted}

\includegraphics[height=6cm]{stat_mcmc_03.png}

MCMC

Yine $p(x)$'den örnekleme yapýlamadýðý durum, bu sefer $p(x)$ yerine onu
yaklaþýksal olarak temsil eden bir $\pi(x)$'i elde etmekle uðraþýlýyor. Bu
$\pi(x)$ iþe bir Markov Zincirinin (Markov Chain -yine MC harfleri!-) duraðan
daðýlýmý olarak hayal ediliyor.

Markov Zinciri teorisinde bir geçiþ matrisi, yan Markov Zincirinin kendisi
verilir, ve duraðan daðýlýmýn hesaplanmasý istenir. MCMC problemlerinde
ise, yani Monte Carlo entegralý için Markov Zinciri kullanýldýðý durumlarda
elimizde bir $\pi(x)$ daðýlýmý vardýr ve bir Markov Zinciri oluþturmamýz
gerekir. Nihai daðýlýmý biliriz, ve bu daðýlýma ``giden'' geçiþleri
üretiriz. Bu geçiþleri öyle ayarlayabiliriz ki üretilen rasgele sayýlar
hedef daðýlýmýndan geliyormuþ gibi olur. 

Geçiþleri üretmek için literatürde bir çok teknik vardýr.  Önemsel
Örnekleme (Importance Sampling), Örnekleme ve Öneme Göre Tekrar Örnekleme
(Sampling Ýmportance Resampling), Metropolis-Hastings, Gibbs Örneklemesi
gibi teknikleri vardýr, ve detaylarý deðiþik olsa da hepsi de MCMC
kategorisine girer, ve yapmaya çalýþtýklarý $\pi(x)$'e giderken bir þekilde
bir geçiþleri, zinciri ortaya çýkartmak ve bu geçiþleri entegral hesabýnda
kullanmaktýr.

Üstteki tekniklerden en yaygýn kullanýlaný Metropolis-Hastings
algoritmasýdýr. 

Þunu vurgulamak önemli, geçiþleri üretmek, ``bir tür sanal Markov Matrisi''
yaratmaktýr aslýnda. Ve her MCMC algoritmasý bunu farklý þekillerde
yapabilir; mesela MH daha basit baþka bir daðýlým ile ana daðýlým arasýnda
sürekli karþýlaþtýrmalar yapar, belli aralýklarda geçiþ yapar, diðerlerinde
yapmaz, ve bunun bir yan etkisi olarak ortaya bir Markov Zinciri çýkartmýþ
/ onu kullanmýþ olur. O geçiþlerin bir Markov Zinciri'ne eþdeðer olduðunun
matematiksel olarak ispatý da vardýr.

Not: Bu alandaki makalelerde bir daðýlýmýn ``belli bir çarpýmsal sabite
kadar'' bilindiði (known up to a multiplicative constant) söylenir. Bu söz
aslýnda þu anlama gelir. Mesela ayrýksal bir daðýlýmýmýz var, ama bu
daðýlýmýn kendisini, þu halini biliyoruz

\verb![ 4.3  2.   8.4  8.7  1.8]!

Bu bir daðýlým deðil, çünkü öðelerin toplamý 1 deðil. Onu bir daðýlým
haline çevirmek için, tüm öðeleri toplamak ve bu vektördeki tüm sayýlarý bu
toplam ile bölmek gerekir. Toplam 25.2, bölersek

\verb![ 0.17063492  0.07936508  0.33333333  0.3452381   0.07142857]!

Ýlk vektör ``belli bir çarpýmsal sabite kadar'' bilinen daðýlým, çarpýmsal
sabit 25.2. Esas daðýlým ikinci vektör. 

Peki niye bu sözü söyleyenler toplamý hesaplayýp gerçek daðýlýmý
hesaplamýyorlar? Sebep performans. Bazen ayrýksal daðýlým o kadar yüksek
boyutlu, fazla öðe içeren bir halde oluyor ki, performans açýsýndan bu
basit toplam hesabýný yapmak bile çok pahalý oluyor. Ýþte MCMC metotlarýnýn
bir güzel tarafý daha burada, daðýlýmýn kendisi olmasa bile belli bir
çarpýmsal sabite kadar bilinen versiyonlarý ile gayet rahat bir þekilde
iþliyorlar.

Metropolis-Hastings

\begin{minted}[fontsize=\footnotesize]{python}
def p(x):
    mu1 = 3; mu2 = 10
    v1 = 10; v2 = 3
    return 0.3*np.exp(-(x-mu1)**2/v1) + 0.7* np.exp(-(x-mu2)**2/v2)

def q(x):
    mu = 5; sigma = 10
    return np.exp(-(x-mu)**2/(sigma**2))

stepsize = 0.5
x = np.arange(-10,20,stepsize)
px = np.zeros(x.shape)
for i in range(len(x)): px[i] = p(x[i])
N = 5000

# independence chain
u = np.random.rand(N)
mu = 5
sigma = 10
y = np.zeros(N)
y[0] = np.random.normal(mu,sigma)
for i in range(N-1):
    ynew = np.random.normal(mu,sigma)
    alpha = min(1,p(ynew)*q(y[i])/(p(y[i])*q(ynew)))
    if u[i] < alpha:
        y[i+1] = ynew
    else:
        y[i+1] = y[i]

# random walk chain
u2 = np.random.rand(N)
sigma = 10
y2 = np.zeros(N)
y2[0] = np.random.normal(0,sigma)
for i in range(N-1):
    y2new = y2[i] + np.random.normal(0,sigma)
    alpha = min(1,p(y2new)/p(y2[i]))
    if u2[i] < alpha:
        y2[i+1] = y2new
    else:
        y2[i+1] = y2[i]

plt.figure(1)
nbins = 30
plt.hist(y, bins = x)
plt.plot(x, px*N/np.sum(px), color='r', linewidth=2)

plt.savefig('stat_mcmc_01.png')
\end{minted}

\includegraphics[height=6cm]{stat_mcmc_01.png}

Gibbs Örneklemesi

Bu örnekleme metodu Metropolis yönteminin bir versiyonu olarak kabul
edilir, Metropolis yöntemlerinde bir teklif (proposal) daðýlýmý $Q$ vardýr,
ve bu daðýlýmýn örneklenmek istenen $P$ ile iliþkisine göre zar atýlýp elde
edilen yeni nokta kabul edilir, ya da kenara atýlýr. Gibbs ile de bir $Q$
vardýr, ama bir cinlik yapýlmýþtýr, $Q$ için $P$'nin kendisi, daha doðrusu
onun koþullu daðýlým hali kullanýlýr. Bu koþullu daðýlým her $i$ için
$P(x_i|\{x_j\}_{j \ne i}$, burada $x_i$ çok boyutlu $x$'in bir öðesidir,
$\{x_j\}_{j \ne i}$ iþe $i$ olmayan diðer tüm öðelerdir. Yani $i$ {\em
  olmayan} her deðiþken koþulunda $i$ örneklenir. Bu kullanýmýn Metropolis
yöntemi ile ayný olduðu ispatlanmýþtýr. Koþullu daðýlýmýn kullanýlmasýnýn
ana sebebi ise çok boyutlu $P$ zor bir daðýlým olsa bile çoðunlukla onun
koþullu ve tek boyutlu daðýlýmýnýn rahatça örneklenebilir halde olmasýdýr.

Algoritma þöyle; Rasgele bir baþlangýç noktasýndan baþlanýr, ve biri harici
tüm deðiþkenler sabit tutulup sabit olmayan deðiþken örneklenir. Bu iþlem
sürekli uygulanýr, bu yapýlýnca sanki örneklenen daðýlýmýn en olasý yerleri
gezilmiþ olur. 

Mesela 2 boyutta, bir $x^{(t)}$ noktasýndan baþladýðýmýzý farzedelim,
$P(x_1|x_2)$ daðýlýmýndan bir $x_1$ örneklenir, (b) þeklinde koþullu
daðýlýmýn tek boyutlu bir Gaussian olduðunu görüyoruz (çünkü ana daðýlým
iki boyutlu Gaussian), bu tek boyutlu daðýlýmdan bir örneklem alýnýyor,
doðal olarak o tek boyutlu daðýlýmýn tepe noktasýnýn altýna yakýn bir
yerden.

\includegraphics[height=8cm]{gibbs.png}

Sonra bu nokta sabitleniyor (yani yeni bir koþullu daðýlým yaratýlýyor, ve
örneklenen $x_1$ þimdi sað tarafta), yani $P(x_2|x_1)$ daðýlýmýndan. Bu
bizi $x^{(t+1)}$ konumuna (state) götürüyor, bu böyle devam ediyor. $K$
deðiþken (boyutundaki) içeren bir sistemde, genel formüller þöyle,

$$ x_1^{(t+1)} \sim P(x_1 | x_2^{t},x_3^{t},..,x_K^{t}) $$ 

$$ x_2^{(t+1)} \sim P(x_2 | x_1^{t},x_3^{t},..,x_K^{t}) $$ 

$$ x_3^{(t+1)} \sim P(x_3 | x_1^{t},x_2^{t},..,x_K^{t}) $$ 

vs..

Monte Carlo ile $pi$ Hesabý

Yaklaþýk olarak $\pi$'yi nasýl hesaplarýz? Ýçinde $\pi$ olan hangi formülü
biliyoruz? Çember alaný formülü. Bunu nasýl kullanabiliriz. Yarýçapý $r$
olan bir çember düþünelim, ve bu çember bir kare içinde olsun, yani karenin
kenarlarý $2r$,

\includegraphics[width=10em]{stat_mcmc_05.png}

Bu durumda kýrmýzýyla iþaretli bölgenin alaný $r^2$. Mavi çemberin alaný
ise $\pi r^2$, çemberin kýrmýzý bölge içine düþen kýsmi $\pi r^2 / 4$. O
zaman ufak karenin oradaki çember kýsminin alanýna olan oraný
$p = \frac{\pi r^2}{4}$ olur. O zaman, eðer iki boyutlu birörnek bir
daðýlýmdan (yani üstteki karenin içine düþecek sayýlar) örneklem alýrsak,
bu her sayý için çember içine mi düþüyor, dýþýna mý düþüyor hesabý kolay,
$x,y$ sayýsý için $x^2+y^2 < r^2$ ise çember içinde, deðilse dýþýnda. Ýçeri
düþen sayýlarýn oranýný $p$ kabul ederiz, bu sayýyý 4 ile çarpýnca yaklaþýk
$\pi$ elde edilir.

\includegraphics[width=15em]{stat_mcmc_04.png}

\begin{minted}[fontsize=\footnotesize]{python}
import random

NB_POINTS = 10**4
LENGTH = 10**5
CENTER = [LENGTH/2,LENGTH/2]

def in_circle(point):
    x,y = point
    center_x, center_y = CENTER
    radius = LENGTH/2
    return (x - center_x)**2. + (y - center_y)**2. < radius**2.

def compute_pi(nb_it):
    inside_count = sum(1.0 for _ in range(nb_it) if \
        in_circle( (random.randint(1,LENGTH),random.randint(1,LENGTH)) ) )
    return (inside_count / nb_it) * 4.

if __name__ == "__main__":
    print u'yaklaþýk', compute_pi(NB_POINTS), u'gerçek', np.pi
\end{minted}

\begin{verbatim}
yaklaþýk 3.1432 gerçek 3.14159265359
\end{verbatim}

Kaynaklar

[1] Marsland, {\em Algorithmic Machine Learning}

[2] MacKay, {\em Information Theory, Inference and Learning Algorithms}

[3] Turner, {\em An Introduction to Particle Filtering}, \url{http://www.lancaster.ac.uk/pg/turnerl/PartileFiltering.pdf}

\end{document}
