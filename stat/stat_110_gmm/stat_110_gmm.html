<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>

  <title>Gaussian Karışım Modeli (GMM) ile Kümelemek</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="gaussian-karışım-modeli-gmm-ile-kümelemek">Gaussian Karışım Modeli (GMM) ile Kümelemek</h1>
<p>Gaussian (normal) dağılımı tek tepesi olan (unimodal) bir dağılımdır. Bu demektir ki eğer birden fazla tepe noktası olan bir veriyi modellemek istiyorsak, değişik yaklaşımlar kullanmamız gerekir. Birden fazla Gaussian'ı &quot;karıştırmak (mixing)&quot; bu tür bir yaklaşım. Karıştırmak, karışım içindeki her Gaussian'dan gelen sonuçları toplamaktır, yani kelimenin tam anlamıyla her veri noktasını teker teker karışımdaki tüm dağılımlara geçip sonuçları ve bir ağırlık üzerinden toplamaktır. Çok boyutlu Gaussian'lar için mesela,</p>
<p><span class="math display">\[ f(x) = \sum_z \pi_k N(x | \mu_k,\Sigma_k) \]</span></p>
<p><span class="math inline">\(\pi_k\)</span> karıştırma oranlarıdır (mixing proportions). Bernoulli karışımlarını anlatan yazıya kıyasla, oradaki <span class="math inline">\(\theta\)</span>'yi 0/1 hücreleri için olasılıklar olarak aldık, şimdi <span class="math inline">\(\theta\)</span> içinde <span class="math inline">\(\mu_k,\Sigma_k\)</span> var, yani <span class="math inline">\(\theta=(\mu_k,\Sigma_k)\)</span>.</p>
<p>İki Gaussian olsa <span class="math inline">\(\pi_1,\pi_2\)</span> oranları 0.2, 0.8 olabilir ve her nokta her Gaussian'a verildikten sonra tekabül eden ağırlıkla mesela sırayla <span class="math inline">\(0.2,0.8\)</span> ile çarpılıp toplanır.</p>
<p>Maksimizasyon adımı için gereken hesapların türetilmesi [5, sf. 392]'de bulunabilir.</p>
<p>Örnek olarak alttaki veriye bakalım.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">data <span class="op">=</span> np.loadtxt(<span class="st">&#39;biometric_data_simple.txt&#39;</span>,delimiter<span class="op">=</span><span class="st">&#39;,&#39;</span>)

women <span class="op">=</span> data[data[:,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">1</span>]
men <span class="op">=</span> data[data[:,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">2</span>]

plt.xlim(<span class="dv">55</span>,<span class="dv">80</span>)
plt.ylim(<span class="dv">80</span>,<span class="dv">280</span>)
plt.plot (women[:,<span class="dv">1</span>],women[:,<span class="dv">2</span>], <span class="st">&#39;b.&#39;</span>)
plt.hold(<span class="va">True</span>)
plt.plot (men[:,<span class="dv">1</span>],men[:,<span class="dv">2</span>], <span class="st">&#39;r.&#39;</span>)
plt.xlabel(<span class="st">&#39;boy (inch)&#39;</span>)
plt.ylabel(<span class="st">&#39;agirlik (pound)&#39;</span>)
plt.savefig(<span class="st">&#39;mixnorm_1.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="mixnorm_1.png" />

</div>
<p>Bu grafik kadınlar ve erkeklerin boy (height) ve kilolarını (weight) içeren bir veri setinden geliyor, veri setinde erkekler ve kadınlara ait olan ölçümler önceden işaretlenmiş / etiketlenmiş (labeled), biz de bu işaretleri kullanarak kadınları kırmızı erkekleri mavi ile grafikledik. Ama bu işaretler / etiketler verilmiş olsun ya da olmasın, kavramsal olarak düşünürsek eğer bu veriye bir dağılım uydurmak (fit) istersek bir karışım kullanılması gerekli, çünkü iki tepe noktasiyle daha rahat temsil edileceğini düşündüğümüz bir durum var ortada.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Multivariate gaussian, contours</span>
<span class="co">#</span>
<span class="im">import</span> scipy.stats
<span class="im">import</span> em

data <span class="op">=</span> np.loadtxt(<span class="st">&#39;biometric_data_simple.txt&#39;</span>,delimiter<span class="op">=</span><span class="st">&#39;,&#39;</span>)

women <span class="op">=</span> data[data[:,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">1</span>]
men <span class="op">=</span> data[data[:,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">2</span>]

plt.xlim(<span class="dv">55</span>,<span class="dv">80</span>)
plt.ylim(<span class="dv">80</span>,<span class="dv">280</span>)
plt.plot (women[:,<span class="dv">1</span>],women[:,<span class="dv">2</span>], <span class="st">&#39;b.&#39;</span>)
plt.hold(<span class="va">True</span>)
plt.plot (men[:,<span class="dv">1</span>],men[:,<span class="dv">2</span>], <span class="st">&#39;r.&#39;</span>)
plt.xlabel(<span class="st">&#39;boy (inch)&#39;</span>)
plt.ylabel(<span class="st">&#39;agirlik (pound)&#39;</span>)
plt.hold(<span class="va">True</span>)

x <span class="op">=</span> np.arange(<span class="fl">55.</span>, <span class="fl">80.</span>, <span class="dv">1</span>)
y <span class="op">=</span> np.arange(<span class="fl">80.</span>, <span class="fl">280.</span>, <span class="dv">1</span>)
X, Y <span class="op">=</span> np.meshgrid(x, y)

Z <span class="op">=</span> np.zeros(X.shape)
nx, ny <span class="op">=</span> X.shape
mu1 <span class="op">=</span> np.array([  <span class="fl">72.89350086</span>,  <span class="fl">193.21741426</span>])
sigma1 <span class="op">=</span> np.matrix([[    <span class="fl">7.84711283</span>,    <span class="fl">25.03111826</span>],
                    [   <span class="fl">25.03111826</span>,  <span class="fl">1339.70289046</span>]])
<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">xrange</span>(nx):
    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">xrange</span>(ny):
        Z[i,j] <span class="op">=</span> em.norm_pdf(np.array([X[i,j], Y[i,j]]),mu1,sigma1)
        
levels <span class="op">=</span> np.linspace(Z.<span class="bu">min</span>(), Z.<span class="bu">max</span>(), <span class="dv">4</span>)

plt.contour(X, Y, Z, colors<span class="op">=</span><span class="st">&#39;b&#39;</span>, levels<span class="op">=</span>levels)
plt.hold(<span class="va">True</span>)

Z <span class="op">=</span> np.zeros(X.shape)
nx, ny <span class="op">=</span> X.shape
mu2 <span class="op">=</span> np.array([  <span class="fl">66.15903841</span>,  <span class="fl">135.308125</span>  ])
sigma2 <span class="op">=</span> np.matrix([[  <span class="fl">14.28189396</span>,   <span class="fl">51.48931033</span>],
                    [  <span class="fl">51.48931033</span>,  <span class="fl">403.09566456</span>]])
<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">xrange</span>(nx):
    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">xrange</span>(ny):
        Z[i,j] <span class="op">=</span> em.norm_pdf(np.array([X[i,j], Y[i,j]]),mu2,sigma2)
        
levels <span class="op">=</span> np.linspace(Z.<span class="bu">min</span>(), Z.<span class="bu">max</span>(), <span class="dv">4</span>)

plt.contour(X, Y, Z, colors<span class="op">=</span><span class="st">&#39;r&#39;</span>, levels<span class="op">=</span>levels)
plt.savefig(<span class="st">&#39;mixnorm_2.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="mixnorm_2.png" />

</div>
<p>Bu karışım içindeki Gaussian'ları üstteki gibi çizebilirdik (gerçi üstteki aslında ileride yapacağımız net bir hesaptan bir geliyor, ona birazdan geliyoruz, ama çıplak gözle de bu şekil uydurulabilirdi). Modeli kontrol edelim, elimizde bir karışım var, nihai olasılık değeri <span class="math inline">\(p(x)\)</span>'i nasıl kullanırız? Belli bir noktanın olasılığını hesaplamak için bu noktayı her iki Gaussian'a teker teker geçeriz (örnekte iki tane), ve gelen olasılık sonuçlarını karışım oranları ile çarparak toplarız. Ağırlıklar sayesinde karışım entegre edilince hala 1 değeri çıkıyor zaten bir dağılımın uyması gereken şartlardan biri bu. Ayrıca bir dağılımın diğerinden daha önemli olduğu ağırlıklar üzerinden modele verilmiş oluyor.</p>
<p>Etiketler Bilinmiyorsa</p>
<p>Eğer etiketler bize önceden verilmemiş olsaydı, hangi veri noktalarının kadınlara, hangilerinin erkeklere ait olduğunu bilmeseydik o zaman ne yapardık? Bu veriyi grafiklerken etiketleri renkleyemezdik tabii ki, şöyle bir resim çizebilirdik ancak,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> scipy.stats
data <span class="op">=</span> np.loadtxt(<span class="st">&#39;biometric_data_simple.txt&#39;</span>,delimiter<span class="op">=</span><span class="st">&#39;,&#39;</span>)

women <span class="op">=</span> data[data[:,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">1</span>]
men <span class="op">=</span> data[data[:,<span class="dv">0</span>] <span class="op">==</span> <span class="dv">2</span>]

plt.xlim(<span class="dv">55</span>,<span class="dv">80</span>)
plt.ylim(<span class="dv">80</span>,<span class="dv">280</span>)
plt.plot (data[:,<span class="dv">1</span>],data[:,<span class="dv">2</span>], <span class="st">&#39;k.&#39;</span>)
plt.xlabel(<span class="st">&#39;boy (inch)&#39;</span>)
plt.ylabel(<span class="st">&#39;agirlik (pound)&#39;</span>)
plt.savefig(<span class="st">&#39;mixnorm_3.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="mixnorm_3.png" />

</div>
<p>Fakat yine de şekil olarak iki kümeyi görebiliyoruz. Acaba öyle bir yapay öğrenim algoritması olsa da, biz bir karışım olduğunu tahmin edip, sonra o karışımı veriye uydururken, etiket değerlerini de kendiliğinden tahmin etse?</p>
<p>Alttaki kod Beklenti-Maksimizasyon üzerinden kümeleme yapar. Konunun teorik kısmı altta ve [6] yazısında bulunabilir.</p>
<p>Türetmek</p>
<p>Karışımda birden fazla çok boyutlu Gaussian olacak, bu Gaussian'lardan <span class="math inline">\(i\)</span>'inci Gaussian</p>
<p><span class="math display">\[ 
f_i(x) = f(x;\mu_i,\Sigma_i) = 
\frac{1}{(2\pi)^{d/2} |\Sigma_i|^{1/2}} \exp 
\bigg\{
-\frac{(x-\mu)^T\Sigma_i^{-1}(x-\mu_i)}{2} 
\bigg\} 
\qquad (1)
\]</span></p>
<p>olur, <span class="math inline">\(x\)</span> çok boyutlu veri noktasıdır, ve kümeleme başlamadan önce <span class="math inline">\(\mu_i,\Sigma_i\)</span> bilinmez, küme sayısı <span class="math inline">\(k\)</span> bilinir. O zaman karışım modeli</p>
<p><span class="math display">\[ f(x) = \sum_{i=1}^{k} f_i(x)P(C_i) =
\sum_{i=1}^{k} f(x;\mu_i,\Sigma_i)P(C_i)
\]</span></p>
<p><span class="math inline">\(P(C_i)\)</span>'a karışım oranları deniyor, ki <span class="math inline">\(\sum_i P(C_i) = 1\)</span>. Bazı metinlerde bu <span class="math inline">\(\pi_i\)</span> olarak ta gösterilebiliyor. Tüm veri için maksimum olurluk</p>
<p><span class="math display">\[ 
L = \sum_{j=1}^{n} \ln f(x_j) = \sum_{j=1}^{n} \ln \bigg(
\sum_{i=1}^{k} f(x_j;\mu_i,\Sigma_i)P(C_i)
\bigg)
\]</span></p>
<p>Şimdi herhangi bir parametre <span class="math inline">\(\theta_i\)</span> için (yani <span class="math inline">\(\mu_i\)</span> ya da <span class="math inline">\(\Sigma_i\)</span>),</p>
<p><span class="math display">\[ 
\frac{\partial L}{\partial \theta_i} = 
\frac{\partial }{\partial \theta_i} 
\bigg(\sum_{j=1}^{n} \ln f(x_j) \bigg)
\]</span></p>
<p><span class="math display">\[ 
= \sum_{j=1}^{n} \big( \frac{1}{f(x_j)} \cdot
\frac{\partial f(x_j)}{\partial \theta_i} \big)
\]</span></p>
<p><span class="math display">\[ 
\sum_{j=1}^{n} \bigg(
\frac{1}{f(x_j)} \sum_{a=1}^{k} \frac{\partial }{\partial \theta_i}
\big( f(x_j;\sigma_a,\Sigma_a)P(C_a) \big)
\bigg)
\]</span></p>
<p><span class="math display">\[ 
\sum_{j=1}^{n} \bigg(
\frac{1}{f(x_j)} \cdot \frac{\partial }{\partial \theta_i}
\big( f(x_j;\sigma_i,\Sigma_i)P(C_i) \big)
\bigg)
\]</span></p>
<p>En son adım mümkün çünkü <span class="math inline">\(\theta_i\)</span> parametresi <span class="math inline">\(i\)</span>'inci kümeye (Gaussian'a) ait, ve diğer kümelerin bakış açısına göre (onlara göre kısmi türev alınınca) bu parametre sabit sayılıyor.</p>
<p>Şimdi <span class="math inline">\(|\Sigma_i| = \frac{1}{|\Sigma^{-1}|}\)</span> eşitliğinden hareketle (1)'deki çok boyutlu Gaussian'ı şöyle yazabiliriz,</p>
<p><span class="math display">\[ 
f(x_j;\sigma_i,\Sigma_i) = (2\pi)^{-d/2} |\Sigma^{-1}|^{1/2} 
\exp \big[ g(\mu_i,\Sigma_i) \big]
\]</span> ki</p>
<p><span class="math display">\[ g(\mu_i,\Sigma_i) = -\frac{1}{2}(x_j-\mu_i)^T\Sigma_i^{-1}(x_j-\mu_i) \]</span></p>
<p>Yani log-olurluk fonksiyonunun türevi şu şekilde yazılabilir,</p>
<p><span class="math display">\[ 
\frac{\partial L}{\partial \theta_i} =
\sum_{j=1}^{n} \bigg(
\frac{1}{f(x_j)} \frac{\partial }{\partial \theta_i} \big(
(2\pi)^{-d/2} |\Sigma_i^{-1}|^{1/2} \exp\big[ 
g(\mu_i,\Sigma_i) \big] P(C_i)
\big) \bigg)
\qquad (3)
\]</span></p>
<p><span class="math inline">\(\mu_i\)</span> için maksimum-olurluk kestirme hesabı yapmak için log olurluğun <span class="math inline">\(\theta_i=\mu_i\)</span>'a göre türevini almamız gerekiyor. Üstteki formülde gördüğümüz gibi <span class="math inline">\(\mu_i\)</span>'a bağlı olan tek terim <span class="math inline">\(\exp\big[ g(\mu_i,\Sigma_i) \big]\)</span>. Şimdi</p>
<p><span class="math display">\[ 
\frac{\partial }{\partial \theta_i} \exp \big[ g(\mu_i,\Sigma_i) \big] =
\exp \big[ g(\mu_i,\Sigma_i) \big] \cdot 
\frac{\partial }{\partial \theta_i} g(\mu_i,\Sigma_i)
\qquad (2)
\]</span></p>
<p>ve <span class="math display">\[ \frac{\partial }{\partial \mu_i}g(\mu_i,\Sigma_i) = 
\Sigma_i^{-1}(x_j-\mu_i)
\]</span></p>
<p>formüllerini kullanarak log olurluğun <span class="math inline">\(\mu_i\)</span>'ya göre türevi</p>
<p><span class="math display">\[ \frac{\partial L}{\partial \mu_i} = 
\sum_{j=1}^{n} \bigg(
\frac{1}{f(x_j)} 
(2\pi)^{-d/2} |\Sigma_i^{-1}|^{1/2} \exp\big[ 
g(\mu_i,\Sigma_i) \big] P(C_i) \Sigma^{-1} 
(x_j-\mu_i)
\bigg)
\]</span></p>
<p><span class="math display">\[ 
= \sum_{j=1}^{n} \bigg(
\frac{f(x_j;\mu_i,\Sigma_i)P(C_i)}{f(x_j)} \cdot 
\Sigma_i^{-1} (x_j-\mu_i)
\bigg)
\]</span></p>
<p><span class="math display">\[ =
\sum_{j=1}^{n} w_{ij} \Sigma_i^{-1}(x_j-\mu_i)
\]</span></p>
<p>Üstteki forma erişmek için (2) ve alttaki formülü kullandık.</p>
<p><span class="math display">\[ P(C_i|x_j) = \frac{P(x_j|C_i)P(C_i)}{\sum_{a=1}^{k}P(x_j|C_a)P(C_a)}\]</span></p>
<p>ki bunun anlamı</p>
<p><span class="math display">\[ w_{ij} = P(C_i|x_j) = \frac{f(x_j;\mu_i,\Sigma_i)P(C_i)}{f(x_j)}\]</span></p>
<p>Üstteki kısmi türevi sıfıra eşitleyip çözer ve her iki tarafı <span class="math inline">\(\Sigma_i\)</span> ile çarparsak,</p>
<p><span class="math display">\[ \sum_{j=1}^{n} w_{ij} (x_j-\mu_i) = 0 \]</span></p>
<p>elde ederiz, bu demektir ki</p>
<p><span class="math display">\[ \sum_{j=1}^{n} w_{ij}x_j = \mu_i\sum_{j=1} w_{ij}  \]</span></p>
<p>o zaman</p>
<p><span class="math display">\[ \mu_i = \frac{\sum_{j=1}^{n} w_{ij}x_j}{\sum_{j=1}^{n} w_{ij}}\]</span></p>
<p>Kovaryans Matrisi <span class="math inline">\(\Sigma_i\)</span>'i Hesaplamak</p>
<p><span class="math inline">\(\Sigma_i\)</span> hesabı için (3) kısmi türevinin <span class="math inline">\(|\Sigma_i^{-1}|^{1/2} \exp(g(\mu_i,\Sigma_i))\)</span> üzerindeki çarpım kuralı (product rule) kullanılarak <span class="math inline">\(\Sigma_i^{-1}\)</span>'ye göre alınması gerekiyor.</p>
<p>Her kare matris <span class="math inline">\(A\)</span> için <span class="math inline">\(\frac{\partial |A|}{\partial A} = |A| \cdot (A^{-1})^T\)</span> olduğundan hareketle, <span class="math inline">\(|\Sigma_i^{-1}|^{1/2}\)</span>'nin <span class="math inline">\(\Sigma_i^{-1}\)</span>'ya göre türevi</p>
<p><span class="math display">\[ 
\frac{\partial |\Sigma_i^{-1}|^{1/2}}{\partial \Sigma_i^{-1}} = 
\frac{1}{2} \cdot |\Sigma_i^{-1}|^{-1/2} \cdot |\Sigma_i^{-1}| \cdot \Sigma_i  = 
\frac{1}{2} |\Sigma_i^{-1}|^{1/2} \cdot \Sigma_i
\]</span></p>
<p>Şimdi <span class="math inline">\(A \in \mathbb{R}^{d \times d}\)</span> ve vektörler <span class="math inline">\(a,b \in \mathbb{R}^d\)</span> için <span class="math inline">\(\frac{\partial }{\partial A}a^TAb = ab^T\)</span> olmasından hareketle (3)'teki <span class="math inline">\(\exp [g(\mu_i,\Sigma_i)]\)</span>'in <span class="math inline">\(\Sigma_i^{-1}\)</span> gore türevi,</p>
<p><span class="math display">\[ 
\frac{\partial }{\partial \Sigma^{-1}} \exp\big[ g(\mu_i,\Sigma_i)\big] = 
-\frac{1}{2} \exp \big[  g(\mu_i,\Sigma_i) (x_j-\mu_i)(x_j-\mu_i)^T \big]
\]</span></p>
<p>Üstteki ve iki üstteki formül üzerinde türev çarpım kuralını kullanırsak,</p>
<p><span class="math display">\[ 
\frac{\partial }{\partial \Sigma_i^{-1}} |\Sigma_i^{-1}|^{1/2} 
\exp\big[ g(\mu_i,\Sigma_i) \big] =
\]</span></p>
<p><span class="math display">\[ 
= \frac{1}{2} |\Sigma_i^{-1}|^{1/2}  \Sigma_i \exp\big[ g(\mu_i,\Sigma_i) \big]-
\frac{1}{2} |\Sigma_i^{-1}|^{1/2} \exp\big[ g(\mu_i,\Sigma_i) \big]
(x_j-\mu_i)(x_j-\mu_i)^T
\]</span></p>
<p><span class="math display">\[ 
= \frac{1}{2} \cdot  |\Sigma_i^{-1}|^{1/2} \cdot 
\exp\big[ g(\mu_i,\Sigma_i) \big] \big( 
\Sigma_i - (x_j-\mu_i)(x_j-\mu_i)^T
\big)
\]</span></p>
<p>Üstteki son formülü (3)'e sokarsak, <span class="math inline">\(\Sigma_i^{-1}\)</span>'e göre log olurluğun türevi</p>
<p><span class="math display">\[ 
\frac{\partial L}{\partial \Sigma_i^{-1}} =
\frac{1}{2} \sum_{j=1}^{n} \frac
{(2\pi)^{-d/2} |\Sigma_i^{-1}|^{1/2}\exp\big[ g(\mu_i,\Sigma_i) P(C_i) }{f(x_j)}
\big( \Sigma_i - (x_j-\mu_i)(x_j-\mu_i)^T \big)
\]</span></p>
<p><span class="math display">\[ =
\frac{1}{2} \sum_{j=1}^{n} \frac{f(x_j;\mu_i,\Sigma_i) P(C_i)}{f(x_j)} \cdot
\big( \Sigma_i - (x_j-\mu_i)(x_j-\mu_i)^T \big)
\]</span></p>
<p><span class="math display">\[ 
= \frac{1}{2} \sum_{j=1}^{n} w_{ij} 
\big( \Sigma_i - (x_j-\mu_i)(x_j-\mu_i)^T \big)
\]</span></p>
<p>Türevi sıfıra eşitlersek,</p>
<p><span class="math display">\[ \sum_{j=1}^{n} w_{ij} \big( \Sigma_i - (x_j-\mu_i)(x_j-\mu_i)^T \big) = 0 \]</span></p>
<p>olur, ve devam edersek alttaki sonucu elde ederiz,</p>
<p><span class="math display">\[ 
\Sigma_i = \frac{\sum_{j=1}^{n} w_{ij} (x_j-\mu_i)(x_j-\mu_i)^T}
{\sum_{j=1}^{n} w_{ij}}
\]</span></p>
<p>Karışım Ağırlıkları <span class="math inline">\(P(C_i)\)</span>'i Hesaplamak</p>
<p>Bu hesabı yapmak için (3) türevinin <span class="math inline">\(P(C_i)\)</span>'a göre alınması lazım fakat <span class="math inline">\(\sum_{a=1}^{k}P(C_a)=1\)</span> şartını zorlamak için Lagrange çarpanları tekniğini kullanmamız gerekiyor. Yani türevin alttaki gibi alınması lazım,</p>
<p><span class="math display">\[ 
\frac{\partial }{\partial P(C_i)} \bigg(
\ln L + \alpha \big( \sum_{a=1}^{k} P(C_a)-1 \big)
\bigg)
\]</span></p>
<p>Log olurluğun <span class="math inline">\(P(C_i)\)</span>'a göre kısmi türevi alınınca,</p>
<p><span class="math display">\[ 
\frac{\partial L}{\partial P(C_i)} = 
\sum_{j=1}^{n} \frac{f(x_j;\mu_i,\Sigma_i)}{f(x_j)}
\]</span></p>
<p>O zaman iki üstteki türevin tamamı şu hale gelir,</p>
<p><span class="math display">\[ 
\bigg( \sum_{j=1}^{n} \frac{f(x_j;\mu_i,\Sigma_i)}{f(x_j)} \bigg) + \alpha
\]</span></p>
<p>Türevi sıfıra eşitlersek ve her iki tarafı <span class="math inline">\(P(C_i)\)</span> ile çarparsak,</p>
<p><span class="math display">\[ 
\sum_{j=1}^{n} \frac{f(x_j;\mu_i,\Sigma_i) P(C_i)}{f(x_j)} = -\alpha P(C_i)
\]</span></p>
<p><span class="math display">\[ 
\sum_{j=1}^{n} w_{ij} = -\alpha P(C_i)
\qquad (4)
\]</span></p>
<p>Üstteki toplamı tüm kümeler üzerinden alırsak</p>
<p><span class="math display">\[ 
\sum_{i=1}^{k} \sum_{j=1}^{n} w_{ij} = -\alpha \sum_{i=1}^{k} P(C_i)
\]</span></p>
<p>ya da <span class="math inline">\(n = -\alpha\)</span>.</p>
<p>Son adım <span class="math inline">\(\sum_{i=1}^{k}w_{ij}=1\)</span> sayesinde mümkün oldu. <span class="math inline">\(n = -\alpha\)</span>'yi (4) içine sokunca <span class="math inline">\(P(C_i)\)</span>'in maksimum olurluk hesabını elde ediyoruz,</p>
<p><span class="math display">\[ P(C_i) = \frac{\sum_{j=1}^{n}w_{ij}}{n}\]</span></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal <span class="im">as</span> mvn
<span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> numpy.linalg <span class="im">as</span> linalg
<span class="im">import</span> math, random, copy, sys
<span class="im">import</span> scipy.stats

<span class="kw">class</span> Cov_problem(<span class="pp">Exception</span>): <span class="cf">pass</span>

<span class="kw">def</span> norm_pdf(b,mean,cov):
   k <span class="op">=</span> b.shape[<span class="dv">0</span>]
   part1 <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>k<span class="op">*</span>np.log(<span class="dv">2</span><span class="op">*</span>np.pi))
   part2 <span class="op">=</span> np.power(np.linalg.det(cov),<span class="op">-</span><span class="fl">0.5</span>)
   dev <span class="op">=</span> b<span class="op">-</span>mean
   part3 <span class="op">=</span> np.exp(<span class="op">-</span><span class="fl">0.5</span><span class="op">*</span>np.dot(np.dot(dev.transpose(),np.linalg.inv(cov)),dev))
   dmvnorm <span class="op">=</span> part1<span class="op">*</span>part2<span class="op">*</span>part3
   <span class="cf">return</span> dmvnorm

<span class="kw">def</span> gm_log_likelihood(X, center_list, cov_list, p_k):
    <span class="co">&quot;&quot;&quot;Finds the likelihood for a set of samples belongin to a Gaussian mixture</span>
<span class="co">    model.</span>
<span class="co">    </span>
<span class="co">    Return log likelighood</span>
<span class="co">    &quot;&quot;&quot;</span>
    samples <span class="op">=</span> X.shape[<span class="dv">0</span>]
    K <span class="op">=</span>  <span class="bu">len</span>(center_list)
    log_p_Xn <span class="op">=</span> np.zeros(samples)
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):
        p <span class="op">=</span> logmulnormpdf(X, center_list[k], cov_list[k]) <span class="op">+</span> np.log(p_k[k])
        <span class="cf">if</span> k <span class="op">==</span> <span class="dv">0</span>:
            log_p_Xn <span class="op">=</span> p
        <span class="cf">else</span>:
            pmax <span class="op">=</span> np.<span class="bu">max</span>(np.concatenate((np.c_[log_p_Xn], np.c_[p]), axis<span class="op">=</span><span class="dv">1</span>), axis<span class="op">=</span><span class="dv">1</span>)
            log_p_Xn <span class="op">=</span> pmax <span class="op">+</span> np.log( np.exp( log_p_Xn <span class="op">-</span> pmax) <span class="op">+</span> np.exp( p<span class="op">-</span>pmax))
    logL <span class="op">=</span> np.<span class="bu">sum</span>(log_p_Xn)
    <span class="cf">return</span> logL

<span class="kw">def</span> gm_assign_to_cluster(X, center_list, cov_list, p_k):
    samples <span class="op">=</span> X.shape[<span class="dv">0</span>]
    K <span class="op">=</span> <span class="bu">len</span>(center_list)
    log_p_Xn_mat <span class="op">=</span> np.zeros((samples, K))
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):
        log_p_Xn_mat[:,k] <span class="op">=</span> logmulnormpdf(X, center_list[k], cov_list[k]) <span class="op">+</span> np.log(p_k[k])
    pmax <span class="op">=</span> np.<span class="bu">max</span>(log_p_Xn_mat, axis<span class="op">=</span><span class="dv">1</span>)
    log_p_Xn <span class="op">=</span> pmax <span class="op">+</span> np.log( np.<span class="bu">sum</span>( np.exp(log_p_Xn_mat.T <span class="op">-</span> pmax), axis<span class="op">=</span><span class="dv">0</span>).T)
    logL <span class="op">=</span> np.<span class="bu">sum</span>(log_p_Xn)
    
    log_p_nk <span class="op">=</span> np.zeros((samples, K))
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):
        log_p_nk[:,k] <span class="op">=</span> log_p_Xn_mat[:,k] <span class="op">-</span> log_p_Xn
    
    maxP_k <span class="op">=</span> np.c_[np.<span class="bu">max</span>(log_p_nk, axis<span class="op">=</span><span class="dv">1</span>)] <span class="op">==</span> log_p_nk
    maxP_k <span class="op">=</span> maxP_k <span class="op">*</span> (np.array(<span class="bu">range</span>(K))<span class="op">+</span><span class="dv">1</span>)
    <span class="cf">return</span> np.<span class="bu">sum</span>(maxP_k, axis<span class="op">=</span><span class="dv">1</span>) <span class="op">-</span> <span class="dv">1</span>

<span class="kw">def</span> logmulnormpdf(X, MU, SIGMA):
    <span class="cf">if</span> MU.ndim <span class="op">!=</span> <span class="dv">1</span>:
        <span class="cf">raise</span> <span class="pp">ValueError</span>, <span class="st">&quot;MU must be a 1 dimensional array&quot;</span>
    mu <span class="op">=</span> MU
    x <span class="op">=</span> X.T
    <span class="cf">if</span> x.ndim <span class="op">==</span> <span class="dv">1</span>:
        x <span class="op">=</span> np.atleast_2d(x).T
    sigma <span class="op">=</span> np.atleast_2d(SIGMA) <span class="co"># So we also can use it for 1-d distributions</span>
    N <span class="op">=</span> <span class="bu">len</span>(MU)
    ex1 <span class="op">=</span> np.dot(linalg.inv(sigma), (x.T<span class="op">-</span>mu).T)
    ex <span class="op">=</span> <span class="fl">-0.5</span> <span class="op">*</span> (x.T<span class="op">-</span>mu).T <span class="op">*</span> ex1
    <span class="cf">if</span> ex.ndim <span class="op">==</span> <span class="dv">2</span>: ex <span class="op">=</span> np.<span class="bu">sum</span>(ex, axis <span class="op">=</span> <span class="dv">0</span>)
    K <span class="op">=</span> <span class="op">-</span>(N<span class="op">/</span><span class="dv">2</span>)<span class="op">*</span>np.log(<span class="dv">2</span><span class="op">*</span>np.pi) <span class="op">-</span> <span class="fl">0.5</span><span class="op">*</span>np.log(np.linalg.det(SIGMA))
    <span class="cf">return</span> ex <span class="op">+</span> K

<span class="kw">def</span> gmm_init(X, K, verbose <span class="op">=</span> <span class="va">False</span>,
                    cluster_init <span class="op">=</span> <span class="st">&#39;sample&#39;</span>, <span class="op">\</span>
                    cluster_init_prop <span class="op">=</span> {}, <span class="op">\</span>
                    max_init_iter <span class="op">=</span> <span class="dv">5</span>, <span class="op">\</span>
                    cov_init <span class="op">=</span> <span class="st">&#39;var&#39;</span>):
    samples, dim <span class="op">=</span> np.shape(X)
    <span class="cf">if</span> cluster_init <span class="op">==</span> <span class="st">&#39;sample&#39;</span>:
        <span class="cf">if</span> verbose: <span class="bu">print</span> <span class="st">&quot;Using sample GMM initalization.&quot;</span>
        center_list <span class="op">=</span> []
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):
            center_list.append(X[np.random.randint(samples), :])
    <span class="cf">elif</span> cluster_init <span class="op">==</span> <span class="st">&#39;box&#39;</span>:
        <span class="cf">if</span> verbose: <span class="bu">print</span> <span class="st">&quot;Using box GMM initalization.&quot;</span>
        center_list <span class="op">=</span> []
        X_max <span class="op">=</span> np.<span class="bu">max</span>(X, axis<span class="op">=</span><span class="dv">0</span>)
        X_min <span class="op">=</span> np.<span class="bu">min</span>(X, axis<span class="op">=</span><span class="dv">0</span>)
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):
            init_point <span class="op">=</span> ((X_max<span class="op">-</span>X_min)<span class="op">*</span>np.random.rand(<span class="dv">1</span>,dim)) <span class="op">+</span> X_min
            center_list.append(init_point.flatten())            
    <span class="cf">elif</span> cluster_init <span class="op">==</span> <span class="st">&#39;kmeans&#39;</span>:
        <span class="cf">if</span> verbose: <span class="bu">print</span> <span class="st">&quot;Using K-means GMM initalization.&quot;</span>
        <span class="co"># Normalize data (K-means is isotropic)</span>
        normalizerX <span class="op">=</span> preproc.Normalizer(X)
        nX <span class="op">=</span> normalizerX.transform(X)
        center_list <span class="op">=</span> []
        best_icv <span class="op">=</span> np.inf
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_init_iter):
            m, kcc <span class="op">=</span> kmeans.kmeans(nX, K, <span class="bu">iter</span><span class="op">=</span><span class="dv">100</span>, <span class="op">**</span>cluster_init_prop)
            icv <span class="op">=</span> kmeans.find_intra_cluster_variance(X, m, kcc)
            <span class="cf">if</span> best_icv <span class="op">&gt;</span> icv:
                membership <span class="op">=</span> m
                cc <span class="op">=</span> kcc
                best_icv <span class="op">=</span> icv
        cc <span class="op">=</span> normalizerX.invtransform(cc)
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(cc.shape[<span class="dv">0</span>]):
            center_list.append(cc[i,:])
        <span class="bu">print</span> cc
    <span class="cf">else</span>:
        <span class="cf">raise</span> <span class="st">&quot;Unknown initialization of EM of MoG centers.&quot;</span>

    <span class="co"># Initialize co-variance matrices</span>
    cov_list <span class="op">=</span> []
    <span class="cf">if</span> cov_init<span class="op">==</span><span class="st">&#39;iso&#39;</span>:
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):
            cov_list.append(np.diag(np.ones(dim)<span class="op">/</span><span class="fl">1e10</span>))
            <span class="co">#cov_list.append(np.diag(np.ones(dim)))</span>
    <span class="cf">elif</span> cov_init<span class="op">==</span><span class="st">&#39;var&#39;</span>:
        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(K):
            cov_list.append(np.diag(np.var(X, axis<span class="op">=</span><span class="dv">0</span>)<span class="op">/</span><span class="fl">1e10</span>))
    <span class="cf">else</span>:
        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">&#39;Unknown option used for cov_init&#39;</span>)

    p_k <span class="op">=</span> np.ones(K) <span class="op">/</span> K <span class="co"># Uniform prior on P(k)</span>
    <span class="cf">return</span> (center_list, cov_list, p_k)


<span class="kw">def</span> em_gm(X, K, max_iter <span class="op">=</span> <span class="dv">50</span>, verbose <span class="op">=</span> <span class="va">False</span>, <span class="op">\</span>
                iter_call <span class="op">=</span> <span class="va">None</span>,<span class="op">\</span>
                delta_stop <span class="op">=</span> <span class="fl">1e-6</span>,<span class="op">\</span>
                init_kw <span class="op">=</span> {}, <span class="op">\</span>
                max_tries <span class="op">=</span> <span class="dv">10</span>,<span class="op">\</span>
                diag_add <span class="op">=</span> <span class="fl">1e-3</span>):

    samples, dim <span class="op">=</span> np.shape(X)
    clusters_found <span class="op">=</span> <span class="va">False</span>
    <span class="cf">while</span> clusters_found<span class="op">==</span><span class="va">False</span> <span class="kw">and</span> max_tries<span class="op">&gt;</span><span class="dv">0</span>:
        max_tries <span class="op">-=</span> <span class="dv">1</span>
        <span class="co"># Initialized clusters</span>
        center_list, cov_list, p_k <span class="op">=</span> gmm_init(X, K, <span class="op">**</span>init_kw)
        <span class="co"># Now perform the EM-steps:</span>
        <span class="cf">try</span>:
            center_list, cov_list, p_k, logL <span class="op">=</span> <span class="op">\</span>
                gmm_em_continue(X, center_list, cov_list, p_k,
                        max_iter<span class="op">=</span>max_iter, verbose<span class="op">=</span>verbose,
                        iter_call<span class="op">=</span>iter_call,
                        delta_stop<span class="op">=</span>delta_stop,
                        diag_add<span class="op">=</span>diag_add)
            clusters_found <span class="op">=</span> <span class="va">True</span>
        <span class="cf">except</span> Cov_problem:
            <span class="cf">if</span> verbose:
                <span class="bu">print</span> <span class="st">&quot;Problems with the co-variance matrix, tries left &quot;</span>, max_tries

    <span class="cf">if</span> clusters_found:
        <span class="cf">return</span> center_list, cov_list, p_k, logL
    <span class="cf">else</span>:
        <span class="cf">raise</span> Cov_problem()


<span class="kw">def</span> gmm_em_continue(X, center_list, cov_list, p_k,
                    max_iter <span class="op">=</span> <span class="dv">50</span>, verbose <span class="op">=</span> <span class="va">False</span>, <span class="op">\</span>
                    iter_call <span class="op">=</span> <span class="va">None</span>,<span class="op">\</span>
                    delta_stop <span class="op">=</span> <span class="fl">1e-6</span>,<span class="op">\</span>
                    diag_add <span class="op">=</span> <span class="fl">1e-3</span>,<span class="op">\</span>
                    delta_stop_count_end<span class="op">=</span><span class="dv">10</span>):
    <span class="co">&quot;&quot;&quot;</span>
<span class="co">    &quot;&quot;&quot;</span>
    delta_stop_count <span class="op">=</span> <span class="dv">0</span>
    samples, dim <span class="op">=</span> np.shape(X)
    K <span class="op">=</span> <span class="bu">len</span>(center_list) <span class="co"># We should do some input checking</span>
    <span class="cf">if</span> diag_add<span class="op">!=</span><span class="dv">0</span>:
        feature_var <span class="op">=</span> np.var(X, axis<span class="op">=</span><span class="dv">0</span>)
        diag_add_vec <span class="op">=</span> diag_add <span class="op">*</span> feature_var
    old_logL <span class="op">=</span> np.NaN
    logL <span class="op">=</span> np.NaN
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">xrange</span>(max_iter):
        <span class="cf">try</span>:
            center_list, cov_list, p_k, logL <span class="op">=</span> __em_gm_step(X, center_list,<span class="op">\</span>
                cov_list, p_k, K, diag_add_vec)
        <span class="cf">except</span> np.linalg.linalg.LinAlgError: <span class="co"># Singular cov matrix</span>
            <span class="cf">raise</span> Cov_problem()
        <span class="cf">if</span> iter_call <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:
            iter_call(center_list, cov_list, p_k, i)
        <span class="co"># Check if we have problems with cluster sizes</span>
        <span class="cf">for</span> i2 <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(center_list)):
            <span class="cf">if</span> np.<span class="bu">any</span>(np.isnan(cov_list[i2])):
                <span class="bu">print</span> <span class="st">&quot;problem&quot;</span>
                <span class="cf">raise</span> Cov_problem()

        <span class="cf">if</span> old_logL <span class="op">!=</span> np.NaN:
            <span class="cf">if</span> verbose:
                <span class="bu">print</span> <span class="st">&quot;iteration=&quot;</span>, i, <span class="st">&quot; delta log likelihood=&quot;</span>, <span class="op">\</span>
                    old_logL <span class="op">-</span> logL
            <span class="cf">if</span> np.<span class="bu">abs</span>(logL <span class="op">-</span> old_logL) <span class="op">&lt;</span> delta_stop: <span class="co">#* samples:</span>
                delta_stop_count <span class="op">+=</span> <span class="dv">1</span>
                <span class="cf">if</span> verbose: <span class="bu">print</span> <span class="st">&quot;gmm_em_continue: delta_stop_count =&quot;</span>, delta_stop_count
            <span class="cf">else</span>:
                delta_stop_count <span class="op">=</span> <span class="dv">0</span>
            <span class="cf">if</span> delta_stop_count<span class="op">&gt;=</span>delta_stop_count_end:
                <span class="cf">break</span> <span class="co"># Sufficient precision reached</span>
        old_logL <span class="op">=</span> logL
    <span class="cf">try</span>:
        gm_log_likelihood(X, center_list, cov_list, p_k)
    <span class="cf">except</span> np.linalg.linalg.LinAlgError: <span class="co"># Singular cov matrix</span>
        <span class="cf">raise</span> Cov_problem()
    <span class="cf">return</span> center_list, cov_list, p_k, logL

<span class="kw">def</span> __em_gm_step(X, center_list, cov_list, p_k, K, diag_add_vec):
    samples <span class="op">=</span> X.shape[<span class="dv">0</span>]
    <span class="co"># New way of calculating the log likelihood:</span>
    log_p_Xn_mat <span class="op">=</span> np.zeros((samples, K))
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):
        log_p_Xn_mat[:,k] <span class="op">=</span> logmulnormpdf(X, center_list[k], cov_list[k]) <span class="op">+</span> np.log(p_k[k])
    pmax <span class="op">=</span> np.<span class="bu">max</span>(log_p_Xn_mat, axis<span class="op">=</span><span class="dv">1</span>)
    log_p_Xn <span class="op">=</span> pmax <span class="op">+</span> np.log( np.<span class="bu">sum</span>( np.exp(log_p_Xn_mat.T <span class="op">-</span> pmax), axis<span class="op">=</span><span class="dv">0</span>).T) <span class="co"># Maybe move this down</span>
    logL <span class="op">=</span> np.<span class="bu">sum</span>(log_p_Xn)

    log_p_nk <span class="op">=</span> np.zeros((samples, K))
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):
        log_p_nk[:,k] <span class="op">=</span> log_p_Xn_mat[:,k] <span class="op">-</span> log_p_Xn

    p_Xn <span class="op">=</span> np.e<span class="op">**</span>log_p_Xn
    p_nk <span class="op">=</span> np.e<span class="op">**</span>log_p_nk
       
    <span class="co"># M-step:    </span>
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):
        ck <span class="op">=</span> np.<span class="bu">sum</span>(p_nk[:,k] <span class="op">*</span> X.T, axis <span class="op">=</span> <span class="dv">1</span>) <span class="op">/</span> np.<span class="bu">sum</span>(p_nk[:,k])
        center_list[k] <span class="op">=</span> ck
        cov_list[k] <span class="op">=</span> np.dot(p_nk[:,k] <span class="op">*</span> ((X <span class="op">-</span> ck).T), (X <span class="op">-</span> ck)) <span class="op">/</span> <span class="bu">sum</span>(p_nk[:,k])
        p_k[k] <span class="op">=</span> np.<span class="bu">sum</span>(p_nk[:,k]) <span class="op">/</span> samples

    <span class="cf">return</span> (center_list, cov_list, p_k, logL)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">data <span class="op">=</span> np.loadtxt(<span class="st">&#39;biometric_data_simple.txt&#39;</span>,delimiter<span class="op">=</span><span class="st">&#39;,&#39;</span>)
data <span class="op">=</span> data[:,<span class="dv">1</span>:<span class="dv">3</span>]
<span class="im">import</span> em
mc <span class="op">=</span> [<span class="fl">0.4</span>, <span class="fl">0.4</span>, <span class="fl">0.2</span>] 
centroids <span class="op">=</span> [ np.array([<span class="dv">0</span>,<span class="dv">0</span>]), np.array([<span class="dv">3</span>,<span class="dv">3</span>]), np.array([<span class="dv">0</span>,<span class="dv">4</span>]) ]
ccov <span class="op">=</span> [ np.array([[<span class="dv">1</span>,<span class="fl">0.4</span>],[<span class="fl">0.4</span>,<span class="dv">1</span>]]), np.diag((<span class="dv">1</span>,<span class="dv">2</span>)), np.diag((<span class="fl">0.4</span>,<span class="fl">0.1</span>)) ]
cen_lst, cov_lst, p_k, logL <span class="op">=</span> em.em_gm(data, K <span class="op">=</span> <span class="dv">2</span>, max_iter <span class="op">=</span> <span class="dv">400</span>)
<span class="cf">for</span> cen <span class="kw">in</span> cen_lst: <span class="bu">print</span> cen
<span class="cf">for</span> cov <span class="kw">in</span> cov_lst: <span class="bu">print</span> cov</code></pre></div>
<pre><code>[  66.22733783  135.69250285]
[  72.92994695  194.55997484]
[[  14.62653617   53.38371315]
 [  53.38371315  414.95573112]]
[[    7.77047547    24.7439079 ]
 [   24.7439079   1369.68034031]]</code></pre>
<p>Kod <code>biometric_data_simple.txt</code> verisi üzerinde işletildiğinde rapor edilen <span class="math inline">\(\mu,\Sigma\)</span> değerlerini grafikleyince başta paylaştığımız grafik görüntüleri çıkacaktır, yani kümeleme başarıyla işletilmiştir.</p>
<p>En İyi K Nasıl Bulunur</p>
<p>Bu sayıyı keşfetmek artık kolay; K-Means ile atılan bir sürü taklaya, ki çoğu gayrı matematiksel, sezgisel, uydurulmuş (heuristic) yöntemlerdi, artık gerek yok. Mesela 10 ila 30 arasındaki tüm küme sayılarını deneriz, ve en iyi AIC vereni seçeriz.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
ff <span class="op">=</span> <span class="st">&#39;../../app_math/kmeans/synthetic.txt&#39;</span>
df <span class="op">=</span> pd.read_csv(ff,comment<span class="op">=</span><span class="st">&#39;#&#39;</span>,names<span class="op">=</span>[<span class="st">&#39;a&#39;</span>,<span class="st">&#39;b&#39;</span>],sep<span class="op">=</span><span class="st">&quot;   &quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.mixture <span class="im">import</span> GMM
<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>,<span class="dv">30</span>):
   g <span class="op">=</span> GMM(n_components<span class="op">=</span>i).fit(df)
   <span class="bu">print</span> i, <span class="st">&#39;clusters&#39;</span>, g.aic(df)</code></pre></div>
<pre><code>10 clusters 124325.897319
11 clusters 124132.382945
12 clusters 123931.508911
13 clusters 123865.913489
14 clusters 123563.524338
15 clusters 123867.79925
16 clusters 123176.509776
17 clusters 123239.708813
18 clusters 123019.873822
19 clusters 122728.247239
20 clusters 122256.554363
21 clusters 122259.954752
22 clusters 122271.805211
23 clusters 122265.886637
24 clusters 122265.344662
25 clusters 122277.924153
26 clusters 122184.54412
27 clusters 122356.971927
28 clusters 122195.916167
29 clusters 122203.347265</code></pre>
<p>Görüldüğü gibi AIC azalıyor, azalıyor, ve K=20'de azıcık artıyor, sonra 25'e kadar artmaya devam ediyor, sonra tekrar düşmeye başlıyor ama bizi ilgilendiren uzun süreli düşüşten sonraki bu ilk çıkış. O nokta optimal K değerini verecektir, ki bu sayı 20.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.mixture <span class="im">import</span> GMM
g <span class="op">=</span> GMM(n_components<span class="op">=</span><span class="dv">20</span>).fit(df)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">plt.scatter(df.a,df.b)
plt.hold(<span class="va">True</span>)
plt.plot(g.means_[:,<span class="dv">0</span>], g.means_[:,<span class="dv">1</span>],<span class="st">&#39;ro&#39;</span>)
plt.savefig(<span class="st">&#39;stat_gmm_03.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_gmm_03.png" />

</div>
<p>Gaussian Karışımları ile Deri Rengi Saptamak</p>
<p>Bir projemizde dijital resimlerdeki deri rengi içeren kısımları çıkartmamız gerekiyordu; çünkü fotoğrafın diğer renkleri ile ilgileniyorduk (resimdeki kişinin üzerindeki kıyafetin renkleri) ve bu sebeple deri renklerini ve o bölgeleri resimde saptamak gerekti. Bizim de önceden aklımızda kalan bir tembih vardı, Columbia Üniversitesi'nde yapay öğrenim dersi veren Tony Jebara derste paylaşmıştı bir kere (bu tür gayrı resmi, lakırdı seviyesinde tiyolar bazen çok faydalı olur), deri rengi bulmak için bir projesinde tüm deri renklerini R,G,B olarak grafiğe basmışlar, ve beyaz olsun, zenci olsun, ve sonuç grafikte deri renklerinin çok ince bir bölgede yanyana durduğunu görmüşler. İlginç değil mi?</p>
<p>Buradan şu sonuç çıkıyor ki diğer renklerin arasında deri renklerine odaklanan, onları &quot;tanıyan'' bir yapay öğrenim algoritmasının oldukça şansı vardır. Ama ondan önce veriye bakıp grafiksel olarak ne olduğunu görelim.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd, zipfile
<span class="cf">with</span> zipfile.ZipFile(<span class="st">&#39;skin.zip&#39;</span>, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> z:
    d <span class="op">=</span>  pd.read_csv(z.<span class="bu">open</span>(<span class="st">&#39;skin.csv&#39;</span>),sep<span class="op">=</span><span class="st">&#39;,&#39;</span>)
<span class="bu">print</span> d[:<span class="dv">3</span>]</code></pre></div>
<pre><code>   Unnamed: 0   rgbhex   skin         r         g         b         h  \
0           0  #200e08  False  0.125490  0.054902  0.031373  0.041667   
1           1  #6d6565  False  0.427451  0.396078  0.396078  0.000000   
2           2  #1f2c4d  False  0.121569  0.172549  0.301961  0.619565   

          s         v  
0  0.750000  0.125490  
1  0.073394  0.427451  
2  0.597403  0.301961  </code></pre>
<p>Burada önemli olan R,G,B ve H,S,V kolonları. Bu iki grup değişik renk kodlama yöntemini temsil ediyorlar. Grafikleyelim,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nd <span class="op">=</span> d[d[<span class="st">&#39;skin&#39;</span>] <span class="op">==</span> <span class="va">False</span>]
sd <span class="op">=</span> d[d[<span class="st">&#39;skin&#39;</span>] <span class="op">==</span> <span class="va">True</span>]
plt.plot(nd[<span class="st">&#39;r&#39;</span>],nd[<span class="st">&#39;g&#39;</span>],<span class="st">&#39;.&#39;</span>)
plt.hold(<span class="va">True</span>)
plt.plot(sd[<span class="st">&#39;r&#39;</span>],sd[<span class="st">&#39;g&#39;</span>],<span class="st">&#39;rx&#39;</span>)
plt.savefig(<span class="st">&#39;stat_gmm_01.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_gmm_01.png" />

</div>
<p>Ya da H,S üzerinden</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">nd <span class="op">=</span> d[d[<span class="st">&#39;skin&#39;</span>] <span class="op">==</span> <span class="va">False</span>]
sd <span class="op">=</span> d[d[<span class="st">&#39;skin&#39;</span>] <span class="op">==</span> <span class="va">True</span>]
plt.plot(nd[<span class="st">&#39;h&#39;</span>],nd[<span class="st">&#39;s&#39;</span>],<span class="st">&#39;.&#39;</span>)
plt.hold(<span class="va">True</span>)
plt.plot(sd[<span class="st">&#39;h&#39;</span>],sd[<span class="st">&#39;s&#39;</span>],<span class="st">&#39;rx&#39;</span>)
plt.savefig(<span class="st">&#39;stat_gmm_02.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_gmm_02.png" />

</div>
<p>Demek ki Jebara haklıymış. Veriye bakınca bir kabaca / sezgisel (intuitive) bazı çıkarımlar yapmak mümkün. Mesela her iki grafikte de deri renklerini belirten bölgenin grafiği sanki 3 boyutlu bir Gaussian'ın üstten görünen / kontur (contour) hali. Bunu bilmek bir avantaj, bu avantajı kullanmak lazım. Modelimiz gerçek dünya verisine ne kadar yakınsa, yapay öğrenim şansı o kadar fazlalaşacaktır. Eğer o bölgeye bir Gaussian uydurursak (fit) tanıma şansımız artacaktır.</p>
<p>O zaman deri rengi tanıma şu şekilde yapılabilir. Scikit Learn kütüphanesinin Gaussian Karışımları (GMM) paketini kullanabiliriz. Tek problem bu karışımlar olasılık fonksiyonunu öğreniyorlar, sınıflama (classification) yapmıyorlar. Önemli değil, şöyle bir ek kod ile bunu halledebiliriz; iki tane GMM yaratırız, bir tanesi deri renk bölgeleri için, diğeri diğer bölgeler için. Eğitim sırasında her iki GMM'i kendi bölgeleri üzerinde eğitiriz. Sonra, test zamanında, her yeni (bilinmeyen) veri noktasını her iki GMM'e veririz, hangisinden daha yüksek olasılık değeri geliyorsa, etiket değeri olarak o GMM'in değerini alırız.</p>
<p>GMM'leri, ve onların içindeki Gaussian'ların kovaryanslarını kullanmak faydalı, kovaryans bildiğimiz gibi bir Gaussian'ın hangi yönde daha fazla ağırlığının olacağını belirler, eğer kovaryans hesabı yapılmazsa, yani kovaryans matrisinin sadece çaprazında değerler varsa, mesela üç boyutta Gaussian'ın konturu bir çember olarak gözükür [1, sf 90]. Tabii her yönde aynı ağırlıkta olan bir Gaussian her türlü veriyi temsil edemez, en esneği (ki grafiğe bakınca bu gerekliliği görüyoruz) tam kovaryans kullanmaktır. Scikit Learn ile bu seçim GMM için <code>full</code> ile yapılır, sadece çaprazı kullan anlamına gelen <code>diag</code> da olabilirdi.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> zipfile
<span class="im">from</span> sklearn.cross_validation <span class="im">import</span> train_test_split
<span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, auc
<span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score
<span class="im">from</span> sklearn.mixture <span class="im">import</span> GMM
<span class="im">import</span> pandas <span class="im">as</span> pd

<span class="kw">class</span> GMMClassifier():
   <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,k,var):
       <span class="va">self</span>.clfs <span class="op">=</span> [GMM(n_components<span class="op">=</span>k,
                    covariance_type<span class="op">=</span>var,thresh<span class="op">=</span><span class="fl">0.1</span>, 
                    min_covar<span class="op">=</span><span class="fl">0.0001</span>,n_iter<span class="op">=</span><span class="dv">100</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>)]

   <span class="kw">def</span> fit(<span class="va">self</span>,X,y):
       <span class="va">self</span>.clfs[<span class="dv">0</span>].fit(X[y<span class="op">==</span><span class="dv">0</span>])
       <span class="va">self</span>.clfs[<span class="dv">1</span>].fit(X[y<span class="op">==</span><span class="dv">1</span>])

   <span class="kw">def</span> predict(<span class="va">self</span>,X):
       res0 <span class="op">=</span> <span class="va">self</span>.clfs[<span class="dv">0</span>].score(X)
       res1 <span class="op">=</span> <span class="va">self</span>.clfs[<span class="dv">1</span>].score(X)
       res <span class="op">=</span> (res1 <span class="op">&gt;</span> res0)
       <span class="cf">return</span> res.astype(<span class="bu">float</span>)

<span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>: 
 
   <span class="cf">with</span> zipfile.ZipFile(<span class="st">&#39;skin.zip&#39;</span>, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> z:
      df <span class="op">=</span>  pd.read_csv(z.<span class="bu">open</span>(<span class="st">&#39;skin.csv&#39;</span>),sep<span class="op">=</span><span class="st">&#39;,&#39;</span>)
   y <span class="op">=</span> (df[<span class="st">&#39;skin&#39;</span>] <span class="op">==</span> <span class="va">True</span>).astype(<span class="bu">float</span>)
   X <span class="op">=</span> df[[<span class="st">&#39;h&#39;</span>,<span class="st">&#39;s&#39;</span>,<span class="st">&#39;v&#39;</span>,<span class="st">&#39;r&#39;</span>,<span class="st">&#39;g&#39;</span>]]
   
   res <span class="op">=</span> []   
   <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):
      clf <span class="op">=</span> GMMClassifier(k<span class="op">=</span><span class="dv">10</span>,var<span class="op">=</span><span class="st">&#39;full&#39;</span>)
      x_train, x_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="dv">2000</span>)
      clf.fit(x_train,y_train)
      preds <span class="op">=</span> clf.predict(x_test)

      fpr, tpr, thresholds <span class="op">=</span> roc_curve(y_test, preds)
      roc_auc <span class="op">=</span> auc(fpr, tpr)
      res.append(roc_auc)

   <span class="bu">print</span> <span class="st">&#39;deneyler&#39;</span>
   <span class="bu">print</span> res
   <span class="bu">print</span> <span class="st">&#39;nihai ortalama&#39;</span>, np.array(res).mean()</code></pre></div>
<pre><code>deneyler 
[0.99075081610446136, 0.98417442945172173, 0.98641291695170819,
 0.98779826464208242, 0.99239130434782608] 
nihai ortalama 0.9883055463</code></pre>
<p>Başarı oranı yüzde 98.8! Bu problem üzerinde pek çok diğer yöntem denedik, mesela KNN sınıflayıcı, Lojistik Regresyon, vs. gibi, bu yöntem tüm diğerlerini geçti.</p>
<p>İlginç bir yan bir soru, &quot;hangi kolonların kullanılacağı''. Bu bağlamda projede arkadaşlardan &quot;ama HSV değerleri RGB değerlerinden türetilebiliyor, ya birini ya ötekini kullanmak yeterli olmaz mı?'' yorumu yapanlar oldu. Evet, bu verinin diğerinden &quot;türetilmiş'' olduğu doğru, ve beklenir ki ideal bir dünyada mükemmel bir yapay öğrenim algoritmasının bu tür bir yardıma ihtiyacı olmaz, algoritma o kadar iyidir ki ona sanki aynı veriyi tekrar vermiş gibi oluruz, en iyi ihtimalle ek külfet yaratırız. Fakat pratikte bu ek veri algoritmaya ek bazı sinyaller verebilir. Mesela eğer müşterilerin kilosu üzerinden bir öğrenim yapıyor olsaydık, 80 kilodan daha az ya da daha fazla olmayı (problem alanına göre) ayrı bir kolon olarak kodlamak avantaj getirebilirdi. Tabii ki kilo verisi sayısal değer olarak azıyla fazlasıyla oradadır, fakat önem verdiğimiz noktaları türetilmiş veri olarak öğrenim algoritmasına vermenin zararı yoktur. Üstteki örnekte GB değerlerinin HSV ile beraber kullanılmasının başarı şansını biraz daha arttırdığını görebiliriz.</p>
<p>Kaynaklar</p>
<p>[1] Alpaydin, E., <em>Introduction to Machine Learning</em></p>
<p>[2] Jebara, T., <em>Columbia Machine Learning Course</em></p>
<p>[3] Aaron A. D'Souza, {}, <a href="http://www-clmc.usc.edu/~adsouza/notes/mix_gauss.pdf" class="uri">http://www-clmc.usc.edu/~adsouza/notes/mix_gauss.pdf</a></p>
<p>[4] <em>Expectation-Maximization (Python Recipe)</em>, <a href="http://code.activestate.com/recipes/577735-expectation-maximization" class="uri">http://code.activestate.com/recipes/577735-expectation-maximization</a></p>
<p>[5] Zaki, <em>Data Mining and Analysis: Fundamental Concepts and Algorithms</em></p>
<p>[6] Bayramlı, Istatistik, <em>Çok Değişkenli Bernoulli Karışımı</em></p>
</body>
</html>
