\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Ekler

Binom ve $p$ Ýçin Maksimum Olurluk Tahmini [1]

$$ L(p;x) = \prod_{i=1}^n f(x_i;p) = \prod_{i=1}^n {n \choose x} p^x(1-p)^{1-x} $$

Log alalým

$$ \log L(p;x) = 
\sum_{i=1}^n \log {n \choose x} + x \log p + (1-x) \log (1-p) $$

$p$'ye göre türevi alalým, bu sýrada kombinasyon ifadesi ${n \choose x}$
içinde $p$ olmadýðý için o yokolacaktýr,

$$ \frac{\partial \log L(p)}{\partial p} =
\frac{x}{p} - \frac{n-x}{1-p}
$$

Maksimum deðeri bulmak için sýfýra eþitleyelim ve $p$ için çözelim,

$$ 0 = \frac{x}{p} - \frac{n-x}{1-p} $$

$$  \frac{x}{p} = \frac{n-x}{1-p}  $$

$$ p(n-x)  = x(1-p) $$

$$ pn - px = x-px $$

$$ pn = x $$

$$ p = \frac{x}{n} $$

Yani $p$ için maksimum olurluk tahmini $x/n$. 

Bernoulli daðýlýmý Binom daðýlýmýna çok benzer, sadece onun baþ kýsmýnda
kombinasyon ifadesi yoktur. Fakat o ifade $p$'ye göre türevde nasýl olsa
yokolacaðýna göre Bernoulli daðýlýmý için de tahmin edici aynýdýr.

\newpage

Bayes Usulü Güven Aralýðý (Confidence Intervals) 

Bayes ile bu hesabý yapmak için bir daðýlýmý baz almak lazým. Eðer sonuç
olarak bir tek sayý deðil, bir daðýlým elde edersek bu daðýlým üzerinde
güvenlik hesaplarýný yaparýz. Mesela sonuç, sonsal daðýlým (posterior) bir
Gaussian daðýlým ise, bu daðýlýmýn yüzde 95 aðýrlýðýnýn nerede olduðu, ve
nasýl hesaplandýðý bellidir.

Bayes Teorisi

$$ P(A \mid B)  = \frac{P(B \mid A)P(A)}{P(B)} $$

Veri analizi baðlamýnda diyelim ki deneyler yaparak tahmini olarak
hesaplamak (estimate) istediðimiz bir parametre var, bu bir protonun
kütlesi ya da bir ameliyat sonrasý hayatta kalma oraný olabilir. Bu
durumlarda iki ayrý "olaydan" bahsetmemiz gerekir, B olayý spesifik bazý
ölçümlerin elde edilmesi "olayýdýr", mesela ölçüm üç sayýdan oluþuyorsa,
biz bir ölçümde spesifik olarak $\{0.2,4,5.4\}$ deðerlerini elde
etmiþiz. Ýkinci olay bilmediðimiz parametrenin belli bir deðere sahip
olmasý olacak. O zaman Bayes Teorisinin þu þekilde tekrar yazabiliriz, 

$$ P(parametre \mid veri ) \propto P(veri \mid parametre)P(parametre) $$

$\propto$ iþareti orantýlý olmak (proportional to) anlamýna geliyor. Böleni
attýk çünkü o bir sabit (tamamen veriye baðlý, tahmini hesaplamak
istediðimiz parametreye baðlý deðil). Tabii bu durumda sol ve sað taraf
birbirine eþit olmaz, o yüzden eþitlik yerine orantýlý olmak iþaretini
kullandýk. Bu çerçevede "belli bir sayýsal sabit çerçevesinde birbirine
eþit (equal within a numeric constant)" gibi cümleler de görülebilir. 

Örnek

Diyelim ki bir bozuk para ile 10 kere yazý-tura attýk, ve sonuç altta

T H H H H T T H H H

Bu veriye bakarak paranýn hileli olup olmadýðýný anlamaya
çalýþacaðýz. Bayes ifadesini bu veriye göre yazalým,

$$ P(p | \{ \textrm{T H H H H T T H H H} \} \propto 
P(\{ \textrm{T H H H H T T H H H} | p) P(p) \}
$$

$P(p)$ ifadesi ne anlama gelir? Aslýnda bu ifadeyi $P([Dagilim] = p)$ olarak
görmek daha iyi, artýk $p$ parametresini bir daðýlýmdan gelen bir özgün deðer
olarak gördüðümüze göre, o daðýlýmýn belli bir $p$'ye eþit olduðu zamaný
modelliyoruz burada. Her halükarda $P(p)$ daðýlýmýný, yani onsel (prior)
olasýlýðý bilmiyoruz, hesaptan önce her deðerin mümkün olduðunu biliyoruz, o
zaman bu onsel daðýlýmý düz (flat) olarak alýrýz, yani $P(p) = 1$.

$P(\{\textrm{T H H H H T T H H H} | p)$ ifadesi göz korkutucu olabilir, ama
buradaki her öðenin baðýmsýz özdeþçe daðýlmýþ (independent identically
distributed) olduðunu görürsek, ama bu ifadeyi ayrý ayrý
$P(\{\textrm{T}|p)$ ve $P(\{\textrm{H}|p)$ çarpýmlarý olarak
görebiliriz. $P(\{\textrm{T}|p) = p$ ve $P(\{\textrm{H}|p)=1-p$ olduðunu
biliyoruz. O zaman

$$ P(p | \{ \textrm{7 Tura, 3 Yazý} \} \propto
p^7(1-p)^3
$$

Grafiklersek, 

\includegraphics[height=6cm]{stat_appendix_01.png}

Böylece $p$ için bir sonsal daðýlým elde ettik. Artýk bu daðýlýmýn yüzde 95
aðýrlýðýnýn nerede olduðunu rahatça görebiliriz /
hesaplayabiliriz. Daðýlýmýn tepe noktasýnýn $p=0.7$ civarýnda olduðu
görülüyor. Bir daðýlýmla daha fazlasýný yapmak ta mümkün, mesela bu
fonksiyonu $p$'ye baðlý baþka bir fonksiyona karþý entegre etmek mümkün,
mesela beklentiyi bu þekilde hesaplayabiliriz.

Onsel daðýlýmýn her noktaya eþit aðýrlýk veren birörnek (uniform) seçilmiþ
olmasý, yani problemi çözmeye sýfýr bilgiden baþlamýþ olmamýz, yöntemin bir
zayýflýðý olarak görülmemeli. Yöntemin kuvveti elimizdeki bilgiyle baþlayýp
onu net bir þekilde veri ve olurluk üzerinden sonsal tek daðýlýma
götürebilmesi. Baþlangýç ve sonuç arasýndaki baðlantý gayet net. Fazlasý da
var; ilgilendiðimiz alaný (domain) öðrendikçe, baþta hiç bilmediðimiz onsel
daðýlýmý daha net, bilgili bir þekilde seçebiliriz ve bu sonsal daðýlýmý da
daha olmasý gereken modele daha yaklaþtýrabilir. 

\newpage

Moment 

Olasýlýk matematiðinde "moment üreten iþlevler" olarak adlandýrýlan,
baþlangýçta pek yararlý gibi gözükmesede bir takým matematiksel
özellikleri olduðu için, ispatlarda oldukça iþe yarayan bir kavram
vardýr.

Her rasgele deðiþkenin bir daðýlýmý olduðunu biliyoruz. Her rasgele
deðiþkenin de ayrýca bir moment üreten fonksiyonu da vardýr. Ayrýca,
moment üreten fonksiyon ile rasgele deðiþken arasýnda bire-bir olarak
bir iliþki mevcuttur. "Bu neye yarar?" diye sorulabilir; Cevap olarak,
mesela cebirsel olarak türete türete bir moment'e geldiðimiz
düþünelim, ve tekrar baþka bir taraftan, baþka bir formülden gene
türete türete tekrar ayný moment iþlevine geliyorsak, bu demektir ki,
iki taraftan gelen rasgele deðiþkenler (ve tekabül eden daðýlýmlarý)
birbirine eþittir. Bazý þartlarda moment üreten iþlevler ile cebir
yapmak, daðýlým fonksiyonlarýndan daha rahat olmaktadýr.

Her rasgele deðiþken için, moment üreten iþlev þöyle bulunur.

$X$ rasgele degiskenin moment ureten operasyonu

$M(t)=E(e^{tX})$ olarak gösterilir

Ayrýksal operasyonlar için

$$ M(t) = \sum_x e^{tx}p(x) $$

Sürekli iþlevler için

$$ M(t) = \int_{-\infty}^{\infty} e^{tx}f(x) \ud x   $$

Kuram

Gelelim yazýmýzýn esas konusu olan kuramýmýza.

Eðer $X_1, X_2...X_n$ baðýmsýz rasgele deðiþken ise, ve her deðiþkenin
$M_i(t)$ $i=1,2,3,...n$ olarak, öz olarak ayný olan birer moment üreten
iþlevi var ise, o zaman,

$$ Y = \sum_{i=1}^n  aX_i $$

açýlýmý

$$ M_y(t) = \prod_{i=1}^n M(a_i t) $$

olacaktýr. 

Ýspat

$$ M_y(t) = E(e^{tY}=E(e^{t(a_1X_1+a_2X_2+..+a_nX_n)} $$

$$ = E[\exp(ta_1 X_1 ta_2X_2...+ta_nX_n)] $$

$$ = E[\exp(ta_1X_1)+\exp(ta_2X_2)+ ... + \exp(ta_nX_n)] $$

$$ = E[\exp(ta_1X_1)]+E[\exp(ta_2X_2)]+ ... + E[\exp(ta_nX_n)]$$

Daha önce belirttiðimiz gibi

$$ M_i(t) = E[\exp(tX_i)] $$

olduðuna göre ve $t$ yerine $ta_i$ koyulduðunu düþünelim

$$ M_y(t) = \prod_{i=1}^n M_y(a_it) $$

olacaktýr. 

Bunu $M_y(t)= (M_i(a_it))^n$ þeklinde de gösterebiliriz. 

\newpage

Markov'un Eþitsizliði (Markov's Inequality)

$X$ bir negatif olmayan rasgele deðiþken olsun ve farz edelim ki $E(X)$
mevcut [1]. O zaman her $t > 0$ için

$$ P(X>t) \le \frac{E(X)}{t}$$

doðru olmalýdýr. 

Ýspat

$X > 0$ olduðuna göre, 

$$ 
E(X) 
= \int _{0}^{\infty} x f(x) \ud x 
= \int _{0}^{t} x f(x) \ud x + \int _{t}^{\infty} x f(x) \ud x =
$$

$$ 
\ge \int _{t}^{\infty} x f(x) \ud x \ge t \int _{t}^{\infty} f(x) \ud x
= t P(X > t)
$$

Çebiþev Eþitsizliði (Chebyshev's Inequality)

Herhangi bir $t$ deðeri için, 

$$ P(|X-\mu| > t) \le \frac{\sigma^2}{t^2} $$

ve 

$$ P(|Z| \ge k) \le \frac{1}{k^2}$$

ki $Z = (X-\mu)/\sigma$, ve $E(X) = \mu$. Bunun bazý akýlda kalabilecek
ilginç sonuçlarý $P(|Z| > 2) < 1/4$ ve $P(|Z| > 3) < 1/9$ olabilir.

Ýspat

1. Yöntem

Üstteki Markov'un eþitsizliðini kullanýrýz, oradan þu sonuca varýrýz, 

$$ 
P(|X-\mu| \ge t) = P(|X-\mu|^2 \ge t^2 ) \le \frac{E(X-\mu)^2}{t^2} 
= \frac{\sigma^2}{t^2}  
$$

Ýkinci kýsým $t=k\sigma$ kullanýlarak elde edilebilir.

2. Yöntem

Olasýlýk matematiðinde, büyük sayýlar kuramý adýnda anýlan ve olasýlýk
matematiðinin belkemiðini oluþturan kuramý ispatlamak için, diðer bir kuram
olan Çebiþev eþitsizliðini de anlamamýz gerekiyor. Çebiþev eþitsizliði bir
rasgele deðiþken, onun ortalamasý (beklentisi) ve herhangi bir sabit sayý
arasýndaki üçlü arasýnda bir 'eþitsizlik' baðlantýsý kurar, ve bu baðlantý
diðer olasýlýk iþlemlerimizde ispat verisi olarak iþimize yarar.



Ýspata baþlayalým. Entegral ile olasýlýk hesabý yapmak için bize bir $x$
uzayý lazým.

$$ \mathbb{R} = {x: |x-\mu| > t} $$

Yani $\mathbb{R}$ uzayý, $x$ ile ortalamasýnýn farkýnýn, $t$'den büyük olduðu bütün
sayýlarýn kümesidir.

O zaman, 

$$ P(|X-\mu| > t) = \int_R f(x) \ud x $$

Dikkat edelim $P(..)$ içindeki formül, küme tanýmý ile ayný. O yüzden $P()$
hesabý ortada daha olmayan, ama varolduðu kesin bir daðýlým fonksiyonu
tanýmlamýþ da oluyor. Buna $f(x)$ deriz. $P()$'in, $f(x)$ fonksiyonunun $R$
üzerinden entegral olduðunu olasýlýða giriþ dersinden bilmemiz lazým. 

Eger $x \in R$ dersek o zaman

$$ \frac{|x-\mu|^2}{t^2} \ge 1 $$

t'nin denkleme bu þekilde nereden geldiði þaþkýnlýk yaratabilir. Daha önce
tanýmlanan þu ibareye dikkat edelim, $x: |x-u| > t$ diye belirtmiþtik. Bu
ifadeyi deðiþtirerek, yukarýdaki denkleme gelebiliriz.

Devam edersek, elimizdeki 1'den büyük bir deðer var. Bu deðeri kullanarak,
aþaðýdaki tanýmý yapmamýz doðru olacaktýr.

$$
\int_R f(x) \ud x \le \int_R \frac{(x-\mu)^2}{t^2}f(x) \ud x \le
\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x) \ud x 
$$

Ortadaki entegral niye birinci entegralden büyük? Çünkü orta entegraldeki
$f(x)dx$ ibaresinden önce gelen kýsmýn, her zaman 1'den büyük olacaðýný
belirttiðimize göre, ikinci entegralin birinciden büyük olmasý normaldir,
çünkü birinci entegral $f(x)$ olasýlýk daðýlýmýna baðlý, entegral ise bir
alan hesabýdýr ve olasýlýk daðýlýmlarýnýn sonsuzlar arasýndaki entegrali
her zaman 1 çýkar, kaldý ki üstteki $x$'in uzayýný daha da daralttýk.

Evet...Üçüncü entegral ispata oldukça yaklaþtý aslýnda. Standart sapma
iþaretini hala ortada göremiyoruz, fakat son entegraldeki ibare standart
sapma deðerini zaten içeriyor. Önce daha önceki olasýlýk natematiði
bilgimize dayanarak, standart sapmanýn tanýmýný yazýyoruz. Dikkat edelim,
bu ibare þu anki ispatýmýz dahilinden deðil, haricinden önceki bilgimize
dayanarak geldi. Standart sapmanýn tanýmý þöyledir.

$$ \sigma^2 = \int_{-\infty}^{\infty} (x-\mu)^2f(x) \ud x $$

O zaman

$$
\frac{\sigma^2}{t^2}
= \int_{-\infty}^{\infty}\frac{(x-\mu)^2}{t^2}f(x)\ud x
$$

yani

$$
\int_R f(x) \ud x \le \frac{\sigma^2}{t^2} = 
\int_{-\infty}^{\infty} \frac{(x-\mu)^2}{t^2}f(x) \ud x
$$

ki $\int_R f(x) \ud x$ zaten $P(|X-\mu| > t)$ olarak tanýmlanmýþtý. 

Örnek

Diyelim ki bir tahmin edicimiz var, onu test etmek istiyoruz, bu bir yapay
sinir aðý (YSA) olabilir, ve elimizde $n$ tane test verisi var. Eðer tahmin
edici, yani YSA, hatalý ise $X_i=1$ olsun, haklý ise $X_i=0$ olsun. O zaman
gözlenen hata oraný (observed error rate)
$\overline{X}_n = n^{-1}\sum _{i=1}^{n} X_i$ olacaktýr. Rasgele deðiþken
çýktýlarýna bakarak bunu bir $p$'si bilinmeyen bir Bernoulli daðýlýmýndan
geliyormuþ gibi kabul edebileceðimizi görebiliriz. Ýstediðimiz gerçek -ama
bilinmeyen- $p$ hakkýnda irdeleme yapmak. $\overline{X}_n$'in gerçek
$p$'nin $\epsilon$ yakýnýnda olmama olasýlýðý nedir?

Bernoulli'lerin özelliklerinden biliyoruz ki 

$$ V(\overline{X}_n) = V(X_1) / n = p(1-p)/n$$

Çebiþev uygulayýnca, 

$$ 
P(|\overline{X}_n - p| > \epsilon) \le \frac{V(\overline{X}_n)}{\epsilon^2}
= \frac{p(1-p)}{n\epsilon^2} \le \frac{1}{4n\epsilon^2}
$$

Hatýrlarsak Bernoulli için $E(X)=p$. Son geçiþ mümkün oldu çünkü her $p$
için $p(1-p) \le \frac{1}{4}$ olmak zorundadýr. Öyle deðil mi? $p(1-p)$'nin
alabileceði en büyük deðer $p=1/2$ içindir, bundan farklý her $p$ deðeri
$1/4$'ten küçük bir çarpým verir, mesela $p=1/3$ için
$1/3 \cdot 2/3 = 2/9$.

O zaman, ve diyelim ki $\epsilon = .2$ ve $n=100$ için $0.0625$ sýnýrýný
elde ederiz. 

Hoeffding'in Eþitsizliði

Bu eþitsizlik Markov'un eþitsizliðine benziyor, ama daha keskin sonuçlar
verebiliyor, yani ufak güven aralýklarý elde edebiliyoruz, ki bu daha fazla
kesinlik demektir. Bu eþitsizliði iki bölüm olarak vereceðiz, 

$Y_1,Y_2,..,Y_n$ baðýmsýz gözlemler olsunlar, ki $E(Y_i)=0$ ve
$a_i \le Y_i \le b_i$ doðru olacak þekilde. O zaman herhangi bir $t>0$ için 

1. Teori

$$ 
P \bigg( 
\sum _{i=1}^{n} Y_i \ge \epsilon \le e^{-t\epsilon} 
\prod _{i=1}^{n} e^{{t^2}(b_i-a_i)^2 / 8}
\bigg)
$$

2. Teori

$X_1,..,X_n \sim Bernoulli(p)$ olsun. O zaman herhangi bir $\epsilon > 0$ icin

$$ P(|\overline{X}_n -p| > \epsilon ) \le 2e^{-2n\epsilon^2}$$

doðru olmalýdýr ki, daha önce gördüðümüz gibi,
$\overline{X}_n = n^{-1}\sum _{i=1}^{n} X_i$ olacak þekilde. 

Ýspat için bkz [1, sf. 67]. 

Örnek

Diyelim ki $X_1,..,X_n \sim Bernoulli(p)$. $n=100$ ve $\epsilon=.2$
olsun. Çebiþev esitsizligi ile 

$$ P(|\overline{X}_n - p| > \epsilon ) \le 0.0625 $$

elde etmiþtik. Hoeffding'e göre

$$ 
P(|\overline{X}_n - p| > \epsilon ) \le 2e^{-2 (100)(.2)^2} = 0.00067
$$

elde ederiz, ki bu Cebisev'den gelen $0.0625$'e göre çok daha ufak bir
deðerdir.

Kaynaklar

[1] Wasserman, {\em All of Statistics}

\newpage

z-Tablosu

Nasýl okunur? Z-deðeri -0.8994 için z kolonundan aþaðý inilir, ve -0.8
bulunur, x.x9xx yani 9 için .09 kolonuna gidilir ve bu kesiþmedeki deðer
okunur, .1867, yuvarlanarak .19 da kabul edilebilir. 

\includegraphics[height=4cm]{stat_appendix_02.png}

\ \ z \ \ \  .00  \ \ \ \ \ .01 \ \ \ \ \ .02 \ \ \ \ \ .03 \ \ \ \ \ .04 \
\ \ \ .05 \ \ \ .06 \
\ \ .07 \ \ \ .08 \ \ \ .09

-3.4 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0003 .0002

-3.3 .0005 .0005 .0005 .0004 .0004 .0004 .0004 .0004 .0004 .0003

-3.2 .0007 .0007 .0006 .0006 .0006 .0006 .0006 .0005 .0005 .0005

-3.1 .0010 .0009 .0009 .0009 .0008 .0008 .0008 .0008 .0007 .0007

-3.0 .0013 .0013 .0013 .0012 .0012 .0011 .0011 .0011 .0010 .0010

-2.9 .0019 .0018 .0018 .0017 .0016 .0016 .0015 .0015 .0014 .0014

-2.8 .0026 .0025 .0024 .0023 .0023 .0022 .0021 .0021 .0020 .0019

-2.7 .0035 .0034 .0033 .0032 .0031 .0030 .0029 .0028 .0027 .0026

-2.6 .0047 .0045 .0044 .0043 .0041 .0040 .0039 .0038 .0037 .0036

-2.5 .0062 .0060 .0059 .0057 .0055 .0054 .0052 .0051 .0049 .0048

-2.4 .0082 .0080 .0078 .0075 .0073 .0071 .0069 .0068 .0066 .0064

-2.3 .0107 .0104 .0102 .0099 .0096 .0094 .0091 .0089 .0087 .0084

-2.2 .0139 .0136 .0132 .0129 .0125 .0122 .0119 .0116 .0113 .0110

-2.1 .0179 .0174 .0170 .0166 .0162 .0158 .0154 .0150 .0146 .0143

-2.0 .0228 .0222 .0217 .0212 .0207 .0202 .0197 .0192 .0188 .0183

-1.9 .0287 .0281 .0274 .0268 .0262 .0256 .0250 .0244 .0239 .0233

-1.8 .0359 .0351 .0344 .0336 .0329 .0322 .0314 .0307 .0301 .0294

-1.7 .0446 .0436 .0427 .0418 .0409 .0401 .0392 .0384 .0375 .0367

-1.6 .0548 .0537 .0526 .0516 .0505 .0495 .0485 .0475 .0465 .0455

-1.5 .0668 .0655 .0643 .0630 .0618 .0606 .0594 .0582 .0571 .0559

-1.4 .0808 .0793 .0778 .0764 .0749 .0735 .0721 .0708 .0694 .0681

-1.3 .0968 .0951 .0934 .0918 .0901 .0885 .0869 .0853 .0838 .0823

-1.2 .1151 .1131 .1112 .1093 .1075 .1056 .1038 .1020 .1003 .0985

-1.1 .1357 .1335 .1314 .1292 .1271 .1251 .1230 .1210 .1190 .1170

-1.0 .1587 .1562 .1539 .1515 .1492 .1469 .1446 .1423 .1401 .1379

-0.9 .1841 .1814 .1788 .1762 .1736 .1711 .1685 .1660 .1635 .1611

-0.8 .2119 .2090 .2061 .2033 .2005 .1977 .1949 .1922 .1894 .1867

-0.7 .2420 .2389 .2358 .2327 .2296 .2266 .2236 .2206 .2177 .2148

-0.6 .2743 .2709 .2676 .2643 .2611 .2578 .2546 .2514 .2483 .2451

-0.5 .3085 .3050 .3015 .2981 .2946 .2912 .2877 .2843 .2810 .2776

-0.4 .3446 .3409 .3372 .3336 .3300 .3264 .3228 .3192 .3156 .3121

-0.3 .3821 .3783 .3745 .3707 .3669 .3632 .3594 .3557 .3520 .3483

-0.2 .4207 .4168 .4129 .4090 .4052 .4013 .3974 .3936 .3897 .3859

-0.1 .4602 .4562 .4522 .4483 .4443 .4404 .4364 .4325 .4286 .4247

0.0 .5000 .4960 .4920 .4880 .4840 .4801 .4761 .4721 .4681 .4641

\newpage

\ \ z \ \ \  .00  \ \ \ \ \ .01 \ \ \ \ \ .02 \ \ \ \ \ .03 \ \ \ \ \ .04 \
\ \ \ .05 \ \ \ .06 \
\ \ .07 \ \ \ .08 \ \ \ .09

0.0 .5000 .5040 .5080 .5120 .5160 .5199 .5239 .5279 .5319 .5359

0.1 .5398 .5438 .5478 .5517 .5557 .5596 .5636 .5675 .5714 .5753

0.2 .5793 .5832 .5871 .5910 .5948 .5987 .6026 .6064 .6103 .6141

0.3 .6179 .6217 .6255 .6293 .6331 .6368 .6406 .6443 .6480 .6517

0.4 .6554 .6591 .6628 .6664 .6700 .6736 .6772 .6808 .6844 .6879

0.5 .6915 .6950 .6985 .7019 .7054 .7088 .7123 .7157 .7190 .7224

0.6 .7257 .7291 .7324 .7357 .7389 .7422 .7454 .7486 .7517 .7549

0.7 .7580 .7611 .7642 .7673 .7704 .7734 .7764 .7794 .7823 .7852

0.8 .7881 .7910 .7939 .7967 .7995 .8023 .8051 .8078 .8106 .8133

0.9 .8159 .8186 .8212 .8238 .8264 .8289 .8315 .8340 .8365 .8389

1.0 .8413 .8438 .8461 .8485 .8508 .8531 .8554 .8577 .8599 .8621

1.1 .8643 .8665 .8686 .8708 .8729 .8749 .8770 .8790 .8810 .8830

1.2 .8849 .8869 .8888 .8907 .8925 .8944 .8962 .8980 .8997 .9015

1.3 .9032 .9049 .9066 .9082 .9099 .9115 .9131 .9147 .9162 .9177

1.4 .9192 .9207 .9222 .9236 .9251 .9265 .9279 .9292 .9306 .9319

1.5 .9332 .9345 .9357 .9370 .9382 .9394 .9406 .9418 .9429 .9441

1.6 .9452 .9463 .9474 .9484 .9495 .9505 .9515 .9525 .9535 .9545

1.7 .9554 .9564 .9573 .9582 .9591 .9599 .9608 .9616 .9625 .9633

1.8 .9641 .9649 .9656 .9664 .9671 .9678 .9686 .9693 .9699 .9706

1.9 .9713 .9719 .9726 .9732 .9738 .9744 .9750 .9756 .9761 .9767

2.0 .9772 .9778 .9783 .9788 .9793 .9798 .9803 .9808 .9812 .9817

2.1 .9821 .9826 .9830 .9834 .9838 .9842 .9846 .9850 .9854 .9857

2.2 .9861 .9864 .9868 .9871 .9875 .9878 .9881 .9884 .9887 .9890

2.3 .9893 .9896 .9898 .9901 .9904 .9906 .9909 .9911 .9913 .9916

2.4 .9918 .9920 .9922 .9925 .9927 .9929 .9931 .9932 .9934 .9936

2.5 .9938 .9940 .9941 .9943 .9945 .9946 .9948 .9949 .9951 .9952

2.6 .9953 .9955 .9956 .9957 .9959 .9960 .9961 .9962 .9963 .9964

2.7 .9965 .9966 .9967 .9968 .9969 .9970 .9971 .9972 .9973 .9974

2.8 .9974 .9975 .9976 .9977 .9977 .9978 .9979 .9979 .9980 .9981

2.9 .9981 .9982 .9982 .9983 .9984 .9984 .9985 .9985 .9986 .9986

3.0 .9987 .9987 .9987 .9988 .9988 .9989 .9989 .9989 .9990 .9990

3.1 .9990 .9991 .9991 .9991 .9992 .9992 .9992 .9992 .9993 .9993

3.2 .9993 .9993 .9994 .9994 .9994 .9994 .9994 .9995 .9995 .9995

3.3 .9995 .9995 .9995 .9996 .9996 .9996 .9996 .9996 .9996 .9997

3.4 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9997 .9998

Kaynaklar

[1] Gullickson, {\em Sociology G4075: Introduction to Social Data Analysis
  II}, \url{https://web.archive.org/web/20160312151715/http://pages.uoregon.edu/aarong/teaching/G4075_Outline/node13.html}


\newpage

Yunan Harfleri

\includegraphics[width=30em]{../../algs/algs_999_zapp/letters.png}

\end{document}

