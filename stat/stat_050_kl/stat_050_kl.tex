\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Kullback-Leibler (KL) Mesafesi

Ýki olasýlýk daðýlýmýnýn arasýndaki uyumsuzluðu (discrepancy) hesaplayan
bir ölçüt KL mesafesidir. Gerçi bu ölçüt tam tanýmýyla mesafe deðil, $f$
ile $g$ arasýndaki mesafe $g$ ile $f$ arasýndaki mesafeden farklý
olabiliyor, KL mesafesi üçgen eþitsizlik (triangle inequality) kavramýný
takip etmiyor. Tam tanýmlamak gerekirse KL bir yönsel (directed) mesafedir [2].

Kullback-Leibler aslýnda 1951'de bir enformasyon ölçütü bulmuþ oldular, bu
ölçüt ilginç bir þekilde fizikçi Boltzmann'ýn bir sistemdeki düzensizliði
ölçen entropi kavramýnýn negatif deðerli halidir. Ayrýca KL mesafesi
Enformasyon Teorisi'ni keþfeden Shannon'un enformasyon tanýmýnýn da bir
uzantýsýdýr, bu sebeple bazen KL mesafesine ``izafi entropi'' ismi de
veriliyor.

Tüm bu kavramlarýn tabii ki Ýstatistik'teki model seçme uygulamalarýyla
yakýn alakalarý var. Diyelim ki elimizde iki daðýlým var, $f$ yaklaþmaya
çalýþtýðýmýz bir model, $g$ ise onu yaklaþýk olarak temsil etmeye uðraþan
baþka bir model, $\theta$ parametreleri üzerinden tanýmlý, yani
$g(x|\theta)$. $\theta$ çoðunlukla veriden kestirilmeye çalýþýlýr,
$\hat{\theta}$ elde edilir, o zaman $g(x|\hat{\theta})$ olur. Bu iki
daðýlým / model arasýndaki KL mesafesi

$$ I(f,g) = \int f(x) \log \bigg( \frac{f(x)}{g(x;\theta)} \bigg) \ud x$$

(çoðunlukla çok boyutlu) entegrali ile hesaplanýr. Kullback-Leibler
$I(f,g)$ notasyonunu ``$g$, $f$ yerine, onu yaklaþýk olarak temsil edecek
þekilde kullanýldýðýna kaybedilen enformasyon'' þeklinde kullandýlar. Tabii
ki uygulamalarda bu kayýbýn olabildiði kadar az olmasýný isteriz, yani
$I(f,g)$'i $g$ üzerinden minimize etmek önemli bir uygulama alaný.

Ayrýksal daðýlýmlar durumunda üstteki formül,

$$ I(f,g) = \sum _{i=1}^{k} p_i \log \bigg( \frac{p_i}{\pi_i} \bigg)  $$

Burada $k$ deðiþkeni rasgele deðiþkenin alabileceði $k$ farklý deðeri
temsil eder, $i$'inci olayýn olma olasýlýðý $p_i$'dir, $\pi_1,..,\pi_k$ ise
gerçek daðýlýmý yaklaþýk olarak temsil etmeye uðraþan modeldir. Ayrýksal
durumda $0 < p_i < 1, 0 < \pi_i < 1$, ve $\sum p_i = 1 = \sum \pi_i = 1$. 

Formüllere yakýndan bakarsak onlarýn birer beklenti hesabý olduðunu
görebiliriz, $\int f(x) ( \cdot ) \ud x$ þablonundaki formüllerin beklenti
hesabý için kullanýldýðýný biliyoruz. Ayrýksal durumda
$\sum_{i=1}^{k}p_i( \cdot ) $, ve bu beklenti iki daðýlýmýn birbirine olan
oranýnýn negatifinin beklentisi, yani bu oranýn ortalamasý. Bu kavramýn
çýkýþý çok derin ve temel, Boltzmann'ýn 1877'de, Shannon'un sonra
bulduklarý ile derin baðlantýlar var. 

Kabaca tarif etmek gerekirse, bir daðýlýmýn içerdiði enformasyon onun
negatif log'udur, iki daðýlým arasýndaki mesafe için negatif log'larýn
farkýný alýrýz, ki fark cebirsel olarak bölümün log'u olarak tek bir log
altýnda gruplanabilir, ve mümkün tüm sayýlar üzerinden bu farklarýn
beklentisini alýrsak üstteki entegral (ya da toplam) formülünü elde etmiþ
oluruz. 

KL mesafesi her zaman pozitiftir, tek bir durum haricinde, eðer $f,g$
eþitse - o zaman $I(f,g) = 0$.

Bir örnek üzerinde görmek gerekirse, diyelim ki $f$ 2 parametreli bir Gamma
daðýlýmý, $\alpha=4,\beta=4$. Þimdi bu modeli yaklaþýk olarak temsil etmeye
uðraþan 4 tane seçeneði görelim, Weibull, lognormal, ters Gaussian, ve F
daðýlýmý. 

\begin{tabular}{cc}
Yaklaþýk Model & $I(f,g_i)$ \\
\hline \\
Weibull ($\alpha=2,\beta=20$) & 0.04620 \\
Lognormal ($\theta=2,\sigma^2=2$) & 0.67235 \\ 
Ters Gaussian ($\alpha=16,\beta=64$) & 0.06008 \\
F daðýlýmý ($\alpha=4,\beta=10$) & 5.74555
\end{tabular}

Görüldüðü gibi Weibull en yakýn olan (yani yaklaþýk temsil sýrasýnda en az
enformasyon kaybeden o). Lognormal 3. sýrada, F daðýlýmý en uzak olaný.

\includegraphics[width=35em]{stat_kl_01.png}

Bir baþka örnek için {\em Testlere Devam} yazýsýndaki araba sayým verisine
bakalým. Þimdi ham veriye en uygun olan daðýlýmý bulmaya çalýþacaðýz. 

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('../stat_tests2/vehicles.csv',header=None)
df.hist(bins=13)
plt.savefig('stat_kl_02.png')
\end{minted}

\includegraphics[width=20em]{stat_kl_02.png}

Veride Poisson görünümü var. Eþit aralýklarda yapýlan sayýmlarýn Poisson
daðýlýmýný takip etmeye meyilli olduðunu biliyoruz. Bu tezi kontrol
edelim. Eðer, diyelim, Possion ve Gaussian arasýnda seçim yapacak olsak, bu
seçimi KL mesafesi üzerinden yapabilirdik. Her iki durumda da daðýlým
parametrelerini veriden tahmin ediyor olurduk,

\begin{minted}[fontsize=\footnotesize]{python}
print np.float(df.mean()), np.float(df.std())
\end{minted}

\begin{verbatim}
9.09433962264 3.54166574177
\end{verbatim}

Poisson durumunda ortalama hesabý $\hat{\lambda}$ için, Gaussian'da ise
ortalama ve standart sapma $\hat{\mu},\hat{\sigma}$ için
kullanýlýrdý. 

Altta hem verinin hem de hipotez daðýlýmlardan üretilmiþ rasgele sayýlarýn
histogramlarýný hesaplýyoruz. Not: Aslýnda ham verinin histogramýndan sonra
histogram kutularýnýn (bins) sýnýrlarýna bakarak Poisson ve Gaussian
analitik daðýlýmlarýnýn oraya tekabül eden yoðunluklarýný analitik çaðrýlar
ile bulabilirdik, fakat kolay yolu (!)  seçtik, analitik daðýlýmlar için de
rasgele sayý üretiyoruz, hem ham veri hem analitik durum için histogram
hesaplýyoruz.

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.stats
s = 4000
b = 15
r1 = scipy.stats.poisson.rvs(mu=8, size=s)
plt.hist(r1, bins=b,color='b')
plt.title('Poisson $\lambda = 8$')
plt.xlim(0,20)
plt.savefig('stat_kl_04.png')
plt.figure()
r2 = scipy.stats.norm.rvs(2, 1, size=s)
plt.hist(r2, bins=b,color='b')
plt.title('Gaussian $\mu = 2,\sigma=1$')
plt.xlim(0,20)
plt.savefig('stat_kl_06.png')
plt.figure()
r3 = scipy.stats.poisson.rvs(mu=9.0943, size=s)
plt.hist(r3, bins=b,color='b')
plt.title('Poisson $\lambda = 9.1$')
plt.xlim(0,20)
plt.savefig('stat_kl_07.png')
plt.figure()
r4 = scipy.stats.norm.rvs(9.1, 3.54, size=s)
plt.hist(r4, bins=b,color='b')
plt.title('Gaussian $\mu = 9.1,\sigma=3.54$')
plt.xlim(0,20)
plt.savefig('stat_kl_08.png')
\end{minted}

\includegraphics[width=15em]{stat_kl_04.png}
\includegraphics[width=15em]{stat_kl_06.png}

\includegraphics[width=15em]{stat_kl_07.png}
\includegraphics[width=15em]{stat_kl_08.png}

Þimdi veri ve tüm müstakbel analitik yoðunluklar arasýnda KL mesafelerini
hesaplayalým,

\begin{minted}[fontsize=\footnotesize]{python}
def kl(p, q):
    return np.sum(p * np.log(p / q))

b = range(0,30)
eps = 1e-5
dh = np.histogram(df, bins=b, density=True)[0]+eps
h1 = np.histogram(r1, bins=b, density=True)[0]+eps
h2 = np.histogram(r2, bins=b, density=True)[0]+eps
h3 = np.histogram(r3, bins=b, density=True)[0]+eps
h4 = np.histogram(r4, bins=b, density=True)[0]+eps
print 'Poisson lambda = 8', kl(h1, dh)
print 'Gaussian mu = 2,sigma=1', kl(h2, dh)
print 'Poisson lambda = 9.1', kl(h3, dh)
print 'Gaussian mu = 9.1,sigma=3.54', kl(h4, dh)
\end{minted}

\begin{verbatim}
Poisson lambda = 8 0.14722344735
Gaussian mu = 2,sigma=1 6.39721632939
Poisson lambda = 9.1 0.133099166073
Gaussian mu = 9.1,sigma=3.54 0.200156046018
\end{verbatim}

En yakýn olan Poisson $\lambda=9.1$ olarak gözüküyor.

Çok Boyutlu Daðýlýmlar

Eðer bir dijital görüntü üzerinde çalýþýyorsak, o resimdeki piksel
deðerlerinin de bir ``daðýlýmý'' olduðunu düþünebiliriz. Yani resmi, ya da
resmin bir bölgesini bir teorik daðýlýmdan ``üretilmiþ'' bir örneklem
olarak görmek mümkün. Bu daðýlýmý çok boyutlu histogram alarak yaklaþýk
olarak hesaplayabiliriz. Eðer iki farklý resim bölgesini bu þekilde
belirtirsek, bu iki daðýlýmý KL mesafesiyle karþýlaþtýrabililiriz, ve
böylece görüntüsel olarak iki bölgeyi karþýlaþtýrabiliriz.

\begin{minted}[fontsize=\footnotesize]{python}
from PIL import Image, ImageDraw

def draw_boxes_color(bs,imfile):
    im = Image.open(imfile).convert('HSV')
    arr = np.asarray(im)
    draw = ImageDraw.Draw(im)
    colors = ['magenta','green','white','red','yellow']
    for i,b in enumerate(bs):
        fr = b[0]; to = b[1]
        bnew = [(fr[0],arr.shape[0]-fr[1]),(to[0],arr.shape[0]-to[1])]
        draw.rectangle(bnew,outline=colors[i])
    plt.imshow(im)

def get_pixels(box, im):
    arr = np.array(im)
    (yw,xw,d) = arr.shape
    (bx1,by1) = box[0]; (bx2,by2) = box[1]
    by1 = yw-by1; by2 = yw-by2
    x1 = min(bx1,bx2); x2 = max(bx1,bx2)
    y1 = min(by1,by2); y2 = max(by1,by2)
    arr = arr[y1:y2, x1:x2, :]
    return arr
\end{minted}


\begin{minted}[fontsize=\footnotesize]{python}
box1 = [(35,144),(87,292)]
box2 = [(106,183),(158,287)]
box3 = [(117,86),(132,160)]
f = '../../vision/vision_50colreg/castle.png'
draw_boxes_color([box1,box2],f)
plt.savefig('stat_kl_03.png')
draw_boxes_color([box2,box3],f)
plt.savefig('stat_kl_05.png')
\end{minted}

\includegraphics[width=20em]{stat_kl_03.png}
\includegraphics[width=20em]{stat_kl_05.png}

Renklerin HSV kodlamasýný kullanalým, o zaman her piksel kordinatýnda 3
deðer olur. Bu durumda histogram almak demek çok boyutlu histogram
demektir, üç boyut için sýrasýyla 8,8,4 tane kutu tanýmlarsak, 256 tane
kutu elde ederiz. Bu kutularý \verb!numpy.histogramdd! ile hesaplarýz, KL
karþýlaþtýrmasý için kutularý düz vektör haline getirebiliriz -KL hesabýnda
her iki tarafýn birbirine tekabül eden kutularý kullanýldýðý sürece problem
yok- ve böylece nihai hesap yapýlýr.

\begin{minted}[fontsize=\footnotesize]{python}
def box_kl_dist(b1,b2,im):
    im = Image.open(f).convert('HSV')
    arr1 = get_pixels(b1, im)
    r = [(0,255),(0,255),(0,255)]

    arr1 = np.reshape(arr1, (arr1.shape[0]*arr1.shape[1],3))
    H1, edges = np.histogramdd(arr1, bins=(8, 8, 4), normed=True, range=r)
    H1 = np.reshape(H1, (H1.shape[0]*H1.shape[1]*H1.shape[2], 1))

    arr2 = get_pixels(b2, im)
    arr2 = np.reshape(arr2, (arr2.shape[0]*arr2.shape[1],3))
    H2, edges = np.histogramdd(arr2, bins=(8, 8, 4), normed=True, range=r)
    H2 = np.reshape(H2, (H2.shape[0]*H2.shape[1]*H2.shape[2], 1))

    return kl(H1+eps, H2+eps)

print box_kl_dist(box1, box2, f)
print box_kl_dist(box2, box3, f)
\end{minted}

\begin{verbatim}
7.55231179178e-06
7.30926985663e-07
\end{verbatim}

Ýkinci karþýlaþtýrmada mesafe daha yakýn duruyor; hakikaten de resimlere
bakarsak ikinci resimdeki bölgelerin renksel olarak birbirine daha yakýn
olduðunu görebiliyoruz.

Kaynaklar

[1] Cover, {\em Elements of Information Theory}

[2] Burnham, {\em Model Selection and Inference}

\end{document}
