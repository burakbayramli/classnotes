<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Kullback-Leibler (KL) Mesafesi</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="kullback-leibler-kl-mesafesi">Kullback-Leibler (KL)
Mesafesi</h1>
<p>İki olasılık dağılımının arasındaki uyumsuzluğu (discrepancy)
hesaplayan bir ölçüt KL mesafesidir. Gerçi bu ölçüt tam tanımıyla mesafe
değil, <span class="math inline">\(f\)</span> ile <span
class="math inline">\(g\)</span> arasındaki mesafe <span
class="math inline">\(g\)</span> ile <span
class="math inline">\(f\)</span> arasındaki mesafeden farklı olabiliyor,
KL mesafesi üçgen eşitsizlik (triangle inequality) kavramını takip
etmiyor. Tam tanımlamak gerekirse KL bir yönsel (directed) mesafedir
[2].</p>
<p>Kullback-Leibler aslında 1951’de bir enformasyon ölçütü bulmuş
oldular, bu ölçüt ilginç bir şekilde fizikçi Boltzmann’ın bir sistemdeki
düzensizliği ölçen entropi kavramının negatif değerli halidir. Ayrıca KL
mesafesi Enformasyon Teorisi’ni keşfeden Shannon’un enformasyon
tanımının da bir uzantısıdır, bu sebeple bazen KL mesafesine “izafi
entropi’’ ismi de veriliyor.</p>
<p>Tüm bu kavramların tabii ki İstatistik’teki model seçme
uygulamalarıyla yakın alakaları var. Diyelim ki elimizde iki dağılım
var, <span class="math inline">\(f\)</span> yaklaşmaya çalıştığımız bir
model, <span class="math inline">\(g\)</span> ise onu yaklaşık olarak
temsil etmeye uğraşan başka bir model, <span
class="math inline">\(\theta\)</span> parametreleri üzerinden tanımlı,
yani <span class="math inline">\(g(x|\theta)\)</span>. <span
class="math inline">\(\theta\)</span> çoğunlukla veriden kestirilmeye
çalışılır, <span class="math inline">\(\hat{\theta}\)</span> elde
edilir, o zaman <span class="math inline">\(g(x|\hat{\theta})\)</span>
olur. Bu iki dağılım / model arasındaki KL mesafesi</p>
<p><span class="math display">\[ I(f,g) = \int f(x) \log \bigg(
\frac{f(x)}{g(x;\theta)} \bigg) \mathrm{d} x\]</span></p>
<p>(çoğunlukla çok boyutlu) entegrali ile hesaplanır. Kullback-Leibler
<span class="math inline">\(I(f,g)\)</span> notasyonunu “<span
class="math inline">\(g\)</span>, <span class="math inline">\(f\)</span>
yerine, onu yaklaşık olarak temsil edecek şekilde kullanıldığına
kaybedilen enformasyon’’ şeklinde kullandılar. Tabii ki uygulamalarda bu
kayıbın olabildiği kadar az olmasını isteriz, yani <span
class="math inline">\(I(f,g)\)</span>’i <span
class="math inline">\(g\)</span> üzerinden minimize etmek önemli bir
uygulama alanı.</p>
<p>Ayrıksal dağılımlar durumunda üstteki formül,</p>
<p><span class="math display">\[ I(f,g) = \sum_{i=1}^{k} p_i \log \bigg(
\frac{p_i}{\pi_i} \bigg)  \]</span></p>
<p>Burada <span class="math inline">\(k\)</span> değişkeni rasgele
değişkenin alabileceği <span class="math inline">\(k\)</span> farklı
değeri temsil eder, <span class="math inline">\(i\)</span>’inci olayın
olma olasılığı <span class="math inline">\(p_i\)</span>’dir, <span
class="math inline">\(\pi_1,..,\pi_k\)</span> ise gerçek dağılımı
yaklaşık olarak temsil etmeye uğraşan modeldir. Ayrıksal durumda <span
class="math inline">\(0 &lt; p_i &lt; 1, 0 &lt; \pi_i &lt; 1\)</span>,
ve <span class="math inline">\(\sum p_i = 1 = \sum \pi_i =
1\)</span>.</p>
<p>Formüllere yakından bakarsak onların birer beklenti hesabı olduğunu
görebiliriz, <span class="math inline">\(\int f(x) ( \cdot ) \mathrm{d}
x\)</span> şablonundaki formüllerin beklenti hesabı için kullanıldığını
biliyoruz. Ayrıksal durumda <span
class="math inline">\(\sum_{i=1}^{k}p_i( \cdot )\)</span>, ve bu
beklenti iki dağılımın birbirine olan oranının negatifinin beklentisi,
yani bu oranın ortalaması. Bu kavramın çıkışı çok derin ve temel,
Boltzmann’ın 1877’de, Shannon’un sonra buldukları ile derin bağlantılar
var.</p>
<p>Kabaca tarif etmek gerekirse, bir dağılımın içerdiği enformasyon onun
negatif log’udur, iki dağılım arasındaki mesafe için negatif log’ların
farkını alırız, ki fark cebirsel olarak bölümün log’u olarak tek bir log
altında gruplanabilir, ve mümkün tüm sayılar üzerinden bu farkların
beklentisini alırsak üstteki entegral (ya da toplam) formülünü elde
etmiş oluruz.</p>
<p>KL mesafesi her zaman pozitiftir, tek bir durum haricinde, eğer <span
class="math inline">\(f,g\)</span> eşitse - o zaman <span
class="math inline">\(I(f,g) = 0\)</span>.</p>
<p>Bir örnek üzerinde görmek gerekirse, diyelim ki <span
class="math inline">\(f\)</span> 2 parametreli bir Gamma dağılımı, <span
class="math inline">\(\alpha=4,\beta=4\)</span>. Şimdi bu modeli
yaklaşık olarak temsil etmeye uğraşan 4 tane seçeneği görelim, Weibull,
lognormal, ters Gaussian, ve F dağılımı.</p>
<p><span class="math display">\[
\begin{array}{cc}
Yaklaşık Model &amp; I(f,g_i) \\
\hline \\
Weibull (\alpha=2,\beta=20) &amp; 0.04620 \\
Lognormal (\theta=2,\sigma^2=2) &amp; 0.67235 \\
Ters Gaussian (\alpha=16,\beta=64) &amp; 0.06008 \\
F dağılımı (\alpha=4,\beta=10) &amp; 5.74555
\end{array}
\]</span></p>
<p>Görüldüğü gibi Weibull en yakın olan (yani yaklaşık temsil sırasında
en az enformasyon kaybeden o). Lognormal 3. sırada, F dağılımı en uzak
olanı.</p>
<p><img src="stat_kl_01.png" /></p>
<p>Bir başka örnek için <em>Testlere Devam</em> yazısındaki araba sayım
verisine bakalım. Şimdi ham veriye en uygun olan dağılımı bulmaya
çalışacağız.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;../stat_tests2/vehicles.csv&#39;</span>,header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>df.hist(bins<span class="op">=</span><span class="dv">13</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_kl_02.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_kl_02.png" /></p>
<p>Veride Poisson görünümü var. Eşit aralıklarda yapılan sayımların
Poisson dağılımını takip etmeye meyilli olduğunu biliyoruz. Bu tezi
kontrol edelim. Eğer, diyelim, Possion ve Gaussian arasında seçim
yapacak olsak, bu seçimi KL mesafesi üzerinden yapabilirdik. Her iki
durumda da dağılım parametrelerini veriden tahmin ediyor olurduk,</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> np.<span class="bu">float</span>(df.mean()), np.<span class="bu">float</span>(df.std())</span></code></pre></div>
<pre><code>9.09433962264 3.54166574177</code></pre>
<p>Poisson durumunda ortalama hesabı <span
class="math inline">\(\hat{\lambda}\)</span> için, Gaussian’da ise
ortalama ve standart sapma <span
class="math inline">\(\hat{\mu},\hat{\sigma}\)</span> için
kullanılırdı.</p>
<p>Altta hem verinin hem de hipotez dağılımlardan üretilmiş rasgele
sayıların histogramlarını hesaplıyoruz. Not: Aslında ham verinin
histogramından sonra histogram kutularının (bins) sınırlarına bakarak
Poisson ve Gaussian analitik dağılımlarının oraya tekabül eden
yoğunluklarını analitik çağrılar ile bulabilirdik, fakat kolay yolu (!)
seçtik, analitik dağılımlar için de rasgele sayı üretiyoruz, hem ham
veri hem analitik durum için histogram hesaplıyoruz.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> <span class="dv">4000</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="dv">15</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>r1 <span class="op">=</span> scipy.stats.poisson.rvs(mu<span class="op">=</span><span class="dv">8</span>, size<span class="op">=</span>s)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.hist(r1, bins<span class="op">=</span>b,color<span class="op">=</span><span class="st">&#39;b&#39;</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Poisson $\lambda = 8$&#39;</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>,<span class="dv">20</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_kl_04.png&#39;</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>r2 <span class="op">=</span> scipy.stats.norm.rvs(<span class="dv">2</span>, <span class="dv">1</span>, size<span class="op">=</span>s)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>plt.hist(r2, bins<span class="op">=</span>b,color<span class="op">=</span><span class="st">&#39;b&#39;</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Gaussian $\mu = 2,\sigma=1$&#39;</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>,<span class="dv">20</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_kl_06.png&#39;</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>r3 <span class="op">=</span> scipy.stats.poisson.rvs(mu<span class="op">=</span><span class="fl">9.0943</span>, size<span class="op">=</span>s)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.hist(r3, bins<span class="op">=</span>b,color<span class="op">=</span><span class="st">&#39;b&#39;</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Poisson $\lambda = 9.1$&#39;</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>,<span class="dv">20</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_kl_07.png&#39;</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>r4 <span class="op">=</span> scipy.stats.norm.rvs(<span class="fl">9.1</span>, <span class="fl">3.54</span>, size<span class="op">=</span>s)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>plt.hist(r4, bins<span class="op">=</span>b,color<span class="op">=</span><span class="st">&#39;b&#39;</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Gaussian $\mu = 9.1,\sigma=3.54$&#39;</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="dv">0</span>,<span class="dv">20</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_kl_08.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_kl_04.png" /> <img src="stat_kl_06.png" /></p>
<p><img src="stat_kl_07.png" /> <img src="stat_kl_08.png" /></p>
<p>Şimdi veri ve tüm müstakbel analitik yoğunluklar arasında KL
mesafelerini hesaplayalım,</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl(p, q):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">sum</span>(p <span class="op">*</span> np.log(p <span class="op">/</span> q))</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">30</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>dh <span class="op">=</span> np.histogram(df, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>h1 <span class="op">=</span> np.histogram(r1, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>h2 <span class="op">=</span> np.histogram(r2, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>h3 <span class="op">=</span> np.histogram(r3, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>h4 <span class="op">=</span> np.histogram(r4, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;Poisson lambda = 8&#39;</span>, kl(h1, dh)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;Gaussian mu = 2,sigma=1&#39;</span>, kl(h2, dh)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;Poisson lambda = 9.1&#39;</span>, kl(h3, dh)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> <span class="st">&#39;Gaussian mu = 9.1,sigma=3.54&#39;</span>, kl(h4, dh)</span></code></pre></div>
<pre><code>Poisson lambda = 8 0.14722344735
Gaussian mu = 2,sigma=1 6.39721632939
Poisson lambda = 9.1 0.133099166073
Gaussian mu = 9.1,sigma=3.54 0.200156046018</code></pre>
<p>En yakın olan Poisson <span
class="math inline">\(\lambda=9.1\)</span> olarak gözüküyor.</p>
<p>Çok Boyutlu Dağılımlar</p>
<p>Eğer bir dijital görüntü üzerinde çalışıyorsak, o resimdeki piksel
değerlerinin de bir “dağılımı’’ olduğunu düşünebiliriz. Yani resmi, ya
da resmin bir bölgesini bir teorik dağılımdan”üretilmiş’’ bir örneklem
olarak görmek mümkün. Bu dağılımı çok boyutlu histogram alarak yaklaşık
olarak hesaplayabiliriz. Eğer iki farklı resim bölgesini bu şekilde
belirtirsek, bu iki dağılımı KL mesafesiyle karşılaştırabililiriz, ve
böylece görüntüsel olarak iki bölgeyi karşılaştırabiliriz.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image, ImageDraw</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> draw_boxes_color(bs,imfile):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    im <span class="op">=</span> Image.<span class="bu">open</span>(imfile).convert(<span class="st">&#39;HSV&#39;</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    arr <span class="op">=</span> np.asarray(im)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    draw <span class="op">=</span> ImageDraw.Draw(im)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [<span class="st">&#39;magenta&#39;</span>,<span class="st">&#39;green&#39;</span>,<span class="st">&#39;white&#39;</span>,<span class="st">&#39;red&#39;</span>,<span class="st">&#39;yellow&#39;</span>]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i,b <span class="kw">in</span> <span class="bu">enumerate</span>(bs):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        fr <span class="op">=</span> b[<span class="dv">0</span>]<span class="op">;</span> to <span class="op">=</span> b[<span class="dv">1</span>]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        bnew <span class="op">=</span> [(fr[<span class="dv">0</span>],arr.shape[<span class="dv">0</span>]<span class="op">-</span>fr[<span class="dv">1</span>]),(to[<span class="dv">0</span>],arr.shape[<span class="dv">0</span>]<span class="op">-</span>to[<span class="dv">1</span>])]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        draw.rectangle(bnew,outline<span class="op">=</span>colors[i])</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    plt.imshow(im)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_pixels(box, im):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    arr <span class="op">=</span> np.array(im)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    (yw,xw,d) <span class="op">=</span> arr.shape</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    (bx1,by1) <span class="op">=</span> box[<span class="dv">0</span>]<span class="op">;</span> (bx2,by2) <span class="op">=</span> box[<span class="dv">1</span>]</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    by1 <span class="op">=</span> yw<span class="op">-</span>by1<span class="op">;</span> by2 <span class="op">=</span> yw<span class="op">-</span>by2</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    x1 <span class="op">=</span> <span class="bu">min</span>(bx1,bx2)<span class="op">;</span> x2 <span class="op">=</span> <span class="bu">max</span>(bx1,bx2)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    y1 <span class="op">=</span> <span class="bu">min</span>(by1,by2)<span class="op">;</span> y2 <span class="op">=</span> <span class="bu">max</span>(by1,by2)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    arr <span class="op">=</span> arr[y1:y2, x1:x2, :]</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> arr</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>box1 <span class="op">=</span> [(<span class="dv">35</span>,<span class="dv">144</span>),(<span class="dv">87</span>,<span class="dv">292</span>)]</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>box2 <span class="op">=</span> [(<span class="dv">106</span>,<span class="dv">183</span>),(<span class="dv">158</span>,<span class="dv">287</span>)]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>box3 <span class="op">=</span> [(<span class="dv">117</span>,<span class="dv">86</span>),(<span class="dv">132</span>,<span class="dv">160</span>)]</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="st">&#39;../../vision/vision_50colreg/castle.png&#39;</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>draw_boxes_color([box1,box2],f)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_kl_03.png&#39;</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>draw_boxes_color([box2,box3],f)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;stat_kl_05.png&#39;</span>)</span></code></pre></div>
<p><img src="stat_kl_03.png" /> <img src="stat_kl_05.png" /></p>
<p>Renklerin HSV kodlamasını kullanalım, o zaman her piksel kordinatında
3 değer olur. Bu durumda histogram almak demek çok boyutlu histogram
demektir, üç boyut için sırasıyla 8,8,4 tane kutu tanımlarsak, 256 tane
kutu elde ederiz. Bu kutuları <code>numpy.histogramdd</code> ile
hesaplarız, KL karşılaştırması için kutuları düz vektör haline
getirebiliriz -KL hesabında her iki tarafın birbirine tekabül eden
kutuları kullanıldığı sürece problem yok- ve böylece nihai hesap
yapılır.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> box_kl_dist(b1,b2,im):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    im <span class="op">=</span> Image.<span class="bu">open</span>(f).convert(<span class="st">&#39;HSV&#39;</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    arr1 <span class="op">=</span> get_pixels(b1, im)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">255</span>),(<span class="dv">0</span>,<span class="dv">255</span>),(<span class="dv">0</span>,<span class="dv">255</span>)]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    arr1 <span class="op">=</span> np.reshape(arr1, (arr1.shape[<span class="dv">0</span>]<span class="op">*</span>arr1.shape[<span class="dv">1</span>],<span class="dv">3</span>))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    H1, edges <span class="op">=</span> np.histogramdd(arr1, bins<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">4</span>), normed<span class="op">=</span><span class="va">True</span>, <span class="bu">range</span><span class="op">=</span>r)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    H1 <span class="op">=</span> np.reshape(H1, (H1.shape[<span class="dv">0</span>]<span class="op">*</span>H1.shape[<span class="dv">1</span>]<span class="op">*</span>H1.shape[<span class="dv">2</span>], <span class="dv">1</span>))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    arr2 <span class="op">=</span> get_pixels(b2, im)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    arr2 <span class="op">=</span> np.reshape(arr2, (arr2.shape[<span class="dv">0</span>]<span class="op">*</span>arr2.shape[<span class="dv">1</span>],<span class="dv">3</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    H2, edges <span class="op">=</span> np.histogramdd(arr2, bins<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">4</span>), normed<span class="op">=</span><span class="va">True</span>, <span class="bu">range</span><span class="op">=</span>r)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    H2 <span class="op">=</span> np.reshape(H2, (H2.shape[<span class="dv">0</span>]<span class="op">*</span>H2.shape[<span class="dv">1</span>]<span class="op">*</span>H2.shape[<span class="dv">2</span>], <span class="dv">1</span>))</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> kl(H1<span class="op">+</span>eps, H2<span class="op">+</span>eps)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> box_kl_dist(box1, box2, f)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> box_kl_dist(box2, box3, f)</span></code></pre></div>
<pre><code>7.55231179178e-06
7.30926985663e-07</code></pre>
<p>İkinci karşılaştırmada mesafe daha yakın duruyor; hakikaten de
resimlere bakarsak ikinci resimdeki bölgelerin renksel olarak birbirine
daha yakın olduğunu görebiliyoruz.</p>
<p>Kaynaklar</p>
<p>[1] Cover, <em>Elements of Information Theory</em></p>
<p>[2] Burnham, <em>Model Selection and Inference</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
