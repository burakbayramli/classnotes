<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="kullback-leibler-kl-mesafesi">Kullback-Leibler (KL) Mesafesi</h1>
<p>İki olasılık dağılımının arasındaki uyumsuzluğu (discrepancy) hesaplayan bir ölçüt KL mesafesidir. Gerçi bu ölçüt tam tanımıyla mesafe değil, <span class="math inline">\(f\)</span> ile <span class="math inline">\(g\)</span> arasındaki mesafe <span class="math inline">\(g\)</span> ile <span class="math inline">\(f\)</span> arasındaki mesafeden farklı olabiliyor, KL mesafesi üçgen eşitsizlik (triangle inequality) kavramını takip etmiyor. Tam tanımlamak gerekirse KL bir yönsel (directed) mesafedir [2].</p>
<p>Kullback-Leibler aslında 1951'de bir enformasyon ölçütü bulmuş oldular, bu ölçüt ilginç bir şekilde fizikçi Boltzmann'ın bir sistemdeki düzensizliği ölçen entropi kavramının negatif değerli halidir. Ayrıca KL mesafesi Enformasyon Teorisi'ni keşfeden Shannon'un enformasyon tanımının da bir uzantısıdır, bu sebeple bazen KL mesafesine &quot;izafi entropi'' ismi de veriliyor.</p>
<p>Tüm bu kavramların tabii ki İstatistik'teki model seçme uygulamalarıyla yakın alakaları var. Diyelim ki elimizde iki dağılım var, <span class="math inline">\(f\)</span> yaklaşmaya çalıştığımız bir model, <span class="math inline">\(g\)</span> ise onu yaklaşık olarak temsil etmeye uğraşan başka bir model, <span class="math inline">\(\theta\)</span> parametreleri üzerinden tanımlı, yani <span class="math inline">\(g(x|\theta)\)</span>. <span class="math inline">\(\theta\)</span> çoğunlukla veriden kestirilmeye çalışılır, <span class="math inline">\(\hat{\theta}\)</span> elde edilir, o zaman <span class="math inline">\(g(x|\hat{\theta})\)</span> olur. Bu iki dağılım / model arasındaki KL mesafesi</p>
<p><span class="math display">\[ I(f,g) = \int f(x) \log \bigg( \frac{f(x)}{g(x;\theta)} \bigg) \mathrm{d} x\]</span></p>
<p>(çoğunlukla çok boyutlu) entegrali ile hesaplanır. Kullback-Leibler <span class="math inline">\(I(f,g)\)</span> notasyonunu &quot;<span class="math inline">\(g\)</span>, <span class="math inline">\(f\)</span> yerine, onu yaklaşık olarak temsil edecek şekilde kullanıldığına kaybedilen enformasyon'' şeklinde kullandılar. Tabii ki uygulamalarda bu kayıbın olabildiği kadar az olmasını isteriz, yani <span class="math inline">\(I(f,g)\)</span>'i <span class="math inline">\(g\)</span> üzerinden minimize etmek önemli bir uygulama alanı.</p>
<p>Ayrıksal dağılımlar durumunda üstteki formül,</p>
<p><span class="math display">\[ I(f,g) = \sum_{i=1}^{k} p_i \log \bigg( \frac{p_i}{\pi_i} \bigg)  \]</span></p>
<p>Burada <span class="math inline">\(k\)</span> değişkeni rasgele değişkenin alabileceği <span class="math inline">\(k\)</span> farklı değeri temsil eder, <span class="math inline">\(i\)</span>'inci olayın olma olasılığı <span class="math inline">\(p_i\)</span>'dir, <span class="math inline">\(\pi_1,..,\pi_k\)</span> ise gerçek dağılımı yaklaşık olarak temsil etmeye uğraşan modeldir. Ayrıksal durumda <span class="math inline">\(0 &lt; p_i &lt; 1, 0 &lt; \pi_i &lt; 1\)</span>, ve <span class="math inline">\(\sum p_i = 1 = \sum \pi_i = 1\)</span>.</p>
<p>Formüllere yakından bakarsak onların birer beklenti hesabı olduğunu görebiliriz, <span class="math inline">\(\int f(x) ( \cdot ) \mathrm{d} x\)</span> şablonundaki formüllerin beklenti hesabı için kullanıldığını biliyoruz. Ayrıksal durumda <span class="math inline">\(\sum_{i=1}^{k}p_i( \cdot )\)</span>, ve bu beklenti iki dağılımın birbirine olan oranının negatifinin beklentisi, yani bu oranın ortalaması. Bu kavramın çıkışı çok derin ve temel, Boltzmann'ın 1877'de, Shannon'un sonra buldukları ile derin bağlantılar var.</p>
<p>Kabaca tarif etmek gerekirse, bir dağılımın içerdiği enformasyon onun negatif log'udur, iki dağılım arasındaki mesafe için negatif log'ların farkını alırız, ki fark cebirsel olarak bölümün log'u olarak tek bir log altında gruplanabilir, ve mümkün tüm sayılar üzerinden bu farkların beklentisini alırsak üstteki entegral (ya da toplam) formülünü elde etmiş oluruz.</p>
<p>KL mesafesi her zaman pozitiftir, tek bir durum haricinde, eğer <span class="math inline">\(f,g\)</span> eşitse - o zaman <span class="math inline">\(I(f,g) = 0\)</span>.</p>
<p>Bir örnek üzerinde görmek gerekirse, diyelim ki <span class="math inline">\(f\)</span> 2 parametreli bir Gamma dağılımı, <span class="math inline">\(\alpha=4,\beta=4\)</span>. Şimdi bu modeli yaklaşık olarak temsil etmeye uğraşan 4 tane seçeneği görelim, Weibull, lognormal, ters Gaussian, ve F dağılımı.</p>
<p><span class="math display">\[
\begin{array}{cc}
Yaklaşık Model &amp; I(f,g_i) \\
\hline \\
Weibull (\alpha=2,\beta=20) &amp; 0.04620 \\
Lognormal (\theta=2,\sigma^2=2) &amp; 0.67235 \\ 
Ters Gaussian (\alpha=16,\beta=64) &amp; 0.06008 \\
F dağılımı (\alpha=4,\beta=10) &amp; 5.74555
\end{array}
\]</span></p>
<p>Görüldüğü gibi Weibull en yakın olan (yani yaklaşık temsil sırasında en az enformasyon kaybeden o). Lognormal 3. sırada, F dağılımı en uzak olanı.</p>
<div class="figure">
<img src="stat_kl_01.png" />

</div>
<p>Bir başka örnek için <em>Testlere Devam</em> yazısındaki araba sayım verisine bakalım. Şimdi ham veriye en uygun olan dağılımı bulmaya çalışacağız.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
df <span class="op">=</span> pd.read_csv(<span class="st">&#39;../stat_tests2/vehicles.csv&#39;</span>,header<span class="op">=</span><span class="va">None</span>)
df.hist(bins<span class="op">=</span><span class="dv">13</span>)
plt.savefig(<span class="st">&#39;stat_kl_02.png&#39;</span>)</code></pre></div>
<div class="figure">
<img src="stat_kl_02.png" />

</div>
<p>Veride Poisson görünümü var. Eşit aralıklarda yapılan sayımların Poisson dağılımını takip etmeye meyilli olduğunu biliyoruz. Bu tezi kontrol edelim. Eğer, diyelim, Possion ve Gaussian arasında seçim yapacak olsak, bu seçimi KL mesafesi üzerinden yapabilirdik. Her iki durumda da dağılım parametrelerini veriden tahmin ediyor olurduk,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> np.<span class="bu">float</span>(df.mean()), np.<span class="bu">float</span>(df.std())</code></pre></div>
<pre><code>9.09433962264 3.54166574177</code></pre>
<p>Poisson durumunda ortalama hesabı <span class="math inline">\(\hat{\lambda}\)</span> için, Gaussian'da ise ortalama ve standart sapma <span class="math inline">\(\hat{\mu},\hat{\sigma}\)</span> için kullanılırdı.</p>
<p>Altta hem verinin hem de hipotez dağılımlardan üretilmiş rasgele sayıların histogramlarını hesaplıyoruz. Not: Aslında ham verinin histogramından sonra histogram kutularının (bins) sınırlarına bakarak Poisson ve Gaussian analitik dağılımlarının oraya tekabül eden yoğunluklarını analitik çağrılar ile bulabilirdik, fakat kolay yolu (!) seçtik, analitik dağılımlar için de rasgele sayı üretiyoruz, hem ham veri hem analitik durum için histogram hesaplıyoruz.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> scipy.stats
s <span class="op">=</span> <span class="dv">4000</span>
b <span class="op">=</span> <span class="dv">15</span>
r1 <span class="op">=</span> scipy.stats.poisson.rvs(mu<span class="op">=</span><span class="dv">8</span>, size<span class="op">=</span>s)
plt.hist(r1, bins<span class="op">=</span>b,color<span class="op">=</span><span class="st">&#39;b&#39;</span>)
plt.title(<span class="st">&#39;Poisson $\lambda = 8$&#39;</span>)
plt.xlim(<span class="dv">0</span>,<span class="dv">20</span>)
plt.savefig(<span class="st">&#39;stat_kl_04.png&#39;</span>)
plt.figure()
r2 <span class="op">=</span> scipy.stats.norm.rvs(<span class="dv">2</span>, <span class="dv">1</span>, size<span class="op">=</span>s)
plt.hist(r2, bins<span class="op">=</span>b,color<span class="op">=</span><span class="st">&#39;b&#39;</span>)
plt.title(<span class="st">&#39;Gaussian $\mu = 2,\sigma=1$&#39;</span>)
plt.xlim(<span class="dv">0</span>,<span class="dv">20</span>)
plt.savefig(<span class="st">&#39;stat_kl_06.png&#39;</span>)
plt.figure()
r3 <span class="op">=</span> scipy.stats.poisson.rvs(mu<span class="op">=</span><span class="fl">9.0943</span>, size<span class="op">=</span>s)
plt.hist(r3, bins<span class="op">=</span>b,color<span class="op">=</span><span class="st">&#39;b&#39;</span>)
plt.title(<span class="st">&#39;Poisson $\lambda = 9.1$&#39;</span>)
plt.xlim(<span class="dv">0</span>,<span class="dv">20</span>)
plt.savefig(<span class="st">&#39;stat_kl_07.png&#39;</span>)
plt.figure()
r4 <span class="op">=</span> scipy.stats.norm.rvs(<span class="fl">9.1</span>, <span class="fl">3.54</span>, size<span class="op">=</span>s)
plt.hist(r4, bins<span class="op">=</span>b,color<span class="op">=</span><span class="st">&#39;b&#39;</span>)
plt.title(<span class="st">&#39;Gaussian $\mu = 9.1,\sigma=3.54$&#39;</span>)
plt.xlim(<span class="dv">0</span>,<span class="dv">20</span>)
plt.savefig(<span class="st">&#39;stat_kl_08.png&#39;</span>)</code></pre></div>
<p><img src="stat_kl_04.png" /> <img src="stat_kl_06.png" /></p>
<p><img src="stat_kl_07.png" /> <img src="stat_kl_08.png" /></p>
<p>Şimdi veri ve tüm müstakbel analitik yoğunluklar arasında KL mesafelerini hesaplayalım,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> kl(p, q):
    <span class="cf">return</span> np.<span class="bu">sum</span>(p <span class="op">*</span> np.log(p <span class="op">/</span> q))

b <span class="op">=</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">30</span>)
eps <span class="op">=</span> <span class="fl">1e-5</span>
dh <span class="op">=</span> np.histogram(df, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps
h1 <span class="op">=</span> np.histogram(r1, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps
h2 <span class="op">=</span> np.histogram(r2, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps
h3 <span class="op">=</span> np.histogram(r3, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps
h4 <span class="op">=</span> np.histogram(r4, bins<span class="op">=</span>b, density<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]<span class="op">+</span>eps
<span class="bu">print</span> <span class="st">&#39;Poisson lambda = 8&#39;</span>, kl(h1, dh)
<span class="bu">print</span> <span class="st">&#39;Gaussian mu = 2,sigma=1&#39;</span>, kl(h2, dh)
<span class="bu">print</span> <span class="st">&#39;Poisson lambda = 9.1&#39;</span>, kl(h3, dh)
<span class="bu">print</span> <span class="st">&#39;Gaussian mu = 9.1,sigma=3.54&#39;</span>, kl(h4, dh)</code></pre></div>
<pre><code>Poisson lambda = 8 0.14722344735
Gaussian mu = 2,sigma=1 6.39721632939
Poisson lambda = 9.1 0.133099166073
Gaussian mu = 9.1,sigma=3.54 0.200156046018</code></pre>
<p>En yakın olan Poisson <span class="math inline">\(\lambda=9.1\)</span> olarak gözüküyor.</p>
<p>Çok Boyutlu Dağılımlar</p>
<p>Eğer bir dijital görüntü üzerinde çalışıyorsak, o resimdeki piksel değerlerinin de bir &quot;dağılımı'' olduğunu düşünebiliriz. Yani resmi, ya da resmin bir bölgesini bir teorik dağılımdan &quot;üretilmiş'' bir örneklem olarak görmek mümkün. Bu dağılımı çok boyutlu histogram alarak yaklaşık olarak hesaplayabiliriz. Eğer iki farklı resim bölgesini bu şekilde belirtirsek, bu iki dağılımı KL mesafesiyle karşılaştırabililiriz, ve böylece görüntüsel olarak iki bölgeyi karşılaştırabiliriz.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> PIL <span class="im">import</span> Image, ImageDraw

<span class="kw">def</span> draw_boxes_color(bs,imfile):
    im <span class="op">=</span> Image.<span class="bu">open</span>(imfile).convert(<span class="st">&#39;HSV&#39;</span>)
    arr <span class="op">=</span> np.asarray(im)
    draw <span class="op">=</span> ImageDraw.Draw(im)
    colors <span class="op">=</span> [<span class="st">&#39;magenta&#39;</span>,<span class="st">&#39;green&#39;</span>,<span class="st">&#39;white&#39;</span>,<span class="st">&#39;red&#39;</span>,<span class="st">&#39;yellow&#39;</span>]
    <span class="cf">for</span> i,b <span class="kw">in</span> <span class="bu">enumerate</span>(bs):
        fr <span class="op">=</span> b[<span class="dv">0</span>]<span class="op">;</span> to <span class="op">=</span> b[<span class="dv">1</span>]
        bnew <span class="op">=</span> [(fr[<span class="dv">0</span>],arr.shape[<span class="dv">0</span>]<span class="op">-</span>fr[<span class="dv">1</span>]),(to[<span class="dv">0</span>],arr.shape[<span class="dv">0</span>]<span class="op">-</span>to[<span class="dv">1</span>])]
        draw.rectangle(bnew,outline<span class="op">=</span>colors[i])
    plt.imshow(im)

<span class="kw">def</span> get_pixels(box, im):
    arr <span class="op">=</span> np.array(im)
    (yw,xw,d) <span class="op">=</span> arr.shape
    (bx1,by1) <span class="op">=</span> box[<span class="dv">0</span>]<span class="op">;</span> (bx2,by2) <span class="op">=</span> box[<span class="dv">1</span>]
    by1 <span class="op">=</span> yw<span class="op">-</span>by1<span class="op">;</span> by2 <span class="op">=</span> yw<span class="op">-</span>by2
    x1 <span class="op">=</span> <span class="bu">min</span>(bx1,bx2)<span class="op">;</span> x2 <span class="op">=</span> <span class="bu">max</span>(bx1,bx2)
    y1 <span class="op">=</span> <span class="bu">min</span>(by1,by2)<span class="op">;</span> y2 <span class="op">=</span> <span class="bu">max</span>(by1,by2)
    arr <span class="op">=</span> arr[y1:y2, x1:x2, :]
    <span class="cf">return</span> arr</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">box1 <span class="op">=</span> [(<span class="dv">35</span>,<span class="dv">144</span>),(<span class="dv">87</span>,<span class="dv">292</span>)]
box2 <span class="op">=</span> [(<span class="dv">106</span>,<span class="dv">183</span>),(<span class="dv">158</span>,<span class="dv">287</span>)]
box3 <span class="op">=</span> [(<span class="dv">117</span>,<span class="dv">86</span>),(<span class="dv">132</span>,<span class="dv">160</span>)]
f <span class="op">=</span> <span class="st">&#39;../../vision/vision_50colreg/castle.png&#39;</span>
draw_boxes_color([box1,box2],f)
plt.savefig(<span class="st">&#39;stat_kl_03.png&#39;</span>)
draw_boxes_color([box2,box3],f)
plt.savefig(<span class="st">&#39;stat_kl_05.png&#39;</span>)</code></pre></div>
<p><img src="stat_kl_03.png" /> <img src="stat_kl_05.png" /></p>
<p>Renklerin HSV kodlamasını kullanalım, o zaman her piksel kordinatında 3 değer olur. Bu durumda histogram almak demek çok boyutlu histogram demektir, üç boyut için sırasıyla 8,8,4 tane kutu tanımlarsak, 256 tane kutu elde ederiz. Bu kutuları <code>numpy.histogramdd</code> ile hesaplarız, KL karşılaştırması için kutuları düz vektör haline getirebiliriz -KL hesabında her iki tarafın birbirine tekabül eden kutuları kullanıldığı sürece problem yok- ve böylece nihai hesap yapılır.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> box_kl_dist(b1,b2,im):
    im <span class="op">=</span> Image.<span class="bu">open</span>(f).convert(<span class="st">&#39;HSV&#39;</span>)
    arr1 <span class="op">=</span> get_pixels(b1, im)
    r <span class="op">=</span> [(<span class="dv">0</span>,<span class="dv">255</span>),(<span class="dv">0</span>,<span class="dv">255</span>),(<span class="dv">0</span>,<span class="dv">255</span>)]

    arr1 <span class="op">=</span> np.reshape(arr1, (arr1.shape[<span class="dv">0</span>]<span class="op">*</span>arr1.shape[<span class="dv">1</span>],<span class="dv">3</span>))
    H1, edges <span class="op">=</span> np.histogramdd(arr1, bins<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">4</span>), normed<span class="op">=</span><span class="va">True</span>, <span class="bu">range</span><span class="op">=</span>r)
    H1 <span class="op">=</span> np.reshape(H1, (H1.shape[<span class="dv">0</span>]<span class="op">*</span>H1.shape[<span class="dv">1</span>]<span class="op">*</span>H1.shape[<span class="dv">2</span>], <span class="dv">1</span>))

    arr2 <span class="op">=</span> get_pixels(b2, im)
    arr2 <span class="op">=</span> np.reshape(arr2, (arr2.shape[<span class="dv">0</span>]<span class="op">*</span>arr2.shape[<span class="dv">1</span>],<span class="dv">3</span>))
    H2, edges <span class="op">=</span> np.histogramdd(arr2, bins<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">4</span>), normed<span class="op">=</span><span class="va">True</span>, <span class="bu">range</span><span class="op">=</span>r)
    H2 <span class="op">=</span> np.reshape(H2, (H2.shape[<span class="dv">0</span>]<span class="op">*</span>H2.shape[<span class="dv">1</span>]<span class="op">*</span>H2.shape[<span class="dv">2</span>], <span class="dv">1</span>))

    <span class="cf">return</span> kl(H1<span class="op">+</span>eps, H2<span class="op">+</span>eps)

<span class="bu">print</span> box_kl_dist(box1, box2, f)
<span class="bu">print</span> box_kl_dist(box2, box3, f)</code></pre></div>
<pre><code>7.55231179178e-06
7.30926985663e-07</code></pre>
<p>İkinci karşılaştırmada mesafe daha yakın duruyor; hakikaten de resimlere bakarsak ikinci resimdeki bölgelerin renksel olarak birbirine daha yakın olduğunu görebiliyoruz.</p>
<p>Kaynaklar</p>
<p>[1] Cover, <em>Elements of Information Theory</em></p>
<p>[2] Burnham, <em>Model Selection and Inference</em></p>
</body>
</html>
