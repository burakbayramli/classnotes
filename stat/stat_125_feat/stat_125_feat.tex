\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Özellik Ýþlemek (Feature Engineering)

Veri madenciliðinde "veriden veri yaratma" tekniði çok kullanýlýyor; mesela
bir sipariþ veri satýrýnda o sipariþin hangi zamanda (timestamp) olduðunu
belirten bir kolon varsa (ki çoðu zaman vardýr), bu kolonu "parçalayarak"
ek, daha genel, özetsel bilgi kolonlarý yaratýlabilir. Ya da kategoriksel
verileri pek çok farklý þekilde sayýsal hale çevirebiliriz, mesela 1-hot
kodlama ile N kategori N kolon haline gelir, eldeki kategoriye tekabül eden
öðe 1 diðerleri sýfýr yapýlýr. 

Özellik iþlemenin önemi yapay öðrenme açýsýndan önemi var, mesela bir SVM
sýnýflayýcýsýný en basit haliyle siyah/beyaz görüntüden sayý tanýma
probleminde kullandýk, ve diyelim yüzde 70 baþarý elde ettik. Þimdi çok
basit bir yeni özellik yaratalým, görüntüyü dikey ikiye bölelim, ve üstteki
ve alttaki siyah noktalarý toplayarak iki yeni kolon olarak görüntü
matrisine ekleyelim. Bu yeni özellikleri kullanýnca basit sýnýflayýcýnýn
yüzde 20 kadar tanýma baþarýsýnda ilerleme kaydettiðini göreceðiz!

Not: Derin yapay sýnýr aðlarý teknikleri ile özellik iþlemeye artýk gerek
olmadýðý söylenir, bu büyük ölçüde doðru. Bir DYSA farklý seviyelerdeki pek
çok farklý nöronlarý üzerinden aslýnda üstte tarif edilen türden yeni
özellikleri otomatik olarak yaratýr, ögrenir. Fakat yine de yeni
özellikleri elle yaratma tekniklerini bilmek iyi.

Þimdi farklý yöntemlere bakalým.

Zaman Kolonlarýný Zenginleþtirmek

Zaman kolonlarý çoðu zaman saniyeye kadar kaydedilir, bu bilgiyi alýp
mesela ay, mevsim, haftanýn günü, saat, iþ saati mi (9-5 arasý), akþam mý,
sabah mý, öðlen mi, vs. gibi ek bilgiler çýkartýlabilir. Tüm kolonlar veri
madenciliði algoritmasýna verilir, ve algoritma belki öðlen saati ile
sipariþ verilmiþ olmasý arasýnda genel bir baðlantý bulacaktýr.

Python + Pandas ile bir zaman kolonu þöyle parçalanabilir, örnek veri
üzerinde görelim, sadece iki kolon var, müþteri no, ve sipariþ zamaný,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
from StringIO import StringIO
s = """customer_id;order_date
  299;2012-07-20 19:44:55.661000+01:00
  421;2012-02-17 21:54:15.013000+01:00
  437;2012-02-20 22:18:12.021000+01:00
  463;2012-02-20 23:46:21.587000+01:00
  482;2012-05-21 09:50:02.739000+01:00
  607;2012-02-21 11:57:12.462000+01:00
  641;2012-02-21 13:40:28.088000+01:00
  674;2012-08-21 14:53:15.851000+01:00
  780;2012-02-23 10:31:05.571000+01:00
  """
df = pd.read_csv(StringIO(s),sep=';', parse_dates=True)

def f(x):
  tmp = pd.to_datetime(x['order_date'])
  tpl = tmp.timetuple(); yymm = int(tmp.strftime('%m%d'))
  spring = int(yymm >= 321 and yymm < 621)
  summer = int(yymm >= 621 and yymm < 921)
  fall = int(yymm >= 921 and yymm < 1221)
  winter = int( spring==0 and summer==0 and fall==0 )
  warm_season = float(tpl.tm_mon >= 4 and tpl.tm_mon <= 9)
  work_hours = float(tpl.tm_hour > 9 and tpl.tm_hour < 17)
  morning = float(tpl.tm_hour >= 7 and tpl.tm_hour <= 11)
  noon = float(tpl.tm_hour >= 12 and tpl.tm_hour <= 14)
  afternoon = float(tpl.tm_hour >= 15 and tpl.tm_hour <= 19)
  night = int (morning==0 and noon==0 and afternoon==0)

  return pd.Series([tpl.tm_hour, tpl.tm_mon,
                    tpl.tm_wday, warm_season,
                    work_hours, morning, noon, afternoon, night,
                    spring, summer, fall, winter])

cols = ['ts_hour','ts_mon','ts_wday','ts_warm_season',\
  'ts_work_hours','ts_morning','ts_noon','ts_afternoon',\
  'ts_night', 'ts_spring', 'ts_summer', 'ts_fall', 'ts_winter']

df[cols] = df.apply(f, axis=1)
print df[cols]
\end{minted}

\begin{verbatim}
   ts_hour  ts_mon  ts_wday  ts_warm_season  ts_work_hours  ts_morning  \
0     18.0     7.0      4.0             1.0            0.0         0.0   
1     20.0     2.0      4.0             0.0            0.0         0.0   
2     21.0     2.0      0.0             0.0            0.0         0.0   
3     22.0     2.0      0.0             0.0            0.0         0.0   
4      8.0     5.0      0.0             1.0            0.0         1.0   
5     10.0     2.0      1.0             0.0            1.0         1.0   
6     12.0     2.0      1.0             0.0            1.0         0.0   
7     13.0     8.0      1.0             1.0            1.0         0.0   
8      9.0     2.0      3.0             0.0            0.0         1.0   

   ts_noon  ts_afternoon  ts_night  ts_spring  ts_summer  ts_fall  ts_winter  
0      0.0           1.0       0.0        0.0        1.0      0.0        0.0  
1      0.0           0.0       1.0        0.0        0.0      0.0        1.0  
2      0.0           0.0       1.0        0.0        0.0      0.0        1.0  
3      0.0           0.0       1.0        0.0        0.0      0.0        1.0  
4      0.0           0.0       0.0        1.0        0.0      0.0        0.0  
5      0.0           0.0       0.0        0.0        0.0      0.0        1.0  
6      1.0           0.0       0.0        0.0        0.0      0.0        1.0  
7      1.0           0.0       0.0        0.0        1.0      0.0        0.0  
8      0.0           0.0       0.0        0.0        0.0      0.0        1.0  
\end{verbatim}

Sýcak mevsim (warm season) Mart-Eylül aylarýný kapsar, bu ikisel bir
deðiþken hale getirildi. Belki sipariþin, ya da diðer baþka bir verinin
bununla bir alakasý vardýr. Genel 4 sezon tek baþýna yeterli deðil midir?
Olabilir, fakat bazý kalýplar / örüntüler (patterns) belki sýcak / soðuk
mevsim bilgisiyle daha çok baðlantýlýdýr. 

Ayný þekilde saat 1-24 arasýnda bir sayý olarak var, fakat "iþ saatini"
ayrý bir ikisel deðiþken olarak kodlamak yine bir "kalýp yakalama"
þansýmýzý arttýrabilir. Bu kolonun ayrý bir þekilde kodlanmýþ olmasý veri
tasarýmý açýsýndan ona önem verildiðini gösterir, ve madencilik
algoritmalarý bu kolonu, eðer ona baðlý bir kalýp var ise,
yakalayabilirler.

Not: Burada ufak bir pürüz sabah, öðlen, akþamüstü gibi zamanlarý kodlarken
çýktý. Gece 19'dan sonra ve 7'den önce bir sayý olacaktý, fakat bu durumda
$x>19$ ve $x<7$ hiçbir sonuç getirmeyecekti. Burada saatlerin 24 sonrasý baþa
dönmesi durumu problem çýkartýyordu, tabii ki karþýlaþtýrma ifadelerini
çetrefilleþtirerek bu iþ çözülebilir, ama o zaman kod temiz olmaz (mesela
($x>19$ ve $x<24$) ya da ($x>0$ ve $x<7$) yapabilirdik). Temiz kod için gece
haricinde diðer tüm seçenekleri kontrol ediyoruz, ve gece "sabah, öðlen,
akþamüstü olmayan þey" haline geliyor. Ayný durum mevsimler için de
geçerli. Onun için 

\begin{minted}[fontsize=\footnotesize]{python}
night = int (morning==0 and noon==0 and afternoon==0)
\end{minted}

kullanýldý.

Kategorileri Ýkileþtirme

Yapay öðrenim algoritmalarýnýn çoðu zaman hem kategorik hem sayýsal
deðerleri ayný anda bulunduran verilerle iþ yapmasý gerekebiliyor. Ayrýca
literatüre bakýlýnca görülür ki çoðunlukla bir algoritma ya biri, ya diðeri
ile çalýþýr, ikisi ile ayný anda çalýþmaz (çalýþanlar var tabii, mesela
karar aðaçlarý -decision tree-). Bu gibi durumlarda iki seçenek var, ya
numerik veri kategoriselleþtirilir (ayrýksallaþtýrýlýr), ya da kategorik
veri numerik hale getirilir.

Bu durumda, kategorik bir kolon eyalet için, eyaletin Ohio olup olmamasý
baþlý baþýna ayrý bir kolon olarak gösteriliyor. Ayný þekilde Nevada. Bu
kodlamaya literatürde 1-hot kodlamasý adý veriliyor. KMeans, lojistik
regresyon gibi metotlara girdi vermek için bu transformasyon
kullanýlabilir.

\begin{minted}[fontsize=\footnotesize]{python}
import numpy as np
import pandas as pd, os
import scipy.sparse as sps
from sklearn.feature_extraction import DictVectorizer

def one_hot_dataframe(data, cols, replace=False):
    vec = DictVectorizer()
    mkdict = lambda row: dict((col, row[col]) for col in cols)
    vecData = pd.DataFrame(vec.fit_transform(data[cols].apply(mkdict, axis=1)).toarray())
    vecData.columns = vec.get_feature_names()
    vecData.index = data.index
    if replace is True:
        data = data.drop(cols, axis=1)
        data = data.join(vecData)
    return (data, vecData, vec)

data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
        'year': [2000, 2001, 2002, 2001, 2002],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}

df = pd.DataFrame(data)

df2, _, _ = one_hot_dataframe(df, ['state'], replace=True)
print df2
\end{minted}

\begin{verbatim}
   pop  year  state=Nevada  state=Ohio
0  1.5  2000           0.0         1.0
1  1.7  2001           0.0         1.0
2  3.6  2002           0.0         1.0
3  2.4  2001           1.0         0.0
4  2.9  2002           1.0         0.0
\end{verbatim}

Unutmayalým, kategorik deðerler bazen binleri bulabilir (hatta sayfa
týklama tahmini durumunda mesela milyonlar, hatta milyarlar), bu da
binlerce yeni kolon demektir. Yani 1/0 kodlamasý, yani 1-hot iþleminden ele
geçen yeni blok içinde aslýnda oldukca çok sayýda sýfýr deðeri olacak
(sonuçta her satýrda binlerce 'þey' içinde sadece bir tanesi 1 oluyor),
yani bu bloðun bir seyrek matris olmasý iyi olurdu. O zaman matrisin
tamamýný \verb!sps.csr_matrix! ya da \verb!sps.lil_matrix! ile gerçekten
seyrek formata çevirebiliriz, ve mesela scikit-learn paketi, numpy, scipy
iþlemleri seyrek matrisler ile hesap yapabilme yeteneðine
sahip. Seyrekselleþtirince ne elde ediyoruz?  Sýfýrlarý depolamadýðýmýz
için sadece sýfýr olmayan deðerler ile iþlem yapýyoruz, o ölçüde kod
hýzlanýyor, daha az yer tutuyor.

Dikkat etmek gerekir ki yeni kolonlarý üretince deðerlerin yerleri
sabitlenmiþ olur. Her satýr bazýnda bazen state=Ohio, state=Nevada, bazen
sadece state=Ohio üretiyor olamayýz. Üstteki örnekte her zaman 4 tane kolon
elde edilmelidir.

Not: 1-hot yerine bir diðer seçenek kategoriyi bir indise çevirmek (tüm
kategorileri sýralayýp kaçýncý olduðuna bakarak mesela) sonra bu sayýyý
ikisel sistemde belirtmek, eðer 'a' sayýsý 30 indisine tekabül ediyorsa, 30
ikisel sistemde 11110, bu deðer kullanýlýr (aslýnda bu son tarif edilen
sistemin 1-hot sistemden daha iyi iþlediði rapor ediliyor).

Anahtarlama Numarasý (1-Hot Encoding, Hashing Trick)

Fakat bir problem var, dokümaný temsil eden ve içinde 1 ya da
0 hücreli özellik vektörünü (feature vector) oluþturmak için tüm
kelimelerin ne olduðunu bilmeliyiz. Yani veriyi bir kere baþtan sonra
tarayarak bir sözlük oluþturmalýyýz (ki öyle yapmaya mecbur kaldýk) ve
ancak ondan sonra her doküman için hangi kelimenin olup olmadýðýný
saptamaya ve onu kodlamaya baþlayabiliriz. Halbuki belgelere bakar
bakmaz, teker teker giderken bile hemen bir özellik vektörü
oluþturabilseydik daha iyi olmaz mýydý?

Bunu baþarmak için anahtarlama numarasýný kullanmamýz lazým. Bilindiði gibi
temel yazýlým bilime göre bir kelimeyi temsil eden bir anahtar (hash)
üretebiliriz, ki bu hash deðeri bir sayýdýr. Bu sayýnýn en fazla kaç
olabileceðinden hareketle (hatta bu sayýya bir limit koyarak) özellik
vektörümüzün boyutunu önceden saptamýþ oluruz.  Sonra kelimeye bakarýz,
hash üretiriz, sonuç mesela 230 geldi, o zaman özellik vektöründeki
230'uncu kolonun deðerini 1 yaparýz.

\begin{minted}[fontsize=\footnotesize]{python}
d_input = dict()

def add_word(word):
  hashed_token = hash(word) % 127
  d_input[hashed_token] = d_input.setdefault(hashed_token, 0) + 1

add_word("obama")
print d_input
\end{minted}

\begin{verbatim}
{48: 1}
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
add_word("politics")
print d_input
\end{minted}

\begin{verbatim}
{48: 1, 91: 1}
\end{verbatim}

Üstteki kodda bunun örneðini görüyoruz. Hash sonrasý mod uyguladýk (yüzde
iþareti ile) ve hash sonucunu en fazla 127 olacak þekilde sýnýrladýk.
Potansiyel problemler ne olabilir? Hashing mükemmel deðildir, çarpýþma
(collision) olmasý mümkündür yani nadiren farklý kelimelerin ayný numaraya
eþlenebilmesi durumu. Bu problemleri iyi bir anahtarlama algoritmasý
kullanarak, mod edilen sayýyý büyük tutarak çözmek mümkündür, ya da bu tür
nadir çarpýþmalar "kabul edilir hata" olarak addedilebilir.

Pandas kullanarak bir Dataframe'i otomatik olarak anahtarlamak istersek,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
  'year': [2000, 2001, 2002, 2001, 2002],
  'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}

data = pd.DataFrame(data)
print data
\end{minted}

\begin{verbatim}
   pop   state  year
0  1.5    Ohio  2000
1  1.7    Ohio  2001
2  3.6    Ohio  2002
3  2.4  Nevada  2001
4  2.9  Nevada  2002
\end{verbatim}

Þimdi bu veri üzerinde sadece eyalet (state) için bir anahtarlama numarasý
yapalým

\begin{minted}[fontsize=\footnotesize]{python}
def hash_col(df,col,N):
  for i in range(N): df[col + '_' + str(i)] = 0.0
  df[col + '_hash'] = df.apply(lambda x: hash(x[col]) % N,axis=1)    
  for i in range(N):
      idx = df[df[col + '_hash'] == i].index
  df.ix[idx,'%s_%d' % (col,i)] = 1.0
  df = df.drop([col, col + '_hash'], axis=1)
  return df

print hash_col(data,'state',4)
\end{minted}

\begin{verbatim}
   pop  year  state_0  state_1  state_2  state_3
0  1.5  2000      0.0      0.0      0.0      0.0
1  1.7  2001      0.0      0.0      0.0      0.0
2  3.6  2002      0.0      0.0      0.0      0.0
3  2.4  2001      0.0      0.0      0.0      1.0
4  2.9  2002      0.0      0.0      0.0      1.0
\end{verbatim}

Baþtan Seyrek Matris ile Çalýþmak

Büyük Veri ortamýnda, eðer kategorik deðerler milyonlarý buluyorsa, o zaman
üstteki gibi normal Numpy matrisinden seyreðe geçiþ yapmak bile külfetli
olabilir. Bu durumlarda daha en baþtan seyrek matris üretiyor
olmalýyýz. Mevcut tüm deðerleri önceden bildiðimizi farz edersek,

\begin{minted}[fontsize=\footnotesize]{python}
import numpy as np
import pandas as pd, os
import scipy.sparse as sps
import itertools

def one_hot_column(df, cols, vocabs):
    mats = []; df2 = df.drop(cols,axis=1)
    mats.append(sps.lil_matrix(np.array(df2)))
    for i,col in enumerate(cols):
        mat = sps.lil_matrix((len(df), len(vocabs[i])))
        for j,val in enumerate(np.array(df[col])):
            mat[j,vocabs[i][val]] = 1.
        mats.append(mat)

    res = sps.hstack(mats)    
    return res
            
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
        'year': ['2000', '2001', '2002', '2001', '2002'],
        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}

df = pd.DataFrame(data)
print df

vocabs = []
vals = ['Ohio','Nevada']
vocabs.append(dict(itertools.izip(vals,range(len(vals)))))
vals = ['2000','2001','2002']
vocabs.append(dict(itertools.izip(vals,range(len(vals)))))

print vocabs

print one_hot_column(df, ['state','year'], vocabs).todense()
\end{minted}

\begin{verbatim}
   pop   state  year
0  1.5    Ohio  2000
1  1.7    Ohio  2001
2  3.6    Ohio  2002
3  2.4  Nevada  2001
4  2.9  Nevada  2002
[{'Ohio': 0, 'Nevada': 1}, {'2002': 2, '2000': 0, '2001': 1}]
[[ 1.5  1.   0.   1.   0.   0. ]
 [ 1.7  1.   0.   0.   1.   0. ]
 [ 3.6  1.   0.   0.   0.   1. ]
 [ 2.4  0.   1.   0.   1.   0. ]
 [ 2.9  0.   1.   0.   0.   1. ]]
\end{verbatim}

\verb!one_hot_column! çaðrýsýna bir "sözlükler listesi" verdik, sözlük her
kolon için o kolonlardaki mümkün tüm deðerleri bir sýra sayýsý ile
eþliyor. Sözlük listesinin sýrasý kolon sýrasýna uyuyor olmalý.

Niye sözlük verdik? Bunun sebebi eðer azar azar (incremental) ortamda iþ
yapýyorsak, ki Büyük Veri (Big Data) ortamýnda her zaman azar azar yapay
öðrenim yapmaya mecburuz, o zaman bir kategorik kolonun mevcut tüm
deðerlerine azar azar ulaþamazdýk (verinin baþýnda isek, en sonundaki bir
kategorik deðeri nasýl görelim ki?). Fakat önceden bu listeyi baþka
yollarla elde etmiþsek, o zaman her öne-hot iþlemine onu parametre olarak
geçiyoruz.

Sözlük niye \verb!one_hot_dataframe! çaðrýsý dýþýnda yaratýldý? Bu çaðrý
düz bir liste alýp oradaki deðerleri sýrayla bir sayýyla eþleyerek her
seferinde bir sözlük yaratabilirdi. Bunu yapmadýk, çünkü sözlük yaratýmýnýn
sadece bir kere, \verb!one_hot_dataframe! dýþýnda olmasýný istiyoruz. Yine
Büyük Veri ortamýný düþünenelim, eþleme (map) için mesela bir script
yazdýk, bu script içinde (basýnda) hemen sözlükler yaratýlýrdý. Daha sonra
verinin tamamý için, azar azar sürekli \verb!one_hot_dataframe! çaðrýsý
yapýlacaktýr. O zaman arka arkaya sürekli ayný veriyi (sözlükleri) sýfýrdan
tekrar yaratmamýz gerekirdi. Bu gereksiz performans kaybý demek
olacaktý. Unutmayalým, Büyük Veri ortamýnda tek bir kategorik kolonun
milyonlarca deðiþik deðeri olabilir!

Azar Azar Ýþlemek (Incremental, Minibatch Processing)

Çoðu zaman onlarca kategori, birkaç milyonluk satýr içeren bir veriye
bakmamýz gerekiyor; biliyoruz ki bu kadar veri için Büyük Veri
teknolojilerine (mesela Spark, Hadoop gibi) geçmek gereðinden fazla külfet
getirecek, elimizdeki dizüstü, masaüstü bilgisayarý bu iþlemler için
yeterli olmalý, fakat çoðu kütüphane tek makinada azar azar iþlem yapmak
için yazýlmamýþ. Mesela üstte görülen anahtarlama yöntemi anahtarlama
baþlamadan önce tüm verinin hafýzaya alýnmasýný gerektiriyor.

Bu durumda kendimiz çok basit Python kavramlarýný, iyi bir anahtarlama
kodunu, ve lineer cebir hesaplarýnda seyreklik (sparsity) tekniklerini
kullanarak ufak veri parçalarý iþleyen bir ortamý yaratabiliriz.

Örnek veri olarak [4] yazýsýnda görülen oy kalýplarý verisini biraz
deðiþtirerek yeni bir analiz için kullanalým. Veri oy verenlerin ýrk,
cinsiyet, meslek, hangi partiye oy verdikleri ve kazançlarýný kaydetmiþ,
biz analizimizde bahsedilen kategorilerin bu kiþilerin kazancýyla
baðlantýlý olup olmadýðýna bakacaðýz. Veriyi oluþturalým,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('../stat_logit/nes.dat',sep=r'\s+')
df = df[['presvote','year','gender','income','race','occup1']]
df = df.dropna()
df.to_csv('nes2.csv',index=None)
\end{minted}

Önce kategorilerden ne kadar var, sayalým. Basit toplam yani,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('nes2.csv')
print u'tüm veri', len(df)
print 'cinsiyet', np.array(df['gender'].value_counts())
print u'ýrk', np.array(df['race'].value_counts())
print 'parti', np.array(df['presvote'].value_counts())
print u'kazanç', df['income'].mean()
\end{minted}

\begin{verbatim}
tüm veri 13804
cinsiyet [7461 6343]
ýrk [12075  1148   299   180    85    17]
parti [6998 6535  271]
kazanç 3.07649956534
\end{verbatim}

Mesela son sonuçtaki her hücre belli bir partiye verilen oylarýn sayýsý;
veriye göre üç farklý kategori varmýþ demek ki, veri ABD için olduðuna göre
bunlardan ilk ikisi bilinen iki büyük parti, üçüncü hücre de herhalde
baðýmsýz adaylar. 

Kazanç 1 ile 5 arasýnda tam sayýlar (1 az, 5 çok) bu sayýlarý kategorik
olarak kabul edip aslýnda çok çýktýlý bir sýnýflayýcý eðitmeyi de
seçebilirdik, fakat bu örnek için bu sayýlarý reel hedef olarak aldýk: test
verisinde tahminleri bakýlýrsa 2.5'lük kazanç tahminleri görülebilir, bu
yüzden.

Kategorik verileri ikileþtirmeye gelelim. Burada üç nokta önemli, veriyi
azar azar iþleyeceðiz demiþtik, ve veriyi seyrek matris olarak almak
istiyoruz, ve hangi kategorik deðerin hangi kolona eþleneceðini elle
tanýmlamak istemiyoruz (eþleme otomatik olmalý). Seyreklik önemli çünkü
eðer 1000 farklý kategorik deðere sahip olan 10 tane kolon varsa, bu 10000
tane yeni kolon yaratýlmasý demektir - her farklý kategori için o deðere
tekabül eden kolon 1 olacak gerisi 0 olacak. Bu rakamlar orta ölçekte bile
rahatlýkla milyonlara ulaþabilir. Eðer ikileþtirme için seyrek matris
kullanýrsak çoðu sýfýr olan deðerler hafýzada bile tutulmaz. Eþleme
otomatik olmalý, zaten onun için anahtarlama yapacaðýz.

Anahtarlama icin \verb!sklearn.feature_extraction.text.HashingVectorizer!
var,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.feature_extraction.text import HashingVectorizer
import numpy as np
vect = HashingVectorizer(n_features=20)
a = ['aa','bb','cc']
res = vect.transform(a)
print res
\end{minted}

\begin{verbatim}
  (0, 5)	1.0
  (1, 19)	1.0
  (2, 18)	-1.0
\end{verbatim}

Sonuçlar seyrek matris olarak, ve üç deðer için üç ayrý satýr olarak
geldi. Anahtarlama niye bazen -1 bazen +1 veriyor? Aslýnda bu bizim için
çok faydalý, çünkü birazdan PCA iþleteceðiz, ve PCA her veri kolonunun
sýfýrda ortalanmýþ olmasýný ister. Üstteki teknikte anahtar üreten
fonksiyon -1,+1 arasýnda rasgele seçim yapýyor gibi duruyor, bize göre bu
üretilen anahtar kolonlarýnda -1, +1 deðerlerinin doðal olarak dengelenmesi
için yapýlmýþ, böylece otomatik olarak ortalamalarý sýfýra
inecektir. Akýllýca bir teknik.

Devam edelim, sonucu tek satýr olacak þekilde kendimiz tekrar
düzenleyebiliriz. O zaman Python \verb!yield! kavramýný [3] kullanarak
(azar azar satýr okumak için), anahtarlama, ve seyrek matrisler ile þu
þekilde bir kod olabilir,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.feature_extraction.text import HashingVectorizer
import numpy as np
import pandas as pd, csv
import scipy.sparse as sps

HASH = 30
vect = HashingVectorizer(decode_error='ignore',n_features=HASH)

def get_row(cols):
    with open("nes2.csv", 'r') as csvfile:
        rd = csv.reader(csvfile)
        headers = {k: v for v, k in enumerate(next(rd))}
        for row in rd:
            label = float(row[headers['income']])
            rrow = [x + str(row[headers[x]]) for x in headers if x in cols]
            X_train = vect.transform(rrow)
            yield X_train.tocoo(), label            

def get_minibatch(row_getter,size=10):
    X_train = sps.lil_matrix((size,HASH))
    y_train = []
    for i in range(size):
        cx,y = row_getter.next()
        for dummy,j,val in zip(cx.row, cx.col, cx.data): X_train[i,j] = val
        y_train.append(y)
    return X_train, y_train

# tek bir satir goster    
cols = ['gender','income','race','occup1']
row_getter = get_row(cols)
X,y = get_minibatch(row_getter,size=1)
print y, X.todense()
\end{minted}

\begin{verbatim}
[4.0] [[ 0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.
   0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.]]
\end{verbatim}

Ýlgilendiðimiz kolon listesini \verb!get_row!'a verip bir gezici fonksiyon
yarattýk. Bu geziciyi \verb!get_minibatch!'e verdik, kaç tane satýr
istediðimizi ona söylüyoruz, o bize istenen kadar satýrý arka planda
geziciye sorarak seyrek matris olarak veriyor. 10 tane daha isteyelim,

\begin{minted}[fontsize=\footnotesize]{python}
X,y = get_minibatch(row_getter,size=10)
print len(y), X.shape, type(X)
\end{minted}

\begin{verbatim}
10 (10, 30) <class 'scipy.sparse.lil.lil_matrix'>
\end{verbatim}

PCA

Lineer Cebir'in temel bileþen analizi (PCA) tekniðini kullanarak boyut
azaltmasý yapabiliriz. Veriyi yine satýr satýr iþleyerek PCA hesabý yapan
teknikler var, kod veriyi seyrek formatta da alabiliyor. Bu kod lineer
cebir PCA yazýsýnda iþlendi. 

\begin{minted}[fontsize=\footnotesize]{python}
import sys; sys.path.append('../../linear/linear_app05pca')
import ccipca
cols = ['gender','income','race','occup1']
row_getter = get_row(cols)
pca = ccipca.CCIPCA(n_components=10,n_features=30)
for i in range(10000): 
    X,y = get_minibatch(row_getter,size=1)
    pca.partial_fit(X)
pca.post_process()

print 'varyans orani'
print pca.explained_variance_ratio_
\end{minted}

\begin{verbatim}
varyans orani
[ 0.36086926  0.16186391  0.13377998  0.09440711  0.0702763   0.05113956
  0.04768294  0.0343724   0.02336052  0.02224802]
\end{verbatim}

Her bileþenin verideki varyansýn ne kadarýný açýkladýðý görülüyor. 

Peki 30 kolonu 10 kolona indirdik, acaba veri temsilinde, tahmin etmek
amacýnda ilerleme elde ettik mi?  Veriyi PCA'nýn bulduðu uzaya yansýtýp bu
boyutu azaltýlmýþ veriyi regresyonda kullansak ne olur acaba? Yansýtma ve
regresyon,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.linear_model import SGDRegressor
clf = SGDRegressor(random_state=1, n_iter=1)
row_getter = get_row(cols)
P = pca.components_.T
for i in range(10000):
    X_train, y_train = get_minibatch(row_getter,1)
    Xp = np.dot((X_train-pca.mean_),P)
    clf.partial_fit(Xp, y_train)
\end{minted}

Þimdi sonraki 1000 satýrý test için kullanalým,

\begin{minted}[fontsize=\footnotesize]{python}
y_predict = []
y_real = []
for i in range(1000):
    X_test,y_test = get_minibatch(row_getter,1)
    Xp = np.dot((X_test-pca.mean_),P)
    y_predict.append(clf.predict(Xp)[0])
    y_real.append(y_test[0])
y_predict = np.array(y_predict)
y_real = np.array(y_real)

err = np.sqrt(((y_predict-y_real)**2).sum()) / len(y_predict)
print 'ortalama tahmin hatasi', err
print 'maksimum deger', np.max(y_real)
\end{minted}

\begin{verbatim}
ortalama tahmin hatasi 0.0105872845541
maksimum deger 5.0
\end{verbatim}

1 ile 5 arasýnda gidip gelen deðerlerin tahmininde 0.01 civarý ortalama
hata var. Fena deðil. Peki verinin kendisini olduðu gibi alýp regresyonda
kullansaydýk? Hedef verisi kazanç, kaynak kolonlarý geri kalan
kategoriler. Üstte olduðu gibi veri parçalarý 1000'er satýr, 10 parça
olarak alacaðýz, yani 10,000 satýr modeli eðitmek için kullanýlacak. Geri
kalanlar test verisi olacak. 

\verb!sklearn.linear_model.SGDRegressor! ufak seyrek matris parçalarý ile
eðitilebiliyor,

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.linear_model import SGDRegressor
clf = SGDRegressor(random_state=1, n_iter=1)
row_getter = get_row(cols)

y_predict = []; y_real = []

for i in range(10):
    X_train, y_train = get_minibatch(row_getter,1000)
    clf.partial_fit(X_train, y_train)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
X_test,y_test = get_minibatch(row_getter,1000)
y_predict = clf.predict(X_test) 

err = np.sqrt(((y_predict-y_test)**2).sum()) / len(y_predict)
print 'ortalama tahmin hatasi', 
\end{minted}

\begin{verbatim}
ortalama tahmin hatasi 0.0208096951078
\end{verbatim}

Bu sonuç ta hiç fena deðil.  Sonuç olarak veri içinde bazý kalýplar
olduðunu gördük, tahmin yapabiliyoruz. Hangi kolonlarýn daha önemli
olduðunu bulmak için her kolonu teker teker atýp hatanýn yukarý mý aþaðý mý
indiðine bakabilirdik.

Tekrar vurgulamak gerekirse: üstteki analizde aslýnda çok fazla kategorik
veri yok, yani \verb!statsmodels.formula.api! üzerinden güzel formüllerle,
regresyon çýktýsýnda her kategorik deðerin güzelce listelendiði türden bir
kullanýma da gidebilirdik. Bu yazýda göstermeye çalýþtýðýmýz çok fazla
veri, çok fazla kolon / kategori olduðunda ve tek makina ortamýnda takip
edilebilecek çözümler.

Zaman Karþýlaþtýrmak

Eðer 23:50 ile sabah 00:10 zamanýný karþýlaþtýrmak istersek ne yaparýz?
Eðer saat ve dakika farkýný direk hesaplasak bu iki zamanýn çok uzak
olduðunu düþünebilirdik. Fakat aslýnda aralarýnda 20 dakika var, zaman
dönüp baþa gelen bir kavram. 

\includegraphics[width=20em]{stat_feat_01.png}

Bu hesabý yapmak için bir yöntem çember, açýlar kullanmak. Gün içindeki
zamaný 0 ile 1 arasýnda kodlarýz, sonra bu büyüklüðü $2\pi$ ile çarparýz,
bu bize çember üzerindeki bir noktayý verir, yani zamaný açýya çevirmiþ
oluruz. Sonra açýnýn $\sin$, $\cos$ deðerini hesaplayýp iki rakam elde
ederiz, bu iki sayý bize gün içindeki zamaný temsil eden bir büyüklük
verir.

Bu büyüklükleri birbirleri ile karþýlaþtýrmak daha kolay, üstteki þekilde
$\theta_2$ ve $\theta_3$ birbirine yakýn, karþýlaþtýrma yaparken $\sin$
bize dikey eksendeki izdüþümü, $\cos$ yatay eksendeki izdüþümünü verir,
$\theta_2,\theta_3$ için y eksenindeki yansýma birbirine çok yakýn. Eksen x
üzerindeki yansýma farklý biri eksi biri artý yönde fakat yine de mutlak
deðer baðlamýnda birbirlerine çok yakýnlar. Ýstediðimiz de bu zaten. 

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.linalg as lin

t1 = 0.12 * 2*np.pi
t2 = 0.97 * 2*np.pi
t3 = 0.03 * 2*np.pi

d1 = (np.cos(t1), np.sin(t1))
d2 = (np.cos(t2), np.sin(t2))
d3 = (np.cos(t3), np.sin(t3))

print ("%f %f" % d1)
print ("%f %f" % d2)
print ("%f %f" % d3)

print u'uzaklýk 1-2 =', lin.norm(np.array(d1)-np.array(d2))
print u'uzaklýk 2-3 =', lin.norm(np.array(d2)-np.array(d3))
\end{minted}

\begin{verbatim}
0.728969 0.684547
0.982287 -0.187381
0.982287 0.187381
uzaklýk 1-2 = 0.907980999479
uzaklýk 2-3 = 0.374762629171
\end{verbatim}

Kaynaklar 

[1] Teetor, {\em R Cookbook}

[2] Scikit-Learn Documentation, {\em 4.2. Feature extraction}, \url{http://scikit-learn.org/dev/modules/feature_extraction.html}

[3] Bayramli, 
    {\em Fonksiyon Gezmek ve Yield}, 
    \url{https://burakbayramli.github.io/dersblog/sk/2011/02/fonksiyon-gezmek-ve-yield.html}

[4] Bayramli, Istatistik, {\em Lineer Regresyon}

\end{document}



