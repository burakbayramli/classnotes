\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Parçacýk Filtreleri (Particle Filters)

Parcaçýk filtreleri Kalman filtrelerinde olduðu gibi saklý bir konum
bilgisi hakkýnda dýþ ölçümler üzerinden kestirme hesabý yapabilir. Her
parçacýk bir hipotezi, farklý bir konum bilgisini temsil eder, olasýlýðý,
olurluðu ölçüm fonksiyonudur.  Eðer bu olasýlýk deðeri problemden direk
elde edilebilen bir þey deðilse, bir ölçüm / hipotez / tahmin arasýndaki
mesafeyi (hatayý) olurluða çevirmek mümkün. Burada genellikle

$$ p(y_t|x_t) \sim e^{-\lambda \varepsilon^2}$$

fonksiyonu kullanýlýr, $\lambda$ bir tür hassaslýk parametresi, bu
parametre üzerinden olurluk ya daha az ya da daha fazla etkin hale gelir,
$\varepsilon$ ölçüm ve tahmin arasýndaki bir mesafe olacaktýr. 

\begin{minted}[fontsize=\footnotesize]{python}
x = np.linspace(0,10,100)
def f(x,lam): return np.exp(-lam * x**2)
plt.plot(x,f(x,lam=0.1))
plt.plot(x,f(x,lam=0.5))
plt.plot(x,f(x,lam=1.0))
plt.savefig('tser_pf_03.png')
\end{minted}

\includegraphics[width=20em]{tser_pf_03.png}

Kalman Filtrelerine ve Saklý Markov Modellerinde gördüðümüz modeli
hatýrlayalým, 

\includegraphics[height=2.5cm]{tser_pf_02.png}

Bu modelde gözlemler, yani dýþarýdan görülen ölçümler $y_1,y_2,..$ ve bu rasgele
deðiþkenler þartsal olarak eðer $x_0,x_1,.$ verili ise birbirlerinden
baðýmsýzlar. Model,

$\pi(x_0)$ baþlangýç daðýlýmý

$f(x_t|x_{t-1})$, $t \ge 1$ geçiþ fonksiyonu

$g(y_t|x_t)$, $t \ge 1$, gözlemlerin daðýlýmý

$x_{0:t} = (x_0,..,x_t)$, $t$ anýna kadar olan gizli konum zinciri

$y_{1:t} = (y_1,..,y_t)$, $t$ anýna kadar olan gözlemler

Genel olarak filtreleme iþleminin yaptýðý þudur: nasýl davrandýðýný, ve
dýþarýdan görülebilen bir ölçütü olasýlýksal olarak dýþarý nasýl yansýttýðýný
bildiðimiz bir sistemi, sadece bu ölçümlerine bakarak nasýl davrandýðýný
anlamak, ve bunu sadece en son noktaya bakarak yapmak, yani sistemin konumu
hakkýndaki tahminimizi sürekli güncellemek.

Mesela bir obje zikzak çizerek hareket ediyor. Bu zikzak hareketinin formülleri
vardýr, bu hareketi belli bir hata payýyla modelleriz. Fakat bu hareket 3
boyutta, diyelim ki biz sadece 2 boyutlu dijital imajlar üzerinden bu objeyi
görüyoruz. 3D/2D geçiþi bir yansýtma iþlemidir ve bir matris çarpýmý ile temsil
edilebilir, fakat bu geçiþ sýrasýnda bir kayýp olur, derinlik bilgisi gider,
artý bir ölçüm gürültüsü orada eklenir diyelim. Fakat tüm bunlara raðmen, sadece
eldeki en son imaja bakarak bu objenin yerini tahmin etmek mümkündür.

Mesela zikzaklý harekete yandan bakýyor olsak obje saða giderken bir bizden
uzaklaþacak yani 2 boyutta küçülecek, ya da yakýnlaþacak yani 2 boyutta
büyüyecek. Tüm bu acaipliðe (!) raðmen eðer yansýtma modeli doðru kodlanmýþ
ise filtre yeri tespit eder. Her parçacýk farklý bir obje konumu hakkýnda
bir hipotez olur, sonra objenin hareketi zikzak modeline göre, algoritmanin
kendi zihninde yapýlýr, bu geçiþ tüm parçacýklar / hipotezler üzerinde
iþletilir, sonra yine tüm parçacýklar ölçüm modeli üzerinden
yansýtýlýr. Son olarak eldeki veri ile bu yansýtma arasýndaki farka
bakýlýr. Hangi parçacýklar daha yakýn ise (daha doðrusu hangi ölçümün
olasýlýðý mevcut modele göre daha yüksek ise) o parçacýklar hayatta kalýr,
çünkü o parçacýklarýn hipotezi daha doðrudur, onlar daha ``önemli'' hale
gelir, diðerleri devreden çýkmaya baþlar. Böylece yavaþça elimizde hipotez
doðru olana yaklaþmaya baþlar.

Matematiksel olarak belirtmek gerekirse, elde etmek istediðimiz sonsal daðýlým
$p(x_{0:t} | y_{1:t})$ ve ondan elde edilebilecek yan sonuçlar, mesela $p(x_t |
y_{1:t})$. Bu kýsmi (marginal) daðýlýma {\em filtreleme daðýlýmý} ismi de
veriliyor, kýsmi çünkü $x_{1:t-1}$ entegre edilip dýþarý çýkartýlmýþ. Bir diðer
ilgilenen yan ürün $\phi$ üzerinden $p(x_{0:t} | y_{1:t})$'nin beklentisi, ona
$I$ diyelim,

$$ I(f_t) = \int \phi_t(x_{0:t}) p(x_{0:t} | y_{1:t}) \ud x_{0:t} $$

En basit durumda eðer $\phi_t(x_{0:t}) =x_{0:t}$ alýrsak, o zaman þartsal
ortalama (conditional mean) elde ederiz. Farklý fonksiyonlar da mümkündür [1].

Üstteki entegrali $x_{0:t} | y_{1:t}$'den örneklem alarak ve entegrali
toplam haline getirerek yaklaþýksal þekilde hesaplayabileceðimizi [2]
yazýsýnda gördük. Fakat $x_{0:t} | y_{1:t}$'den örnekleyemiyoruz. Bu
durumda yine ayný yazýda görmüþtük ki örneklenebilen baþka bir daðýlýmý baz
alarak örneklem yapabiliriz, bu tekniðe önemsel örnekleme (importance
sampling) adý veriliyordu. Mesela mesela herhangi bir yoðunluk $h(x_{0:t})$
üzerinden,

$$ I = \int
\phi(x_{0:t})
\frac{ p(x_{0:t}|y_{1:t}) }{ h(x_{0:t}) } h_{0:t} \ud x_{0:t}
$$

yaklaþýksal olarak

$$ \hat{I} = \frac{1}{N} \sum _{i=1}^{N} \phi (x^i_{0:t}) w^i_t  $$

ki

$$ 
w^i_t = \frac{p(x^i_{0:t}|y_{1:t})}{h(x^i_{0:t})} 
\mlabel{1} 
$$

ve baðýmsýz özdeþçe daðýlmýþ (i.i.d.) $x^1_{0:t}, .., x^N_{0:t} \sim h$
olacak þekilde. Yani örneklem $h$'den alýnýyor.

Bu güzel, fakat acaba $w^i_t$ formülündeki $p(x^i_{0:t}|y_{1:t})$'yi nasýl
hesaplayacaðýz? Ayrýca $h$ nasýl seçilecek? Acaba üstteki hesap özyineli olarak
yapýlamaz mý, yani tüm $1:t$ ölçümlerini bir kerede kullanmadan, $t$ andaki
hesap sadece $t-1$ adýmýndaki hesaba baðlý olsa hesapsal olarak daha iyi olmaz
mý?

Bu mümkün. Mesela önemsel daðýlým $h$ için,

$$ 
h(x_{0:t}) = h(x_t | x_{0:t-1}) h(x_{{0:t-1}}) 
\mlabel{2}
$$

Üstteki ifade koþulsal olasýlýðýn doðal bir sonucu. Peki aðýrlýklar özyineli
olarak hesaplanabilir mi? Bayes Teorisini kullanarak (1)'in bölünen kýsmýný
açabiliriz,

$$
w_t =
\frac{p(x_{0:t}|y_{1:t})}{h(x_{0:t})} =
\frac{p(y_{1:t}|x_{0:t}) p(x_{0:t})}{h(x_{0:t})p(y_{1:t}) }
\mlabel{3}
$$

çünkü hatýrlarsak $P(A|B) = P(B|A)P(A) / P(B)$, teknik iþliyor çünkü
$P(B,A)=P(A,B)$.  

Þimdi $h(x_{0:t})$ için (2)'de gördüðümüz açýlýmý yerine koyalým,

$$ w_t =
\frac{p(y_{1:t}|x_{0:t}) p(x_{0:t})}{h(x_t | x_{0:t-1}) h(x_{{0:t-1}}) p(y_{1:t}) }
$$

Ayrýca gözlem daðýlýmý $g$'yi $p(y_{1:t}|x_{0:t})$'yi, ve gizli geçiþ daðýlýmý
$f$'i $p(x_{0:t})$ açmak için kullanýrsak,

$$ = \frac
{g(y_t|x_t) p(y_{1:t-1}|x_{0:t-1}) f(x_t|x_{t-1})p(x_{0:t-1}) }
{h(x_t|x_{0:t-1}) h(x_{{0:t-1}}) p(y_{1:t})}
$$

Ustteki formülde bolunendeki 2. carpan 4. carpan ve bolende ortadaki
carpana bakalým, bu aslýnda (3)'e göre $w_{t-1}$'in tanýmý deðil mi?

Neredeyse; arada tek bir fark var, bir $p(y_{1:t-1})$ lazým, o üstteki formülde
yok, ama onu bölünene ekleyebiliriz, o zaman

$$ =
w_{t-1} \frac{g(y_t|x_t) f(x_t|x_{t-1})p(y_{1:t-1}) }
{h(x_t|x_{0:t-1}) p(y_{1:t})}
$$

Hem $p(y_{1:t})$ hem de $p(y_{1:t})$ birer sabittir, o zaman o deðiþkenleri
atarak üstteki eþitliðin oransal doðru olduðunu söyleyebiliriz. Ayrýca bu
aðýrlýklarý artýk normalize edilmiþ parçacýklar bazýnda düþünürsek,
$\tilde{w}^i_t = \frac{w_t^i}{\sum_j w_t^j}$, o zaman 

$$
\tilde{w}^i_{t} \propto
\tilde{w}^i_{t-1} \frac{g(y_t|x_t) f(x_t|x_{t-1}) } {h(x_t|x_{0:t-1}) }
$$

Eðer baþlangýç daðýlýmý $x_0^{(1)}, ..., x_0^{(N)} \sim \pi(x_0)$'dan geliyor
ise, ve biz $h(x_0) = \pi(x_0)$ dersek, ayrýca önem daðýlýmý $h$ için
$h(x_t|x_{0:t-1}) = f(x_t|x_{t-1})$ kullanýrsak, geriye 

$$
\tilde{w}^i_{t} \propto \tilde{w}^i_{t-1} g(y_t|x_t)
$$

kalacaktýr.

Burada ilginç bir nokta sistemin geçiþ modeli $f$'in önemlilik örneklemindeki
teklif (proposal) daðýlýmý olarak kullanýlmýþ olmasý. 

Tekrar Örnekleme

Buraya kadar gördüklerimiz sýralý önemsel örnekleme (sequential importance
sampling) algoritmasý olarak biliniyor. Fakat gerçek dünya uygulamalarýnda
görüldü ki aðýrlýklar her adýmda çarpýla çarpýla dejenere hale geliyorlar. Bir
ilerleme olarak aðýrlýklarý her adýmda çarpmak yerine her adýmda $w_t$ $g$
üzerinden hesaplanýr, ve bir ek iþlem daha yapýlýr, eldeki aðýrlýklara göre
parçacýklardan ``tekrar örneklem'' alýnýr. Bu sayede daha kuvvetli olan
hipotezlerin hayatta kalmasý diðerlerinin yokolmasý saðlanýr. 

Nihai parcaçýk filtre algoritmasý þöyledir,


\verb!particle_filter!$\left( f, g, y_{1:t} \right)$

\begin{itemize}
  \item Her $i=1,..,N$ için 
  \begin{enumerate}
     \item $\tilde{x}_t^{(i)} \sim f(x_t|x_{t-1}^{(i)})$ örneklemini al, ve
       $\tilde{x}_{0:t}^{(i)} = ( \tilde{x}_{0:t-1}^{(i)},\tilde{x}_{t}^{(i)})$ yap. 
     \item Önemsel aðýrlýklar $\tilde{w}_t^{(i)} = g(y_t|\tilde{x}^{{i}})$'ý hesapla.
     \item $N$ tane yeni parçacýk $(x_{0:t}^{(i)}; i=1,..,N )$ eski parçacýklar 
       $\{ \tilde{x}^{(i)}_{0:t},...,\tilde{x}^{(i)}_{0:t} \}$ içinden
       normalize edilmiþ önemsel aðýrlýklara göre örnekle. 
     \item $t = t + 1$ 
  \end{enumerate}
\end{itemize}

Örnek

Ali'nin ruh halini modelleyelim. Ali mutlu ya da üzgün olabiliyor, her 10
dakikada Ali'nin ruh hali 0.1 olasýlýkla deðiþiyor, mutluysa üzgün, üzgünse
mutlu olabiliyor. Eðer Ali mutlu ise 0.8 þansýyla gülmesi mümkün, üzgün ise 0.2
olasýlýkla gülebilir. Önce kendimiz verili olasýlýklara göre yapay bir veri
üreteceðiz. Ardýndan bu veriye bakýp sadece gülme / gülmeme verilerine,
ölçümlerine bakarak Ali'nin hangi ruh halinde olduðunu takip etmeye
uðraþacaðýz. 
  
\begin{minted}[fontsize=\footnotesize]{python}
import smile
  
y,Ttotal,a,b,xs = smile.prepare_data()
            
M=100

xp=np.ones((M,Ttotal))
x= np.random.randint(2,size=(M,Ttotal))

#contains weights for each particle at each time step
w=np.ones((M,Ttotal))

#normalize weights
w=w/M

k=0
for t in range(1,Ttotal):
    r1 = np.random.rand(M) 
    for i in range(M):
        if r1[i] < a:
            xp[i,t] = 1-x[i,t-1] 
            k=k+1
        else:
            xp[i,t] = x[i,t-1] 
        if y[t] == xp[i,t]:
            w[i,t] = b
        else:
            w[i,t] = 1-b

    w[:,t] = w[:,t] / sum(w[:,t])    
    j=0
    while j < M-1:
        i = np.random.randint(M)
        if np.random.rand() < w[i,t]:
            x[j,t] = xp[i,t]
            j = j+1

pred = np.zeros(Ttotal)
for t in range(Ttotal):
    pred[t] = (sum(xp[:,t])/M)

plt.plot([i for i in range(Ttotal)], xs)
plt.ylim([-1,2])
plt.plot([i for i in range(Ttotal)], pred)
plt.legend([u'gerçek gizli konum', 'tahmin edilen gizli konum'])
plt.xlabel('time')
plt.ylabel('mood')
plt.savefig('tser_pf_01.png')
\end{minted}

\includegraphics[height=6cm]{tser_pf_01.png}

Hata fonksiyonu 

$$
w^{[i]} = \frac{1}{1 + (y^{[i]} - p^{[i]})^2  )}
$$

olan parçacýk filtreleri için kod þurada. Bu filtrenin kullanýmý için
bakýnýz [3] yazýsý. 

\inputminted[fontsize=\footnotesize]{python}{PF.py}

Kaynaklar

[1] Gandy, {\em LTCC - Advanced Computational Methods in Statistics},
\url{http://wwwf.imperial.ac.uk/~agandy/ltcc.html}

[2] Bayramli, Istatistik, {\em Ýstatistik, Monte Carlo, Entegraller, MCMC}

[3] Bayramli, Yapay Görüþ, {\em Obje Takibi}

\end{document}



