<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Ders 2.19</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h1 id="ders-2.19">Ders 2.19</h1>
<p>Eşlenik Gradyan (Conjugate Gradient) Yöntemi</p>
<p>Arnoldi metotu Gram-Schmidt'e benzeyen bir yöntemdir ve bir dikgen baz ortaya çıkartır. Bu baz, Krylov altuzayının bazıdır, ki bu altuzaydaki her yeni baz vektör, <span class="math inline">\(e\)</span>'nin başka bir üstü alınıp çarpılarak elde edilir. Fakat bu pek iyi bir baz değildir, bazların dikgenleştirilmesi gerekir, ve Arnoldi'nin yaptığı budur.</p>
<p>Arnoldi-Lanczos yöntemi özdeğerler (eigenvalue) bulmak için de kullanılır.</p>
<p><span class="math display">\[ AQ = QH \]</span></p>
<p>eşitliğindeki <span class="math inline">\(H\)</span> matrisinin alt-matrisine bakılırsa, aranılan özdeğerler buradan okunabilir. Bu alt-matris simetrik ve üst köşegendir. (upperdiagonal).</p>
<p><span class="math display">\[ H = Q^{-1}AQ \]</span></p>
<p>formülünde <span class="math inline">\(H,A\)</span> matrisleri birbirine benzerdir (similar) ve benzer matrislerin özdeğerleri aynıdır.</p>
<p>Bu kavramlardan şöyle bir bahsetmek istedim, belki günün birinde çok büyük bir matrisin özdeğerlerini bulmak gerekir, akılda olsun. Yazılım <code>arpack</code> bunun için kullanılabiliyor. Bahsi yaptık bir diğer sebep lineer cebirin yarısı lineer sistemlerse, diğer yarısı özdeğer problemleridir denebilir. Buraya gelmişken üstteki özdeğer yönteminden bahsetmemek olmazdı.</p>
<p>Konumuza dönelim.</p>
<p><span class="math inline">\(A\)</span> pozitif kesin ve simetrik olmalı. Eğer değilse birazdan gösgtereceğimiz formülleri kullanmak biraz riskli olur, işleyebilirler ama garanti olmaz.</p>
<p>$r_K = b - Ax_k $, <span class="math inline">\(K_k\)</span>'ye dikgen, <span class="math inline">\(x_k \in \mathscr{K}_K\)</span>.</p>
<p>Demek ki <span class="math inline">\(x_k\)</span>'yi özyineli olarak yaratabiliriz, ve her adımda sadece <span class="math inline">\(A\)</span> ile çarpmamız gerekir. Üstteki formülde <span class="math inline">\(A\)</span> ile çarpım olduğuna göre, <span class="math inline">\(r_K\)</span> bir sonraki uzay <span class="math inline">\(k+1\)</span> içinde olacaktır. Arnoldi'den biliyoruz ki <span class="math inline">\(q_{k+1}\)</span> aynı uzay içindedir. O zaman <span class="math inline">\(r_k\)</span>, <span class="math inline">\(q_{k+1}\)</span>'in bir katıdır. Yani <span class="math inline">\(r\)</span> ile gösterilen &quot;artıklar (residuals)'' birbirine dikgen. Yani</p>
<p><span class="math display">\[ r_i^Tr_k = 0, \ i &lt; k \]</span></p>
<p>Artıkların birbirine dikgen olmasının sebebi içlerinde <span class="math inline">\(A\)</span> olması.</p>
<p>Başlangıç değerleri</p>
<p><span class="math display">\[ d_0 = b \]</span></p>
<p><span class="math display">\[ x_0 = 0 \]</span></p>
<p><span class="math display">\[ r_0 = b - Ax_0  = b\]</span></p>
<p>Simetrik Pozitif Kesin <span class="math inline">\(A\)</span> İçin Eşlenik Gradyan Metodu</p>
<p>Algoritma <code>eslenik_gradyan</code></p>
<ul>
<li><span class="math inline">\(\alpha_k = r_{k-1}^T r_{k-1} / d_{k-1}^T A d_{k-1}\)</span></li>
<li><span class="math inline">\(\alpha_k = \alpha_{k-1} + \alpha_k d_{k-1}\)</span></li>
<li><span class="math inline">\(r_k = r_{k-1} - \alpha_k Ad_{k-1}\)</span></li>
<li><span class="math inline">\(\beta_k = r_k^Tr_k / r_{k-1}^Tr_{k-1}\)</span></li>
<li><span class="math inline">\(d_k = r_k + \beta_k d_{k-1}\)</span></li>
</ul>
<p><span class="math inline">\(d\)</span> &quot;arama yönüdür'', optimizasyon ilerlerken gideceğimiz istikamettir. 2. adımda güncellemeyi yapıyorum. Peki bir sonraki yönüm ne olmalı?</p>
<p>Her Döngüde:</p>
<ul>
<li><span class="math inline">\(Ad\)</span> çarpımını görüyoruz, çünkü <span class="math inline">\(A\)</span> ile çarpım bize yeni Krylov altuzayını veriyor.\</li>
<li>2 içsel çarpım \</li>
<li>2 ya da 3 vektör güncellemesi</li>
</ul>
<p>Peki <span class="math inline">\(k\)</span> adım sonra hata <span class="math inline">\(||e_k||\)</span> nedir ve ilk baştaki hata <span class="math inline">\(||e_0||\)</span> ile bağlantısı nedir?</p>
<p><span class="math display">\[ ||e_k|| \le 2  \bigg(
\frac{ \sqrt{ \lambda_{maks} - \lambda_{min}}}
{\lambda_{maks} + \lambda_{min}}
\bigg)^k||e_0||
\]</span></p>
<p>Hala bir kelimeye açıklık getirmedik; gradyan. Niye bir &quot;gradyan'' kelimesi kullanıyoruz, neyin gradyanından bahsediyoruz, bu teknik için gradyanlar ne anlama geliyor?</p>
<p>Lineer problemlerde <span class="math inline">\(Ax = b\)</span> eşitliği vardır ve bu eşitlik enerjinin gradyanından gelir. Yani</p>
<p><span class="math display">\[ E(x) = \frac{ 1}{2}x^TAx - b^Tx \]</span></p>
<p>enerjisinin gradyanından. Üstteki formül nereden geldik diye düşünebilirsiniz, hep lineer sistemlerden bahsettik, ve bu sistemlerde her şey <span class="math inline">\(Ax = b\)</span> formatına uyar. Şimdi birdenbire matematiğin farklı bir koluna geçiyorum sanki, üstteki formülü minimize etmeye uğraşıyorum, yani optimizasyona giriyorum. Fakat cebirsel olarak düşünürsek,</p>
<p><span class="math display">\[ grad \ E = [\frac{\partial E}{\partial x} ]  = Ax - b \]</span></p>
<p>olacaktır. Minimumda üstteki sıfır olacağına göre</p>
<p><span class="math display">\[ Ax - b = 0 \]</span></p>
<p><span class="math display">\[ Ax = b \]</span></p>
<p>Yani karesel enerjinin lineer gradyanı vardır, ve onun minimumu <span class="math inline">\(Ax = b\)</span>'dir. Bu demektir ki lineer denklemi çözmek ve enerjiyi minimize etmek aslında aynı şeydir! Minimum kelimesini kullanabiliyorum bu arada, çünkü <span class="math inline">\(A\)</span>'nin pozitif kesin olduğunu biliyorum.</p>
<p>Minimize işlemi nasıl yapılır? Diyelim ki alttaki gibi bir <span class="math inline">\(E(x)\)</span>'im var, kap şeklinin herhangi bir noktasındayım, ve aşağı inmem lazım. En fazla artış gradyan <span class="math inline">\(g\)</span> ise, dibe inmek için <span class="math inline">\(-g\)</span> yönünde gidebilirim.</p>
<div class="figure">
<img src="19_1.png" />

</div>
<p>Bu yön doğal bir yöndür, ilk akla gelen fikirdir ve mantıklıdır. Fakat en iyi yön değildir. Şimdi minimizasyon çözümü olarak eşlenik gradyan açısından bakıyoruz olaya, işin gradyan tarafı da böylelikle açıklığa kavuşacak.</p>
<p>Negatif gradyanın aynı zamanda artığın da (residual) negatif yönüdür. Artığın yönünde hareket etmek iyi midir? Negatif gradyanı takip etmenin bir diğer ismi &quot;en dik iniş (steepest descent)''tir. Fakat, başlangıç noktasına göre bu değişir ama, çok fazla iniş çıkış ta yaşanabilir.</p>
<p><span class="math inline">\(r\)</span>'ler hesapsal bilimde çok aranan bir özelliğe sahip değildir, dikgenlik. Bir şekilde dikgenlik her zaman doğru yönde hareket ettiğimizin garantisidir. Gidilmesi gereken doğru yön, üstteki kodda 5. satırda hesaplanan yöndür. Bu yöne &quot;<span class="math inline">\(A\)</span>-dikgen'' denir.</p>
<p>Bir resimle göstermek gerekirse, alta bakalım, soldaki en dik iniş, sağdaki eşlenik gradyan. Enerji fonksiyonunu kesit seviyesinden (level set), kontur (contour) olarak gösteriyoruz, her kontur bir enerji seviyesine tekabül edecek, mesela en dıştaki kontur 5, bir içerideki 4 olabilir, ve en ortadaki nokta tam sıfır olabilir, çünkü en düşüktür.</p>
<div class="figure">
<img src="19_2.png" />

</div>
<p>Her iki tekniğin gidişatı resimde görülmektedir.</p>
<p>[gerisi atlandı]</p>
<p>Ekler</p>
<p>Üstteki anlatımda Krylov altuzaylarının eşlenik gradyan metotunun işleyişinde tam olarak nasıl rol oynadığı belirtilmemiş. Aslında Krylov altuzayları gerektirmeden bu metotu anlatmak mümkün.</p>
<p>İki vektör <span class="math inline">\(u,v\)</span> birbirine A-dikgendir eğer</p>
<p><span class="math display">\[ u^TAv = 0 \]</span></p>
<p>işe. Dikkat, bu iki vektör, tek başlarına, <span class="math inline">\(u^Tv\)</span> olarak birbirine dikgen olmayabilir, ama ortada <span class="math inline">\(A\)</span> olduğu halde çarpım sıfır çıkarsa dikgen olmasalar da A-dikgen olurlar. Bu dikgenliğin bir diğer ismi eşlenik (conjugate) olmaktır.</p>
<p>Şimdi diyelim ki elimizde herbiri birbirine dikgen olan <span class="math inline">\(n\)</span> tane <span class="math inline">\(\{d_k\}\)</span> yönü / vektörü var. O zaman <span class="math inline">\(d_k\)</span> <span class="math inline">\(\mathbb{R}^n\)</span> için bir baz oluşturur ve biz de <span class="math inline">\(Ax = b\)</span> denkleminin çözümü <span class="math inline">\(x_*\)</span>'i bu bazı temel alarak temsil ederiz. Yani baz vektörlerini çarpan bazı katsayılar vardır, ve bu çarpımların toplamı <span class="math inline">\(x_*\)</span> olur.</p>
<p><span class="math display">\[ x_* = \sum_{ i=1}^{n} \alpha_i d_i \]</span></p>
<p>Böylece <span class="math inline">\(Ax = b\)</span>'yi çözmek için bir metot elde ediyoruz, eğer <span class="math inline">\(n\)</span> tane eşlenik yön bulabilirsek, <span class="math inline">\(\alpha\)</span> değerlerini hemen hesaplayabiliriz. Ayrıca eğer eşlenik vektörler <span class="math inline">\(d_k\)</span>'leri dikkatlice seçersek, yaklaşık çözüm <span class="math inline">\(x_*\)</span> için hepsine ihtiyacımız olmaz. Özyineli <span class="math inline">\(x\)</span> formülünü kullanabiliriz,</p>
<p><span class="math display">\[
x_{k+1} = x_k + \alpha_k d_{k+1} 
\qquad (1)
\]</span></p>
<p>Bu formül niye mantıklı? Eğer çözüm <span class="math inline">\(x_*\)</span> dikgen <span class="math inline">\(d_k\)</span> vektörlerinin bir lineer kombinasyonu ise, çözüm vektörleri birbiri ardına dizilmiş ve &quot;bir yere giden'' bir zincir olarak görülebilir. Üstteki formül sadece bu zinciri yavaş yavaş kurmakta..</p>
<p>İlk önce özyineli olarak artıklar <span class="math inline">\(r_k\)</span> arasında bir ilişki kuralım, (1)'nin iki tarafı <span class="math inline">\(A\)</span> ile çarpıp, <span class="math inline">\(b\)</span>'den çıkartalım (çünkü <span class="math inline">\(r_i = b - Ax_i\)</span>'a erişmek istiyoruz),</p>
<p><span class="math display">\[b - A x_{k+1} = b - A x_k  + \alpha_k A d_{k} \]</span></p>
<p><span class="math display">\[r_{k+1} = r_k + \alpha_k A d_{k} \]</span></p>
<p><span class="math display">\[
r_{k+1} = r_k + \alpha_k A d_{k} 
\qquad (8)
\]</span></p>
<p>Şimdi hata terimini hesaplayalım. <span class="math inline">\(e_i\)</span>, yani <span class="math inline">\(i\)</span>'inci tahminin hatası,</p>
<p><span class="math display">\[ e_i = x - x_i  \]</span></p>
<p>İki tarafı <span class="math inline">\(A\)</span> ile çarpalım</p>
<p><span class="math display">\[ Ae_i = Ax - Ax_i  \]</span></p>
<p><span class="math display">\[ Ae_i = b - Ax_i  \]</span></p>
<p>Sağ taraf <span class="math inline">\(r_i\)</span> tanımının aynısı değil mi? O zaman</p>
<p><span class="math display">\[
Ae_i = r_i 
\qquad (5)
\]</span></p>
<p><span class="math inline">\(e\)</span>'yi özyineli olarak temsil etmek te mümkündür, (1)'nin her iki tarafından <span class="math inline">\(x\)</span> çıkartırsam,</p>
<p><span class="math display">\[ x_{k+1} - x = x_k - x + \alpha_k d_{k} \]</span></p>
<p><span class="math display">\[ e_{k+1} = e_k + \alpha_k d_k 
\qquad (2)
\]</span></p>
<p>Bu her adımı <span class="math inline">\(\alpha_k\)</span>'ye bağlı özyineli bir tanımdır.</p>
<p><span class="math inline">\(\alpha\)</span> katsayılarını bulmak için bir sonraki yönden gelen hatanın önceki tüm arama yönlerine, özelde bir önceki arama yönüne A-dikgen olmasını istiyoruz. Yani</p>
<p><span class="math display">\[ d_i^TA e_{i+1}  = 0\]</span></p>
<p>olmalı.</p>
<p><span class="math display">\[ d_i^TA (x_{i+1}-x)  = 0\]</span></p>
<p><span class="math display">\[ d_i^TA (x_{i} + \alpha_i d_i -x)  = 0\]</span></p>
<p><span class="math display">\[ d_i^TA (e_i + \alpha_i d_i )  = 0\]</span></p>
<p><span class="math display">\[ d_i^Tr_i + \alpha_i d_i^TA d_i   = 0\]</span></p>
<p><span class="math display">\[ 
\alpha_i = -\frac{ d_i^Tr_i}{d_i^T A d_i} 
\qquad (6)
\]</span></p>
<p>Şimdi hata terimine dönelim, diyelim ki <span class="math inline">\(e_0\)</span> vektörü, bu vektör, diğer her vektör gibi içinde olduğumuz uzayın bazlarının bir kombinasyonu olarak temsil edilebilir. Bizim bazlarımız <span class="math inline">\(d_j\)</span> olduğuna göre,</p>
<p><span class="math display">\[ e_0 = \sum_{ j=0}^{n-1} \delta_j d_j \]</span></p>
<p>Katsayı olarak <span class="math inline">\(\delta_j\)</span> seçtik, <span class="math inline">\(\alpha\)</span> ile karışıklık olmasın diye. Şimdi iki tarafı <span class="math inline">\(d_k^T A\)</span> ile çarpalım,</p>
<p><span class="math display">\[ d_k^T A e_0 = \sum_{ j=0}^{n-1} \delta_j d_k^T A d_j \]</span></p>
<p>Yine aynı dikgenlik numarası, toplam içinde <span class="math inline">\(j\)</span> olmayan tüm diğer <span class="math inline">\(p\)</span> çarpımları sıfırdır,</p>
<p><span class="math display">\[ d_k^T A e_0 =  \delta_j d_j^T A d_j \]</span></p>
<p><span class="math display">\[ \delta_j = \frac{ d_k^T A e_0}{ d_j^T A d_j } 
\qquad (4)
\]</span></p>
<p>Şimdi <span class="math inline">\(e_0\)</span>'in yerine (2)'teki özyineli tanımdan türeteceğim bir şey koymak istiyorum. Diyelim ki <span class="math inline">\(e_0\)</span>'dan başlayıp teker teker bir sonraki <span class="math inline">\(e\)</span>'yi hesaplayıp alt alta yazdım, ve topladım</p>
<p><span class="math display">\[ \cancel{e_{1}} = e_0 + \alpha_0 d_0\]</span></p>
<p><span class="math display">\[ \cancel{e_{2}} = \cancel{e_1} + \alpha_1 d_1\]</span></p>
<p><span class="math display">\[ ... \]</span></p>
<p><span class="math display">\[ e_{k} = \cancel{e_{k-1}} + \alpha_{k-1} d_{k-1}\]</span></p>
<p>Sağ kalan tek terimler</p>
<p><span class="math display">\[ e_k = e_0 + \sum_{ j=0}^{k-1} \alpha_j d_j \]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>içinde <span class="math inline">\(e_0\)</span> yerine koyalım</li>
</ol>
<p><span class="math display">\[ \delta_j = \frac{ d_k^T A e_k - \cancel{\sum_{ j=0}^{k-1} \alpha_j d_k^T A d_j}}
{ d_j^T A d_j } 
\]</span></p>
<p>Niye iptal? Yine A-dikgenliği. Dikkat edilirse <span class="math inline">\(j\)</span>'ler <span class="math inline">\(k-1\)</span>'e kadar çıkıyor, <span class="math inline">\(k\)</span>'ye bile erişmiyor, çarpım hep sıfır. Kalanlar,</p>
<p><span class="math display">\[  = \frac{ d_k^T A e_k}
{ d_j^T A d_j } 
\]</span></p>
<p>(5)'i kullanırsak,</p>
<p><span class="math display">\[ \delta_j = \frac{ d_k^T r_k}
{ d_j^T A d_j } 
\]</span></p>
<ol start="6" style="list-style-type: decimal">
<li>ile bu formülün benzerliği bariz, sadece eksi işareti farklı. O zaman</li>
</ol>
<p><span class="math display">\[ \delta_k = -\alpha_k \]</span></p>
<p>diyebiliriz. Bu demektir ki hata formülünde <span class="math inline">\(\alpha\)</span> yerine <span class="math inline">\(\delta\)</span> kullanabiliriz,</p>
<p><span class="math display">\[ e_0 = -\sum_{ j=0}^{n-1} \alpha_j d_j \]</span></p>
<p>Hataların özyineli denklemi (2)'yi üste uygularsak,</p>
<p><span class="math display">\[ e_i = -\sum_{ j=i}^{n-1} \alpha_j d_j 
\qquad (3)
\]</span></p>
<p>Şimdi artıkların ve önceki gidiş yönlerinin dikgen olduklarını gösterelim. (3)'u <span class="math inline">\(d_k^TA\)</span> ile çarpalım,</p>
<p><span class="math display">\[ d_k^TAe_i = -\sum_{ j=i}^{n-1} \alpha_j  d_k^TAd_j 
\]</span></p>
<p><span class="math display">\[ d_k^Tr_i = -\sum_{ j=i}^{n-1} \alpha_j  d_k^TAd_j 
\]</span></p>
<p><span class="math inline">\(d\)</span>'ler arasındaki A-ortogonallik sayesinde ve <span class="math inline">\(k &lt; i\)</span> için</p>
<p><span class="math display">\[ d_k^Tr_i = 0
\qquad (9)
\]</span></p>
<p>Madem ki eski yönler ve artıklar birbirine dikgen, Gram-Schmidt işleminin A-dikgen halini artıklardan yön üretmek için kullanabiliriz. Her artığı alıp, içinden ona dikgen bir yön çıkartmak mümkün.</p>
<p><span class="math display">\[ d_i = r_i + \sum_{ j=0}^{i-1} \beta_{i,j}d_j 
\qquad (10)
\]</span></p>
<p><span class="math display">\[ \beta_{i,j} = - \frac{ r_i^TAd_j}{d_j^TAd_j} \]</span></p>
<p>Yani Gram-Schmidt formülasyonunun A-dikgenlik kullanan hali (bkz Lineer Cebir Ders 17 notları). Ama üstteki ifadeyi daha da basitleştirebiliriz, ve verimli hale getirebiliriz. Üstteki yöntemde tüm vektörleri etrafta tutmamız gerekiyor, ayrıca <span class="math inline">\(A\)</span>'lardan kurtulmak iyi olur.</p>
<p>Kurtulmak için <span class="math inline">\(r_i^TAd_j\)</span> ifadesine içinde ulaşmaya çalışacağız, ve eşitliğin diğer tarafında içinde <span class="math inline">\(A\)</span> olmayan bir ifade olmasına gayret göstereceğiz. <span class="math inline">\(r_i^Tr_{j+1}\)</span> ile başlayalım, ve <span class="math inline">\(r_{j+1}\)</span> üzerinde özyineli denklem (8)'i uygulayalım.</p>
<p><span class="math display">\[ r_i^Tr_{j+1} = r_i^T (r_i + \alpha_k A d_k)  = r_i^Tr_j + \alpha_i r_i^TAd_j \]</span></p>
<p><span class="math display">\[ = \frac{ r_i^Tr_{j+1} - r_i^Tr_j }{\alpha_j} =  r_i^TAd_j \]</span></p>
<p>Eşitliğin sağındaki ifadenin <span class="math inline">\(\beta_{i,j}\)</span> ifadesinin bölünen kısmı ile aynı olduğuna dikkat, ve eşitliğin sol tarafında <span class="math inline">\(A\)</span> yok. Yerine koyalım,</p>
<p><span class="math display">\[ \beta_{i,j} = - \frac{ 1}{\alpha_j}\frac{  r_i^Tr_{j+1} - r_i^Tr_j }{d_j^TAd_j} \]</span></p>
<p><span class="math inline">\(j = i -1\)</span>, yani <span class="math inline">\(\beta_{i,i-1}\)</span> için</p>
<p><span class="math display">\[ \beta_{i,i-1} = - \frac{ 1}{\alpha_{j-1}}\frac{  r_i^Tr_i  }{d_{i-1}^TAd_{i-1}} \]</span></p>
<p><span class="math inline">\(r_i^Tr_j\)</span> terimi <span class="math inline">\(r_i^Tr_{i-1}\)</span> olunca sıfır oldu, çünkü artıklar birbirine dikgen. Dikkat bu sefer dikgen, A-dikgen değil. Bunu nasıl ispat ederiz? (10)'u alıp <span class="math inline">\(r_k\)</span> ile çarpalım, ve <span class="math inline">\(k,i\)</span> indislerini değiştirelim</p>
<p><span class="math display">\[ d_k^Tr_i = r_k^Tr_i + \sum_{ j=0}^{i-1} \beta_{k,j}d_j ^Tr_i = 0
\]</span></p>
<p>Sıfıra eşitlik (9) sayesinde. Ama bu sıfır durumu toplam içindekiler için de geçerli, çünkü toplamın üst sınırı <span class="math inline">\(i-1\)</span>, ve en yüksek indisli yön <span class="math inline">\(d_{j-1}\)</span> olabilir, o zaman toplam da sıfırdır. Yani</p>
<p><span class="math display">\[  r_k^Tr_i  = 0 \]</span></p>
<p><span class="math inline">\(\beta\)</span> ile işimize devam edelim. Bölen kısmında hala bir <span class="math inline">\(A\)</span> var, onu yokedelim. Önce (6)'daki <span class="math inline">\(\alpha\)</span> tanımının <span class="math inline">\(i-1\)</span> indisli haline bakalım, ve tersine çevirelim,</p>
<p><span class="math display">\[ \frac{ 1}{\alpha_{i-1}} = -\frac{d_{i-1}^T A d_{i-1} }{d_{i-1}^Tr_{i-1}} 
\]</span></p>
<p>Son <span class="math inline">\(\beta\)</span> formülünde yerine koyalım</p>
<p><span class="math display">\[ \beta_{i,i-1} = 
\frac{  r_i^Tr_i  }{ \cancel{d_{i-1}^TAd_{i-1}}} 
\frac{\cancel{d_{i-1}^T A d_{i-1}}}{d_{i-1}^Tr_{i-1}}
= 
\frac{  r_i^Tr_i  }{d_{i-1}^Tr_{i-1}}
\]</span></p>
<p>Son bir eşitlik daha var, bu da <span class="math inline">\(d_{i-1} = r_{i-1}\)</span> eşitliği, nereden geliyor? <span class="math inline">\(j &lt; i-1\)</span> için <span class="math inline">\(r_i^Tr_{j+1}\)</span> ve <span class="math inline">\(r_i^Tr_{j}\)</span> çarpımlarının ikisi de sıfırdır, o zaman (10) formülü</p>
<p><span class="math display">\[ d_i = r_i + \beta_{i,j}d_j \]</span></p>
<p>haline gelir çünkü pek çok değer için <span class="math inline">\(\beta_{i,j} = 0\)</span> olacaktır. Şimdi üsttekini bir önceki indis değerleri için tekrar yazalım,</p>
<p><span class="math display">\[ d_{i-1} = r_{i-1} + \beta_{i-1,i-2}d_{i-2} \]</span></p>
<p>Yine dikgenlik sayesinde <span class="math inline">\(\beta\)</span> değeri iptal olur, ve geriye sadece</p>
<p><span class="math display">\[ d_{i-1} = r_{i-1} \]</span></p>
<p>kalır. Böylece son formül</p>
<p><span class="math display">\[  \beta_{i,i-1} = 
\frac{  r_i^Tr_i  }{r_{i-1}^Tr_{i-1}}
\]</span></p>
<p>haline geliyor, ve kodlama çok temizleşiyor.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> scipy.linalg <span class="im">as</span> lin

A <span class="op">=</span> np.array([[<span class="fl">6.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>],
              [<span class="fl">1.</span>,<span class="fl">7.</span>,<span class="fl">1.</span>],
              [<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">8.</span>]])

b <span class="op">=</span> np.array([<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="fl">1.</span>])

xreal <span class="op">=</span> lin.solve(A, b)<span class="op">;</span> <span class="bu">print</span> <span class="st">&quot;solution&quot;</span>, xreal

p <span class="op">=</span> b
r <span class="op">=</span> b
x <span class="op">=</span> b<span class="op">*</span><span class="dv">0</span><span class="op">;</span>
r2 <span class="op">=</span> np.dot(r.T,r)
<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):
    Ap <span class="op">=</span> np.dot(A,p)
    alpha <span class="op">=</span> r2 <span class="op">/</span> np.dot(p.T,Ap)
    x <span class="op">=</span> x <span class="op">+</span> np.dot(alpha,p)
    r <span class="op">=</span> r<span class="op">-</span>np.dot(alpha,Ap)
    r2old <span class="op">=</span> r2
    r2 <span class="op">=</span> np.dot(r.T,r)
    beta <span class="op">=</span> r2 <span class="op">/</span> r2old
    p <span class="op">=</span> r <span class="op">+</span> np.dot(beta,p)
    <span class="bu">print</span> x             </code></pre></div>
<pre><code>solution [ 0.13249211  0.11041009  0.09463722]
[ 0.11111111  0.11111111  0.11111111]
[ 0.13125  0.1125   0.09375]
[ 0.13249211  0.11041009  0.09463722]
[ 0.13249211  0.11041009  0.09463722]
[ 0.13249211  0.11041009  0.09463722]</code></pre>
<p>Seyrek Matrisler</p>
<p>Eğer üstteki kodu seyrek formda ve artımsal (incremental) olarak kodlamak istesek, <span class="math inline">\(A\)</span> matrisini bir Python &quot;sözlükler sözlüğü'' olarak temsil edebilirdik, her satır 0,1,.. gibi anahtar değerlerine sahip olur, ve her satırda ayrı sözlükler kodlanabilir, bu sözlük içinde o satırda mevcut kolonların değerleri anahtar/değer olarak kayıtlıdır. Yüksek ölçekli ortamlarda bu yaklaşımın faydaları açık, satırlar büyük bir dosyada ayrı ayrı okunabilir, hatta paralelize yaklaşımlar mesela <span class="math inline">\(A p\)</span> çarpımı paralel şekilde işletebilirler. Örnek bir kodlama şu şekilde olabilir,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">A <span class="op">=</span> <span class="bu">dict</span>({<span class="dv">0</span>: {<span class="dv">0</span>: <span class="fl">6.</span>, <span class="dv">1</span>:<span class="fl">1.</span>, <span class="dv">2</span>:<span class="fl">1.</span>},
          <span class="dv">1</span>: {<span class="dv">0</span>: <span class="fl">1.</span>, <span class="dv">1</span>:<span class="fl">7.</span>, <span class="dv">2</span>:<span class="fl">1.</span>},
          <span class="dv">2</span>: {<span class="dv">0</span>:<span class="fl">1.</span>, <span class="dv">1</span>:<span class="fl">1.</span>, <span class="dv">2</span>:<span class="fl">8.</span>}})

b <span class="op">=</span> <span class="bu">dict</span>({<span class="dv">0</span>:<span class="fl">1.</span>, <span class="dv">1</span>:<span class="fl">1.</span>, <span class="dv">2</span>:<span class="fl">1.</span>})

<span class="kw">def</span> mult_A_b(A,b):
    <span class="cf">return</span> <span class="bu">dict</span>((k, <span class="bu">sum</span>(b[key]<span class="op">*</span>row.get(key, <span class="dv">0</span>) <span class="cf">for</span> key <span class="kw">in</span> b)) <span class="cf">for</span> k,row <span class="kw">in</span> A.items()  )

<span class="kw">def</span> dot(a,b):
    <span class="cf">return</span> <span class="bu">sum</span>(b[key]<span class="op">*</span>a.get(key, <span class="dv">0</span>) <span class="cf">for</span> key <span class="kw">in</span> b)

<span class="kw">def</span> vec_sum(a,b):
    <span class="cf">return</span> {k: a.get(k, <span class="dv">0</span>) <span class="op">+</span> b.get(k, <span class="dv">0</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">set</span>(a) <span class="op">|</span> <span class="bu">set</span>(b)}

<span class="kw">def</span> vec_subt(a,b):
    <span class="cf">return</span> {k: a.get(k, <span class="dv">0</span>) <span class="op">-</span> b.get(k, <span class="dv">0</span>) <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">set</span>(a) <span class="op">|</span> <span class="bu">set</span>(b)}

<span class="kw">def</span> vec_scalar_times (vec,a):
    <span class="cf">return</span> <span class="bu">dict</span>((k,v<span class="op">*</span>a) <span class="cf">for</span> k,v <span class="kw">in</span> vec.items())

<span class="kw">def</span> vec_scalar_sum (vec,a):
    <span class="cf">return</span> <span class="bu">dict</span>((k,v<span class="op">+</span>a) <span class="cf">for</span> k,v <span class="kw">in</span> vec.items())

<span class="kw">def</span> vec_scalar_subt (vec,a):
    <span class="cf">return</span> <span class="bu">dict</span>((k,v<span class="op">-</span>a) <span class="cf">for</span> k,v <span class="kw">in</span> vec.items())

p <span class="op">=</span> b<span class="op">;</span> r <span class="op">=</span> b<span class="op">;</span> x <span class="op">=</span> {}

r2 <span class="op">=</span> dot(r,r)

<span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):

    Ap <span class="op">=</span> mult_A_b(A,p)

    alpha <span class="op">=</span> r2 <span class="op">/</span> dot(Ap, p)

    x <span class="op">=</span> vec_sum(x,vec_scalar_times(p,alpha))

    r <span class="op">=</span> vec_subt(r,vec_scalar_times(Ap,alpha))

    r2old <span class="op">=</span> r2

    r2 <span class="op">=</span> dot(r,r)

    beta <span class="op">=</span> r2 <span class="op">/</span> r2old

    p <span class="op">=</span> vec_sum(r,vec_scalar_times(p,beta))

    <span class="bu">print</span> (x)
    </code></pre></div>
<pre><code>{0: 0.1111111111111111, 1: 0.1111111111111111, 2: 0.1111111111111111}
{0: 0.13125, 1: 0.11249999999999999, 2: 0.09374999999999999}
{0: 0.13249211356466878, 1: 0.11041009463722397, 2: 0.09463722397476339}
{0: 0.13249211356466878, 1: 0.11041009463722397, 2: 0.09463722397476339}
{0: 0.13249211356466878, 1: 0.11041009463722397, 2: 0.09463722397476339}</code></pre>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
