\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Ders 19

Eþlenik Gradyan (Conjugate Gradient) Yöntemi 

Arnoldi metotu Gram-Schmidt'e benzeyen bir yöntemdir ve bir dikgen baz
ortaya çýkartýr. Bu baz, Krylov altuzayýnýn bazidir, ki bu altuzaydaki her
yeni baz vektör, $e$'nin baþka bir üstü alýnýp çarpýlarak elde
edilir. Fakat bu pek iyi bir baz deðildir, bazlarýn dikgenleþtirilmesi
gerekir, ve Arnoldi'nin yaptýðý budur.

Arnoldi-Lanczos yöntemi özdeðerler (eigenvalue) bulmak için de kullanýlýr.

$$ AQ = QH $$

eþitliðindeki $H$ matrisinin alt-matrisine bakýlýrsa, aranýlan özdeðerler
buradan okunabilir. Bu alt-matris simetrik ve üst köþegendir.
(upperdiagonal). 

$$ H = Q^{-1}AQ $$

formülünde $H,A$ matrisleri birbirine benzerdir (similar) ve benzer
matrislerin özdeðerleri aynýdýr. 

Bu kavramlardan þöyle bir bahsetmek istedim, belki günün birinde çok büyük
bir matrisin özdeðerlerini bulmak gerekir, akýlda olsun. Yazýlým
\verb!arpack! bunun için kullanýlabiliyor. Bahsi yaptýk bir diðer sebep
lineer cebirin yarýsý lineer sistemlerse, diðer yarýsý özdeðer
problemleridir denebilir. Buraya gelmiþken üstteki özdeðer yönteminden
bahsetmemek olmazdý.

Konumuza dönelim. 

$A$ pozitif kesin ve simetrik olmalý. Eðer deðilse birazdan gösgtereceðimiz
formülleri kullanmak biraz riskli olur, iþleyebilirler ama garanti olmaz. 

$r_K = b - Ax_k $, $K_k$'ye dikgen, $x_k \in \mathscr{K}_K$. 

Demek ki $x_k$'yi özyineli olarak yaratabiliriz, ve her adýmda sadece $A$
ile çarpmamýz gerekir. Üstteki formülde $A$ ile çarpým olduðuna göre, $r_K$
bir sonraki uzay $k+1$ içinde olacaktýr. Arnoldi'den biliyoruz ki $q_{k+1}$
ayný uzay içindedir. O zaman 

$r_k$, $q_{k+1}$'in bir katýdýr. Yani $r$ ile gösterilen ``artýklar
(residuals)'' birbirine dikgen. Yani 

$$ r_i^Tr_k = 0, \ i < k $$

Artýklarýn birbirine dikgen olmasýnýn sebebi içlerinde $A$ olmasý. 

Baþlangýç deðerleri

$$ d_0 = b $$

$$ x_0 = 0 $$

$$ r_0 = b - Ax_0  = b$$

Simetrik Pozitif Kesin $A$ Ýçin Eþlenik Gradyan Metodu

Algoritma \verb!eslenik_gradyan!
\begin{enumerate}
  \item $\alpha_k = r_{k-1}^T r_{k-1} / d_{k-1}^T A d_{k-1}$ 
  \item $\alpha_k = \alpha_{k-1} + \alpha_k d_{k-1}$
  \item $r_k = r_{k-1} - \alpha_k Ad_{k-1}$
  \item $\beta_k = r_k^Tr_k / r_{k-1}^Tr_{k-1}$
  \item $d_k = r_k + \beta_k d_{k-1}$
\end{enumerate}

$d$ ``arama yönüdür'', optimizasyon ilerlerken gideceðimiz
istikamettir. 2. adýmda güncellemeyi yapýyorum. Peki bir sonraki yönüm ne
olmalý?

Her Döngüde:

- $Ad$ çarpýmýný görüyoruz, çünkü $A$ ile çarpým bize yeni Krylov altuzayýný
veriyor.\\
- 2 içsel çarpým \\
- 2 ya da 3 vektör güncellemesi

Peki $k$ adým sonra hata $||e_k||$ nedir ve ilk baþtaki hata $||e_0||$ ile
baðlantýsý nedir? 

$$ ||e_k|| \le 2  \bigg(
\frac{ \sqrt{ \lambda_{maks} - \lambda_{min}}}
{\lambda_{maks} + \lambda_{min}}
\bigg)^k||e_0||
$$

Hala bir kelimeye açýklýk getirmedik; gradyan. Niye bir ``gradyan''
kelimesi kullanýyoruz, neyin gradyanýndan bahsediyoruz, bu teknik için
gradyanlar ne anlama geliyor?

Lineer problemlerde $Ax = b$ eþitliði vardýr ve bu eþitlik enerjinin
gradyanýndan gelir. Yani 

$$ E(x) = \frac{ 1}{2}x^TAx - b^Tx $$

enerjisinin gradyanýndan. Üstteki formül nereden geldik diye
düþünebilirsiniz, hep lineer sistemlerden bahsettik, ve bu sistemlerde her
þey $Ax = b$ formatýna uyar. Þimdi birdenbire matematiðin farklý bir koluna
geçiyorum sanki, üstteki formülü minimize etmeye uðraþýyorum, yani
optimizasyona giriyorum. Fakat cebirsel olarak düþünürsek, 

$$ grad \ E = [\frac{\partial E}{\partial x} ]  = Ax - b $$

olacaktýr. Minimumda üstteki sýfýr olacaðýna göre 

$$ Ax - b = 0 $$

$$ Ax = b $$

Yani karesel enerjinin lineer gradyaný vardýr, ve onun minimumu $Ax = b$'dir. Bu
demektir ki lineer denklemi çözmek ve enerjiyi minimize etmek aslýnda ayný
þeydir! Minimum kelimesini kullanabiliyorum bu arada, çünkü $A$'nin pozitif
kesin olduðunu biliyorum.

Minimize iþlemi nasýl yapýlýr? Diyelim ki alttaki gibi bir $E(x)$'im var,
kap þeklinin herhangi bir noktasýndayým, ve aþaðý inmem lazým. En fazla
artýþ gradyan $g$ ise, dibe inmek için $-g$ yönünde gidebilirim. 

\includegraphics[height=2cm]{19_1.png}

Bu yön doðal bir yöndür, ilk akla gelen fikirdir ve mantýklýdýr. Fakat en
iyi yön deðildir. Þimdi minimizasyon çözümü olarak eþlenik gradyan
açýsýndan bakýyoruz olaya, iþin gradyan tarafý da böylelikle açýklýða
kavuþacak. 

Negatif gradyanýn ayný zamanda artýðýn da (residual) negatif
yönüdür. Artýðýn yönünde hareket etmek iyi midir? Negatif gradyaný takip
etmenin bir diðer ismi ``en dik iniþ (steepest descent)''tir. Fakat,
baþlangýç noktasýna göre bu deðiþir ama, çok fazla iniþ çýkýþ ta
yaþanabilir.

$r$'ler hesapsal bilimde çok aranan bir özelliðe sahip deðildir,
dikgenlik. Bir þekilde dikgenlik her zaman doðru yönde hareket
ettiðimizin garantisidir. Gidilmesi gereken doðru yön, üstteki kodda
5. satýrda hesaplanan yöndür. Bu yöne ``$A$-dikgen'' denir. 

Bir resimle göstermek gerekirse, alta bakalým, soldaki en dik iniþ, saðdaki
eþlenik gradyan. Enerji fonksiyonunu kesit seviyesinden (level set), kontur
(contour) olarak gösteriyoruz, her kontur bir enerji seviyesine tekabül edecek,
mesela en dýþtaki kontur 5, bir içerideki 4 olabilir, ve en ortadaki nokta tam
sýfýr olabilir, çünkü en düþüktür.

\includegraphics[height=4cm]{19_2.png}

Her iki tekniðin gidiþatý resimde görülmektedir. 

[gerisi atlandý]

Ekler 

Üstteki anlatýmda Krylov altuzaylarýnýn eþlenik gradyan metotunun
iþleyiþinde tam olarak nasýl rol oynadýðý belirtilmemiþ. Aslýnda Krylov
altuzaylarý gerektirmeden bu metotu anlatmak mümkün. 

Ýki vektör $u,v$ birbirine A-dikgendir eðer

$$ u^TAv = 0 $$ 

iþe. Dikkat, bu iki vektör, tek baþlarýna, $u^Tv$ olarak birbirine
dikgen olmayabilir, ama ortada $A$ olduðu halde çarpým sýfýr çýkarsa
dikgen olmasalar da A-dikgen olurlar. Bu dikgenliðin bir diðer
ismi eþlenik (conjugate) olmaktýr.

Þimdi diyelim ki elimizde herbiri birbirine dikgen olan $n$ tane
$\{d_k\}$ yönü / vektörü var. O zaman $d_k$ $\mathbb{R}^n$ için bir baz
oluþturur ve biz de $Ax = b$ denkleminin çözümü $x_*$'i bu bazý temel
alarak temsil ederiz. Yani baz vektörlerini çarpan bazý katsayýlar vardýr,
ve bu çarpýmlarýn toplamý $x_*$ olur. 

$$ x_* = \sum _{ i=1}^{n} \alpha_i d_i $$

Böylece $Ax = b$'yi çözmek için bir metot elde ediyoruz, eðer $n$ tane
eþlenik yön bulabilirsek, $\alpha$ deðerlerini hemen hesaplayabiliriz.
Ayrýca eðer eþlenik vektörler $d_k$'leri dikkatlice seçersek, yaklaþýk çözüm $x_*$
için hepsine ihtiyacýmýz olmaz. Özyineli $x$ formülünü kullanabiliriz,

$$
x_{k+1} = x_k + \alpha_k d_{k+1} 
\mlabel{1}
$$

Bu formül niye mantýklý? Eðer çözüm $x_*$ dikgen $d_k$ vektörlerinin bir
lineer kombinasyonu ise, çözüm vektörleri birbiri ardýna dizilmiþ ve ``bir
yere giden'' bir zincir olarak görülebilir. Üstteki formül sadece bu
zinciri yavaþ yavaþ kurmakta..

Ýlk önce özyineli olarak artýklar $r_k$ arasýnda bir iliþki kuralým,
(1)'nin iki tarafý $A$ ile çarpýp, $b$'den çýkartalým (çünkü
$r_i = b - Ax_i$'a eriþmek istiyoruz),

$$b - A x_{k+1} = b - A x_k  + \alpha_k A d_{k} $$

$$r_{k+1} = r_k + \alpha_k A d_{k} $$

$$
r_{k+1} = r_k + \alpha_k A d_{k} 
\mlabel{8}
$$

Þimdi hata terimini hesaplayalým. $e_i$, yani $i$'inci tahminin hatasý, 

$$ e_i = x - x_i  $$

Ýki tarafý $A$ ile çarpalým

$$ Ae_i = Ax - Ax_i  $$

$$ Ae_i = b - Ax_i  $$

Sað taraf $r_i$ tanýmýnýn aynýsý deðil mi? O zaman 

$$
Ae_i = r_i 
\mlabel{5}
$$

$e$'yi özyineli olarak temsil etmek te mümkündür, (1)'nin her iki
tarafýndan $x$ çýkartýrsam, 

$$ x_{k+1} - x = x_k - x + \alpha_k d_{k} $$

$$ e_{k+1} = e_k + \alpha_k d_k 
\mlabel{2}
$$

Bu her adýmý $\alpha_k$'ye baðlý özyineli bir tanýmdýr. 

$\alpha$ katsayýlarýný bulmak için bir sonraki yönden gelen hatanýn önceki
tüm arama yönlerine, özelde bir önceki arama yönüne A-dikgen
olmasýný istiyoruz. Yani

$$ d_i^TA e_{i+1}  = 0$$

olmalý. 

$$ d_i^TA (x_{i+1}-x)  = 0$$

$$ d_i^TA (x_{i} + \alpha_i d_i -x)  = 0$$

$$ d_i^TA (e_i + \alpha_i d_i )  = 0$$

$$ d_i^Tr_i + \alpha_i d_i^TA d_i   = 0$$

$$ 
\alpha_i = -\frac{ d_i^Tr_i}{d_i^T A d_i} 
\mlabel{6}
$$

Þimdi hata terimine dönelim, diyelim ki $e_0$ vektörü, bu vektör, diðer
her vektör gibi içinde olduðumuz uzayýn bazlarýnýn bir kombinasyonu olarak
temsil edilebilir. Bizim bazlarýmýz $d_j$ olduðuna göre, 

$$ e_0 = \sum _{ j=0}^{n-1} \delta_j d_j $$

Katsayý olarak $\delta_j$ seçtik, $\alpha$ ile karýþýklýk olmasýn
diye. Þimdi iki tarafý $d_k^T A$ ile çarpalým, 

$$ d_k^T A e_0 = \sum _{ j=0}^{n-1} \delta_j d_k^T A d_j $$

Yine ayný dikgenlik numarasý, toplam içinde $j$ olmayan tüm diðer $p$
çarpýmlarý sýfýrdýr, 

$$ d_k^T A e_0 =  \delta_j d_j^T A d_j $$

$$ \delta_j = \frac{ d_k^T A e_0}{ d_j^T A d_j } 
\mlabel{4}
$$

Þimdi $e_0$'in yerine (2)'teki özyineli tanýmdan türeteceðim bir þey koymak
istiyorum. Diyelim ki $e_0$'dan baþlayýp teker teker bir sonraki $e$'yi
hesaplayýp alt alta yazdým, ve topladým

$$ \cancel{e_{1}} = e_0 + \alpha_0 d_0$$

$$ \cancel{e_{2}} = \cancel{e_1} + \alpha_1 d_1$$

$$ ... $$

$$ e_{k} = \cancel{e_{k-1}} + \alpha_{k-1} d_{k-1}$$

Sað kalan tek terimler 

$$ e_k = e_0 + \sum _{ j=0}^{k-1} \alpha_j d_j $$

(4) içinde $e_0$ yerine koyalým

$$ \delta_j = \frac{ d_k^T A e_k - \cancel{\sum _{ j=0}^{k-1} \alpha_j d_k^T A d_j}}
{ d_j^T A d_j } 
$$

Niye iptal? Yine A-dikgenliði. Dikkat edilirse $j$'ler $k-1$'e kadar
çýkýyor, $k$'ye bile eriþmiyor, çarpým hep sýfýr. Kalanlar,

$$  = \frac{ d_k^T A e_k}
{ d_j^T A d_j } 
$$

(5)'i kullanýrsak, 

$$ \delta_j = \frac{ d_k^T r_k}
{ d_j^T A d_j } 
$$


(6) ile bu formülün benzerliði bariz, sadece eksi iþareti farklý. O zaman 

$$ \delta_k = -\alpha_k $$

diyebiliriz. Bu demektir ki hata formülünde $\alpha$ yerine $\delta$
kullanabiliriz, 

$$ e_0 = -\sum _{ j=0}^{n-1} \alpha_j d_j $$

Hatalarýn özyineli denklemi (2)'yi üste uygularsak, 

$$ e_i = -\sum _{ j=i}^{n-1} \alpha_j d_j 
\mlabel{3}
$$

Þimdi artýklarýn ve önceki gidiþ yönlerinin dikgen olduklarýný
gösterelim. (3)'u $d_k^TA$ ile çarpalým, 

$$ d_k^TAe_i = -\sum _{ j=i}^{n-1} \alpha_j  d_k^TAd_j 
$$

$$ d_k^Tr_i = -\sum _{ j=i}^{n-1} \alpha_j  d_k^TAd_j 
$$

$d$'ler arasýndaki A-ortogonallik sayesinde ve $k < i$ için 

$$ d_k^Tr_i = 0
\mlabel{9}
$$

Madem ki eski yönler ve artýklar birbirine dikgen, Gram-Schmidt
iþleminin A-dikgen halini artýklardan yön üretmek için
kullanabiliriz. Her artýðý alýp, içinden ona dikgen bir yön çýkartmak
mümkün.

$$ d_i = r_i + \sum _{ j=0}^{i-1} \beta_{i,j}d_j 
\mlabel{10}
$$

$$ \beta_{i,j} = - \frac{ r_i^TAd_j}{d_j^TAd_j} $$

Yani Gram-Schmidt formülasyonunun A-dikgenlik kullanan hali (bkz Lineer
Cebir Ders 17 notlarý). Ama üstteki ifadeyi daha da basitleþtirebiliriz, ve
verimli hale getirebiliriz. Üstteki yöntemde tüm vektörleri etrafta
tutmamýz gerekiyor, ayrýca $A$'lardan kurtulmak iyi olur. 

Kurtulmak için $r_i^TAd_j$ ifadesine içinde ulaþmaya çalýþacaðýz, ve
eþitliðin diðer tarafýnda içinde $A$ olmayan bir ifade olmasýna gayret
göstereceðiz. $r_i^Tr_{j+1}$ ile baþlayalým, ve $r_{j+1}$ üzerinde özyineli
denklem (8)'i uygulayalým. 

$$ r_i^Tr_{j+1} = r_i^T (r_i + \alpha_k A d_k)  = r_i^Tr_j + \alpha_i r_i^TAd_j $$

$$ = \frac{ r_i^Tr_{j+1} - r_i^Tr_j }{\alpha_j} =  r_i^TAd_j $$

Eþitliðin saðýndaki ifadenin $\beta_{i,j}$ ifadesinin bölünen kýsmý ile
ayný olduðuna dikkat, ve eþitliðin sol tarafýnda $A$ yok. Yerine koyalým, 

$$ \beta_{i,j} = - \frac{ 1}{\alpha_j}\frac{  r_i^Tr_{j+1} - r_i^Tr_j }{d_j^TAd_j} $$

$j = i -1$, yani $\beta_{i,i-1}$ için

$$ \beta_{i,i-1} = - \frac{ 1}{\alpha_{j-1}}\frac{  r_i^Tr_i  }{d_{i-1}^TAd_{i-1}} $$

$r_i^Tr_j$ terimi $r_i^Tr_{i-1}$ olunca sýfýr oldu, çünkü artýklar
birbirine dikgen.  Dikkat bu sefer dikgen, A-dikgen deðil. Bunu
nasýl ispat ederiz?  (10)'u alýp $r_k$ ile çarpalým, ve $k,i$ indislerini
deðiþtirelim

$$ d_k^Tr_i = r_k^Tr_i + \sum _{ j=0}^{i-1} \beta_{k,j}d_j ^Tr_i = 0
$$

Sýfýra eþitlik (9) sayesinde. Ama bu sýfýr durumu toplam içindekiler için
de geçerli, çünkü toplamýn üst sýnýrý $i-1$, ve en yüksek indisli yön
$d_{j-1}$ olabilir, o zaman toplam da sýfýrdýr. Yani

$$  r_k^Tr_i  = 0 $$

$\beta$ ile iþimize devam edelim. Bölen kýsmýnda hala bir $A$ var, onu
yokedelim. Önce (6)'daki $\alpha$ tanýmýnýn $i-1$ indisli haline bakalým, ve
tersine çevirelim, 

$$ \frac{ 1}{\alpha_{i-1}} = -\frac{d_{i-1}^T A d_{i-1} }{d_{i-1}^Tr_{i-1}} 
$$

Son $\beta$ formülünde yerine koyalým

$$ \beta_{i,i-1} = 
\frac{  r_i^Tr_i  }{ \cancel{d_{i-1}^TAd_{i-1}}} 
\frac{\cancel{d_{i-1}^T A d_{i-1}}}{d_{i-1}^Tr_{i-1}}
= 
\frac{  r_i^Tr_i  }{d_{i-1}^Tr_{i-1}}
$$


Son bir eþitlik daha var, bu da $d_{i-1} = r_{i-1}$ eþitliði, nereden
geliyor?  $j < i-1$ için $r_i^Tr_{j+1}$ ve $r_i^Tr_{j}$ çarpýmlarýnýn ikisi
de sýfýrdýr, o zaman (10) formülü

$$ d_i = r_i + \beta_{i,j}d_j $$

haline gelir çünkü pek çok deðer için $\beta_{i,j} = 0$ olacaktýr. Þimdi
üsttekini bir önceki indis deðerleri için tekrar yazalým,

$$ d_{i-1} = r_{i-1} + \beta_{i-1,i-2}d_{i-2} $$

Yine dikgenlik sayesinde $\beta$ deðeri iptal olur, ve geriye sadece 

$$ d_{i-1} = r_{i-1} $$

kalýr. Böylece son formül

$$  \beta_{i,i-1} = 
\frac{  r_i^Tr_i  }{r_{i-1}^Tr_{i-1}}
$$ 

haline geliyor, ve kodlama çok temizleþiyor. 

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.linalg as lin

A = np.array([[6.,1.,1.],
              [1.,7.,1.],
              [1.,1.,8.]])

b = np.array([1.,1.,1.])

xreal = lin.solve(A, b); print "solution", xreal

p = b
r = b
x = b*0;
r2 = np.dot(r.T,r)
for i in range(5):
    Ap = np.dot(A,p)
    alpha = r2 / np.dot(p.T,Ap)
    x = x + np.dot(alpha,p)
    r = r-np.dot(alpha,Ap)
    r2old = r2
    r2 = np.dot(r.T,r)
    beta = r2 / r2old
    p = r + np.dot(beta,p)
    print x             
\end{minted}

\begin{verbatim}
solution [ 0.13249211  0.11041009  0.09463722]
[ 0.11111111  0.11111111  0.11111111]
[ 0.13125  0.1125   0.09375]
[ 0.13249211  0.11041009  0.09463722]
[ 0.13249211  0.11041009  0.09463722]
[ 0.13249211  0.11041009  0.09463722]
\end{verbatim}

\end{document}
