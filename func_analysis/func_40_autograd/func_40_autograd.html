<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Genel Optimizasyon, Paketler, Autograd</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="genel-optimizasyon-paketler-autograd">Genel Optimizasyon,
Paketler, Autograd</h1>
<p>Otomatik türevin nasıl işlediğini [1] yazısında gördük. Programlama
dilinde yazılmış, içinde <code>if</code>, <code>case</code>, hatta
döngüler bile içerebilen herhangi bir kod parçasının türevini
alabilmemizi sağlayan otomatik türev almak pek çok alanda işimize yarar.
Optimizasyon alanı bunların başında geliyor. Düşünürsek, eğer sembolik
olarak türev alması çok çetrefil bir durum varsa, tasaya gerek yok; bir
fonksiyonu kodlayabildiğimiz anda onun türevini de alabiliriz
demektir.</p>
<p>Autograd</p>
<p>Çok boyutlu bir fonksiyonun gradyani ve Hessian’ı,</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> grad, hessian</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(X): </span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    x, y, z <span class="op">=</span> X</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> z<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>x,y,z <span class="op">=</span> <span class="fl">1.0</span>,<span class="fl">1.0</span>,<span class="fl">1.0</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> hessian(objective, <span class="dv">0</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> h(np.array([x, y, z]))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (res)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> grad(objective, <span class="dv">0</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> g(np.array([x, y, z]))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (res)</span></code></pre></div>
<pre class="text"><code>[[2. 0. 0.]
 [0. 2. 0.]
 [0. 0. 2.]]
[2. 2. 2.]</code></pre>
<p>Ya da</p>
<p>Hessian</p>
<p>Mesela <span class="math inline">\(f(x_1,x_2) = x_2^3 + x_2^3 +
x_1^2x_2^2\)</span> gibi bir fonksiyon var diyelim. Belli bir noktadaki
Hessian</p>
<p><span class="math display">\[
H = \left[\begin{array}{rr}
\frac{\partial f}{\partial x_1x_1} &amp; \frac{\partial f}{\partial
x_1x_2} \\
\frac{\partial f}{\partial x_2x_1} &amp; \frac{\partial f}{\partial
x_2x_2}
\end{array}\right]
\]</span></p>
<p>hesaplatmak için <code>autograd.hessian</code> kullanırız,</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    x1,x2<span class="op">=</span>x[<span class="dv">0</span>],x[<span class="dv">1</span>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x1<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> x2<span class="op">**</span><span class="dv">3</span> <span class="op">+</span> (x1<span class="op">**</span><span class="dv">2</span>)<span class="op">*</span>(x2<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>xx <span class="op">=</span> np.array([<span class="fl">1.0</span>,<span class="fl">1.0</span>])</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> autograd.hessian(f)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (h(xx))</span></code></pre></div>
<pre class="text"><code>[[8. 4.]
 [4. 8.]]</code></pre>
<p>Şimdi bazı genel optimizasyon konularını işleyelim.</p>
<p>Sınırlanmamış optimizasyonda (unconstrained optimization) <span
class="math inline">\(f(x)\)</span> fonksiyonunu minimum değerde tutacak
<span class="math inline">\(x\)</span> değerini bulmaya uğraşıyoruz, ki
<span class="math inline">\(x\)</span> tek boyutlu skalar, ya da çok
boyutlu <span class="math inline">\(x \in \mathbb{R}^n\)</span>
olabilir. Yani yapmaya uğraştığımız</p>
<p><span class="math display">\[
\min_x f(x)
\]</span></p>
<p>işlemi. Peki minimumu nasıl tanımlarız? Bir nokta <span
class="math inline">\(x^\ast\)</span> global minimize edicidir eğer tüm
<span class="math inline">\(x\)</span>’ler için <span
class="math inline">\(f(x^\ast) \le f(x)\)</span> ise, ki <span
class="math inline">\(x \in \mathbb{R}^n\)</span>, en azından <span
class="math inline">\(x\)</span> modelleyeni ilgilendiren tüm küme
öğeleri için.</p>
<p>Fakat çoğu zaman bir global <span class="math inline">\(f\)</span>’i
kullanmak mümkün olmayabilir, fonksiyon çok çetrefil, çok boyutlu,
bilinmez durumdadır, ve elimizde sadece yerel bilgi vardır. Bu durumda
üstteki tanımı “bir <span class="math inline">\(N\)</span> bölgesi
içinde’’ olacak şekilde değiştiririz ki bölge, <span
class="math inline">\(x^\ast\)</span> etrafındaki, yakınındaki
bölgedir.</p>
<p>Üstteki tanımı okuyunca <span
class="math inline">\(x^\ast\)</span>’in yerel minimum olup olmadığını
anlamanın tek yolunun yakındaki diğer tüm noktalara teker teker bakmak
olduğu anlamı çıkabilir, fakat eğer <span
class="math inline">\(f\)</span> pürüzsüz bir fonksiyon ise yerel
minimumu doğrulamanın çok daha hızlı bir yöntemi vardır. Hatta ve hatta
eğer fonksiyon <span class="math inline">\(f\)</span> iki kez türevi
alınabilir haldeyse <span class="math inline">\(x^\ast\)</span>’in yerel
minimum olduğunu ispatlamak daha kolaylaşır, <span
class="math inline">\(\nabla f(x^\ast)\)</span> ve Hessian <span
class="math inline">\(\nabla^2 f(x^\ast)\)</span>’e bakarak bunu
yapabiliriz.</p>
<p>Minimallik için 1. ve 2. derece şartlar var. 1. derece gerekli şart
(ama yeterli değil) <span class="math inline">\(\nabla f = 0\)</span>
olması. Bu standard Calculus’tan bildiğimiz bir şey, minimum ya da
maksimumda birinci türev sıfırdır. Ama türevin sıfır olup minimum ya da
maksimum olmadığı durum da olabilir, mesela <span
class="math inline">\(f(x) = x^3\)</span>. <span
class="math inline">\(f&#39;(0) = 0\)</span>’dir fakat <span
class="math inline">\(x=0\)</span> ne maksimum ne de minimumdur. Daha
iyi bir termioloji <span class="math inline">\(\nabla f = 0\)</span>
noktalarını {} olarak tanımlamaktır. <span
class="math inline">\(x=0\)</span> noktasında bir değişim oluyor, bu
değişim kritik bir değişim, her ne kadar minimum ya da maksimum olmasa
da.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">100</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>plt.plot(x,x<span class="op">**</span><span class="dv">3</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;func_40_autograd_01.png&#39;</span>)</span></code></pre></div>
<p><img src="func_40_autograd_01.png" /></p>
<p>Bir kritik noktanın yerel maksimum ya da yerel minimum olup
olmadığını anlamak için fonksiyonun ikinci türevine bakabiliriz. Bir
<span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> var
ve <span class="math inline">\(x^\ast\)</span> noktasının kritik nokta
olduğunu düşünelim, yani <span class="math inline">\(\nabla f(x^\ast) =
0\)</span>. Şimdi çok ufak bir <span class="math inline">\(h\)</span>
adımı için <span class="math inline">\(f(x^\ast + h)\)</span>’a ne
olduğuna bakalım. Burada Taylor açılımı kullanabiliriz [2],</p>
<p><span class="math display">\[
f(x + h^\ast) =
f(x^\ast) + \nabla f(x^\ast) h +
\frac{1}{2} h^T f(x^\ast) \nabla^2 (x^\ast) f(x^\ast) h +
O(3)
\]</span></p>
<p><span class="math inline">\(\nabla^2 (x^\ast)\)</span> bir matristır
içinde <span class="math inline">\(f\)</span>’nin ikinci derece
türevleri vardır [6]. Şimdi, kritik noktada olduğumuz için <span
class="math inline">\(\nabla f(x^\ast) = 0\)</span>, ve <span
class="math inline">\(O(3)\)</span> terimlerini iptal edersek,
üstteki</p>
<p><span class="math display">\[
f(x^\ast + h^\ast) - f(x^\ast) = \frac{1}{2} h^T \nabla^2 (x^\ast)  h +
O(3)
\]</span></p>
<p>haline gelir. Simdi “bir noktanın mesela yerel maksimum olması’’
sözünü <span class="math inline">\(f(x^\ast + h^\ast) - f(x^\ast) &lt;
0\)</span> ile ifade edebiliriz, çünkü <span
class="math inline">\(x^\ast\)</span> etrafındaki tüm <span
class="math inline">\(x\)</span>’lerin <span
class="math inline">\(f\)</span>’in daha az değerlerinden olma şartını
aramış oluyoruz (adım atılıyor, çıkartma yapılıyor, sonuç sıfırdan
küçük). Tabii bu”tüm’’ söylemi yaklaşıksal, o sebeple minimumluk ifadesi
<em>yerel</em>.</p>
<p>Devam edersek <span class="math inline">\(f(x^\ast + h^\ast) -
f(x^\ast) &lt; 0\)</span> olması şartı aynı zamanda <span
class="math inline">\(\frac{1}{2} h^T \nabla^2 (x^\ast) h &lt;
0\)</span> anlamına gelir, bu da <span class="math inline">\(\nabla^2
(x^\ast )\)</span> negatif kesin demektir. Çünkü <span
class="math inline">\(A\)</span> simetrik bir matris olduğu zaman</p>
<p><span class="math inline">\(x^TAx &lt; 0\)</span> ise matris negatif
kesin</p>
<p><span class="math inline">\(x^TAx \le 0\)</span> ise matris negatif
yarı-kesin (negatif semi-definite)</p>
<p><span class="math inline">\(x^TAx &gt; 0\)</span> ise matris pozitif
kesin</p>
<p><span class="math inline">\(x^TAx \ge 0\)</span> ise matris pozitif
yarı-kesin (positive semi-definite)</p>
<p>Gradyan Inisi</p>
<p>Optimizasyonun mekaniğine gelelim. Diyelim ki basit, tek boyutlu bir
<span class="math inline">\(f(x) = x^2\)</span> fonksiyonumuz var. Tek
boyutlu bu ortamda bir noktadan başlayıp gradyanın (1. türev) işaret
ettiği yönde ufak bir adım atmak bizi minimuma daha yaklaştırır, ve bunu
ardı ardına yaparak yerel bir minimuma erisebiliriz. Örnek <span
class="math inline">\(f(x)\)</span> dışbükey (convex) olduğu için bu
bizi global minimuma götürür [3]. Formül</p>
<p><span class="math display">\[
x_{i+1} = x_i + \alpha \nabla f(x_i)
\]</span></p>
<p>Başlangıç <span class="math inline">\(x_0\)</span> herhangi bir
nokta, üstteki formülle adım ata ata ilerliyoruz, adım boyutunu bizim
tanımladığımız bir <span class="math inline">\(\alpha\)</span> sabitiyle
ayarlayabiliyoruz.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fun(x):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_desc(x, fun, alpha<span class="op">=</span><span class="fl">0.1</span>, max_iter<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    xs <span class="op">=</span> np.zeros(<span class="dv">1</span> <span class="op">+</span> max_iter)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    xs[<span class="dv">0</span>] <span class="op">=</span> x</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> autograd.grad(fun)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">-</span> alpha <span class="op">*</span> grad(x)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        xs[step <span class="op">+</span> <span class="dv">1</span>] <span class="op">=</span> x</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> xs</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>x_opt <span class="op">=</span> grad_desc(x0, fun, alpha <span class="op">=</span> alpha, max_iter <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>y_opt <span class="op">=</span> fun(x_opt)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>x_true <span class="op">=</span> np.linspace(<span class="op">-</span><span class="fl">1.2</span>, <span class="fl">1.2</span>, <span class="dv">100</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> fun(x_true)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>plt.plot(x_true, y_true)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>plt.plot(x_opt, y_opt, <span class="st">&#39;o-&#39;</span>, c<span class="op">=</span><span class="st">&#39;red&#39;</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (x, y) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(x_opt, y_opt), <span class="dv">1</span>):</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>      plt.text(x <span class="op">-</span> <span class="fl">0.1</span>, y <span class="op">+</span> <span class="fl">0.1</span>, i, fontsize<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&quot;func_40_autograd_02.png&quot;</span>)</span></code></pre></div>
<p><img src="func_40_autograd_02.png" /></p>
<p>Türevi <code>autograd</code> ile aldık, bu örnekte sembolik türev
kolaydı, elle <span class="math inline">\(f&#39;(x)=2x\)</span>
diyebilirdik ama gösterim amaçlı direk yazılımla türevi aldık.</p>
<p>Kısıtlanmış Optimizasyon</p>
<p>Mühendislik problemlerinde kısıtlanmış optimizasyon çok ortaya çıkar.
Prototipik örnek bir düzlem üzerindeki orijine en yakın noktayı bulmak.
Mesela düzlem <span class="math inline">\(2x - y + z = 3\)</span> olsun,
ve mesafeyi minimize etmek istiyoruz, bunu <span
class="math inline">\(x^2+y^2+z^2\)</span> ile hesaplayabiliriz. Yani
optimizasyon problemi düzlem denklemi ile sınırlanan mesafe formülünün
minimal noktasını bulmak [5].</p>
<p>Problemi direk <code>scipy.optimize.minimize</code> ile çözelim.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(X): <span class="co"># hedef</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    x, y, z <span class="op">=</span> X</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> z<span class="op">**</span><span class="dv">2</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cons2(X): <span class="co"># kisitlama</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    x, y, z <span class="op">=</span> X</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">2</span> <span class="op">*</span> x <span class="op">-</span> y <span class="op">+</span> z <span class="op">-</span> <span class="dv">3</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>sol <span class="op">=</span> minimize(objective, x0, constraints<span class="op">=</span>{<span class="st">&#39;type&#39;</span>: <span class="st">&#39;eq&#39;</span>, <span class="st">&#39;fun&#39;</span>: cons2})</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (sol)</span></code></pre></div>
<pre class="text"><code> message: Optimization terminated successfully
 success: True
  status: 0
     fun: 1.5000000035790053
       x: [ 1.000e+00 -5.001e-01  5.000e-01]
     nit: 4
     jac: [ 2.000e+00 -1.000e+00  9.999e-01]
    nfev: 18
    njev: 4</code></pre>
<p>Fonksiyon <code>minimize</code> için kısıtlamalar <code>eq</code> ile
sıfıra eşit olma üzerinden tanımlanır. Eğer <code>ineq</code>
kullanılırsa sıfırdan büyük olma tanımlanıyor o zaman mesela <span
class="math inline">\(x&gt;0\)</span> ve <span
class="math inline">\(x&lt;5\)</span> kısıtlamalarını getirmek
istersek,</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>cons<span class="op">=</span>({<span class="st">&#39;type&#39;</span>: <span class="st">&#39;ineq&#39;</span>,<span class="st">&#39;fun&#39;</span>: <span class="kw">lambda</span> xvec: <span class="fl">5.0</span><span class="op">-</span>xvec[<span class="dv">1</span>]}, <span class="co"># y&lt;5</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>      {<span class="st">&#39;type&#39;</span>: <span class="st">&#39;ineq&#39;</span>,<span class="st">&#39;fun&#39;</span>: <span class="kw">lambda</span> xvec: xvec[<span class="dv">1</span>]}) <span class="co"># y&gt;0</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>sol <span class="op">=</span> minimize(objective, x0, method <span class="op">=</span> <span class="st">&#39;SLSQP&#39;</span>, constraints<span class="op">=</span>cons)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (sol)</span></code></pre></div>
<pre class="text"><code> message: Optimization terminated successfully
 success: True
  status: 0
     fun: 1.1090612774580318e-16
       x: [-7.447e-09  2.739e-24 -7.447e-09]
     nit: 4
     jac: [ 7.798e-12  1.490e-08  7.799e-12]
    nfev: 16
    njev: 4</code></pre>
<p>Not: <code>SLSQP</code> metotu gradyana ihtiyaç duymuyor.</p>
<p>Bazen her şeyi kendimiz yaparak tüm adımların ne yaptığından emin
olmak isteyebiliriz. Mesela kısıtlama şartlarını kendimiz bir Lagrange
çarpanı <span class="math inline">\(f(x) f(x) - \lambda g(x)\)</span>
ifadesi üzerinden tanımlayıp, türevi alıp sıfıra eşitleyip, <span
class="math inline">\(f_x(x)=f_y(x)=f_z(x)=g(x)=0\)</span> ile, elde
edilen kısıtsız optimizasyonu çözmeyi tercih edebiliriz. Türevin
alınmasını direk <code>autograd</code>’a yaptırırız.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd.numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> grad</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> F(L):</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    x, y, z, _lambda <span class="op">=</span> L</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> objective([x, y, z]) <span class="op">-</span> _lambda <span class="op">*</span> cons2([x, y, z])</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>dfdL <span class="op">=</span> grad(F, <span class="dv">0</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Find L that returns all zeros in this function.</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> obj(L):</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    x, y, z, _lambda <span class="op">=</span> L</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    dFdx, dFdy, dFdz, dFdlam <span class="op">=</span> dfdL(L)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [dFdx, dFdy, dFdz, cons2([x, y, z])]</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> fsolve</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>x, y, z, _lam <span class="op">=</span> fsolve(obj, [<span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">1.0</span>])</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (x,y,z)</span></code></pre></div>
<pre class="text"><code>1.0 -0.5 0.5</code></pre>
<p>Aynı sonuç bulundu. Şimdi merak ediyoruz, bu sonuç gerçekten minimum
mu? Üstteki noktada Hessian’ın pozitif kesin olup olmadığını kontrol
edebiliriz. Hessian’ı da <code>autograd</code> hesaplar! Once
gradyan,</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> hessian</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> hessian(objective, <span class="dv">0</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> h(np.array([x,y,z]))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (res)</span></code></pre></div>
<pre class="text"><code>[[2. 0. 0.]
 [0. 2. 0.]
 [0. 0. 2.]]</code></pre>
<p>Bu matris pozitif kesin, ama çıplak gözle bariz değilse, tüm
özdeğerleri pozitif olup olmadığına bakabiliriz,</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (np.linalg.eig(h(np.array([x, y, z])))[<span class="dv">0</span>])</span></code></pre></div>
<pre class="text"><code>[2. 2. 2.]</code></pre>
<p>Birden Fazla Gradyan Değişkeni</p>
<p>Diyelim ki elimizde</p>
<p><span class="math display">\[
g(w_1,w_2) = \tanh (w_1w_2)
\]</span></p>
<p>fonksiyonu var, bu üç boyutlu bir fonksiyon, ve optimizasyon amaçlı
gradyan gerekiyor, gradyanın iki değişken üzerinden alınması gerekli
[7].</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> autograd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> autograd <span class="im">import</span> numpy <span class="im">as</span> anp</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(w_1,w_2):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> anp.tanh(w_1<span class="op">*</span>w_2)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> mpl_toolkits.mplot3d <span class="im">import</span> Axes3D</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> cm</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">20</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">20</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>xx,yy <span class="op">=</span> np.meshgrid(x,y)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>zz <span class="op">=</span> g(xx,yy)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">1</span>, subplot_kw<span class="op">=</span>{<span class="st">&#39;projection&#39;</span>: <span class="st">&#39;3d&#39;</span>})</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>surf <span class="op">=</span> ax.plot_surface(xx, yy, zz, cmap<span class="op">=</span>cm.coolwarm)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;func_40_autograd_03.png&#39;</span>)</span></code></pre></div>
<p><img src="func_40_autograd_03.png" /></p>
<p><span class="math inline">\(g\)</span>’nin her iki kısmi türevini ve
gradyanını,</p>
<p><span class="math display">\[
\nabla g(w_1,w_2) = \left[\begin{array}{r}
\frac{\partial }{\partial w_1} g(w_1,w_2) \\
\frac{\partial }{\partial w_2} g(w_1,w_2)
\end{array}\right]
\]</span></p>
<p><code>autograd</code> ile hesaplamak için</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>dgdw1 <span class="op">=</span> autograd.grad(g,<span class="dv">0</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>dgdw2 <span class="op">=</span> autograd.grad(g,<span class="dv">1</span>)</span></code></pre></div>
<p>Dikkat edersek, 0 ve 1 parametreleri geçildi, bunlar sırasıyla <span
class="math inline">\(w_1\)</span> ve <span
class="math inline">\(w_2\)</span> değişkenlerine tekabül ediyorlar
(<code>g</code> tanımındaki sıralarına göre, 0. ve 1. parametreler).
Şimdi mesela (1.0,2.0) noktasındaki gradyanı hesaplayabiliriz,</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>gradg <span class="op">=</span> [dgdw1(<span class="fl">1.0</span>,<span class="fl">2.0</span>), dgdw2(<span class="fl">1.0</span>,<span class="fl">2.0</span>)]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (gradg)</span></code></pre></div>
<pre class="text"><code>[np.float64(0.14130164970632894), np.float64(0.07065082485316447)]</code></pre>
<p>Tabii çok boyutlu ortamda yazının başındaki teknikleri kullanmak daha
iyi, üstteki bir seçenek.</p>
<p>Kaynaklar</p>
<p>[1] Bayramlı, Ders Notları, <em>Otomatik Türev Almak (Automatic
Differentiation -AD-)</em></p>
<p>[2] Schrimpf, <a
href="http://faculty.arts.ubc.ca/pschrimpf/526/526.html">http://faculty.arts.ubc.ca/pschrimpf/526/526.html</a></p>
<p>[3] Stoyanov, <a
href="https://nikstoyanov.me/post/2019-04-14-numerical-optimizations">https://nikstoyanov.me/post/2019-04-14-numerical-optimizations</a></p>
<p>[5] Kitchin, <a
href="http://kitchingroup.cheme.cmu.edu/blog/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd/">http://kitchingroup.cheme.cmu.edu/blog/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd/</a></p>
<p>[6] Bayramlı, Cok Boyutlu Calculus, <em>Vektör Calculus, Kurallar,
Matris Türevleri</em></p>
<p>[7] Watt, <em>Automatic Differentiation</em>, <a
href="https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_5_Automatic.html">https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_5_Automatic.html</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
