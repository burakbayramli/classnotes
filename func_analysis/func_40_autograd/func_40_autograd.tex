\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Genel Optimizasyon, Paketler, Autograd

Otomatik türevin nasýl iþlediðini [1] yazýsýnda gördük. Programlama dilinde
yazýlmýþ, içinde \verb!if!, \verb!case!, hatta döngüler bile içerebilen
herhangi bir kod parçasýnýn türevini alabilmemizi saðlayan otomatik türev
almak pek çok alanda iþimize yarar. Optimizasyon alaný bunlarýn baþýnda
geliyor. Düþünürsek, eðer sembolik olarak türev almasý çok çetrefil bir
durum varsa, tasaya gerek yok; bir fonksiyonu kodlayabildiðimiz anda onun
türevini de alabiliriz demektir.

Autograd

Çok boyutlu bir fonksiyonun gradyani ve Hessian'ý,

\begin{minted}[fontsize=\footnotesize]{python}
from autograd import grad, hessian

def objective(X): 
    x, y, z = X
    return x**2 + y**2 + z**2

x,y,z = 1.0,1.0,1.0

h = hessian(objective, 0)
res = h(np.array([x, y, z]))
print (res)

g = grad(objective, 0)
res = g(np.array([x, y, z]))
print (res)
\end{minted}

\begin{verbatim}
[[2. 0. 0.]
 [0. 2. 0.]
 [0. 0. 2.]]
[2. 2. 2.]
\end{verbatim}

Ya da

Hessian

Mesela $f(x_1,x_2) = x_2^3 + x_2^3 + x_1^2x_2^2$ gibi bir fonksiyon var
diyelim. Belli bir noktadaki Hessian

$$
H = \left[\begin{array}{rr}
\frac{\partial f}{\partial x_1x_1} & \frac{\partial f}{\partial x_1x_2} \\
\frac{\partial f}{\partial x_2x_1} & \frac{\partial f}{\partial x_2x_2} 
\end{array}\right]
$$

hesaplatmak için \verb!autograd.hessian! kullanýrýz,

\begin{minted}[fontsize=\footnotesize]{python}
import autograd 

def f(x):
    x1,x2=x[0],x[1]
    return x1**3 + x2**3 + (x1**2)*(x2**2)

print
xx = np.array([1.0,1.0])
h = autograd.hessian(f)
print (h(xx))
\end{minted}

\begin{verbatim}
[[8. 4.]
 [4. 8.]]
\end{verbatim}

Þimdi bazý genel optimizasyon konularýný iþleyelim. 

Sýnýrlanmamýþ optimizasyonda (unconstrained optimization) $f(x)$
fonksiyonunu minimum deðerde tutacak $x$ deðerini bulmaya uðraþýyoruz, ki
$x$ tek boyutlu skalar, ya da çok boyutlu $x \in \mathbb{R}^n$
olabilir. Yani yapmaya uðraþtýðýmýz

$$
\min_x f(x)
$$

iþlemi. Peki minimumu nasýl tanýmlarýz? Bir nokta $x^*$ global minimize
edicidir eðer tüm $x$'ler için $f(x^*) \le f(x)$ ise, ki
$x \in \mathbb{R}^n$, en azýndan $x$ modelleyeni ilgilendiren tüm küme
öðeleri için.

Fakat çoðu zaman bir global $f$'i kullanmak mümkün olmayabilir, fonksiyon
çok çetrefil, çok boyutlu, bilinmez durumdadýr, ve elimizde sadece yerel
bilgi vardýr. Bu durumda üstteki tanýmý ``bir $N$ bölgesi içinde'' olacak
þekilde deðiþtiririz ki bölge, $x^*$ etrafýndaki, yakýnýndaki bölgedir.

Üstteki tanýmý okuyunca  $x^*$'in yerel minimum olup olmadýðýný anlamanýn
tek yolunun yakýndaki diðer tüm noktalara teker teker bakmak olduðu anlamý
çýkabilir, fakat eðer $f$ pürüzsüz bir fonksiyon ise yerel minimumu
doðrulamanýn çok daha hýzlý bir yöntemi vardýr. Hatta ve hatta eðer
fonksiyon $f$ iki kez türevi alýnabilir haldeyse $x^*$'in yerel minimum
olduðunu ispatlamak daha kolaylaþýr, $\nabla f(x^*)$ ve Hessian $\nabla^2
f(x^*)$'e bakarak bunu yapabiliriz.

Minimallik için 1. ve 2. derece þartlar var. 1. derece gerekli þart (ama
yeterli deðil) $\nabla f = 0$ olmasý. Bu standard Calculus'tan bildiðimiz
bir þey, minimum ya da maksimumda birinci türev sýfýrdýr. Ama türevin sýfýr
olup minimum ya da maksimum olmadýðý durum da olabilir, mesela
$f(x) = x^3$. $f'(0) = 0$'dir fakat $x=0$ ne maksimum ne de
minimumdur. Daha iyi bir termioloji $\nabla f = 0$ noktalarýný {\em kritik
  nokta} olarak tanýmlamaktýr. $x=0$ noktasýnda bir deðiþim oluyor, bu
deðiþim kritik bir deðiþim, her ne kadar minimum ya da maksimum olmasa da.

\begin{minted}[fontsize=\footnotesize]{python}
x = np.linspace(-3,3,100)
plt.plot(x,x**3)
plt.grid(True)
plt.savefig('func_40_autograd_01.png')
\end{minted}

\includegraphics[height=6cm]{func_40_autograd_01.png}

Bir kritik noktanýn yerel maksimum ya da yerel minimum olup olmadýðýný
anlamak için fonksiyonun ikinci türevine bakabiliriz. Bir
$f: \mathbb{R}^n \to \mathbb{R}$ var ve $x^*$ noktasýnýn kritik nokta
olduðunu düþünelim, yani $\nabla f(x^*) = 0$. Þimdi çok ufak bir $h$ adýmý
için $f(x^* + h)$'a ne olduðuna bakalým. Burada Taylor açýlýmý
kullanabiliriz [2], 

$$
f(x + h^*) = 
f(x^*) + \nabla f(x^*) h + 
\frac{1}{2} h^T f(x^*) \nabla^2 (x^*) f(x^*) h + 
O(3)
$$

$\nabla^2 (x^*)$ bir matristýr içinde $f$'nin ikinci derece türevleri
vardýr [6]. Þimdi, kritik noktada olduðumuz için $\nabla f(x^*) = 0$, ve
$O(3)$ terimlerini iptal edersek, üstteki

$$
f(x^* + h^*) - f(x^*) = \frac{1}{2} h^T \nabla^2 (x^*)  h + O(3)
$$

haline gelir. Simdi ``bir noktanýn mesela yerel maksimum olmasý'' sözünü
$f(x^* + h^*) - f(x^*) < 0$ ile ifade edebiliriz, çünkü $x^*$ etrafýndaki
tüm $x$'lerin $f$'in daha az deðerlerinden olma þartýný aramýþ oluyoruz
(adým atýlýyor, çýkartma yapýlýyor, sonuç sýfýrdan küçük). Tabii bu ``tüm''
söylemi yaklaþýksal, o sebeple minimumluk ifadesi {\em yerel}.

Devam edersek $f(x^* + h^*) - f(x^*) < 0$ olmasý þartý ayný zamanda
$\frac{1}{2} h^T \nabla^2 (x^*) h < 0$ anlamýna gelir, bu da
$\nabla^2 (x^*)$ negatif kesin demektir. Çünkü $A$ simetrik bir matris
olduðu zaman

$x^TAx < 0$ ise matris negatif kesin

$x^TAx \le 0$ ise matris negatif yarý-kesin (negatif semi-definite)

$x^TAx > 0$ ise matris pozitif kesin

$x^TAx \ge 0$ ise matris pozitif yarý-kesin (positive semi-definite)

Gradyan Inisi

Optimizasyonun mekaniðine gelelim. Diyelim ki basit, tek boyutlu bir $f(x)
= x^2$ fonksiyonumuz var. Tek boyutlu bu ortamda bir noktadan baþlayýp
gradyanýn (1. türev) iþaret ettiði yönde ufak bir adým atmak bizi minimuma
daha yaklaþtýrýr, ve bunu ardý ardýna yaparak yerel bir minimuma
erisebiliriz. Örnek $f(x)$ dýþbükey (convex) olduðu için bu bizi global
minimuma götürür [3]. Formül

$$
x_{i+1} = x_i + \alpha \nabla f(x_i)
$$

Baþlangýç $x_0$ herhangi bir nokta, üstteki formülle adým ata ata
ilerliyoruz, adým boyutunu bizim tanýmladýðýmýz bir $\alpha$ sabitiyle
ayarlayabiliyoruz. 


\begin{minted}[fontsize=\footnotesize]{python}
import autograd

def fun(x):
    return x**2

def grad_desc(x, fun, alpha=0.1, max_iter=100):
    xs = np.zeros(1 + max_iter)
    xs[0] = x
    grad = autograd.grad(fun)
    
    for step in range(max_iter):
        x = x - alpha * grad(x)
        xs[step + 1] = x
        
\includegraphics[height=6cm]{func_40_autograd_02.png}
    return xs

alpha = 0.1
x0 = 1.

x_opt = grad_desc(x0, fun, alpha = alpha, max_iter = 10)
y_opt = fun(x_opt)

x_true = np.linspace(-1.2, 1.2, 100)
y_true = fun(x_true)

plt.plot(x_true, y_true)
plt.plot(x_opt, y_opt, 'o-', c='red')

for i, (x, y) in enumerate(zip(x_opt, y_opt), 1):
      plt.text(x - 0.1, y + 0.1, i, fontsize=15)

plt.show()
\end{minted}


\includegraphics[height=6cm]{func_40_autograd_02.png}

Türevi \verb!autograd! ile aldýk, bu örnekte sembolik türev kolaydý, elle
$f'(x)=2x$ diyebilirdik ama gösterim amaçlý direk yazýlýmla türevi aldýk.

Kýsýtlanmýþ Optimizasyon

Mühendislik problemlerinde kýsýtlanmýþ optimizasyon çok ortaya
çýkar. Prototipik örnek bir düzlem üzerindeki orijine en yakýn noktayý
bulmak. Mesela düzlem $2x - y + z = 3$ olsun, ve mesafeyi minimize etmek
istiyoruz, bunu $x^2+y^2+z^2$ ile hesaplayabiliriz. Yani optimizasyon
problemi düzlem denklemi ile sýnýrlanan mesafe formülünün minimal noktasýný
bulmak [5]. 

Problemi direk \verb!scipy.optimize.minimize! ile çözelim. 

\begin{minted}[fontsize=\footnotesize]{python}
from scipy.optimize import minimize

def objective(X): # hedef
    x, y, z = X
    return x**2 + y**2 + z**2

def cons(X): # kisitlama
    x, y, z = X
    return 2 * x - y + z - 3

x0 = [1, 1, 1]
sol = minimize(objective, x0, constraints={'type': 'eq', 'fun': cons})
print (sol)
\end{minted}

\begin{verbatim}
     fun: 1.5000000035790053
     jac: array([ 1.99997392, -1.00010441,  0.99994774])
 message: 'Optimization terminated successfully.'
    nfev: 22
     nit: 4
    njev: 4
  status: 0
 success: True
       x: array([ 0.99998696, -0.50005221,  0.49997386])
\end{verbatim}

Fonksiyon \verb!minimize! için kýsýtlamalar \verb!eq! ile sýfýra eþit olma
üzerinden tanýmlanýr. Eðer \verb!ineq! kullanýlýrsa sýfýrdan büyük olma
tanýmlanýyor o zaman mesela $x>0$ ve $x<5$ kýsýtlamalarýný getirmek
istersek, 

\begin{minted}[fontsize=\footnotesize]{python}
cons=({'type': 'ineq','fun': lambda xvec: 5.0-xvec[1]}, # y<5
      {'type': 'ineq','fun': lambda xvec: xvec[1]}) # y>0
sol = minimize(objective, x0, method = 'SLSQP', constraints=cons)
print (sol)
\end{minted}

Not: \verb!SLSQP! metotu gradyana ihtiyaç duymuyor. 

\begin{verbatim}
     fun: 1.1090612774580318e-16
     jac: array([7.79817877e-12, 1.49011612e-08, 7.79860898e-12])
 message: 'Optimization terminated successfully.'
    nfev: 20
     nit: 4
    njev: 4
  status: 0
 success: True
       x: array([-7.44668151e-09,  2.73897702e-24, -7.44668129e-09])
\end{verbatim}

Bazen her þeyi kendimiz yaparak tüm adýmlarýn ne yaptýðýndan emin olmak
isteyebiliriz. Mesela kýsýtlama þartlarýný kendimiz bir Lagrange çarpaný
$f(x) f(x) - \lambda g(x)$ ifadesi üzerinden tanýmlayýp, türevi alýp sýfýra
eþitleyip, $f_x(x)=f_y(x)=f_z(x)=g(x)=0$ ile, elde edilen kýsýtsýz
optimizasyonu çözmeyi tercih edebiliriz. Türevin alýnmasýný direk
\verb!autograd!'a yaptýrýrýz.

\begin{minted}[fontsize=\footnotesize]{python}
import autograd.numpy as np
from autograd import grad

def F(L):
    x, y, z, _lambda = L
    return objective([x, y, z]) - _lambda * eq([x, y, z])

dfdL = grad(F, 0)

# Find L that returns all zeros in this function.
def obj(L):
    x, y, z, _lambda = L
    dFdx, dFdy, dFdz, dFdlam = dfdL(L)
    return [dFdx, dFdy, dFdz, eq([x, y, z])]

from scipy.optimize import fsolve
x, y, z, _lam = fsolve(obj, [0.0, 0.0, 0.0, 1.0])
print (x,y,z)
\end{minted}

\begin{verbatim}
1.0 -0.5 0.5
\end{verbatim}

Ayný sonuç bulundu. Þimdi merak ediyoruz, bu sonuç gerçekten minimum mu?
Üstteki noktada Hessian'ýn pozitif kesin olup olmadýðýný kontrol
edebiliriz. Hessian'ý da \verb!autograd! hesaplar! Once gradyan,

\begin{minted}[fontsize=\footnotesize]{python}
from autograd import hessian
h = hessian(objective, 0)
res = h(np.array([x,y,z]))
print (res)
\end{minted}

\begin{verbatim}
[[2. 0. 0.]
 [0. 2. 0.]
 [0. 0. 2.]]
\end{verbatim}

Bu matris pozitif kesin, ama çýplak gözle bariz deðilse, tüm özdeðerleri
pozitif olup olmadýðýna bakabiliriz, 

\begin{minted}[fontsize=\footnotesize]{python}
print (np.linalg.eig(h(np.array([x, y, z])))[0])
\end{minted}

\begin{verbatim}
[2. 2. 2.]
\end{verbatim}

Birden Fazla Gradyan Deðiþkeni

Diyelim ki elimizde 

$$
g(w_1,w_2) = \tanh (w_1w_2)
$$

fonksiyonu var, bu üç boyutlu bir fonksiyon, ve optimizasyon amaçlý gradyan
gerekiyor, gradyanýn iki deðiþken üzerinden alýnmasý gerekli [7]. 

\begin{minted}[fontsize=\footnotesize]{python}
import autograd
from autograd import numpy as anp

def g(w_1,w_2):
    return anp.tanh(w_1*w_2)

from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
x = np.linspace(-4,4,20)
y = np.linspace(-4,4,20)
xx,yy = np.meshgrid(x,y)
zz = g(xx,yy)
fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_surface(xx, yy, zz, cmap=cm.coolwarm)
plt.savefig('func_40_autograd_03.png')
\end{minted}

\includegraphics[width=20em]{func_40_autograd_03.png}

$g$'nin her iki kýsmi türevini ve gradyanýný, 

$$
\nabla g(w_1,w_2) = \left[\begin{array}{r}
\frac{\partial }{\partial w_1} g(w_1,w_2) \\
\frac{\partial }{\partial w_2} g(w_1,w_2) 
\end{array}\right]
$$

\verb!autograd! ile hesaplamak için 

\begin{minted}[fontsize=\footnotesize]{python}
dgdw1 = autograd.grad(g,0)
dgdw2 = autograd.grad(g,1)
\end{minted}

Dikkat edersek, 0 ve 1 parametreleri geçildi, bunlar sýrasýyla $w_1$ ve
$w_2$ deðiþkenlerine tekabül ediyorlar (\verb!g! tanýmýndaki sýralarýna
göre, 0. ve 1. parametreler). Þimdi mesela (1.0,2.0) noktasýndaki gradyaný
hesaplayabiliriz,

\begin{minted}[fontsize=\footnotesize]{python}
gradg = [dgdw1(1.0,2.0), dgdw2(1.0,2.0)]
print (gradg)
\end{minted}

\begin{verbatim}
[0.14130164970632894, 0.07065082485316447]
\end{verbatim}

Tabii çok boyutlu ortamda yazýnýn baþýndaki teknikleri kullanmak daha iyi,
üstteki bir seçenek.

Kaynaklar 

[1] Bayramlý, Ders Notlarý, {\em Otomatik Türev Almak (Automatic Differentiation -AD-)}

[2] Schrimpf, \url{http://faculty.arts.ubc.ca/pschrimpf/526/526.html}

[3] Stoyanov, \url{https://nikstoyanov.me/post/2019-04-14-numerical-optimizations}

[5] Kitchin, \url{http://kitchingroup.cheme.cmu.edu/blog/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd/}

[6] Bayramli, Cok Boyutlu Calculus, {\em Vektör Calculus, Kurallar, Matris Türevleri}

[7] Watt, {\em Automatic Differentiation}, 
    \url{https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_5_Automatic.html}


\end{document}



