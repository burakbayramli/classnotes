\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Autograd ile Optimizasyon

Otomatik türevin nasýl iþlediðini [1] yazýsýnda gördük. Programlama dilinde
yazýlmýþ, içinde \verb!if!, \verb!case!, hatta döngüler bile içerebilen
herhangi bir kod parçasýnýn türevini alabilmemizi saðlayan otomatik türev
almak pek çok alanda iþimize yarar. Optimizasyon alaný bunlarýn baþýnda
geliyor. Düþünürsek, eðer sembolik olarak türev almasý çok çetrefil bir
durum varsa, tasaya gerek yok; bir fonksiyonu kodlayabildiðimiz anda onun
türevini de alabiliriz demektir.

Önce bazý genel optimizasyon konularýný iþleyelim. 

Sýnýrlanmamýþ optimizasyonda (unconstrained optimization) $f(x)$
fonksiyonunu minimum deðerde tutacak $x$ deðerini bulmaya uðraþýyoruz, ki
$x$ tek boyutlu skalar, ya da çok boyutlu $x \in \mathbb{R}^n$
olabilir. Yani yapmaya uðraþtýðýmýz

$$
\min_x f(x)
$$

iþlemi. Peki minimumu nasýl tanýmlarýz? Bir nokta $x^*$ global minimize
edicidir eðer tüm $x$'ler için $f(x^*) \le f(x)$ ise, ki
$x \in \mathbb{R}^n$, en azýndan $x$ modelleyeni ilgilendiren tüm küme
öðeleri için.

Fakat çoðu zaman bir global $f$'i kullanmak mümkün olmayabilir, fonksiyon
çok çetrefil, çok boyutlu, bilinmez durumdadýr, ve elimizde sadece yerel
bilgi vardýr. Bu durumda üstteki tanýmý ``bir $N$ bölgesi içinde'' olacak
þekilde deðiþtiririz ki bölge, $x^*$ etrafýndaki, yakýnýndaki bölgedir.

Üstteki tanýmý okuyunca  $x^*$'in yerel minimum olup olmadýðýný anlamanýn
tek yolunun yakýndaki diðer tüm noktalara teker teker bakmak olduðu anlamý
çýkabilir, fakat eðer $f$ pürüzsüz bir fonksiyon ise yerel minimumu
doðrulamanýn çok daha hýzlý bir yöntemi vardýr. Hatta ve hatta eðer
fonksiyon $f$ iki kez türevi alýnabilir haldeyse $x^*$'in yerel minimum
olduðunu ispatlamak daha kolaylaþýr, $\nabla f(x^*)$ ve Hessian $\nabla^2
f(x^*)$'e bakarak bunu yapabiliriz.

Minimallik için 1. ve 2. derece þartlar var. 1. derece gerekli þart (ama
yeterli deðil) $\nabla f = 0$ olmasý. Bu standard Calculus'tan bildiðimiz
bir þey, minimum ya da maksimumda birinci türev sýfýrdýr. Ama türevin sýfýr
olup minimum ya da maksimum olmadýðý durum da olabilir, mesela
$f(x) = x^3$. $f'(0) = 0$'dir fakat $x=0$ ne maksimum ne de
minimumdur. Daha iyi bir termioloji $\nabla f = 0$ noktalarýný {\em kritik
  nokta} olarak tanýmlamaktýr. $x=0$ noktasýnda bir deðiþim oluyor, bu
deðiþim kritik bir deðiþim, her ne kadar minimum ya da maksimum olmasa da.

\begin{minted}[fontsize=\footnotesize]{python}
x = np.linspace(-3,3,100)
plt.plot(x,x**3)
plt.grid(True)
plt.savefig('func_40_autograd_01.png')
\end{minted}

\includegraphics[height=6cm]{func_40_autograd_01.png}

Bir kritik noktanýn yerel maksimum ya da yerel minimum olup olmadýðýný
anlamak için fonksiyonun ikinci türevine bakabiliriz. Bir
$f: \mathbb{R}^n \to \mathbb{R}$ var ve $x^*$ noktasýnýn kritik nokta
olduðunu düþünelim, yani $\nabla f(x^*) = 0$. Þimdi çok ufak bir $h$ adýmý
için $f(x^* + h)$'a ne olduðuna bakalým. Burada Taylor açýlýmý
kullanabiliriz [2], 

$$
f(x + h^*) = 
f(x^*) + \nabla f(x^*) h + 
\frac{1}{2} h^T f(x^*) \nabla^2 (x^*) f(x^*) h + 
O(3)
$$

$\nabla^2 (x^*)$ bir matristýr içinde $f$'nin ikinci derece türevleri
vardýr [6]. Þimdi, kritik noktada olduðumuz için $\nabla f(x^*) = 0$, ve
$O(3)$ terimlerini iptal edersek, üstteki

$$
f(x^* + h^*) - f(x^*) = \frac{1}{2} h^T \nabla^2 (x^*)  h + O(3)
$$

haline gelir. Simdi ``bir noktanýn mesela yerel maksimum olmasý'' sözünü
$f(x^* + h^*) - f(x^*) < 0$ ile ifade edebiliriz, çünkü $x^*$ etrafýndaki
tüm $x$'lerin $f$'in daha az deðerlerinden olma þartýný aramýþ oluyoruz
(adým atýlýyor, çýkartma yapýlýyor, sonuç sýfýrdan küçük). Tabii bu ``tüm''
söylemi yaklaþýksal, o sebeple minimumluk ifadesi {\em yerel}.

Devam edersek $f(x^* + h^*) - f(x^*) < 0$ olmasý þartý ayný zamanda
$\frac{1}{2} h^T \nabla^2 (x^*) h < 0$ anlamýna gelir, bu da
$\nabla^2 (x^*)$ negatif kesin demektir. Çünkü $A$ simetrik bir matris
olduðu zaman

$x^TAx < 0$ ise matris negatif kesin

$x^TAx \le 0$ ise matris negatif yarý-kesin (negatif semi-definite)

$x^TAx > 0$ ise matris pozitif kesin

$x^TAx \ge 0$ ise matris pozitif yarý-kesin (positive semi-definite)

Gradyan Inisi

Optimizasyonun mekaniðine gelelim. Diyelim ki basit, tek boyutlu bir $f(x)
= x^2$ fonksiyonumuz var. Tek boyutlu bu ortamda bir noktadan baþlayýp
gradyanýn (1. türev) iþaret ettiði yönde ufak bir adým atmak bizi minimuma
daha yaklaþtýrýr, ve bunu ardý ardýna yaparak yerel bir minimuma
erisebiliriz. Örnek $f(x)$ dýþbükey (convex) olduðu için bu bizi global
minimuma götürür [3]. Formül

$$
x_{i+1} = x_i + \alpha \nabla f(x_i)
$$

Baþlangýç $x_0$ herhangi bir nokta, üstteki formülle adým ata ata
ilerliyoruz, adým boyutunu bizim tanýmladýðýmýz bir $\alpha$ sabitiyle
ayarlayabiliyoruz. 


\begin{minted}[fontsize=\footnotesize]{python}
import autograd

def fun(x):
    return x**2

def grad_desc(x, fun, alpha=0.1, max_iter=100):
    xs = np.zeros(1 + max_iter)
    xs[0] = x
    grad = autograd.grad(fun)
    
    for step in range(max_iter):
        x = x - alpha * grad(x)
        xs[step + 1] = x
        
\includegraphics[height=6cm]{func_40_autograd_02.png}
    return xs

alpha = 0.1
x0 = 1.

x_opt = grad_desc(x0, fun, alpha = alpha, max_iter = 10)
y_opt = fun(x_opt)

x_true = np.linspace(-1.2, 1.2, 100)
y_true = fun(x_true)

plt.plot(x_true, y_true)
plt.plot(x_opt, y_opt, 'o-', c='red')

for i, (x, y) in enumerate(zip(x_opt, y_opt), 1):
      plt.text(x - 0.1, y + 0.1, i, fontsize=15)

plt.show()
\end{minted}


\includegraphics[height=6cm]{func_40_autograd_02.png}

Türevi \verb!autograd! ile aldýk, bu örnekte sembolik türev kolaydý, elle
$f'(x)=2x$ diyebilirdik ama gösterim amaçlý direk yazýlýmla türevi aldýk.

Kýsýtlanmýþ Optimizasyon

Mühendislik problemlerinde kýsýtlanmýþ optimizasyon çok ortaya
çýkar. Prototipik örnek bir düzlem üzerindeki orijine en yakýn noktayý
bulmak. Mesela düzlem $2x - y + z = 3$ olsun, ve mesafeyi minimize etmek
istiyoruz, bunu $x^2+y^2+z^2$ ile hesaplayabiliriz. Yani optimizasyon
problemi düzlem denklemi ile sýnýrlanan mesafe formülünün minimal noktasýný
bulmak [5]. 

Problemi direk \verb!scipy.optimize.minimize! ile çözelim. 

\begin{minted}[fontsize=\footnotesize]{python}
from scipy.optimize import minimize

def objective(X): # hedef
    x, y, z = X
    return x**2 + y**2 + z**2

def cons(X): # kisitlama
    x, y, z = X
    return 2 * x - y + z - 3

x0 = [1, 1, 1]
sol = minimize(objective, x0, constraints={'type': 'eq', 'fun': cons})
print (sol)
\end{minted}

\begin{verbatim}
     fun: 1.5000000035790053
     jac: array([ 1.99997392, -1.00010441,  0.99994774])
 message: 'Optimization terminated successfully.'
    nfev: 22
     nit: 4
    njev: 4
  status: 0
 success: True
       x: array([ 0.99998696, -0.50005221,  0.49997386])
\end{verbatim}

Fonksiyon \verb!minimize! için kýsýtlamalar \verb!eq! ile sýfýra eþit olma
üzerinden tanýmlanýr. Eðer \verb!ineq! kullanýlýrsa sýfýrdan büyük olma
tanýmlanýyor o zaman mesela $x>0$ ve $x<5$ kýsýtlamalarýný getirmek
istersek, 

\begin{minted}[fontsize=\footnotesize]{python}
cons=({'type': 'ineq','fun': lambda xvec: 5.0-xvec[1]}, # y<5
      {'type': 'ineq','fun': lambda xvec: xvec[1]}) # y>0
sol = minimize(objective, x0, method = 'SLSQP', constraints=cons)
print (sol)
\end{minted}

Not: \verb!SLSQP! metotu gradyana ihtiyaç duymuyor. 

\begin{verbatim}
     fun: 1.1090612774580318e-16
     jac: array([7.79817877e-12, 1.49011612e-08, 7.79860898e-12])
 message: 'Optimization terminated successfully.'
    nfev: 20
     nit: 4
    njev: 4
  status: 0
 success: True
       x: array([-7.44668151e-09,  2.73897702e-24, -7.44668129e-09])
\end{verbatim}

Bazen her þeyi kendimiz yaparak tüm adýmlarýn ne yaptýðýndan emin olmak
isteyebiliriz. Mesela kýsýtlama þartlarýný kendimiz bir Lagrange çarpaný
$f(x) f(x) - \lambda g(x)$ ifadesi üzerinden tanýmlayýp, türevi alýp sýfýra
eþitleyip, $f_x(x)=f_y(x)=f_z(x)=g(x)=0$ ile, elde edilen kýsýtsýz
optimizasyonu çözmeyi tercih edebiliriz. Türevin alýnmasýný direk
\verb!autograd!'a yaptýrýrýz.

\begin{minted}[fontsize=\footnotesize]{python}
import autograd.numpy as np
from autograd import grad

def F(L):
    x, y, z, _lambda = L
    return objective([x, y, z]) - _lambda * eq([x, y, z])

dfdL = grad(F, 0)

# Find L that returns all zeros in this function.
def obj(L):
    x, y, z, _lambda = L
    dFdx, dFdy, dFdz, dFdlam = dfdL(L)
    return [dFdx, dFdy, dFdz, eq([x, y, z])]

from scipy.optimize import fsolve
x, y, z, _lam = fsolve(obj, [0.0, 0.0, 0.0, 1.0])
print (x,y,z)
\end{minted}

\begin{verbatim}
1.0 -0.5 0.5
\end{verbatim}

Ayný sonuç bulundu. Þimdi merak ediyoruz, bu sonuç gerçekten minimum mu?
Üstteki noktada Hessian'ýn pozitif kesin olup olmadýðýný kontrol
edebiliriz. Hessian'ý da \verb!autograd! hesaplar!

\begin{minted}[fontsize=\footnotesize]{python}
from autograd import hessian
h = hessian(objective, 0)
res = h(np.array([x, y, z]))
print (res)
\end{minted}

\begin{verbatim}
[[2. 0. 0.]
 [0. 2. 0.]
 [0. 0. 2.]]
\end{verbatim}

Bu matris pozitif kesin, ama çýplak gözle bariz deðilse, tüm özdeðerleri
pozitif olup olmadýðýna bakabiliriz, 

\begin{minted}[fontsize=\footnotesize]{python}
print (np.linalg.eig(h(np.array([x, y, z])))[0])
\end{minted}

\begin{verbatim}
[2. 2. 2.]
\end{verbatim}

Birden Fazla Gradyan Deðiþkeni

Diyelim ki elimizde 

$$
g(w_1,w_2) = \tanh (w_1w_2)
$$

fonksiyonu var, bu üç boyutlu bir fonksiyon, ve optimizasyon amaçlý gradyan
gerekiyor, gradyanýn iki deðiþken üzerinden alýnmasý gerekli [7]. 

\begin{minted}[fontsize=\footnotesize]{python}
import autograd
from autograd import numpy as anp

def g(w_1,w_2):
    return anp.tanh(w_1*w_2)

from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
x = np.linspace(-4,4,20)
y = np.linspace(-4,4,20)
xx,yy = np.meshgrid(x,y)
zz = g(xx,yy)
fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_surface(xx, yy, zz, cmap=cm.coolwarm)
plt.savefig('func_40_autograd_03.png')
\end{minted}

\includegraphics[width=20em]{func_40_autograd_03.png}

$g$'nin her iki kýsmi türevini ve gradyanýný, 

$$
\nabla g(w_1,w_2) = \left[\begin{array}{r}
\frac{\partial }{\partial w_1} g(w_1,w_2) \\
\frac{\partial }{\partial w_2} g(w_1,w_2) 
\end{array}\right]
$$

\verb!autograd! ile hesaplamak için 

\begin{minted}[fontsize=\footnotesize]{python}
dgdw1 = autograd.grad(g,0)
dgdw2 = autograd.grad(g,1)
\end{minted}

Dikkat edersek, 0 ve 1 parametreleri geçildi, bunlar sýrasýyla $w_1$ ve
$w_2$ deðiþkenlerine tekabül ediyorlar (\verb!g! tanýmýndaki sýralarýna
göre, 0. ve 1. parametreler). Þimdi mesela (1.0,2.0) noktasýndaki gradyaný
hesaplayabiliriz,

\begin{minted}[fontsize=\footnotesize]{python}
gradg = [dgdw1(1.0,2.0), dgdw2(1.0,2.0)]
print (gradg)
\end{minted}

\begin{verbatim}
[0.14130164970632894, 0.07065082485316447]
\end{verbatim}


Esitsizlik Sýnýrlamalarý 

Optimizasyon problemimizde 

$$
\min_x f(x), 
\quad 
\textrm{oyle ki}, 
\quad 
c_i(x) \ge 0, 
\quad i=1,2,..,m
$$

$c_i$ ile gösterilen eþitsizlik içeren (üstte büyüklük türünden)
kýsýtlamalar olduðunu düþünelim. Bu problemi nasýl çözeriz?

Bir fikir, problemin eþitizliklerini bir gösterge (indicator)
fonksiyonu üzerinden, Lagrange yönteminde olduðu gibi, ana hedef
fonksiyonuna dahil etmek, ve elde edilen yeni hedefi kýsýtlanmamýþ bir
problem gibi çözmek. Yani üstteki yerine, alttaki problemi çözmek,

$$
\min_x f(x) + \sum_{i=1}^{m} I(c_i(x))
$$

ki $I$ pozitif reel fonksiyonlar için göstergeç fonksiyonu,

$$
I(u) = 
\left\{ \begin{array}{ll}
0 & u \le 0 \\
\infty & u > 0
\end{array} \right.
$$

Bu yaklaþýmýn nasýl iþleyeceðini kabaca tahmin edebiliriz. $I$ fonksiyonu
0'dan büyük deðerler için müthiþ büyük deðerler veriyor, bu sebeple
optimizasyon sýrasýnda o deðerlerden tabii ki kaçýnýlacak, ve arayýþ
istediðimiz noktalara doðru kayacak. Tabii $x_1 > 3$ gibi bir þart varsa
onu $x_1 - 3 > 0$ þartýna deðiþtiriyoruz ki üstteki göstergeci
kullanabilelim. Bu yaklaþýma "bariyer metotu" ismi veriliyor çünkü $I$ ile
bir bariyer yaratýlmýþ oluyor.

Fakat bir problem var, göstergeç fonksiyonunun türevini almak, ve pürüzsüz
rahat kullanýlabilen bir yeni fonksiyon elde etmek kolay deðil. Acaba $I$
yerine onu yaklaþýk temsil edebilen bir baþka sürekli fonksiyon kullanamaz
mýyýz?

Log fonksiyonunu kullanabiliriz. Altta farklý $\mu$ deðerleri için
$-\mu \log(-u)$ fonksiyonun deðerlerini görüyoruz. Fonksiyon görüldüðü gibi
$I$'ya oldukca yakýn.

\begin{minted}[fontsize=\footnotesize]{python}
def I(u): 
   if u<0: return 0.
   else: return 10.0

u = np.linspace(-3,1,100)
Is = np.array([I(x) for x in u])

import pandas as pd
df = pd.DataFrame(index=u)

df['I'] = Is
df['$\mu$=0.5'] = -0.5*np.log(-u)
df['$\mu$=1.0'] = -1.0*np.log(-u)
df['$\mu$=2.0'] = -2.0*np.log(-u)

df.plot()
plt.savefig('func_40_autograd_04.png')
\end{minted}

\includegraphics[width=25em]{func_40_autograd_04.png}

O zaman eldeki tüm $c_i(x) \ge 0$ kýsýtlamalarýný

$$
- \sum_{i=1}^{m} \log c_i(x)
$$

ile hedef fonksiyonuna dahil edebiliriz, yeni birleþik fonksiyon,

$$
P(x;\mu) = f(x) - \mu \sum_{i=1}^{m} \log c_i(x) 
$$

olur. Böylece elde edilen yaklaþým log-bariyer yaklaþýmý olacaktýr. 

Algoritma olarak optimizasyon þu þekilde gider;

1) Bir $x$ ve $\mu$ deðerinden baþla.

2) Newton metotu ile birkaç adým at (durma kriteri yaklaþýma göre deðisebilir)

3) $\mu$'yu küçült

4) Ana durma kriterine bak, tamamsa dur. Yoksa baþa dön

Bu yaklaþýmýn dýþbükey (convex) problemler için global minimuma
gittiði ispatlanmýþtýr [8, sf. 504].

Örnek

$\min (x_1 + 0.5)^2 + (x_2 - 0.5)^2$ problemini çöz, $x_1 \in [0,1]$ ve $x_2 \in
[0,1]$ kriterine göre.

Üstteki fonksiyon için log-bariyer,

$$
P(x;\mu) = (x_1 + 0.5)^2 + (x_2-0.5)^2 -
\mu 
\big[
\log x_1 + \log (1-x_1) + \log x_2 + \log (1-x_2)
\big]
$$

Bu formülasyonu nasýl elde ettiðimiz bariz herhalde, $x_1 \ge 0$ ve
$x_1 \le 1$ kýsýtlamalarý var mesela, ikinci ifadeyi büyüktür
iþaretine çevirmek için eksi ile çarptýk, $-x_1 \ge 1$, ya da $1-x_1
\ge 0$ böylece $\log(1-x_1)$ oldu.

Artýk Newton yöntemini kullanarak sanki elimizde bir kýsýtlanmasý
olmayan fonksiyon varmýþ gibi kodlama yapabiliriz, $P$'yi minimize
edebiliriz. Newton yönü $d$ için gereken Hessian ve Jacobian
matrislerini otomatik türevle hesaplayacaðýz, belli bir noktadan
baþlayacaðýz, ve her adýmda $d = -H(x)^{-1} \nabla f(x)$ yönünde adým
atacaðýz.

\begin{minted}[fontsize=\footnotesize]{python}
from autograd import numpy as anp, grad, hessian, jacobian
import numpy.linalg as lin

x = np.array([0.8,0.2])
mu = 2.0
for i in range(10):
    def P(x):
    	x1,x2=x[0],x[1]
    	return (x1+0.5)**2 + (x2-0.5)**2 - mu * \
	   (anp.log(x1) + anp.log(1-x1) + anp.log(x2)+anp.log(1-x2))

    h = hessian(P)
    j = jacobian(P)
    J = j(np.array(x))
    H = h(np.array(x))
    d = np.dot(-lin.inv(H), J)
    x = x + d
    print (i, x, np.round(mu,5))
    mu = mu*0.1
\end{minted}

\begin{verbatim}
0 [0.61678005 0.34693878] 2.0
1 [-0.00858974  0.486471  ] 0.2
2 [-0.02078755  0.49999853] 0.02
3 [-0.18014768  0.5       ] 0.002
4 [-0.49963245  0.5       ] 0.0002
5 [-0.50002667  0.5       ] 2e-05
6 [-0.50000267  0.5       ] 0.0
7 [-0.50000027  0.5       ] 0.0
8 [-0.50000003  0.5       ] 0.0
9 [-0.5  0.5] 0.0
\end{verbatim}

Görüldüðü gibi 5. adýmda optimal noktaya gelindi, o noktada $\mu$
oldukca küçük, ve bariyerle tanýmladýðýmýz yerlerden uzak duruldu,
optimal nokta $x_1=-0.5,x_2=0.5$ bulundu.

[devam edecek]

Kaynaklar 

[1] Bayramlý, Ders Notlarý, {\em Otomatik Türev Almak (Automatic Differentiation -AD-)}

[2] Schrimpf, \url{http://faculty.arts.ubc.ca/pschrimpf/526/526.html}

[3] Stoyanov, \url{https://nikstoyanov.me/post/2019-04-14-numerical-optimizations}

[5] Kitchin, \url{http://kitchingroup.cheme.cmu.edu/blog/2018/11/03/Constrained-optimization-with-Lagrange-multipliers-and-autograd/}

[6] Bayramli, Cok Boyutlu Calculus, {\em Vektör Calculus, Kurallar, Matris Türevleri}

[7] Watt, {\em Automatic Differentiation}, 
    \url{https://jermwatt.github.io/machine_learning_refined/notes/3_First_order_methods/3_5_Automatic.html}

[8] Nocedal, {\em Numerical Optimization}

\end{document}



