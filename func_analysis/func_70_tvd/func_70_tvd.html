<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Tam Varyasyon ile Gürültüyü Yoketmek (Total Variation Denoising)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1
id="tam-varyasyon-ile-gürültüyü-yoketmek-total-variation-denoising">Tam
Varyasyon ile Gürültüyü Yoketmek (Total Variation Denoising)</h1>
<p>Bir sinyalden, görüntüden gürültüyü silmek için optimizasyon
kullanılabilir. Orijinal sinyal <span
class="math inline">\(x\)</span>’in <span class="math inline">\(y = B x
+ n\)</span> ile bir <span class="math inline">\(n\)</span> gürültüsü
eklenerek bozulduğu (corrupted) farzedilebilir (<span
class="math inline">\(B\)</span> bir değişim matrisidir, tutarlı,
bilinen değişimleri temsil eder) biz eldeki <span
class="math inline">\(y\)</span> ile <span
class="math inline">\(x\)</span>’i kestirmeye uğraşırız. Fakat
literatürde iyi bilindiği üzere <span class="math inline">\(x\)</span>’i
<span class="math inline">\(y\)</span>’den tahmin etmeye uğraşmak kötü
konumlanmış (ill-posed) bir sorudur. Çözüm olabilecek pek çok <span
class="math inline">\(x\)</span> bulunabilir, bu sebeple arama alanını
bir şekilde daraltmak gerekir, ve bunun için bir tür düzenlileştirme /
regülarizasyon (regularization) kullanılması şarttır [3].</p>
<p>Bir sayısal resimden gürültü çıkartma alanında iyi bilinen bir yöntem
problemi çift hedefli bir halde konumlandırmak [4],</p>
<p><span class="math display">\[
|| x-x_{cor}||_2, \qquad \phi_{tv} (x) = \sum_{i=1}^{n-1} | x_{i+1} -
x_i |
\qquad (1)
\]</span></p>
<p>Burada <span class="math inline">\(x_{cor} \in \mathbb{R}^n\)</span>
bize verilen bozulmuş sinyal, <span class="math inline">\(x \in
\mathbb{R}^n\)</span> ise bulmak istediğimiz, gürültüsü çıkartılmış
sinyal, <span class="math inline">\(\phi_{tv}\)</span> ise tam varyasyon
fonksiyonu. Üstteki iki hedefi minimize etmek istiyoruz, böylece aynı
anda hem sinyalin kendi içindeki varyasyonu azaltan hem de bozulmuş
sinyale mümkün olduğunca yakın duran bir gerçek <span
class="math inline">\(x\)</span> elde edebilelim.</p>
<p>Her iki hedef fonksiyonunu birleştirip tek bir fonksiyon haline
getirip onu kısıtlanmamış (unconstrained) bir optimizasyon problemi
olarak çözebiliriz,</p>
<p><span class="math display">\[
\psi = || x-x_{cor}||_2^2 + \mu \phi_{tv}
\]</span></p>
<p>ki <span class="math inline">\(\mu\)</span> bizim seçeceğimiz bir
parametre olabilir. Çözüm için mesela Newton metodunu kullanabiliriz,
fakat tek bir problem var, Newton ve ona benzer diğer optimizasyon
metotları için türev almak gerekli, fakat <span
class="math inline">\(\phi_{tv}\)</span>’deki L1-norm’unun (tek boyutta
mutlak değer fonksiyonu) <span class="math inline">\(x=0\)</span>’da
türevi yoktur (birinci terimdeki Oklit normunun karesi alındığı için
onun iki kere türevi alınabilir). Bu durumda <span
class="math inline">\(\phi_{tv}\)</span>’yi yaklaşık olarak temsil
edebilirsek, onun da türevi alınır hale gelmesi sağlayabiliriz. Bu yeni
fonksiyona <span class="math inline">\(\phi_{atv}\)</span> diyelim,</p>
<p><span class="math display">\[
\phi_{atv} = \sum_{i=1}^{n-1}
\left( \sqrt{ \epsilon^2 + (x_{i+1})-x_i  } - \epsilon \right)
\]</span></p>
<p>ki <span class="math inline">\(\epsilon &gt; 0\)</span>
yaklaşıklamanın seviyesini ayarlıyor. Bu fonksiyonun iyi bir
yaklaşıklama olduğunu görmek zor değil, toplam içindeki kısmı deneyerek
görelim,</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-6</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="fl">50.0</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> norm_tv(x):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(np.diff(x)))</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> norm_atv(x):</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> np.<span class="bu">sum</span>(np.sqrt(eps <span class="op">+</span> np.power(np.diff(x),<span class="dv">2</span>)) <span class="op">-</span> eps)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>xcor <span class="op">=</span> np.random.randn(<span class="dv">1000</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (norm_tv(xcor))</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (norm_atv(xcor))</span></code></pre></div>
<pre><code>1103.2561038302395
1103.2571969067808</code></pre>
<p>Üstteki fonksiyonun iki kez türevi alınabilir. Şimdi analitik şekilde
devam etmeden önce pür sayısal açıdan bir çözüme bakalım. Üstteki
fonksiyonları direk kodlayarak ve sayısal türev üzerinden işleyebilen
bir kütüphane çağrısıyla hedefi minimize edelim, eldeki sinyal,</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;xcor.csv&#39;</span>,header<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>xcor <span class="op">=</span> np.reshape(np.array(df[<span class="dv">0</span>]), (<span class="dv">5000</span>,<span class="dv">1</span>))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="bu">len</span>(xcor)), xcor)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;func_60_tvd_01.png&#39;</span>)</span></code></pre></div>
<p><img src="func_60_tvd_01.png" /></p>
<p>Kütüphane çağrısı ile</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> np.zeros(<span class="bu">len</span>(xcor))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.optimize <span class="im">import</span> minimize, Bounds, SR1, BFGS</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> phi(x):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> np.<span class="bu">sum</span>(np.power(x<span class="op">-</span>xcor, <span class="dv">2</span>)) <span class="op">+</span> mu<span class="op">*</span>norm_atv(x)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>opts <span class="op">=</span> {<span class="st">&#39;maxiter&#39;</span>: <span class="dv">400</span>, <span class="st">&#39;verbose&#39;</span>: <span class="dv">2</span>}</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> minimize (fun<span class="op">=</span>phi,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>                x0<span class="op">=</span>x0,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>                options<span class="op">=</span>opts,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>                jac<span class="op">=</span><span class="st">&#39;2-point&#39;</span>,</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>                hess<span class="op">=</span>BFGS(),</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>                method<span class="op">=</span><span class="st">&#39;trust-constr&#39;</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5000</span>), res.x)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;func_60_tvd_02.png&#39;</span>)</span></code></pre></div>
<p><img src="func_60_tvd_02.png" /></p>
<p>Sonuç fena olmadı. Fakat üstteki yaklaşımın hesabı uzun sürecektir,
eğer eldeki problem hakkında bazı ek şeyler biliyorsak, bu bilgileri
dahil ederek elde edilen çözüm daha hızlı olabilir. Mesela analitik
olarak türevler Jacobian ve Hessian bulunabilir, Newton adımı elle
kodlanabilir, ayrıca problemdeki matrislerde muhtemel bir seyreklikten
(sparsity) faydalanılabilir.</p>
<p>Hedef fonksiyonu, <span class="math inline">\(\psi(x)\)</span>
diyelim, için birinci ve ikinci türev,</p>
<p><span class="math display">\[
\nabla \psi(x) = 2 (x-x_{cor}) + \mu \nabla \phi_{atv}(x), \qquad
\nabla^2 \psi(x) = 2 I + \mu \nabla^2 \phi_{atv} (x)
\]</span></p>
<p>Zincirleme Kuralı uygulandı tabii, ve şimdi <span
class="math inline">\(\phi_{atv}\)</span> üzerindeki türevleri bulmak
gerekiyor. Sorun değil, daha önceki yaklaşıklamayı bunun için yapmıştık
zaten. Yaklaşık fonksiyonu genel olarak belirtirsek,</p>
<p><span class="math display">\[
f(u) = \sqrt{\epsilon^2 + u^2} - \epsilon
\]</span></p>
<p>Bu fonksiyonun 1. ve 2. türevi</p>
<p><span class="math display">\[
f&#39;(u) = u(\epsilon^2 + u^{-1/2} ), \qquad
f&#39;&#39;(u) = \epsilon^2 (\epsilon^2 + u^2)^{-3/2}
\]</span></p>
<p>Şimdi bir <span class="math inline">\(F\)</span> tanımlayalım,</p>
<p><span class="math display">\[
F(u_1,..., u_{n-1}) = \sum_{i=1}^{n-1} f(u_i)
\]</span></p>
<p>Yani <span class="math inline">\(F(u)\)</span> <span
class="math inline">\(u\)</span>’nun bileşenlerinin yaklaşık L1
norm’unun toplamıdır. Nihai amacımız bu tanımdan bir <span
class="math inline">\(\phi_{atv}\)</span> ifadesine ulaşmak. <span
class="math inline">\(F\)</span>’in gradyanı ve Hessian’ı</p>
<p><span class="math display">\[
\nabla F(u) = \left[\begin{array}{ccc} f&#39;(u_1) &amp; \dots &amp;
f&#39;(u_{n-1}) \end{array}\right]
\]</span></p>
<p><span class="math display">\[
\nabla^2 F(u) =
\mathrm{diag}
\left[\begin{array}{ccc} f&#39;&#39;(u_1) &amp; \dots &amp;
f&#39;&#39;(u_{n-1}) \end{array}\right]
\]</span></p>
<p>Eğer bir ileri farklılık matrisi <span
class="math inline">\(D\)</span> tanımlarsak,</p>
<p><span class="math display">\[
D = \left[\begin{array}{ccccc}
-1  &amp; 1 &amp; &amp; &amp;    \\
&amp; -1 &amp; 1 &amp; &amp;   \\
&amp;  &amp; \ddots  &amp; \ddots &amp;  \\
&amp;  &amp;  &amp;  -1 &amp; 1
\end{array}\right]
\]</span></p>
<p>O zaman <span class="math inline">\(\phi_{atv}(x) = F(Dx)\)</span>
diyebiliriz. Bir <span class="math inline">\(x\)</span> vektörünü
üstteki matris ile soldan çarpınca öğeleri <span
class="math inline">\(\left[\begin{array}{ccc} x_2-x_1 &amp; x_3-x_2
&amp; \dots \end{array}\right]\)</span> şeklinde giden bir yeni vektör
elde edeceğimizi doğrulamak zor değil. Yine Zincirleme Kuralını
uygularsak,</p>
<p><span class="math display">\[
\nabla \phi_{atv}(x) = D^T \nabla F(Dx), \qquad
\nabla^2 \phi_{atv}(x) = D^T \nabla^2 F(Dx) D
\]</span></p>
<p>Hepsini bir araya koyarsak</p>
<p><span class="math display">\[
\nabla \psi(x) = 2(x-x_{cor}) + \mu D^T \nabla F(Dx)
\]</span></p>
<p><span class="math display">\[
\nabla^2 \psi(x) = 2 I  + \mu D^T \nabla^2 F(Dx) D
\]</span></p>
<p>Kodlamayı alttaki gibi yapabiliriz,</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sps</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse.linalg <span class="im">as</span> slin</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>MU <span class="op">=</span> <span class="fl">50.0</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>EPSILON <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>ALPHA <span class="op">=</span> <span class="fl">0.01</span><span class="op">;</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>BETA <span class="op">=</span> <span class="fl">0.5</span><span class="op">;</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>MAXITERS <span class="op">=</span> <span class="dv">100</span><span class="op">;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>NTTOL <span class="op">=</span> <span class="fl">1e-10</span><span class="op">;</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(xcor)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1</span><span class="op">*</span>np.ones(n), np.ones(n)])</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>diags <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> sps.spdiags(data, diags, n<span class="op">-</span><span class="dv">1</span>, n)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.zeros((<span class="bu">len</span>(xcor),<span class="dv">1</span>))</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(MAXITERS):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>   d <span class="op">=</span> D.dot(x)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>   val1 <span class="op">=</span> np.dot((x<span class="op">-</span>xcor).T,(x<span class="op">-</span>xcor))</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>   val2 <span class="op">=</span> np.sqrt(EPSILON<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> np.power(d,<span class="dv">2</span>))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>   val3 <span class="op">=</span> EPSILON<span class="op">*</span>np.ones((n<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>   val <span class="op">=</span> np.<span class="bu">float</span>(val1 <span class="op">+</span> MU<span class="op">*</span>np.<span class="bu">sum</span>(val2 <span class="op">-</span> val3))</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>   grad1 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>(x<span class="op">-</span>xcor)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>   grad2 <span class="op">=</span> MU<span class="op">*</span>D.T.dot(d <span class="op">/</span> np.sqrt(EPSILON<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> d<span class="op">**</span><span class="dv">2</span>))</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>   grad <span class="op">=</span> grad1 <span class="op">+</span> grad2</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>   hess1 <span class="op">=</span> <span class="dv">2</span><span class="op">*</span>sps.eye(n)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>   hess2 <span class="op">=</span> EPSILON<span class="op">**</span><span class="dv">2</span><span class="op">*</span>(EPSILON<span class="op">**</span><span class="dv">2</span><span class="op">+</span>d<span class="op">**</span><span class="dv">2</span>)<span class="op">**</span>(<span class="op">-</span><span class="dv">3</span><span class="op">/</span><span class="dv">2</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>   hess2 <span class="op">=</span> hess2.reshape((n<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>   hess3 <span class="op">=</span> sps.spdiags(hess2, <span class="dv">0</span>, n<span class="op">-</span><span class="dv">1</span>, n<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>   hess <span class="op">=</span> hess1 <span class="op">+</span> MU<span class="op">*</span>hess3.dot(D).T.dot(D)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>   v <span class="op">=</span> slin.spsolve(<span class="op">-</span>hess, grad)</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>   v <span class="op">=</span> np.reshape(v, (n,<span class="dv">1</span>))</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>   lambdasqr <span class="op">=</span> np.<span class="bu">float</span>(np.dot(<span class="op">-</span>grad.T,v))</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>   <span class="cf">if</span> lambdasqr<span class="op">/</span><span class="dv">2</span> <span class="op">&lt;</span> NTTOL: <span class="cf">break</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>   t <span class="op">=</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>   <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>      tmp1 <span class="op">=</span> np.<span class="bu">float</span>(np.dot((x<span class="op">+</span>t<span class="op">*</span>v<span class="op">-</span>xcor).T,(x<span class="op">+</span>t<span class="op">*</span>v<span class="op">-</span>xcor)))</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>      tmp2 <span class="op">=</span> MU<span class="op">*</span>np.<span class="bu">sum</span>(np.sqrt(EPSILON<span class="op">**</span><span class="dv">2</span><span class="op">+</span>(D<span class="op">*</span>(x<span class="op">+</span>t<span class="op">*</span>v))<span class="op">**</span><span class="dv">2</span>)<span class="op">-</span>EPSILON<span class="op">*</span>np.ones((n<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)))</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>      tmp3 <span class="op">=</span> val <span class="op">-</span> ALPHA<span class="op">*</span>t<span class="op">*</span>lambdasqr</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>      <span class="cf">if</span> tmp1 <span class="op">+</span> tmp2 <span class="op">&lt;</span> tmp3: <span class="cf">break</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>      t <span class="op">=</span> BETA<span class="op">*</span>t</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>   x <span class="op">=</span> x<span class="op">+</span>t<span class="op">*</span>v</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(n),xcor)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(n),x,<span class="st">&#39;r&#39;</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;func_60_tvd_03.png&#39;</span>)</span></code></pre></div>
<p><img src="func_60_tvd_03.png" /></p>
<p>Çok daha iyi bir gürültüsüz sonuç elde ettik, üstteki bu işlem çok
daha hızlı.</p>
<p>Görüntüden Gürültü Silmek</p>
<p>Aynen tek boyutlu sinyalden gürültü silebildiğimiz gibi iki boyutlu
görüntüden de gürültü silmek mümkün. Bu durumda tam varyasyon</p>
<p><span class="math display">\[
\sum_{i=2}^{m} \sum_{j=2}^{n} (|U_{i,j} - U_{i-1,j}| + |U_{i,j} -
U_{i,j-1}|)
\]</span></p>
<p>olabilir, yani her pikselin bir yanindaki ve bir altındaki pikselle
olan uzaklığının L1-norm’unu almak. Üstteki hesabı yapmak için aslında
yine daha önce hesapladığımız <span class="math inline">\(D\)</span>
matrisini kullanabiliriz. Bir <span class="math inline">\(X\)</span>
imajı üzerinde <span class="math inline">\(DX\)</span> hesabı, yani
<span class="math inline">\(D\)</span> ile soldan çarpım dikey
farklılıkları, sağdan çarpım <span class="math inline">\(XD\)</span> ise
yatay farklılıkları verecektir.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sps</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> [[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>],</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>],</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>],</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">7</span>, <span class="dv">8</span>]]</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array(X)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (X)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1</span><span class="op">*</span>np.ones(n), np.ones(n)])</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>diags <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> sps.lil_matrix(sps.spdiags(data, diags, n, n))</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (D.todense())</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;Dikey Farklilik&#39;</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (D.dot(X))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;Yatay Farklilik&#39;</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (D.transpose().dot(X.T))</span></code></pre></div>
<pre><code>[[1 2 3 4]
 [5 6 7 8]
 [1 2 3 4]
 [5 6 7 8]]
[[-1.  1.  0.  0.]
 [ 0. -1.  1.  0.]
 [ 0.  0. -1.  1.]
 [ 0.  0.  0. -1.]]
Dikey Farklilik
[[ 4.  4.  4.  4.]
 [-4. -4. -4. -4.]
 [ 4.  4.  4.  4.]
 [-5. -6. -7. -8.]]
Yatay Farklilik
[[-1. -5. -1. -5.]
 [-1. -1. -1. -1.]
 [-1. -1. -1. -1.]
 [-1. -1. -1. -1.]]</code></pre>
<p>L1 norm yaklaşıksallığı için daha önceki yöntemi kullanabiliriz.</p>
<p>Gradyan almak için ise bu sefer <code>tensorflow</code> paketini
kullanacağız [5]. Bir vektöre göre değil bir matrise göre türev
alıyoruz, bunu sembolik yapmak yerine sembolik yaklaşım kadar kuvvetli
olan otomatik türev ile gradyanı elde edebiliriz.</p>
<p>Üstteki tüm hesapları TF ile bir hesap grafiği içinde kodlayıp,
<code>tf.gradients</code> ile hedef fonksiyonunun gradyanını alacağız,
ve standart gradyan inişi optimizasyonu ile bir noktadan başlayıp
gradyan yönü tersinde adım atarak minimum noktaya varmaya
uğraşacağız.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> io</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>MU <span class="op">=</span> <span class="fl">50.0</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>EPSILON <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">225</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> io.imread(<span class="st">&#39;lena.jpg&#39;</span>, as_gray<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>io.imsave(<span class="st">&#39;lenad0.jpg&#39;</span>, img)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> io.imread(<span class="st">&#39;lena-noise.jpg&#39;</span>, as_gray<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>io.imsave(<span class="st">&#39;lenad1.jpg&#39;</span>, img)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>xorig <span class="op">=</span> tf.cast(tf.constant( io.imread(<span class="st">&#39;lena-noise.jpg&#39;</span>, as_gray<span class="op">=</span><span class="va">True</span>)),dtype<span class="op">=</span>tf.float32)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.placeholder(dtype<span class="op">=</span><span class="st">&quot;float&quot;</span>,shape<span class="op">=</span>[n,n],name<span class="op">=</span><span class="st">&quot;x&quot;</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.zeros((n,n))</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>idx1, idx2 <span class="op">=</span> [], []</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    idx1.append([i,i])</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">&lt;</span>n<span class="op">-</span><span class="dv">1</span>: idx2.append([i,i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> idx1 <span class="op">+</span> idx2</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>ones <span class="op">=</span> [<span class="fl">1.0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n)]</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>ones[n<span class="op">-</span><span class="dv">1</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>negs <span class="op">=</span> [<span class="op">-</span><span class="fl">1.0</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n<span class="op">-</span><span class="dv">1</span>)]</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>negs[n<span class="op">-</span><span class="dv">2</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> ones <span class="op">+</span> negs</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> np.array(vals).astype(np.float32)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> tf.SparseTensor(indices<span class="op">=</span>idx, values<span class="op">=</span>vals, dense_shape<span class="op">=</span>[n, n])</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>diff <span class="op">=</span> tf.square(tf.norm(xorig<span class="op">-</span>x, <span class="bu">ord</span><span class="op">=</span><span class="st">&#39;euclidean&#39;</span>))</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>Ux <span class="op">=</span> tf.sparse_tensor_dense_matmul(D, x)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>Uy <span class="op">=</span> tf.sparse_tensor_dense_matmul(tf.sparse_transpose(D), tf.transpose(x))</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>Uy <span class="op">=</span> tf.transpose(Uy)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>fUx <span class="op">=</span> tf.reduce_sum(tf.sqrt(EPSILON<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> tf.square(Ux)) <span class="op">-</span> EPSILON)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>fUy <span class="op">=</span> tf.reduce_sum(tf.sqrt(EPSILON<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> tf.square(Uy)) <span class="op">-</span> EPSILON)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>phi_atv <span class="op">=</span> fUx <span class="op">+</span> fUy</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>psi <span class="op">=</span> diff <span class="op">+</span> MU<span class="op">*</span>phi_atv</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> tf.gradients(psi, x)</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>g <span class="op">=</span> tf.reshape(g,[n<span class="op">*</span>n])</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> tf.global_variables_initializer()</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>sess <span class="op">=</span> tf.Session()</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>sess.run(init)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tv(xvec):</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>    xmat <span class="op">=</span> xvec.reshape(n,n)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> sess.run(psi, {x: xmat} )</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tv_grad(xvec):</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    xmat <span class="op">=</span> xvec.reshape(n,n)</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a>    gres <span class="op">=</span> sess.run(g, {x: xmat} )</span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> gres</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> np.zeros(n<span class="op">*</span>n)</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a>xcurr <span class="op">=</span> x0</span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">130</span></span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,N):</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>    gcurr <span class="op">=</span> tv_grad(xcurr)</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>    gcurr <span class="op">/=</span> gcurr.<span class="bu">max</span>()<span class="op">/</span><span class="fl">0.3</span></span>
<span id="cb8-68"><a href="#cb8-68" aria-hidden="true" tabindex="-1"></a>    chg <span class="op">=</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(xcurr))</span>
<span id="cb8-69"><a href="#cb8-69" aria-hidden="true" tabindex="-1"></a>    xcurr <span class="op">=</span> xcurr <span class="op">-</span> gcurr</span>
<span id="cb8-70"><a href="#cb8-70" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb8-71"><a href="#cb8-71" aria-hidden="true" tabindex="-1"></a>xcurr <span class="op">/=</span> xcurr.<span class="bu">max</span>()<span class="op">/</span><span class="fl">255.0</span></span>
<span id="cb8-72"><a href="#cb8-72" aria-hidden="true" tabindex="-1"></a>io.imsave(<span class="st">&#39;lenad2.jpg&#39;</span>, np.reshape(xcurr,(n,n)))</span></code></pre></div>
<p><img src="lenad0.jpg" /> <img src="lenad1.jpg" /> <img
src="lenad2.jpg" /></p>
<p>Yine total varyasyon kullanan ama farklı optimizasyon çözücüyle
hesabı yapan bir yöntem <code>tlv_prim_dual.py</code> kodunda [1], sonuç
(soldaki)</p>
<p><img src="lenad3.jpg" /> <img src="lenad4.jpg" /></p>
<p>Ayrıca <code>cvxpy</code> adlı bir paket üzerinden aynı şeyi
kodlayabiliriz, yani</p>
<p><span class="math display">\[
\min_{\beta \in \mathbb{R}^n}
\frac{1}{2} \sum_{i=1}^{n} (y_i - \beta_i)^2 +
\lambda \sum_{(i,j) \in E)}  |\beta_i - \beta_j|
\]</span></p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cvxpy</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>lam <span class="op">=</span> <span class="fl">35.0</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>u_corr <span class="op">=</span> plt.imread(<span class="st">&quot;lenad1.jpg&quot;</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>rows, cols <span class="op">=</span> u_corr.shape</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>U <span class="op">=</span> cvxpy.Variable(shape<span class="op">=</span>(rows, cols))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>obj <span class="op">=</span> cvxpy.Minimize(<span class="fl">0.5</span> <span class="op">*</span> cvxpy.sum_squares(u_corr<span class="op">-</span>U) <span class="op">+</span> lam<span class="op">*</span>cvxpy.tv(U))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> cvxpy.Problem(obj)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>prob.solve(verbose<span class="op">=</span><span class="va">True</span>, solver<span class="op">=</span>cvxpy.SCS)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>plt.imshow(U.value, cmap<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>plt.imsave(lena4.jpg<span class="st">&#39;, U.value, cmap=&#39;</span>gray<span class="st">&#39;)</span></span></code></pre></div>
<p>Üstteki sağdaki resim bu sonucu gösteriyor. Bu yaklaşımda
<code>cvxpy.tv</code> ile tam varyasyon hesabını yapan kütüphanenin
kendi iç çağrısını kullandık.</p>
<p>Kaynaklar</p>
<p>[1] Mordvintsev, <em>ROF and TV-L1 denoising with Primal-Dual
algorithm</em>, <a
href="https://github.com/znah/notebooks/blob/master/TV_denoise.ipynb">https://github.com/znah/notebooks/blob/master/TV_denoise.ipynb</a></p>
<p>[2] Chambolle, <em>An introduction to continuous optimization for
imaging</em>, <a
href="https://hal.archives-ouvertes.fr/hal-01346507/document">https://hal.archives-ouvertes.fr/hal-01346507/document</a></p>
<p>[3] Afonso, <em>Fast Image Recovery Using Variable Splitting and
Constrained Optimization</em>, <a
href="http://www.lx.it.pt/~mtf/Afonso_BioucasDias_Figueiredo_twocolumn_v7.pdf">http://www.lx.it.pt/~mtf/Afonso_BioucasDias_Figueiredo_twocolumn_v7.pdf</a></p>
<p>[4] Boyd, <em>Additional Exercises for Convex Optimization</em> <a
href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook_extra_exercises.pdf">https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook_extra_exercises.pdf</a></p>
<p>[5] Bayramlı, <em>Bilgisayar Bilim, Yapay Zeka, Tensorflow</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
