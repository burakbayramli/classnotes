\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Tam Varyasyon ile Gürültüyü Yoketmek (Total Variation Denoising)

Bir sinyalden, görüntüden gürültüyü silmek için optimizasyon
kullanýlabilir. Orijinal sinyal $x$'in $y = B x + n$ ile bir $n$ gürültüsü
eklenerek bozulduðu (corrupted) farzedilebilir ($B$ bir deðiþim matrisidir,
tutarlý, bilinen deðiþimleri temsil eder) biz eldeki $y$ ile $x$'i
kestirmeye uðraþýrýz.  Fakat literatürde iyi bilindiði üzere $x$'i $y$'den
tahmin etmeye uðraþmak kötü konumlanmýþ (ill-posed) bir sorudur. Çözüm
olabilecek pek çok $x$ bulunabilir, bu sebeple arama alanýný bir þekilde
daraltmak gerekir, ve bunun için bir tür düzenlileþtirme / regülarizasyon
(regularization) kullanýlmasý þarttýr [3].

Bir sayýsal resimden gürültü çýkartma alanýnda iyi bilinen bir yöntem
problemi çift hedefli bir halde konumlandýrmak [4],

$$
|| x-x_{cor}||_2, \qquad \phi_{tv} (x) = \sum_{i=1}^{n-1} | x_{i+1} - x_i | 
\mlabel{1}
$$

Burada $x_{cor} \in \mathbb{R}^n$ bize verilen bozulmuþ sinyal,
$x \in \mathbb{R}^n$ ise bulmak istediðimiz, gürültüsü çýkartýlmýþ sinyal,
$\phi_{tv}$ ise tam varyasyon fonksiyonu. Üstteki iki hedefi minimize etmek
istiyoruz, böylece ayný anda hem sinyalin kendi içindeki varyasyonu azaltan
hem de bozulmuþ sinyale mümkün olduðunca yakýn duran bir gerçek $x$ elde
edebilelim.

Her iki hedef fonksiyonunu birleþtirip tek bir fonksiyon haline getirip onu
kýsýtlanmamýþ (unconstrained) bir optimizasyon problemi olarak çözebiliriz,

$$
\psi = || x-x_{cor}||_2^2 + \mu \phi_{tv} 
$$

ki $\mu$ bizim seçeceðimiz bir parametre olabilir. Çözüm için mesela Newton
metodunu kullanabiliriz, fakat tek bir problem var, Newton ve ona benzer
diðer optimizasyon metotlarý için türev almak gerekli, fakat
$\phi_{tv}$'deki L1-norm'unun (tek boyutta mutlak deðer fonksiyonu)
$x=0$'da türevi yoktur (birinci terimdeki Oklit normunun karesi alýndýðý
için onun iki kere türevi alýnabilir). Bu durumda $\phi_{tv}$'yi yaklaþýk
olarak temsil edebilirsek, onun da türevi alýnýr hale gelmesi
saðlayabiliriz. Bu yeni fonksiyona $\phi_{atv}$ diyelim,

$$
\phi_{atv} = \sum _{i=1}^{n-1} 
\left( \sqrt{ \epsilon^2 + (x_{i+1})-x_i  } - \epsilon \right)
$$

ki $\epsilon > 0$ yaklaþýklamanýn seviyesini ayarlýyor. Bu fonksiyonun iyi
bir yaklaþýklama olduðunu görmek zor deðil, toplam içindeki kýsmý deneyerek
görelim,

\begin{minted}[fontsize=\footnotesize]{python}
import numpy as np

eps = 1e-6
mu = 50.0

def norm_tv(x):
   return np.sum(np.abs(np.diff(x)))
   
def norm_atv(x):
   return np.sum(np.sqrt(eps + np.power(np.diff(x),2)) - eps)
   
xcor = np.random.randn(1000)

print (norm_tv(xcor))
print (norm_atv(xcor))
\end{minted}

\begin{verbatim}
1103.2561038302395
1103.2571969067808
\end{verbatim}

Üstteki fonksiyonun iki kez türevi alýnabilir. Þimdi analitik þekilde devam
etmeden önce pür sayýsal açýdan bir çözüme bakalým. Üstteki fonksiyonlarý
direk kodlayarak ve sayýsal türev üzerinden iþleyebilen bir kütüphane
çaðrýsýyla hedefi minimize edelim, eldeki sinyal,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
df = pd.read_csv('xcor.csv',header=None)
xcor = np.reshape(np.array(df[0]), (5000,1))
plt.plot(range(len(xcor)), xcor)
plt.savefig('func_60_tvd_01.png')
\end{minted}

\includegraphics[width=25em]{func_60_tvd_01.png}

Kütüphane çaðrýsý ile

\begin{minted}[fontsize=\footnotesize]{python}
x0 = np.zeros(len(xcor))

from scipy.optimize import minimize, Bounds, SR1, BFGS

def phi(x):
   return np.sum(np.power(x-xcor, 2)) + mu*norm_atv(x)

opts = {'maxiter': 400, 'verbose': 2}

res = minimize (fun=phi,
                x0=x0,
                options=opts,
                jac='2-point',
                hess=BFGS(),
                method='trust-constr'
                )

plt.plot(range(5000), res.x)
plt.savefig('func_60_tvd_02.png')
\end{minted}

\includegraphics[width=25em]{func_60_tvd_02.png}

Sonuç fena olmadý. Fakat üstteki yaklaþýmýn hesabý uzun sürecektir, eðer
eldeki problem hakkýnda bazý ek þeyler biliyorsak, bu bilgileri dahil
ederek elde edilen çözüm daha hýzlý olabilir. Mesela analitik olarak
türevler Jacobian ve Hessian bulunabilir, Newton adýmý elle kodlanabilir,
ayrýca problemdeki matrislerde muhtemel bir seyreklikten (sparsity)
faydalanýlabilir.

Hedef fonksiyonu, $\psi(x)$ diyelim, için birinci ve ikinci türev,

$$
\nabla \psi(x) = 2 (x-x_{cor}) + \mu \nabla \phi_{atv}(x), \qquad
\nabla^2 \psi(x) = 2 I + \mu \nabla^2 \phi_{atv} (x)
$$

Zincirleme Kuralý uygulandý tabii, ve þimdi $\phi_{atv}$ üzerindeki
türevleri bulmak gerekiyor. Sorun deðil, daha önceki yaklaþýklamayý bunun
için yapmýþtýk zaten. Yaklaþýk fonksiyonu genel olarak belirtirsek, 

$$
f(u) = \sqrt{\epsilon^2 + u^2} - \epsilon
$$

Bu fonksiyonun 1. ve 2. türevi

$$
f'(u) = u(\epsilon^2 + u^{-1/2} ), \qquad
f''(u) = \epsilon^2 (\epsilon^2 + u^2)^{-3/2}
$$

Þimdi bir $F$ tanýmlayalým,

$$
F(u_1,..., u_{n-1}) = \sum_{i=1}^{n-1} f(u_i)
$$

Yani $F(u)$ $u$'nun bileþenlerinin yaklaþýk L1 norm'unun toplamýdýr. Nihai
amacýmýz bu tanýmdan bir $\phi_{atv}$ ifadesine ulaþmak. $F$'in gradyaný ve
Hessian'ý

$$
\nabla F(u) = \left[\begin{array}{ccc} f'(u_1) & \dots & f'(u_{n-1}) \end{array}\right]
$$

$$
\nabla^2 F(u) = 
\diag 
\left[\begin{array}{ccc} f''(u_1) & \dots & f''(u_{n-1}) \end{array}\right] 
$$

Eðer bir ileri farklýlýk matrisi $D$ tanýmlarsak, 

$$
D = \left[\begin{array}{ccccc}
-1  & 1 & & &    \\
 & -1 & 1 & &   \\
 &  & \ddots  & \ddots &  \\
 &  &  &  -1 & 1
\end{array}\right]
$$

O zaman $\phi_{atv}(x) = F(Dx)$ diyebiliriz. Bir $x$ vektörünü
üstteki matris ile soldan çarpýnca öðeleri 
$\left[\begin{array}{ccc} x_2-x_1 & x_3-x_2 & \dots \end{array}\right]$ 
þeklinde giden bir yeni vektör elde edeceðimizi doðrulamak zor deðil. Yine
Zincirleme Kuralýný uygularsak,

$$
\nabla \phi_{atv}(x) = D^T \nabla F(Dx), \qquad
\nabla^2 \phi_{atv}(x) = D^T \nabla^2 F(Dx) D
$$

Hepsini bir araya koyarsak 

$$
\nabla \psi(x) = 2(x-x_{cor}) + \mu D^T \nabla F(Dx)
$$

$$
\nabla^2 \psi(x) = 2 I  + \mu D^T \nabla^2 F(Dx) D
$$

Kodlamayý alttaki gibi yapabiliriz,

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
import scipy.sparse as sps
import scipy.sparse.linalg as slin

MU = 50.0
EPSILON = 0.001

ALPHA = 0.01;
BETA = 0.5;
MAXITERS = 100;
NTTOL = 1e-10;

n = len(xcor)
data = np.array([-1*np.ones(n), np.ones(n)])
diags = np.array([0, 1])
D = sps.spdiags(data, diags, n-1, n)

x = np.zeros((len(xcor),1))

for iter in range(MAXITERS):
   d = D.dot(x)
   val1 = np.dot((x-xcor).T,(x-xcor))
   val2 = np.sqrt(EPSILON**2 + np.power(d,2))
   val3 = EPSILON*np.ones((n-1,1))
   val = np.float(val1 + MU*np.sum(val2 - val3))
   grad1 = 2*(x-xcor)
   grad2 = MU*D.T.dot(d / np.sqrt(EPSILON**2 + d**2))
   grad = grad1 + grad2
   hess1 = 2*sps.eye(n)
   hess2 = EPSILON**2*(EPSILON**2+d**2)**(-3/2)
   hess2 = hess2.reshape((n-1))
   hess3 = sps.spdiags(hess2, 0, n-1, n-1)

   hess = hess1 + MU*hess3.dot(D).T.dot(D)
   v = slin.spsolve(-hess, grad)
   v = np.reshape(v, (n,1))
   lambdasqr = np.float(np.dot(-grad.T,v))
   if lambdasqr/2 < NTTOL: break
   t = 1;
   while True:
      tmp1 = np.float(np.dot((x+t*v-xcor).T,(x+t*v-xcor)))
      tmp2 = MU*np.sum(np.sqrt(EPSILON**2+(D*(x+t*v))**2)-EPSILON*np.ones((n-1,1)))
      tmp3 = val - ALPHA*t*lambdasqr
      if tmp1 + tmp2 < tmp3: break
      t = BETA*t

   x = x+t*v

plt.plot(range(n),xcor)
plt.plot(range(n),x,'r')
plt.savefig('func_60_tvd_03.png')
\end{minted}

\includegraphics[width=25em]{func_60_tvd_03.png}

Çok daha iyi bir gürültüsüz sonuç elde ettik, üstteki bu iþlem çok daha
hýzlý. 

Görüntüden Gürültü Silmek

Aynen tek boyutlu sinyalden gürültü silebildiðimiz gibi iki boyutlu
görüntüden de gürültü silmek mümkün. Bu durumda tam varyasyon 

$$
\sum_{i=2}^{m} \sum_{j=2}^{n} (|U_{i,j} - U_{i-1,j}| + |U_{i,j} - U_{i,j-1}|)
$$

olabilir, yani her pikselin bir yanindaki ve bir altýndaki pikselle olan
uzaklýðýnýn L1-norm'unu almak. Üstteki hesabý yapmak için aslýnda yine daha
önce hesapladýðýmýz $D$ matrisini kullanabiliriz. Bir $X$ imajý üzerinde
$DX$ hesabý, yani $D$ ile soldan çarpým dikey farklýlýklarý, saðdan çarpým
$XD$ ise yatay farklýlýklarý verecektir.

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.sparse as sps

X = [[1, 2, 3, 4],
     [5, 6, 7, 8],
     [1, 2, 3, 4],
     [5, 6, 7, 8]]

X = np.array(X)
print (X)
n = X.shape[0]
data = np.array([-1*np.ones(n), np.ones(n)])
diags = np.array([0, 1])
D = sps.lil_matrix(sps.spdiags(data, diags, n, n))
print (D.todense())
print ('Dikey Farklilik')
print (D.dot(X))
print ('Yatay Farklilik')
print (D.transpose().dot(X.T))
\end{minted}

\begin{verbatim}
[[1 2 3 4]
 [5 6 7 8]
 [1 2 3 4]
 [5 6 7 8]]
[[-1.  1.  0.  0.]
 [ 0. -1.  1.  0.]
 [ 0.  0. -1.  1.]
 [ 0.  0.  0. -1.]]
Dikey Farklilik
[[ 4.  4.  4.  4.]
 [-4. -4. -4. -4.]
 [ 4.  4.  4.  4.]
 [-5. -6. -7. -8.]]
Yatay Farklilik
[[-1. -5. -1. -5.]
 [-1. -1. -1. -1.]
 [-1. -1. -1. -1.]
 [-1. -1. -1. -1.]]
\end{verbatim}

L1 norm yaklaþýksallýðý için daha önceki yöntemi kullanabiliriz.  

Gradyan almak için ise bu sefer \verb!tensorflow! paketini kullanacaðýz
[5]. Bir vektöre göre deðil bir matrise göre türev alýyoruz, bunu sembolik
yapmak yerine sembolik yaklaþým kadar kuvvetli olan otomatik türev ile
gradyaný elde edebiliriz.

Üstteki tüm hesaplarý TF ile bir hesap grafiði içinde kodlayýp,
\verb!tf.gradients! ile hedef fonksiyonunun gradyanýný alacaðýz, ve
standart gradyan iniþi optimizasyonu ile bir noktadan baþlayýp gradyan yönü
tersinde adým atarak minimum noktaya varmaya uðraþacaðýz. 

\begin{minted}[fontsize=\footnotesize]{python}
from skimage import io
import tensorflow as tf

MU = 50.0
EPSILON = 0.001
n = 225

img = io.imread('lena.jpg', as_gray=True)
io.imsave('lenad0.jpg', img)
img = io.imread('lena-noise.jpg', as_gray=True)
io.imsave('lenad1.jpg', img)
xorig = tf.cast(tf.constant( io.imread('lena-noise.jpg', as_gray=True)),dtype=tf.float32)
x = tf.placeholder(dtype="float",shape=[n,n],name="x")

D = np.zeros((n,n))
idx1, idx2 = [], []
for i in range(n):
    idx1.append([i,i])
    if i<n-1: idx2.append([i,i+1])
idx = idx1 + idx2
ones = [1.0 for i in range(n)]
ones[n-1] = 0
negs = [-1.0 for i in range(n-1)]
negs[n-2] = 0
vals = ones + negs
vals = np.array(vals).astype(np.float32)
D = tf.SparseTensor(indices=idx, values=vals, dense_shape=[n, n])


diff = tf.square(tf.norm(xorig-x, ord='euclidean'))

Ux = tf.sparse_tensor_dense_matmul(D, x)
Uy = tf.sparse_tensor_dense_matmul(tf.sparse_transpose(D), tf.transpose(x))
Uy = tf.transpose(Uy)

fUx = tf.reduce_sum(tf.sqrt(EPSILON**2 + tf.square(Ux)) - EPSILON)
fUy = tf.reduce_sum(tf.sqrt(EPSILON**2 + tf.square(Uy)) - EPSILON)

phi_atv = fUx + fUy

psi = diff + MU*phi_atv
g = tf.gradients(psi, x)
g = tf.reshape(g,[n*n])

init = tf.global_variables_initializer()

sess = tf.Session()

sess.run(init)

def tv(xvec):
    xmat = xvec.reshape(n,n)
    p = sess.run(psi, {x: xmat} )
    return p

def tv_grad(xvec):
    xmat = xvec.reshape(n,n)
    gres = sess.run(g, {x: xmat} )
    return gres
    
x0 = np.zeros(n*n)
xcurr = x0

N = 130
for i in range(1,N):
    gcurr = tv_grad(xcurr)
    gcurr /= gcurr.max()/0.3
    chg = np.sum(np.abs(xcurr))
    xcurr = xcurr - gcurr
 
xcurr /= xcurr.max()/255.0
io.imsave('lenad2.jpg', np.reshape(xcurr,(n,n)))
\end{minted}


\includegraphics[width=10em]{lenad0.jpg}
\includegraphics[width=10em]{lenad1.jpg}
\includegraphics[width=10em]{lenad2.jpg}

Yine total varyasyon kullanan ama farklý optimizasyon çözücüyle hesabý
yapan bir yöntem \verb!tlv_prim_dual.py! kodunda [1], sonuç (soldaki)

\includegraphics[width=10em]{lenad3.jpg}
\includegraphics[width=10em]{lenad4.jpg}

Ayrýca \verb!cvxpy! adlý bir paket üzerinden ayný þeyi kodlayabiliriz, yani 

$$
\min_{\beta \in \mathbb{R}^n} 
\frac{1}{2} \sum _{i=1}^{n} (y_i - \beta_i)^2 + 
\lambda \sum _{(i,j) \in E)}  |\beta_i - \beta_j|
$$

\begin{minted}[fontsize=\footnotesize]{python}
import cvxpy

lam = 35.0

u_corr = plt.imread("lenad1.jpg")

rows, cols = u_corr.shape

U = cvxpy.Variable(shape=(rows, cols))

obj = cvxpy.Minimize(0.5 * cvxpy.sum_squares(u_corr-U) + lam*cvxpy.tv(U))

prob = cvxpy.Problem(obj)

prob.solve(verbose=True, solver=cvxpy.SCS)

plt.imshow(U.value, cmap='gray')
plt.imsave(lena4.jpg', U.value, cmap='gray')
\end{minted}

Üstteki saðdaki resim bu sonucu gösteriyor. Bu yaklaþýmda
\verb!cvxpy.tv! ile tam varyasyon hesabýný yapan kütüphanenin kendi iç
çaðrýsýný kullandýk. 


Kaynaklar

[1] Mordvintsev, {\em ROF and TV-L1 denoising with Primal-Dual algorithm},
    \url{https://github.com/znah/notebooks/blob/master/TV_denoise.ipynb}

[2] Chambolle, {\em An introduction to continuous optimization for imaging},
    \url{https://hal.archives-ouvertes.fr/hal-01346507/document}

[3] Afonso, {\em Fast Image Recovery Using Variable Splitting and Constrained Optimization},
    \url{http://www.lx.it.pt/~mtf/Afonso_BioucasDias_Figueiredo_twocolumn_v7.pdf}

[4] Boyd, 
    {\em Additional Exercises for Convex Optimization}
    \url{https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook_extra_exercises.pdf}

[5] Bayramli, 
    {\em Bilgisayar Bilim, Yapay Zeka, Tensorflow}

\end{document}




