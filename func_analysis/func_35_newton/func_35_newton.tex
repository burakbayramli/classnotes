\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Newton'un Metodu (Newton's Method)

Newton birazdan bahsedeceðimiz yöntemi tek boyutlu problemler için kullandý
[2]. Rhapson adlý bilimci yöntemi çok boyutlu problemler için
geniþletti. Biz bu yönteme optimizasyon çerçevesinde bakacaðýz.  Konunun
tarihinden biraz bahsetmek istiyorum, bu dersi öðretmeye baþladýðýmda 1986
senesiydi, Newton'un metodunu nasýl gördüðümüz o zamandan beri deðiþime
uðradý. NM o zamanlar son baþvurulan metot diye öðretiliyordu, çünkü metodu
kullanmak için ``büyük'' bir denklem sistemi çözmek gerekiyordu, 500 x 500
bir sistem mesela. Bugüne gelelim artýk NM ilk baþvurulan metot haline
geldi, 50,000 x 50,000 boyutlarýnda bir sistem çözmek ``yetiyor'' ve böyle
bir sistem artýk idare edilebilen bir boyut haline geldi. Yani hesapsal
kapasite NM'in optimizasyon alanýnda oynadýðý rolü tamamen deðiþtirdi.

Diðer bir faktör ileride öðreneceðimiz iç nokta (interior-point)
metotlarýnýn Newton'un metodunu kullanýyor olmalarý. Ýç nokta metotlarý
icbukey optimizayonda çok popüler, onlar için NM gerekiyor, bu da NM'in
popülaritesini arttýrýyor.

NM nedir? Elimde bir kýsýtlanmamýþ (unconstrained) problemim var diyelim,

$$
\min f(x), \quad \textrm{ öyle ki } \quad x \in X = \mathbb{R}^n
$$

Bir Taylor açýlýmý yapabiliriz,

$$
f(x) \approx 
f(\bar{x}) + 
\nabla f(\bar{x})^T (x-\bar{x}) + 
\frac{1}{2} (x-\bar{x})^T F (x-\bar{x}) 
$$

ki $F$ Hessian matrisi. Üstteki formüle $h(x)$ diyelim. Böylece bir karesel
model ortaya çýkartmýþ oldum, formülün sað tarafýndaki çarpým onu karesel
yapýyor, ve þimdi onu kesin olarak çözmek istiyorum. Bunu nasýl yaparým?
Formülün gradyanýný sýfýra eþitleyebilirim. Üstteki fonksiyonun $x$'teki
gradyaný nedir? 

Gradyaný $x$'e göre aldýðýmýzý unutmayalým, $h(x)$'in ikinci terimi
$\nabla f(\bar{x})^T$ bir sabit sayý, ikinci gradyan alýnýrken sýfýrlanýr,
ve tüm ikinci terim sýfýrlanýr. Üçüncü terimin gradyanýný almak bir nevi
$\frac{\partial (x^TAx)}{\partial x} $ almak gibi [1], $F$ belli bir
noktadaki ikinci türev matrisi olduðu için $A$ gibi bir sabit matris kabul
edilebilir, $A$ simetrik olunca gradyan $2Ax$ sonucunu veriyordu, $F$
simetrik, o zaman üçüncü terimde $F$ kalýr, 2 ve $1/2$ birbirini iptal
eder, sonuç

$$
\nabla h(\bar{x}) = \nabla f(\bar{x}) + F(\bar{x})(x-\bar{x}) 
$$

Ýki üstteki karesel yaklaþýksal ifadenin gradyaný bu iþte. Onu sýfýra
eþitleriz ve çözeriz. $F$ tersi alinabilir bir matristir, o zaman 

$$
\nabla h(\bar{x}) = \nabla f(\bar{x}) + F(\bar{x})(x-\bar{x}) = 0
$$


$$
(x-\bar{x}) = -F^{-1} \nabla f(\bar{x})
$$
 
Üstteki ifadeye $d$ diyebilirim, ve bu $d$ benim Newton yönüm olarak
görülebilir, yön derken optimizasyon baðlamýnda minimuma giden yön. 

Bu bizi gayet basit 4 adýmlýk bir algoritmaya taþýyor,

0) $x^0$ verildi, bu baþlangýç noktasý, $k = 0$ yap.

1) $d^k = -F(x^k)^{-1} \nabla f(x^k)$. Eðer $d^k=0$ ise dur.

2) $\alpha^k = 1$ adým boyu seç

3) $x^{k+1} = x^k + \alpha^k d^k$, $k = k + 1$ yap, ve 1. adýma geri dön.

Bu metodun önemli bir özelliðinin her adýmda sadece bir lineer sistemi
çözmek olduðunu görüyoruz (tersini alma iþlemi). Bir lineer sistemi çözmek
kolay mýdýr? Sisteme göre deðiþir, 100 x 100 sistem, problem yok. 10,000 x
10,000 yoðun bir sistem var ise (seyrek matrisle temsil edilen lineer
sisteme nazaran) iþimiz daha zor olacaktýr. Bu tür sistemlerde Gaussian
eliminasyon iþlemeyebilir, bir tür özyineli metot gerekli. Demek istediðim
Newton yönteminin darboðazý bir lineer denklem sistemini her seferinde
sýfýrdan baþlayarak çözmek, ve bunu her döngüde yapmak.

Fakat bu çözümün bize pek çok þey kazandýrdýðýný da görmek lazým;
bahsedilen sistemi çözmek bize pek çok bilgi kazandýrýyor çünkü çözülen
problem içinde 1. ve 2. türev bilgisi var. Bu bilgi minimizasyon açýsýndan
daha akýllýca adým atýlabilmesini saðlýyor. 

Metot Hessian'ýn her adýmda tersi alýnabilir olduðunu farzediyor, bu her
zaman doðru olmayabilir. O sebeple bunun doðru olduðu türden problemler ile
uðraþacaðýz, ya da Hessian'ýn tersi alýnabilir olmasýný saðlayan
mekanizmalarý göreceðiz. $F$'nin özünü bozmadan deðiþtirerek tersi
alýnabilir olmasýný saðlayan yöntemler var. 

Ayrýca hedef her adýmda fonksiyonunu oluþturduðumda bu fonksiyonun azalma
garantisi yok. Öyle ya akýllý bir algoritmanin her adýmda hedef
fonksiyonumu daha iyiye götürdüðümü düþünebilirdim, ama þu anda kadar
gördüklerimiz ýþýðýnda, bunun garantisi yok. Bu konuya sonra deðineceðiz. 

Bir diðer nokta 2. adýmýn çizgi arama ile geniþletilebilmesi [bu konuya
altta baska kaynaklardan deginiyoruz]

NY'nin en çekici tarafý, eðer yakýnsama (convergence) mümkün ise bu
yakýnsamanýn çok hýzlý bir þekilde olmasý, ki bu iyi. Bu konuya gelmeden
metodun bazý ek özelliklerini görelim.

Terminoloji: bir matrise SPD denir eger matris simetrik, pozitif kesin ise
(simetric positive-definite). 

Teklif (Proposition) 1: 

Eðer $F(x)$ SPD ise $d \ne 0$, o zaman $d$ $\bar{x}$ noktasýnda bir iniþ
yönüne iþaret eder. Ýniþ yönü olmasý demek, eðer makul ufak bir adým
çerçevesinde gidilen noktada $f$'in deðerinin o an olduðumuz noktadan daha
az olmasý demektir.

Nasýl ispatlarým? Önceki dersten hatýrlarsak, eðer yönüm gradyan ile
negatif iç çarpýma sahip ise, o zaman yönüm kesinlikle bir iniþ yönüydü.

Teori 

Diyelim ki $f(x)$ fonksiyonu $\bar{x}$ noktasýnda türevi alýnabilir halde
[2, sf. 9]. Eðer elimizde $\nabla f(\bar{x})^T d < 0$ sonucunu veren bir
$d$ vektörü var ise, öyle ki her yeterince küçük $\lambda > 0$ için
$f(\bar{x}+\lambda d) < f(\bar{x})$ olacak þekilde, o zaman $d$ bir iniþ
yönüdür.

Ýspat 

Taylor açýlýmý ile yönsel türev tanýmýna bakarsak,

$$
f(\bar{x} + \lambda d ) = 
f(\bar{x}) + \lambda \nabla f(\bar{x})^T d + 
\lambda ||d|| \alpha(\bar{x},\lambda d)
$$

öyle ki $\alpha(\bar{x},\lambda d) \to 0$, $\lambda \to 0$ olurken. Not:
Norm içeren üçüncü terimdeki $\lambda ||d|| \alpha(\bar{x},\lambda d)$
ifadesi Taylor serisinin artýklý tanýmýndan geliyor. Detaylar için [3,
sf. 360]'a bakýlabilir.

Üstteki ifadeyi tekrar düzenlersek,

$$
\frac{f(\bar{x} + \lambda d) - f(\bar{x})}{\lambda} = 
\nabla f(\bar{x})^T + ||d||\alpha(\bar{x},\lambda d)
$$

$\nabla f(\bar{x})^T d < 0$ olduðuna göre (aradýðýmýz þart bu) o zaman, ve
$\alpha(\bar{x},\lambda d) \to 0$, $\lambda \to 0$ iken, her yeterince
küçük $\lambda > 0$ için $f(\bar{x} + \lambda d)-f(\bar{x}) < 0$ olmalýdýr,
yani her hangi bir yönde atýlan adým bir önceki $f$ deðerinden bizi daha
ufak bir $f$ deðerine götürmelidir. 

Ana Teklif'e dönelim. Newton adýmýnýdaki SPD $F$ için $0 < d^T \nabla f$
olduðunu göstermemiz lazým (ki böylece iniþ yönü olduðunu ispatlayabilelim,
bir önceki teori),

$$
d = -F^{-1} \nabla f(\bar{x})
$$

demiþtik, her iki tarafý $\nabla f(x)$ ile çarpalým,

$$
d \nabla f(x) = -\nabla f(x) F^{-1} \nabla f(x) 
$$

Eþitliðin sað tarafýndaki ifade hangi þartlarda eksi olur? Eðer $F$ matrisi
pozitif kesin ise deðil mi? Genel matrislerden hatýrlarsak, matris $A$ ve
bir vektör için $v$ eðer $A$ pozitif kesin ise $v^TAv > 0$. Daha önce
$F$'nin pozitif kesin olduðunu söylemiþtik, o zaman bir þekilde eðer $F$
pozitif kesin olmasýnýn sadece ve sadece $F$'nin tersinin pozitif kesin
olmasýna baðlý olduðuna gösterebilirsem amacýma ulaþabilirim.

Bunu yapmak aslýnda pek zor deðil. Biliyorum ki $F(x)$ SPD. Simdi herhangi
bir vektor $v$ icin 

$$
0 < v^T F(x)^{-1}v
$$

ifadeyi þöyle geniþletelim, $F(x)F(x)^{-1}$ eklemek hiçbir þeyi deðiþtirmez
çünkü bu çarpým birim matristir, 

$$
v^T F(x)^{-1}v = v^T F(x)^{-1} F(x)F(x)^{-1} v > 0
$$

Geniþlemiþ ifadenin harfiyen pozitif olduðunu biliyorum, iki üstteki
tanýmdan. Ama þimdi üstteki ifadeye farklý bir þekilde bakarsak,

$$
v^T F(x)^{-1}v = \underbrace{v^TF(x)^{-1}} F(x) \underbrace{F(x)^{-1} v} > 0
$$

Ýþaretlenen bölümlerin birer vektör olduðunu görebiliriz, bu durumda
$v^TAv > 0$ pozitif kesinlik formülü farklý bir $v$ için hala geçerlidir, o
zaman ortadaki $A$, bu durumda $F(x)$ pozitif kesin olmalýdýr. 

Örnek 1

$f(x) = 7x - \ln(x)$ olsun. O zaman $\nabla f(x) = 7 - \frac{1}{x}$ ve
$F(x) = f''(x) = \frac{1}{x^2}$. Bu fonksiyonun özgün global minimumunun
$x^* = 1/7 = 1.428..$ olduðunu kontrol etmek zor deðil. $x$ noktasýndaki
Newton yönü

$$
d = -F(x)^{-1} \nabla f(x) = -\frac{f'(x)}{f''(x)} = 
-x^2 \left( 7 - \frac{1}{x}  \right)=
x - 7x^2
$$

Newton yöntemi $\{ x^k \}$ serisini üretecek, öyle ki

$$
x^{k+1} = x^k + ( x^k - 7(x^k)^2  ) = 2x^k - 7(x^k)^2
$$

Altta farklý baþlangýç noktalarýna göre üretilen serileri
görüyoruz. Yakýnsamanýn hangi deðere doðru olduðu bariz, ve global minimum
da o deðer zaten. 

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
pd.set_option('display.notebook_repr_html', False)
pd.set_option('display.max_columns', 20)
pd.set_option('display.max_rows', 30) 
pd.set_option('display.width', 82) 
pd.set_option('precision', 6)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
df = pd.DataFrame(index=np.arange(11))

def calculate_newton_ex1(x):
    arr = []
    for i in range(11):
        arr.append(x)
        x = 2*x - 7*x**2
        if (x > 1e100):  x = np.inf
        if (x < -1e100):  x = -np.inf
    return arr

df['1'] = calculate_newton_ex1(1.0)
df['2'] = calculate_newton_ex1(0.0)
df['3'] = calculate_newton_ex1(0.1)
df['4'] = calculate_newton_ex1(0.01)

print (df)    
\end{minted}

\begin{verbatim}
               1    2         3         4
0   1.000000e+00  0.0  0.100000  0.010000
1  -5.000000e+00  0.0  0.130000  0.019300
2  -1.850000e+02  0.0  0.141700  0.035993
3  -2.399450e+05  0.0  0.142848  0.062917
4  -4.030157e+11  0.0  0.142857  0.098124
5  -1.136952e+24  0.0  0.142857  0.128850
6  -9.048612e+48  0.0  0.142857  0.141484
7  -5.731417e+98  0.0  0.142857  0.142844
8           -inf  0.0  0.142857  0.142857
9           -inf  0.0  0.142857  0.142857
10          -inf  0.0  0.142857  0.142857
\end{verbatim}

Örnek 2

Bu örnekte iki deðiþkenli bir fonksiyon görelim. Global minimum
$(1/3,1/3)$. Bakalým bu deðeri bulabilecek miyiz?

$f(x) = -\ln( 1 - x_1 - x_2) - \ln x_1 - \ln x_2$

$$
\nabla f(x) = 
\left[\begin{array}{r}
\frac{1}{1-x_1-x_2} - \frac{1}{x_1} \\
\frac{1}{1-x_1-x_2} - \frac{1}{x_2} 
\end{array}\right]
$$

$$
F(x) = 
\left[\begin{array}{rr}
(\frac{1}{1-x_1-x_2})^2 - (\frac{1}{x_1})^2 & (\frac{1}{1-x_1-x_2} )^2 \\
(\frac{1}{1-x_1-x_2} )^2 & (\frac{1}{1-x_1-x_2})^2 - (\frac{1}{x_2})^2
\end{array}\right]
$$

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
df = pd.DataFrame(index=np.arange(11))

def calculate_newton_ex2(x):    
    arr = []
    for i in range(8):
        arr.append(x)
        x1,x2 = x[0],x[1]    
        F = [[(1.0/(1.0-x1-x2))**2 + (1.0/x1)**2.0, (1.0/(1.0-x1-x2))**2.0],
             [(1.0/(1.0-x1-x2))**2, (1.0/(1.0-x1-x2))**2.0 + (1.0/x2)**2.0]]
        F = np.array(F)

        Df = [[1.0/(1.0-x1-x2) - (1.0/x1)], [1.0/(1.0-x1-x2)-(1.0/x2)]]
        Df = np.array(Df)

        d = np.dot(-lin.inv(F),Df)
        x = x + d.flatten()

    return np.array(arr)

res = calculate_newton_ex2([0.85,0.05])
print (res)
\end{minted}

\begin{verbatim}
[[0.85  0.05 ]
 [0.717 0.097]
 [0.513 0.176]
 [0.352 0.273]
 [0.338 0.326]
 [0.333 0.333]
 [0.333 0.333]
 [0.333 0.333]]
\end{verbatim}

[diðer yakýnsama konusu atlandý]

Dikkat edilirse þimdiye kadar dýþbükeylik (convexity) farzýný yapmadýk,
sadece Hessian matrinin tersi alýnabilir olduðunu farzettik. 

Devam edersek, özyineli þekilde güncelememizi yaparken Hessian'ýn eþsiz
olduðu bazý noktalara gelmiþ olabiliriz. Bu olduðunda çoðu yazýlým bu
durumu yakalayacak þekilde yazýlmýþtýr, ``yeterince eþsiz'' Hessian
matrislere önceden tanýmlý ufak bir $\epsilon$ çarpý birim matrisi kadar
bir ekleme yaparlar, böylece tersin alýnamama durumundan kurtulunmuþ olur. 
Bu metotlara Newton-umsu (quasi-Newton) ismi de veriliyor. 

Eskiden Newton-umsu metotlar koca bir araþtýrma sahasýydý. Benim bildiðim
kadarýyla tarihte 15 sene kadar geriye gidersek, üstteki görüldüðü gibi her
adýmda büyük bir denklem sistemi çözmek istemiyoruz, $x$ noktasýndayým,
Hessian iþliyorum, Newton yönümü buluyorum, adým atýyorum, yeni bir
noktadayým. Newton-umsu metotlarda bu yeni noktada sil baþtan bir Hessian
iþlemek yerine bir önceki adýmdaki iþlenen Hessian sonuçlarýný, bir
þekilde, az ek iþlem yaparak sonraki adýmda kullanmaya uðraþýyorlar.
Aslýnda pek çok farklý Newton-umsu metot var, hepsi farklý þekilde Newton
metotundan farklý (!)

[atlandý]

Not: Hocanýn söylediklerine ek [4, pg. 522]'de belirtildiði gibi Hessian'ýn
silbaþtan yaratýlmasý yerine güncellenmesi aslýnda paradoksal bir þekilde
daha iyi optimizasyon yapýlmasýný saðlayabiliyor. Newton yönünün iniþ yönü
olmasý için $H$ pozitif kesin olmalýdýr, fakat minimum noktasýndan uzaktaki
yerlerde bunun garantisi yoktur. Yani gerçek Hessian ile gerçek Newton
adýmýný atmak bizi fonksiyonun arttýðý yerlere götürebilir. Newton-umsu
metotlarýn esprisi pozitif kesin bir þekilde baþlamak ve iniþe giderken
azar azar $H$'yi yaklaþýksal þekilde öyle güncellemek ki $H$ hep pozitif
kesin kalsýn.

[teori 1.1 ispatý atlandý]

Ekler

Newton metodunun eðer baþlangýç noktasý nihai minimuma yakýnsa iyi
yakýnsaklýk özellikleri var. Fakat eðer sonuca uzaktan bir yerden
baþlamýþsak yakýnsaklýk garantisi yok. Geldiðimiz yeni noktada Hessian
eþsiz olabilir. Bu sebeple metot sürekli iniþ özelliðine (descent property)
sahip olmayacaktýr, yani $f(x_{x+1}) \ge f(x_k)$ olabilir, ve yakýnsaklýk
garantisi bu durumda kaybolur [5, sf. 167]. Fakat bu algoritmayý biraz
deðiþtirerek sürekli iniþ özelliðine sahip olmasýný saðlayabiliriz. 

Teori

$x_1,x_2,...$ ya da kýsaca $\{x_k\}$ Newton'un metodu tarafýndan üretilmiþ
$f(x)$ hedef fonksiyonunu minimize etme amaçlý bir çözüm dizisi olsun. Eðer
Hessian $F(x)_k$ pozitif kesin ise ve gradyan $g_k = \nabla f(x_k) \ne 0$
ise, o zaman çözüm yönü

$$
d_k = -F(x_k)^{-1} g_k = x_{k+1} - x_k
$$

bir iniþ yönüdür, ki bu ifadeyle kastedilen bir $\bar{\alpha} > 0$
kesinlikle vardýr öyle ki her $\alpha \in  (0,\bar{\alpha})$ için 

$$
f(x_k + \alpha d_k) < f(x_k)
$$

ifadesi doðrudur. 

Ýspat

$\phi(\alpha)$ diye yeni bir eþitlik yaratalým,

$$
\phi(\alpha) = f(x_k + \alpha d_k)
$$

Üstteki formülün türevini alalým. Zincirleme Kuralýný kullanarak,

$$
\phi(\alpha)' = f(x_k + \alpha d_k) d_k
$$

elde ederiz. Þimdi $\phi(0)'$ ne oluyor ona bakalým,

$$
\phi(0)' = \nabla f(x_k) d_k = -g_k^T F(x_k) ^{-1} g_k < 0
$$

$-g_k^T F(x_k) ^{-1} g_k$ ifadesinin sýfýrdan küçük olduðunu biliyoruz
çünkü $F(x_k) ^{-1}$ pozitif kesin, ve $g_k \ne 0$. O zaman diyebiliriz ki 
bir $\bar{\alpha} > 0$ mevcuttur öyle ki her $\alpha \in (0,\bar{\alpha})$
için $\phi(\alpha) < \phi(0)$. Bu da demektir ki her $\alpha \in
(0,\bar{\alpha})$ için

$$
\phi(\alpha) < \phi(0) =  f(x_k + \alpha x_k) < f(x_k)
$$

Ýspat tamamlandý.

Üstteki gördüklerimiz $d_k$ yönünde bir arama yaparsak, muhakkak bir
minimum bulacaðýmýzý söylüyor. Eðer her geldiðimiz noktada, bir sonraki
gidiþ noktasýný hesap için bu minimum yeri ararsak, sürekli iniþ özelliðine
kavuþmuþ olacaðýz, ve böylece Newton metodunu kurtarmýþ olacaðýz. Demek ki
Newton metotunu þu þekilde deðiþtirmemiz gerekiyor, 

$$
x_{k+1} = x_k - \alpha_k F(x_k)^{-1} g_k
$$

ki 

$$
\alpha_k = \arg\min_{\alpha \ge 0} f(x_k -\alpha F(x_k)^{-1} g_k)
$$

Yani döngünün her adýmýnda $-F(x_k)^{-1} g_k$ yönünde bir arama
gerçekleþtiriyoruz, o yöndeki en fazla azalmayý buluyoruz, ve $\alpha_k$
adýmýný o büyüklükte seçiyoruz. Ve üstteki teori sayesinde $g_k \ne 0$
olduðu sürece 

$$
f(x_{k+1}) < f(x_k)
$$

sürekli iniþ özelliðinin mevcut olduðundan emin oluyoruz. 



[devam edecek]

Kaynaklar 

[1] Bayramlý, Çok Boyutlu Calculus, {\em Vektör Calculus, Kurallar, Matris Türevleri}

[2] Freund, {\em MIT OCW Nonlinear Programming Lecture},
    \url{https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/}

[3] Miller, {\em Numerical Analysis for Scientists and Engineers}

[4] Flannery, {\em Numerical Recipes, 3rd Edition}

[5] Zak, {\em An Introduction to Optimization, 4th Edition}

\end{document}
