\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Newton'un Metodu (Newton's Method)

Bu notlar [2] dersinden alýndý.

Newton birazdan bahsedeceðimiz yöntemi tek boyutlu problemler için
kullandý. Rhapson adlý bilimci yöntemi çok boyutlu problemler için
geniþletti. Biz bu yönteme optimizasyon çerçevesinde bakacaðýz.  Konunun
tarihinden biraz bahsetmek istiyorum, bu dersi öðretmeye baþladýðýmda 1986
senesiydi, Newton'un metodunu nasýl gördüðümüz o zamandan beri deðiþime
uðradý. NM o zamanlar son baþvurulan metot diye öðretiliyordu, çünkü metodu
kullanmak için ``büyük'' bir denklem sistemi çözmek gerekiyordu, 500 x 500
bir sistem mesela. Bugüne gelelim artýk NM ilk baþvurulan metot haline
geldi, 50,000 x 50,000 boyutlarýnda bir sistem çözmek ``yetiyor'' ve böyle
bir sistem artýk idare edilebilen bir boyut haline geldi. Yani hesapsal
kapasite NM'in optimizasyon alanýnda oynadýðý rolü tamamen deðiþtirdi.

Diðer bir faktör ileride öðreneceðimiz iç nokta (interior-point)
metotlarýnýn Newton'un metodunu kullanýyor olmalarý. Ýç nokta metotlarý
icbukey optimizayonda çok popüler, onlar için NM gerekiyor, bu da NM'in
popülaritesini arttýrýyor.

NM nedir? Elimde bir kýsýtlanmamýþ (unconstrained) problemim var diyelim,

$$
\min f(x), \quad \textrm{ öyle ki } \quad x \in X = \mathbb{R}^n
$$

Bir Taylor açýlýmý yapabilirim,

$$
f(x) \approx 
f(\bar{x}) + 
\nabla f(\bar{x})^T (x-\bar{x}) + 
\frac{1}{2} (x-\bar{x})^T H (x-\bar{x}) 
$$

ki $H$ Hessian matrisi. Üstteki formüle $h(x)$ diyelim. Böylece bir karesel
model ortaya çýkartmýþ oldum, formülün sað tarafýndaki çarpým onu karesel
yapýyor, ve þimdi onu kesin olarak çözmek istiyorum. Bunu nasýl yaparým?
Formülün gradyanýný sýfýra eþitleyebilirim. Üstteki fonksiyonun $x$'teki
gradyaný nedir? 

Gradyaný $x$'e göre aldýðýmýzý unutmayalým, $h(x)$'in ikinci terimi
$\nabla f(\bar{x})^T$ bir sabit sayý, ikinci gradyan alýnýrken sýfýrlanýr,
ve tüm ikinci terim sýfýrlanýr. Üçüncü terimin gradyanýný almak bir nevi
$\frac{\partial (x^TAx)}{\partial x} $ almak gibi [1], $H$ belli bir
noktadaki ikinci türev matrisi olduðu için $A$ gibi bir sabit matris kabul
edilebilir, $A$ simetrik olunca gradyan $2Ax$ sonucunu veriyordu, $H$
simetrik, o zaman üçüncü terimde $H$ kalýr, 2 ve $1/2$ birbirini iptal
eder, sonuç

$$
\nabla h(\bar{x}) = \nabla f(\bar{x}) + H(\bar{x})(x-\bar{x}) 
$$

Ýki üstteki karesel yaklaþýksal ifadenin gradyaný bu iþte. Onu sýfýra
eþitleriz ve çözeriz. $H$ tersi alinabilir bir matristir, o zaman 

$$
\nabla h(\bar{x}) = \nabla f(\bar{x}) + H(\bar{x})(x-\bar{x}) = 0
$$


$$
(x-\bar{x}) = -H^{-1} \nabla f(\bar{x})
$$
 
Üstteki ifadeye $d$ diyebilirim, ve bu $d$ benim Newton yönüm olarak
görülebilir, yön derken optimizasyon baðlamýnda minimuma giden yön. 

Bu bizi gayet basit 4 adýmlýk bir algoritmaya taþýyor,

0) $x^0$ verildi, bu baþlangýç noktasý, $k = 0$ yap.

1) $d^k = -H(x^k)^{-1} \nabla f(x^k)$. Eðer $d^k=0$ ise dur.

2) $\alpha^k = 1$ adým boyu seç

3) $x^{k+1} = x^k + \alpha^k d^k$, $k = k + 1$ yap, ve 1. adýma geri dön.

Bu metodun önemli bir özelliðinin sadece bir lineer sistemi çözmek olduðunu
görüyoruz. Bir lineer sistemi çözmek kolay midir? Sisteme göre deðiþir, 100
x 100 sistem, problem yok. 10,000 x 10,000 yoðun bir sistem var ise (seyrek
matrisle temsil edilen lineer sisteme nazaran) iþimiz daha zor
olacaktýr. Bu tür sistemlerde Gaussian eliminasyon iþlemeyebilir, bir tür
özyineli metot gerekli. Demek istediðim Newton yönteminin darboðazý bir
lineer denklem sistemini her seferinde sýfýrdan baþlayarak çözmek, ve bunu
her döngüde yapmak. 

Fakat bu çözümün bize pek çok þey kazandýrdýðýný da görmek lazým;
bahsedilen sistemi çözmek bize pek çok bilgi kazandýrýyor çünkü çözülen
problem içinde 1. ve 2. türev bilgisi var. Bu bilgi minimizasyon açýsýndan
daha akýllýca adým atýlabilmesini saðlýyor. 

Metot Hessian'ýn her adýmda tersi alýnabilir olduðunu farzediyor, bu her
zaman doðru olmayabilir. O sebeple bunun doðru olduðu türden problemler ile
uðraþacaðýz, ya da Hessian'ýn tersi alýnabilir olmasýný saðlayan
mekanizmalarý göreceðiz. $H$'nin özünü bozmadan deðiþtirerek tersi
alýnabilir olmasýný saðlayan yöntemler var. 

Ayrýca hedef her adýmda fonksiyonunu oluþturduðumda bu fonksiyonun azalma
garantisi yok. Öyle ya akýllý bir algoritmanin her adýmda hedef
fonksiyonumu daha iyiye götürdüðümü düþünebilirdim, ama þu anda kadar
gördüklerimiz ýþýðýnda, bunun garantisi yok. Bu konuya sonra deðineceðiz. 

Bir diðer nokta 2. adýmýn çizgi arama ile geniþletilebilmesi [atlandý]

NY'nin en çekici tarafý, eðer yakýnsama (convergence) mümkün ise bu
yakýnsamanýn çok hýzlý bir þekilde olmasý, ki bu iyi. Bu konuya gelmeden
metodun bazý ek özelliklerini görelim.

Terminoloji: bir matrise SPD denir eger matris simetrik, pozitif kesin ise
(simetric positive-definite). 

Teklif (Proposition) 1: 

Eðer $H(x)$ SPD ise $d \ne 0$, o zaman $d$ $\bar{x}$ noktasýnda bir iniþ
yönüne iþaret eder. Ýniþ yönü olmasý demek, eðer makul ufak bir adým
çerçevesinde gidilen noktada $f$'in deðerinin o an olduðumuz noktadan daha
az olmasý demektir.

Nasýl ispatlarým? Önceki dersten hatýrlarsak, eðer yönüm gradyan ile
negatif iç çarpýma sahip ise, o zaman yönüm kesinlikle bir iniþ yönüydü.

Teori 

Diyelim ki $f(x)$ fonksiyonu $\bar{x}$ noktasýnda türevi alýnabilir halde
[2, sf. 9]. Eðer elimizde $\nabla f(\bar{x})^T d < 0$ sonucunu veren bir
$d$ vektörü var ise, öyle ki her yeterince küçük $\lambda > 0$ için
$f(\bar{x}+\lambda d) < f(\bar{x})$ olacak þekilde, o zaman $d$ bir iniþ
yönüdür.

Ýspat 

Taylor açýlýmý ile yönsel türev tanýmýna bakarsak,

$$
f(\bar{x} + \lambda d ) = 
f(\bar{x}) + \lambda \nabla f(\bar{x})^T d + 
\lambda ||d|| \alpha(\bar{x},\lambda d)
$$

öyle ki $\alpha(\bar{x},\lambda d) \to 0$, $\lambda \to 0$ olurken. Not:
Norm içeren üçüncü terimdeki $\lambda ||d|| \alpha(\bar{x},\lambda d)$
ifadesi Taylor serisinin artýklý tanýmýndan geliyor. Detaylar için [3,
sf. 360]'a bakýlabilir.

Üstteki ifadeyi tekrar düzenlersek,

$$
\frac{f(\bar{x} + \lambda d) - f(\bar{x})}{\lambda} = 
\nabla f(\bar{x})^T + ||d||\alpha(\bar{x},\lambda d)
$$

$\nabla f(\bar{x})^T d < 0$ olduðuna göre (aradýðýmýz þart bu) o zaman, ve
$\alpha(\bar{x},\lambda d) \to 0$, $\lambda \to 0$ iken, her yeterince
küçük $\lambda > 0$ için $f(\bar{x} + \lambda d)-f(\bar{x}) < 0$ olmalýdýr,
yani her hangi bir yönde atýlan adým bir önceki $f$ deðerinden bizi daha
ufak bir $f$ deðerine götürmelidir. 

Ana Teklif'e dönelim. Newton adýmýnýdaki SPD $H$ için $0 < d^T \nabla f$
olduðunu göstermemiz lazým (ki böylece iniþ yönü olduðunü ispatlayabilelim,
bir önceki teori),

$$
d = -H^{-1} \nabla f(\bar{x})
$$

demiþtik, her iki tarafý $\nabla f(x)$ ile çarpalým,

$$
d \nabla f(x) = -\nabla f(x) H^{-1} \nabla f(x) 
$$

Eþitliðin sað tarafýndaki ifade hangi þartlarda eksi olur? Eðer $H$ matrisi
pozitif kesin ise deðil mi? Genel matrislerden hatýrlarsak, matris $A$ ve
bir vektör için $v$ eðer $A$ pozitif kesin ise $v^TAv > 0$. Daha önce
$H$'nin pozitif kesin olduðunu söylemiþtik, o zaman bir þekilde eðer $H$
pozitif kesin olmasýnýn sadece ve sadece $H$'nin tersinin pozitif kesin
olmasýna baðlý olduðuna gösterebilirsem amacýma ulaþabilirim.

Bunu yapmak aslýnda pek zor deðil. Biliyorum ki $H(x)$ SPD. Simdi herhangi
bir vektor $v$ icin 

$$
0 < v^T H(x)^{-1}v
$$

ifadeyi þöyle geniþletelim, $H(x)H(x)^{-1}$ eklemek hiçbir þeyi deðiþtirmez
çünkü bu çarpým birim matristir, 

$$
v^T H(x)^{-1}v = v^T H(x)^{-1} H(x)H(x)^{-1} v > 0
$$

Geniþlemiþ ifadenin harfiyen pozitif olduðunu biliyorum, iki üstteki
tanýmdan. Ama þimdi üstteki ifadeye farklý bir þekilde bakarsak,

$$
v^T H(x)^{-1}v = \underbrace{v^TH(x)^{-1}} H(x) \underbrace{H(x)^{-1} v} > 0
$$

Ýþaretlenen bölümlerin birer vektör olduðunu görebiliriz, bu durumda
$v^TAv > 0$ pozitif kesinlik formülü farklý bir $v$ için hala geçerlidir, o
zaman ortadaki $A$, bu durumda $H(x)$ pozitif kesin olmalýdýr. 

Örnek 1

$f(x) = 7x - \ln(x)$ olsun. O zaman $\nabla f(x) = 7 - \frac{1}{x}$ ve
$H(x) = f''(x) = \frac{1}{x^2}$. Bu fonksiyonun özgün global minimumunun
$x^* = 1/7 = 1.428..$ olduðunu kontrol etmek zor deðil. $x$ noktasýndaki
Newton yönü

$$
d = -H(x)^{-1} \nabla f(x) = -\frac{f'(x)}{f''(x)} = 
-x^2 \left( 7 - \frac{1}{x}  \right)=
x - 7x^2
$$

Newton yöntemi $\{ x^k \}$ serisini üretecek, öyle ki

$$
x^{k+1} = x^k + ( x^k - 7(x^k)^2  ) = 2x^k - 7(x^k)^2
$$

Altta farklý baþlangýç noktalarýna göre üretilen serileri
görüyoruz. Yakýnsamanýn hangi deðere doðru olduðu bariz, ve global minimum
da o deðer zaten. 

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
pd.set_option('display.notebook_repr_html', False)
pd.set_option('display.max_columns', 20)
pd.set_option('display.max_rows', 30) 
pd.set_option('display.width', 82) 
pd.set_option('precision', 6)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
df = pd.DataFrame(index=np.arange(11))

def calculate_newton_ex1(x):
    arr = []
    for i in range(11):
        arr.append(x)
        x = 2*x - 7*x**2
        if (x > 1e100):  x = np.inf
        if (x < -1e100):  x = -np.inf
    return arr

df['1'] = calculate_newton_ex1(1.0)
df['2'] = calculate_newton_ex1(0.0)
df['3'] = calculate_newton_ex1(0.1)
df['4'] = calculate_newton_ex1(0.01)

print (df)    
\end{minted}

\begin{verbatim}
               1    2         3         4
0   1.000000e+00  0.0  0.100000  0.010000
1  -5.000000e+00  0.0  0.130000  0.019300
2  -1.850000e+02  0.0  0.141700  0.035993
3  -2.399450e+05  0.0  0.142848  0.062917
4  -4.030157e+11  0.0  0.142857  0.098124
5  -1.136952e+24  0.0  0.142857  0.128850
6  -9.048612e+48  0.0  0.142857  0.141484
7  -5.731417e+98  0.0  0.142857  0.142844
8           -inf  0.0  0.142857  0.142857
9           -inf  0.0  0.142857  0.142857
10          -inf  0.0  0.142857  0.142857
\end{verbatim}

Örnek 2

Bu örnekte iki deðiþkenli bir fonksiyon görelim. Global minimum
$(1/3,1/3)$. Bakalým bu deðeri bulabilecek miyiz?

$f(x) = -\ln( 1 - x_1 - x_2) - \ln x_1 - \ln x_2$

$$
\nabla f(x) = 
\left[\begin{array}{r}
\frac{1}{1-x_1-x_2} - \frac{1}{x_1} \\
\frac{1}{1-x_1-x_2} - \frac{1}{x_2} 
\end{array}\right]
$$

$$
H(x) = 
\left[\begin{array}{rr}
(\frac{1}{1-x_1-x_2})^2 - (\frac{1}{x_1})^2 & (\frac{1}{1-x_1-x_2} )^2 \\
(\frac{1}{1-x_1-x_2} )^2 & (\frac{1}{1-x_1-x_2})^2 - (\frac{1}{x_2})^2
\end{array}\right]
$$

\begin{minted}[fontsize=\footnotesize]{python}
import numpy.linalg as lin
df = pd.DataFrame(index=np.arange(11))

def calculate_newton_ex2(x):    
    arr = []
    for i in range(8):
        arr.append(x)
        x1,x2 = x[0],x[1]    
        H = [[(1.0/(1.0-x1-x2))**2 + (1.0/x1)**2.0, (1.0/(1.0-x1-x2))**2.0],
             [(1.0/(1.0-x1-x2))**2, (1.0/(1.0-x1-x2))**2.0 + (1.0/x2)**2.0]]
        H = np.array(H)

        Df = [[1.0/(1.0-x1-x2) - (1.0/x1)], [1.0/(1.0-x1-x2)-(1.0/x2)]]
        Df = np.array(Df)

        d = np.dot(-lin.inv(H),Df)
        x = x + d.flatten()

    return np.array(arr)

res = calculate_newton_ex2([0.85,0.05])
print (res)
\end{minted}

\begin{verbatim}
[[0.85  0.05 ]
 [0.717 0.097]
 [0.513 0.176]
 [0.352 0.273]
 [0.338 0.326]
 [0.333 0.333]
 [0.333 0.333]
 [0.333 0.333]]
\end{verbatim}





54:37

[devam edecek]

Kaynaklar 

[1] Bayramlý, Çok Boyutlu Calculus, {\em Vektör Calculus, Kurallar, Matris Türevleri}

[2] Freund, {\em MIT OCW Nonlinear Programming Lecture},
    \url{https://ocw.mit.edu/courses/sloan-school-of-management/15-084j-nonlinear-programming-spring-2004/}

[3] Miller, {\em Numerical Analysis for Scientists and Engineers}

\end{document}





