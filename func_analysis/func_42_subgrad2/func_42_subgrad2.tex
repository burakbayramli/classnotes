\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Altgradyanlar (Subgradients) - 2

Bir dýþbükey kümesi üzerinden tanýmlý ama pürüzsüz olmayabilecek bir
dýþbükey fonksiyonu minimize etmek için yansýtýlan altgradyan (projected
subgradient) metotu adlý bir metot ta kullanýlabilir [1, 22:04].  

Dýþbükey $f$'yi dýþbükey küme $C$ üzerinden optimize etmek için 

$$
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
$$

$$
x^{(k)} = P_C \left( x^{(k-1)} - t_k g_k^{(k-1)} \right)
$$

Bu metot normal altgradyan metotu gibi, tek fark parantez içinde görülen
altgradyan adýmý atýldýktan sonra elde edilen sonucun $C$ kümesine geri
yansýtýlmasý (projection), çünkü atýlan adým sonucunda olurlu bir sonuç
elde etmemiþ olabiliriz. 

Yakýnsama analizi normal altgraydan metotuna benziyor, bu metotla da benzer
yakýnsama garantisi elde edilebiliyor, yakýnsama oraný da buna dahil.

Yansýtma adýmý bazen zor olabilir, hangi durumlarda kolay olduðunun listesi
aþaðýda [1, 23:53]. Hangi kümelere yansýtmak kolaydýr?

1) Doðrusal görüntüler: $\{  Ax + b: x \in \mathbb{R}^n  \}$

2) $\{ x: Ax = b \}$ sisteminin çözüm kümesi 

3) Negatif olmayan bölge: $\mathbb{R}_{+}^n = \{x: x \ge 0 \}$. Verilen
vektörün negatif deðerlerinin sýfýr yapmak, pozitifleri tutmak bize o
vektörün negatif olmayan bölge karþýlýðýný veriyor. 

4) Bazý norm toplarý $\{ x: ||x||_p \le 1 \}$, $p=1,2,..,\infty$ için. Daha
önce 2-norm topuna yansýtmayý gördük, vektörü alýp normalize edersek bu
kümeye yansýtma yapmýþ oluyoruz aslýnda. Bu yansýtma bizi o vektörün 2-norm
topundaki en yakýn diðer vektöre götürüyor. Sonsuz norm kolay, bir kutuya
yansýtma yapmýþ oluyoruz, bunun ne demek olduðunu düþünmeyi size
býrakýyorum, 1-norm en zoru. 

5) Bazý çokyüzlüler (polyhedra) ve basit koniler.

Bir uyarýda bulunalým, tanýmý basit duran kümeler ortaya çýkartmak
kolaydýr, fakat bu kümelere yansýtma yapan operatörler çok zor
olabilir. Mesela geliþigüzel bir çokyüzlü $C = \{ x: Ax \le b \}$ kümesine
yansýtma yapmak zordur. Bu problemin kendisi apayrý bir optimizasyon
problemi aslýnda, bir QP. 

[stochastic subgradient method atlandý]

Altgradyanlarýn iyi tarafý genel uygulanabilirlik. Altgradyan metotlarý
dýþbükey, Lipschitz fonksiyonlarý üzerinde bir anlamda optimaldir. Ünlü
bilimci Neþterov'un bu baðlamda bir teorisi vardýr [1, 45:12], pürüzsüz
durumlarda ve 1. derece yöntemlerde altgradyanlarýn lineer kombinasyonlarý
üzerinden güncellemenin sonucu olan adýmlarýn baþarýsýna bir alt sýnýr
tanýmlar. 

Proksimal / Yakýnsal Gradyan Metotu (Proximal Gradient Method) 

Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uðraþmak
yerine belli bir yapýya uyan çetrefil fonksiyonlarý optimize etmeye
uðraþýr. Bu yapý

$$
f(x) = g(x) + h(x) 
\mlabel{1}
$$

formundadýr [1, 45:59]. $g,h$'in ikisi de dýþbükey. $g$'nin pürüzsüz,
türevi alýnabilir olduðu farz edilir (çoðunlukla oldukca çetrefil olabilir)
ve $\dom(g) = \mathbb{R}^n$, $h$ ise pürüzsüz olmayabilir. Tabii pürüzsüz
artý pürüzsüz olmayan iki fonksýsyon toplamý kriterin tamamýný pürüzsüz
olmayan hale çevirir. Ama bu toplam daha basittir denebilir, onun üzerinde
proksimal operatörü uygulanabilir.

Hatýrlarsak eðer $f$ türevi alýnabilir olsaydý gradyan iniþ güncellemesi 

$$
x^{+} = x - t \cdot \nabla f(x)
$$

Bu formüle eriþmenin bir yöntemi karesel yaklaþýklama üzerinden idi,
$f$'nin $x$ etrafýndaki yaklaþýklamasýnda $\nabla^2f(x)$ yerine
$\frac{1}{t} I$ koyunca,

$$
x^{+} = \arg\min_x f(x) + 
\underbrace{
  \nabla f(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 
}_{f_t(z)}
$$

elde ediliyordu. Yani gradyan iniþi sanki ardý ardýna geldiði her noktada
bir karesel yaklaþýklama yapýyor, ve onu adým atarak minimize etmeye
uðraþýyor.

Ama (1)'deki $f$ pürüzsüz deðil, o sebeple üstteki mantýk ise
yaramayacak. Fakat, belki de karesel yaklaþýklamanýn bir kýsmýný hala
kullanabiliriz, ve minimizasyonu sadece $g$'ye uygularýz çünkü pürüzsüz
olan kýsým o. Yani niye $g$'nin yerine karesel yaklaþýklama koymayalým?
Þimdi bunu yapacaðýz [1, 50:10]. 

$$
x^{+} = \arg\min_z \tilde{g}_t(z) + h(z)
$$

ki $\tilde{g}_t(z)$ $g$'nin $x$ noktasý etrafýndaki karesel yaklaþýklamasý
oluyor.  Eðer elimizde $h$ olmasaydý üstteki sadece bir gradyan
güncellemesine indirgenebilirdi. Neyse açýlýmý yaparsak

$$
= \arg\min_z g(x) + \nabla g(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 + h(z)
$$

Daha uygun bir formda yazabiliriz, $z$'nin $g$ üzerindeki deðiþikliðe
karesel uzaklýðý olarak,

$$
= \arg\min_z \frac{1}{2t} 
\big|\big| z - \big( x-t\nabla g(x) \big) \big|\big|_2^2 + 
h(z)
$$

Yani söylenmek istenen, hem sadece $g$ olsaydý atacaðýmýz gradyan adýmýna
yakýn durmaya çalýþmak, hem de $h$'nin kendisini ufak tutmak. Dýþarýdan
ayarlanan $t$ parametresi $g$ gradyan adýmý ile $h$ arasýnda bir denge
kurmak gibi görülebilir, eðer $t$ ufak ise o zaman gradyan adýmýna yakýn
durmaya daha fazla önem atfetmiþ olacaðýz, $h$'nin ufak tutulmasýna daha
az. $t$ çok büyük ise tam tersi olacak. 

Bu aslýnda kabaca Proksimal Gradyan Ýniþi yöntemini tarif etmiþ
oluyor. Þimdi proksimal eþlemesi operatörünü göstereceðiz, ki bu
algoritmalarý daha temiz olarak yazmamýza yardýmcý olacak. 

Bir $h$ fonksiyonu için bir $\prox$ operatörü tanýmlýyoruz, 

$$
\prox_{t} (x) = \arg\min_z \frac{1}{2t} ||x-z||_2^2 + h(z)
\mlabel{3}
$$

Üstteki $x$'in bir fonksiyonu olarak $\arg\min$'de gösterilen kriteri
minimize eden $z$'yi buluyor. Operatör $t$'ye baðlý, dýþarýdan verilen
parametre. Tabii ki $h$'ye de baðlý, ki bazen üstteki operatörü
$\prox_{h,t}$ olarak gösteren de oluyor. 

Üstteki ifadenin eþleme olduðunu kontrol edelim. Ciddi tanýmlý bir
fonksiyon üstteki deðil mi? Ona bir $x$ veriyorsunuz, o da size tek bir
sonuç döndürüyor. Ayrýca bu eþleme / fonksiyonun kendisi bir
minimizasyon. Peki bu minimizasyon problemi dýþbükey mi? Evet. Peki bu
problemin özgün bir sonucu var mýdýr? Vardýr çünkü üstteki problem harfiyen
dýþbükey. Deðil mi? Eðer $h$ harfiyen dýþbükey olmasa bile ifadenin tümü
harfiyen dýþbükey olurdu çünkü $\frac{1}{2t} ||x-z||_2^2 $ harfiyen
dýþbükey [1, 55:09], $z-x$'in karesi var [ayrýca $x$ deðiþkeni $h$'ye
geçilmiyor].

Yani harfiyen dýþbükey, o zaman özgün sonuç var. Biz de bu özgün sonucu
alarak bir iniþ algoritmasý yazýyoruz [1, 56:28], 

$$
x^{(k)} = \prox_{t_k} \big( 
x^{(k-1)} - t_k \nabla g(x^{(k-1)})
\big), \quad k=1,2,...
\mlabel{2}
$$

Güncelleme adýmýný tanýdýk þekilde yazmak için 

$$
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k} (x^{(k-1)})
$$

ki $G_t$'ye $f$'nin genelleþtirilmiþ gradyaný denebilir,

$$
G_t(x) = \frac{x - \prox_{t}(x - t \nabla g(x))}{t}
$$

$G$'nin dýþbükey fonksiyonlarýn alýþýlageldik gradyanlarýna benzer pek çok
özelliði vardýr, ki bu özellikler proksimal metotlarýn yakýnsadýðýyla
alakalý ispatlarda kullanýlabilir.

Þimdiye kadar anlattýklarýmýza bakanlara bu komik bir hikaye gibi
gelebilir. Bir $g + h$ toplamýný minimize etmek istiyordum, bunu yapabilmek
için (2) formunda adýmlar atacaðým, bir $\prox$ operatörüm var, ama bu
operatör bir sonuç döndürüyor aslýnda, ve bu sonuç bir baþka
minimizasyondan geliyor. Yani $g + h$ türü bir toplam minimizasyonu yerine
her adýmda, bir sürü minimizasyonlarý koymuþ oldum. Bu nasýl daha iyi bir
sonuç verecek ki?

Þunu belirtmek lazým, sadece eðer proksimal gradyanlarý analitik, ya da
hýzlý bir þekilde hesaplayabiliyorsak onlarý çözüm için düþünürüz. Yani her
ne kadar her adýmda (3) turu optimizasiyonlar yapýyorsak ta, bunu pürüzsüz
olan kýsým $h$ yeterince basit olduðu zaman yapýyoruz ki tüm (3) için
analitik  ya da hýzlý hesapsal çözüm olsun [1, 58:54]. 

Diðer noktalar, dikkat edersek proksimal operatör $g$'ye baðlý deðil,
tamamen $h$ bazlý. Eðer $h$ basit ise ama $g$ müthiþ çetrefil ise bu
proksimal hesaplarýný çok zorlaþtýrmýyor. Eðer o çok çetrefil (ve pürüzsüz)
$g$ için gradyan hesaplanabiliyorsa, durumu kurtardýk demektir. 

Tekrar bir Lasso problemi göreceðiz. Bu Lasso için gördüðümüz ikinci
algoritma, ve belirtmek gerekir ki Proksimal metot altgradyan metotuna göre
çok daha verimlidir, hýzlýdýr.

Verili bir $y \in \mathbb{R}^n$, $X \in \mathbb{R}^{n \times p}$ için Lasso
kriterini hatýrlarsak, 

$$
f(\beta) = \frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1 
$$

Kriterdeki ilk terim en az kareler kayýp fonksiyonu, ikinci terim bir ayar
parametresi üzerinden katsayýlarýn 1. normu. Bu kriteri pürüzsüz, ve
pürüzsüz olmayan ama basitçe olan iki kýsma ayýracaðýz, yani zaten oldukca
bariz, pürüzsüz kýsým 1. terim, $g(\beta)$ diyelim, olmayan 2. terim,
$h(\beta)$ diyelim. Proksimal gradyan iniþi için bize iki þey gerekiyor,
birincisi $g$'nin gradyani, ikincisi $h$ için $\prox$ operatörünü
hesaplayabilmek.

$g$'nin gradyani oldukca basit, onu bu noktada uykumuzda bile bulabiliyor
olmamýz lazým. $h$'nin $\prox$ operatörü,

$$
\prox_t(\beta) = \arg\min_z \frac{1}{2t} ||\beta-z||_2^2 + \lambda ||z||_1
$$

Her þeyi $t$ ile çarparsam,

$$
\prox_t(\beta) = \arg\min_z \frac{1}{2} ||\beta-z||_2^2 + \lambda t ||z||_1
$$

Üstteki minimizasyonun çözümünü daha önce altgradyanlar üzerinden
görmüþtük, 

$$
= S_{\lambda t}(\beta)
$$

Yine yumuþak eþikleme (soft-threshold) operatörüne gelmiþ olduk, 

$$
[ S_{\lambda} (\beta) ]_i = 
\left\{ \begin{array}{ll}
\beta_i - \lambda & \textrm{eðer } \beta_i > \lambda_i \\
0 & \textrm{eðer } -\lambda \ge \beta_i \ge \lambda, \quad i=1,..,n \\
\beta_i + \lambda & \textrm{eðer } \beta_i < -\lambda_i 
\end{array} \right.
$$

Tüm algoritma neye benziyor? Önce $g$'ye göre bir graydan güncellemesi
yaparým, gradyan

$$
\nabla g(\beta) = -X^T (y-X\beta)
$$

O zaman güncelleme 

$$\beta + t X^T (y-X\beta)$$ 

olur. Buna $\prox$ uygularsak,

$$
\beta^+ = S_{\lambda t}(\beta + t X^T (y-X\beta))
$$

Yumuþak eþikleme ne yapar? Her ögeye teker teker bakar, eðer mutlak deðeri
çok ufaksa onu ya sýfýra eþitler, ya da onu $\lambda \cdot t$ kadar sýfýra
yaklaþtýrýr. 

Üstteki Lasso algoritmasina ÝSTA adý da verilir. 

Geriye Çizgisel Ýz Sürme  (Backtracking line search)

Graydan iniþinde görmüþtük ki adým büyüklüklerini dinamik olarak
seçebiliyorduk, her adýmdaki duruma adapte olabiliyorduk, tipik olarak
Lipschitz sabitini bilmiyoruz çünkü. O sebeple geriye iz sürme pratikte
iyi iþlemesiyle beraber, teorik olarak yakýnsamayý da garantiliyordu. 

Proksimal gradyanlarý için benzer bir kavram geçerli. Ayrýca proksimal
durumda geriye doðru iz sürmenin birden fazla yolu var. 

[atlandý]

Örnek kod, Lasso problem çözümü [2]

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
diabetes = pd.read_csv("../../stat/stat_120_regular/diabetes.csv",sep=';')
y = np.array(diabetes['response'].astype(float)).reshape(442,1)
X = np.array(diabetes.drop("response",axis=1))
N,dim = X.shape
print (N,dim)

lam = 1/np.sqrt(N);
w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T

L = (np.linalg.svd(X)[1][0])**2
print(L)
max_iter = 500

def obj(w):
    r = X*w-y;
    return np.sum(np.multiply(r,r))/2 +  lam * np.sum(np.abs(w))

def f_grad(w):
    return  X.T*(X*w-y) 

def soft_threshod(w,mu):
    return np.multiply(np.sign(w), np.maximum(np.abs(w)-mu,0))  

w = np.matrix([0.0]*dim).T
for t in range(0, max_iter):
    obj_val = obj(w)
    w = w - (1/L)* f_grad(w)
    w= soft_threshod(w,lam/L)    
    if (t % 50==0):
        print('iter= {},\tobjective= {:3f}'.format(t, obj_val.item()))

print (w)
\end{minted}

\begin{verbatim}
442 10
4.0242141761466925
iter= 0,	objective= 6425460.500000
iter= 50,	objective= 5751070.568959
iter= 100,	objective= 5750285.357193
iter= 150,	objective= 5749670.506866
iter= 200,	objective= 5749177.635558
iter= 250,	objective= 5748779.527464
iter= 300,	objective= 5748457.810485
iter= 350,	objective= 5748197.804952
iter= 400,	objective= 5747987.670443
iter= 450,	objective= 5747817.840900
[[  -8.71913404]
 [-238.35531517]
 [ 522.93302022]
 [ 323.11825944]
 [-526.09642955]
 [ 265.58097894]
 [ -17.84381222]
 [ 143.15165377]
 [ 652.14114865]
 [  68.55685031]]
\end{verbatim}

Kaynaklar

[1] Tibshirani, {\em Convex Optimization, Lecture Video 8}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

[2] He, {\em IE 598 - Big Data Optimization},  
    \url{http://niaohe.ise.illinois.edu/IE598_2016/}


\end{document}





