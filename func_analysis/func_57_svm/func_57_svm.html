<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Destek Vektor Makinaları (Support Vector Machines)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="destek-vektor-makinaları-support-vector-machines">Destek Vektor
Makinaları (Support Vector Machines)</h1>
<p>En basit halleriyle SVM’ler risk minimize eden lineer
sınıflayıcısıdırlar.</p>
<p><span class="math display">\[
R(\Theta) \leq J(\Theta) = R_{emp}(\Theta) +
\sqrt{ \frac{h \times (\log(\frac{2N}{h}) + 1) -
\log(\frac{\eta}{4})}{N}}
\]</span></p>
<p>h: sınıflayıcının kapasitesi</p>
<p>N: eğitim verisinde kaç veri noktası olduğu</p>
<p>Vapnik ve Chernovenkis <span class="math inline">\(1-\eta\)</span>
olasılıkla ispaladı ki üstteki denklem doğrudur. SVM algoritması hem
<span class="math inline">\(h\)</span> değerini hem de sayısal, ölçümsel
riski aynı anda minimize etmektedir, ve bunu sınır noktalarını
noktalarını ayırmakla yapmaktadır. Türetelim,</p>
<p><img src="svm-planes.png" /></p>
<p>Karar düzlemi: <span class="math inline">\(w^{T}x + b=0\)</span></p>
<p>Şöyle bir tanım yapalım:</p>
<p><span class="math display">\[
q = \min_{x}\big\|x - 0\big\|, \quad w^T x+b=0 \textrm { şartına göre }
\]</span></p>
<p><span class="math inline">\(q\)</span>, <span
class="math inline">\(H^{+}\)</span> ve <span
class="math inline">\(H^{-}\)</span> formüllerini ileride
kullanacağız.</p>
<p>Lagrange:</p>
<p><span class="math display">\[
\min_{x}\frac{1}{2} \big\|x - 0\big\|^2+\lambda(w^{T}x+b)
\]</span></p>
<p>Gradyanı alalım ve 0 değerine eşitleyelim,</p>
<p><span class="math display">\[
\frac{\partial}{\partial x} ( \frac{1}{2} x^T x + \lambda( w^T x + b ) )
= 0
\]</span></p>
<p><span class="math display">\[ x + \lambda w = 0 \]</span></p>
<p><span class="math display">\[ x = -\lambda w \]</span></p>
<p>Üsteki sonucu <span class="math inline">\(w^T x+b=0\)</span> şartına
sokalım,</p>
<p><span class="math display">\[ w^T(-\lambda w) + b = 0 \]</span></p>
<p><span class="math display">\[ \lambda = \frac{b}{w^Tw} \]</span></p>
<p>Yani çözüm</p>
<p><span class="math display">\[ \hat{x} = - \bigg(
\frac{b}{w^Tw}  \bigg) w \]</span></p>
<p>O zaman <span class="math inline">\(q\)</span></p>
<p><span class="math display">\[ q = || \hat{x}-0 || = \bigg|\bigg|
-  \frac{b}{w^Tw} w \bigg|\bigg|  \]</span></p>
<p><span class="math display">\[ \big|\frac{b}{w^Tw}\big| \times
\sqrt{w^Tw}  \]</span></p>
<p><span class="math display">\[ q = \frac{|b|}{||w||} \]</span></p>
<p>Tanım:</p>
<p><span class="math display">\[ H^{+} = w^{T}x + b=+1 \]</span></p>
<p><span class="math display">\[ H^{-} = w^{T}x + b=-1 \]</span></p>
<p>grafikte görüldüğü gibi yani. Üstteki şekilde tanımın bir zararı yok
(çünkü +1,-1 sabit durunca ayraç genişlemesi nasıl olacak diye
düşünülebilir, ama bu tanım genelliği kaybetmeden yapabilabiliyor çünkü
<span class="math inline">\(b,w\)</span> değerlerinde hala
oynanabilir.</p>
<p><span class="math inline">\(q^{+}\)</span> ve <span
class="math inline">\(q^{-}\)</span> değerlerini hesapla</p>
<p><span class="math display">\[ q^{+} = \frac{|b-1|}{||w||}
\]</span></p>
<p><span class="math display">\[ q^{-} = \frac{|-b-1|}{||w||}
\]</span></p>
<p>Ayraç o zaman şöyle</p>
<p><span class="math display">\[ m=q^{+}+q^{-} =
\frac{|b-1-b-1|}{||w||} = \frac{|-2|}{||w||} = \frac{2}{||w||}
\]</span></p>
<p>Ayraçların olabildiğince ayırmasını istiyorsak <span
class="math inline">\(m\)</span>’i arttırız (yani <span
class="math inline">\(\frac{2}{||w||}\)</span>’i maksimize ederiz), ya
da <span class="math inline">\(||w||\)</span> değerini minimize
ederiz.</p>
<p>Sınırlar</p>
<p>Veri noktalarını öyle sınıflamak istiyoruz ki + ve - noktalar
hiperdüzlemlerin doğru noktalarında kalsınlar.</p>
<p><span class="math display">\[ w^{T}x+b \geq +1, \quad \forall
y_{i}=+1   \]</span></p>
<p><span class="math display">\[ w^{T}x+b \leq -1, \quad \forall
y_{i}=-1  \]</span></p>
<p>Bu iki denklemi birleştirelim</p>
<p><span class="math display">\[ y_{i}(w^{T}x+b)-1 \geq 0  \]</span></p>
<p>Her şeyi biraraya koyalım</p>
<p><span class="math display">\[ \min_w \frac{1}{2}{||w||^2}, \quad
y_{i}(w^Tx_{i}+b)-1 \ge 0
\textrm{ olsun. }\]</span></p>
<p>Bu form tanıdık geliyor mu? Bu qp ile çözülebilecek karesel
(quadratic) bir formül, programdır!</p>
<p>qp</p>
<p>Python dilinde cvxopt paketi vardır Matlab Optimization Toolbox’da
qp() var. QP fonksiyonları problemleri genelde</p>
<p><span class="math display">\[\frac{1}{2}x^{T}Px+q^{T}x\]</span></p>
<p>formunda görmek isterler. Biraz önce elde ettiğimiz denklemi bu
istenen formata doğru “masajlayabiliriz’’</p>
<p>İkiz (dual)</p>
<p>SVM ihtiyaçları için ikiz formül (dual) ile çalışmak daha rahattır
Lagrange (tekrar) oluşturalım, türevi alalım, ve sıfıra eşitleyelim.
Bunun sonucunda elimize KKT noktaları geçecektir</p>
<p><span class="math display">\[
L_{p} = \frac{1}{2}||w||^{2}-\sum_{i}\alpha_{i}(y_{i}(w^{T}x_{i}+b)-1)  
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial w} L_{p} =
w-\sum_{i}\alpha_{i}y_{i}x_{i}=0  \]</span></p>
<p><span class="math display">\[
w = \sum_{i}\alpha_{i}y_{i}x_{i}
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial b} L_{p} = -\sum_{i}\alpha_{i}y_{i}=0  
\]</span></p>
<p>Üstteki iki denklemi asal (primal) denkleme koyduğumuz zaman</p>
<p><span class="math display">\[
\textrm{ Maksimize et } L_{D}= \sum_{i}\alpha_{i}-\frac{1}{2}
\sum_{i} \sum_{j} \alpha_{i} \alpha_{j} y_{i}y_{j}x_{i}^{T}x_{j}
\]</span></p>
<p>sınırlar</p>
<p><span class="math display">\[
\sum_{i}\alpha_{i}y_{i}=0  \]</span></p>
<p><span class="math display">\[ \alpha_{i} \geq 0  \]</span></p>
<p>qp</p>
<p>Bu yine qp() formunda bir problem! Sadece bu sefer çözeceğimiz
değişkenler <span class="math inline">\(\alpha_i\)</span>’lar, <span
class="math inline">\(x\)</span>’lar değil. Üstteki denklem şu forma
<span class="math inline">\(\frac{1}{2}x^{T}Px+q^{T}x\)</span>
masajlanabilir Bunun yapmak için <span
class="math inline">\(P_{i,j}\)</span>’ye <span
class="math inline">\(-y_{i}y_{j}x_{i}^{T}x_{j}\)</span> değerini
atarız. Ve qp’yi çağırırız Sonuç bir <span
class="math inline">\(\alpha\)</span>’lar listesi olacaktır.</p>
<p><span class="math inline">\(b\)</span> değerini hesaplamak</p>
<p>KKT koşulunun sebebiyle sıfır olmayan her <span
class="math inline">\(\alpha_{i}\)</span> için ana problemde ona tekabül
eden kısıtlayıcı şart şıkıdır (tight), yani bir eşitliktir. O zaman
sıfır olmayan her <span class="math inline">\(\alpha_{i}\)</span> için
<span class="math inline">\(b\)</span>’yi <span
class="math inline">\(w^{T}x_{i}+b = y_{i}\)</span> ifadesini kullanarak
hesaplarız. Sıfır olmayan her <span
class="math inline">\(\alpha_{i}\)</span>’dan gelen <span
class="math inline">\(b\)</span> yaklaşık olarak diğer other <span
class="math inline">\(b\)</span>’lere eşit olacaktır. Final <span
class="math inline">\(b\)</span>’yi hesaplamak için tüm <span
class="math inline">\(b\)</span>’lerin ortalamasını almak sayısal
(numeric) olarak daha garantidir.</p>
<p>Sınıflayıcı Tamamlandı</p>
<p>Her yeni <span class="math inline">\(x\)</span> noktası için artık
<span class="math inline">\(sign(x^{T}w+b)\)</span> ibaresini
sınıflayıcımız olarak kullanabiliriz. <span
class="math inline">\(-1\)</span> ya da <span
class="math inline">\(+1\)</span> olarak geri gelecek sonuç bize yeni
noktanın hangi sınıfa ait olduğunu söyleyecektir.</p>
<p>Örnek Çıktı</p>
<p><img src="svmlinear.png" /></p>
<p>Çekirdekler (Kernels)</p>
<p>Şimdiye kadar lineer ayraçlardan bahsettik. SVM’ler lineer olmayan
ayraçlarla da çalışabilir. Çok basit: Bir temel fonksiyon kullanarak
girdiyi daha yüksek boyuta doğru bir önişlemden geçirirsek bunu
başarabiliriz. Algoritmanın geri kalanı değişmeden kalacaktır.</p>
<p>Gayri Lineer Çekirdek</p>
<p><img src="svmpoly.png" /></p>
<p>Esneme Payı Bazen bir problem ayrılmaya müsait olmayabilir. Çok üç
noktalardaki bazı noktalar sınıflayıcının çalışmasını imkansız hale
getirebilir Bunun çözümü için sınıflayıcıya “esneme payı” dahil
edebiliriz. Mesela <span class="math inline">\(y_{i}=+1\)</span> için
verinin yanlış tarafa düşmesini şu durumda izin verebiliriz: <span
class="math inline">\(w^{T}+b \geq -0.03\)</span> Fakat eklemek gerekir
ki bu tür noktaların “çok fazla’’ olmasını da istemiyoruz, bu sebeple
bu”yanlış” noktaların sayısına da bir ceza getirebiliriz.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> linalg</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cvxopt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cvxopt.solvers</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> svm(X, y):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    n_samples, n_features <span class="op">=</span> X.shape</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gram matrix</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    K <span class="op">=</span> np.zeros((n_samples, n_samples))</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            K[i,j] <span class="op">=</span> np.dot(X[i], X[j])</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> cvxopt.matrix(np.outer(y,y) <span class="op">*</span> K)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> cvxopt.matrix(np.ones(n_samples) <span class="op">*</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> cvxopt.matrix(y, (<span class="dv">1</span>,n_samples))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> cvxopt.matrix(<span class="fl">0.0</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    G <span class="op">=</span> cvxopt.matrix(np.diag(np.ones(n_samples) <span class="op">*</span> <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> cvxopt.matrix(np.zeros(n_samples))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># solve QP problem</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    solution <span class="op">=</span> cvxopt.solvers.qp(P, q, G, h, A, b)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (solution)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lagrange multipliers</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> np.ravel(solution[<span class="st">&#39;x&#39;</span>])</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&quot;a&quot;</span>, a)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Support vectors have non zero lagrange multipliers</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    ssv <span class="op">=</span> a <span class="op">&gt;</span> <span class="fl">1e-5</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    ind <span class="op">=</span> np.arange(<span class="bu">len</span>(a))[ssv]</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> a[ssv]</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    sv <span class="op">=</span> X[ssv]</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    sv_y <span class="op">=</span> y[ssv]</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&quot;</span><span class="sc">%d</span><span class="st"> support vectors out of </span><span class="sc">%d</span><span class="st"> points&quot;</span> <span class="op">%</span> (<span class="bu">len</span>(a), n_samples))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&quot;sv&quot;</span>, sv)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&quot;sv_y&quot;</span>, sv_y)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Intercept</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(a)):</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        b <span class="op">+=</span> sv_y[n]</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        b <span class="op">-=</span> np.<span class="bu">sum</span>(a <span class="op">*</span> sv_y <span class="op">*</span> K[ind[n],ssv])</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    b <span class="op">/=</span> <span class="bu">len</span>(a)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Weight vector</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.zeros(n_features)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(a)):</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        w <span class="op">+=</span> a[n] <span class="op">*</span> sv_y[n] <span class="op">*</span> sv[n]</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&quot;a&quot;</span>, a)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w, b, sv_y, sv, a</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="fl">3.</span>,<span class="fl">3.</span>],[<span class="fl">4.</span>,<span class="fl">4.</span>],[<span class="fl">7.</span>,<span class="fl">7.</span>],[<span class="fl">8.</span>,<span class="fl">8.</span>]])</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="op">-</span><span class="fl">1.</span>,<span class="op">-</span><span class="fl">1.</span>])</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>w, b, sv_y, sv, a <span class="op">=</span> svm(X, y)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;w&quot;</span>, w)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;b&quot;</span>, b)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;test points&#39;</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (np.dot([<span class="fl">2.</span>,<span class="fl">2.</span>], w) <span class="op">+</span> b) <span class="co"># &gt; 1</span></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (np.dot([<span class="fl">9.</span>,<span class="fl">9.</span>], w) <span class="op">+</span> b) <span class="co"># &lt; -1</span></span></code></pre></div>
<pre class="text"><code>     pcost       dcost       gap    pres   dres
 0: -2.9061e-01 -5.0286e-01  6e+00  2e+00  1e+00
 1: -3.6857e-02 -3.0976e-01  3e-01  3e-16  2e-15
 2: -1.0255e-01 -1.2816e-01  3e-02  3e-17  1e-15
 3: -1.1074e-01 -1.1128e-01  5e-04  2e-17  9e-16
 4: -1.1111e-01 -1.1111e-01  5e-06  3e-17  3e-16
 5: -1.1111e-01 -1.1111e-01  5e-08  4e-17  4e-16
Optimal solution found.
{&#39;x&#39;: &lt;4x1 matrix, tc=&#39;d&#39;&gt;, &#39;y&#39;: &lt;1x1 matrix, tc=&#39;d&#39;&gt;, &#39;s&#39;: &lt;4x1 matrix, tc=&#39;d&#39;&gt;, &#39;z&#39;: &lt;4x1 matrix, tc=&#39;d&#39;&gt;, &#39;status&#39;: &#39;optimal&#39;, &#39;gap&#39;: 5.330207369919097e-08, &#39;relative gap&#39;: 4.797188223915408e-07, &#39;primal objective&#39;: -0.11111107426109383, &#39;dual objective&#39;: -0.11111112756316754, &#39;primal infeasibility&#39;: 4.388541835878497e-17, &#39;dual infeasibility&#39;: 3.646862906299076e-16, &#39;primal slack&#39;: 2.76375125177696e-08, &#39;dual slack&#39;: 7.403425105865937e-08, &#39;iterations&#39;: 5}
a [2.76375125e-08 1.11111073e-01 1.11111073e-01 2.76375125e-08]
2 support vectors out of 4 points
sv [[4. 4.]
 [7. 7.]]
sv_y [ 1. -1.]
a [0.11111107 0.11111107]
w [-0.33333322 -0.33333322]
b 3.666665418062399
test points
2.3333325387669808
-2.3333325387669817</code></pre>
<p>Not: İkizdeki <span class="math inline">\(L_d\)</span>’yi maksimize
ediyoruz, fakat hala <code>qp()</code>’deki minimize ediciyi
çağırıyoruz. Bu sebeple tüm <span
class="math inline">\(\alpha\)</span>’ların toplamını temsil eden <span
class="math inline">\(q\)</span>’ların negatifini alıyoruz,
<code>np.ones(n_samples) *-1</code> işleminde görüldüğü gibi. Formüldeki
karesel kısım içinde zaten <span
class="math inline">\(-\frac{1}{2}\)</span> negatif ibaresi var, böylece
geri kalan formülün değişmesine gerek yok.</p>
<p>Dayanaklı Kayıp Fonksiyonu ile SVM, Pegasos</p>
<p>SVM problemi alttaki fonksiyonu çözmek anlamına geliyordu,</p>
<p><span class="math display">\[ \min_w \frac{1}{2}{||w||^2}, \textrm{
s.t. } \quad y_{i}(w^Tx_{i}+b)-1 \ge 0 \]</span></p>
<p>ki bu bir karesel program idi ve <code>cvxopt</code> paketindeki
<code>qp</code> ile çözülebiliyordu. Bazıları <span
class="math inline">\(b\)</span> terimini de atıyorlar, ve</p>
<p><span class="math display">\[  \min_w \frac{1}{2}{||w||^2} + \sum
\max \{ 0, 1-y_i (w^T x_i) \}  \]</span></p>
<p>olarak yazıyorlar. Ayrıca regülarizasyonu kontrol etmek için bir
<span class="math inline">\(\lambda\)</span> sabiti de ekleniyor, yani
üstte <span class="math inline">\(\lambda ||w||^2 / 2\)</span>
kullanılması lazım. Regülarize işlemi <span
class="math inline">\(w\)</span>’nin norm’unun küçük olmasını tercih
eder, ki bu bazı <span class="math inline">\(w\)</span> değerlerinin
sıfıra gitmesini zorlar, yani bir tür özellik seçme işi bu şekilde
gerçekleşmiş olur. Toplam işleminin içindeki fonksiyona “kayıp
fonksiyonu (loss function)’’ ismi de verilir, eğer bu kayıp fonksiyonu
tam üstteki gibi ise ona dayanaklı kayıp (hinge loss) denir. Üstte
görülen <span class="math inline">\(\max\)</span> ifadesi suna
eşittir,</p>
<p><span class="math display">\[
Loss(w,x_i,y_i) =
\left\{ \begin{array}{ll}
1-y_i \cdot (w \cdot x_i) &amp; \textrm{ eğer } y_i \cdot (w \cdot x_i)
&lt; 1 \\
0 &amp; \textrm { diğer }
\end{array} \right.
\]</span></p>
<p>Eğer kayıp fonksiyonunun gradyanını alırsak,</p>
<p><span class="math display">\[
\nabla L = \frac{\partial Loss(w,x_i,y_i)}{\partial w} =
\left\{ \begin{array}{ll}
-y_i  x_i &amp; \textrm{ eğer } y_i \cdot (w \cdot x_i) &lt; 1 \\
0 &amp; \textrm { diğer }
\end{array} \right.
\]</span></p>
<p>Böylece bir rasgele gradyan iniş (stochastic gradient descent)
yaklaşımını kodlayabiliriz.</p>
<p><span class="math display">\[ w_{t+1} = w_t - \eta (\lambda w_t +
\nabla L )\]</span></p>
<p>ki <span class="math inline">\(\eta\)</span> gradyanın ne kadar
güncellenme yapacağını kontrol eden bir sabittir.</p>
<p>Ufak Toptan Parçalar (Minibatching)</p>
<p>Güncelleme işlemi tüm veri üzerinde, her veri noktası için
yapılabilir, ya da gradyan güncellemeleri toparlanarak belli sayıda adım
sonrası bir toplam güncelleme yapılır. <span
class="math inline">\(b\)</span> büyüklüğündeki ufak parça <span
class="math inline">\(B_t\)</span> de rasgele seçilir, ve <span
class="math inline">\(w\)</span>’ye uygulanır [3].</p>
<p><span class="math display">\[
w_{t+1} = w_t - \eta \bigg(
\lambda w_t + \frac{1}{b} \sum_{x_i,y_i \in B_t} \nabla L
\bigg)
\]</span></p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(w, x):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(w.reshape((<span class="bu">len</span>(x),<span class="dv">1</span>)).T,x)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_sgd(data, labels, lam, <span class="bu">iter</span>, batch_size):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    m,n <span class="op">=</span> data.shape<span class="op">;</span> w <span class="op">=</span> np.zeros(n)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(m)) <span class="co"># Convert range object to a list</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    eta <span class="op">=</span> <span class="fl">0.0001</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">iter</span>):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        w_delta <span class="op">=</span> np.zeros(n)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        np.random.shuffle(idx)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(batch_size):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            i <span class="op">=</span> idx[j]</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            p <span class="op">=</span> predict(w, data[i,:])</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> labels[i]<span class="op">*</span>p <span class="op">&lt;</span> <span class="fl">1.</span>:</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>                w_delta <span class="op">+=</span> labels[i]<span class="op">*</span>data[i,:]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> (<span class="fl">1.0</span> <span class="op">-</span> eta<span class="op">*</span>lam)<span class="op">*</span>w <span class="op">+</span> (eta<span class="op">/</span>batch_size)<span class="op">*</span>w_delta</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd, zipfile</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> zipfile.ZipFile(<span class="st">&#39;svmdata.zip&#39;</span>, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> z:</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span>  pd.read_csv(z.<span class="bu">open</span>(<span class="st">&#39;features.txt&#39;</span>),sep<span class="op">=</span><span class="st">&#39;,&#39;</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span>  pd.read_csv(z.<span class="bu">open</span>(<span class="st">&#39;target.txt&#39;</span>))</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df.shape, labels.shape)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>data_train <span class="op">=</span> df.head(<span class="dv">5413</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>data_test <span class="op">=</span> df.tail(<span class="dv">1000</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>label_train <span class="op">=</span> labels.head(<span class="dv">5413</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>label_test <span class="op">=</span> labels.tail(<span class="dv">1000</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, auc</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_auc(d1, d2):</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    fpr, tpr, thresholds <span class="op">=</span> roc_curve(d1,d2)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    roc_auc <span class="op">=</span> auc(fpr, tpr)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">&#39;AUC&#39;</span>, roc_auc</span></code></pre></div>
<pre class="text"><code>(6413, 122) (6413, 1)</code></pre>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> [<span class="dv">10</span>,<span class="dv">50</span>,<span class="dv">100</span>,<span class="dv">200</span>]:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_size <span class="kw">in</span> [<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">100</span>]:</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> train_sgd(np.array(data_train),labels<span class="op">=</span>np.array(label_train),</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>                              lam<span class="op">=</span><span class="dv">1</span>, <span class="bu">iter</span><span class="op">=</span>epoch,batch_size<span class="op">=</span>batch_size)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> predict(w, data_train.T)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> show_auc(np.array(label_train.T)[<span class="dv">0</span>], pred[<span class="dv">0</span>])</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span> (<span class="st">&#39;iter&#39;</span>, epoch, <span class="st">&#39;batch&#39;</span>, batch_size, <span class="st">&#39;egitim&#39;</span>, score)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> predict(w, data_test.T)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> show_auc(np.array(label_test.T)[<span class="dv">0</span>], pred[<span class="dv">0</span>])</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span> (<span class="st">&#39;iter&#39;</span>, epoch, <span class="st">&#39;batch&#39;</span>, batch_size, <span class="st">&#39;test&#39;</span>, score)</span></code></pre></div>
<pre class="text"><code>iter 10 batch 1 egitim (&#39;AUC&#39;, 0.8047518086158001)
iter 10 batch 1 test (&#39;AUC&#39;, 0.7971786666666667)
iter 10 batch 10 egitim (&#39;AUC&#39;, 0.7894177873794489)
iter 10 batch 10 test (&#39;AUC&#39;, 0.7861306666666668)
iter 10 batch 100 egitim (&#39;AUC&#39;, 0.7668542103627676)
iter 10 batch 100 test (&#39;AUC&#39;, 0.7649733333333333)
iter 50 batch 1 egitim (&#39;AUC&#39;, 0.7562366828358444)
iter 50 batch 1 test (&#39;AUC&#39;, 0.7637626666666666)
iter 50 batch 10 egitim (&#39;AUC&#39;, 0.794759653080582)
iter 50 batch 10 test (&#39;AUC&#39;, 0.7964026666666666)
iter 50 batch 100 egitim (&#39;AUC&#39;, 0.7577277052199766)
iter 50 batch 100 test (&#39;AUC&#39;, 0.7551173333333333)
iter 100 batch 1 egitim (&#39;AUC&#39;, 0.7847942568739994)
iter 100 batch 1 test (&#39;AUC&#39;, 0.7882906666666667)
iter 100 batch 10 egitim (&#39;AUC&#39;, 0.772609410468842)
iter 100 batch 10 test (&#39;AUC&#39;, 0.7707013333333333)
iter 100 batch 100 egitim (&#39;AUC&#39;, 0.7593143760002341)
iter 100 batch 100 test (&#39;AUC&#39;, 0.7570266666666666)
iter 200 batch 1 egitim (&#39;AUC&#39;, 0.7134575904456049)
iter 200 batch 1 test (&#39;AUC&#39;, 0.7176400000000001)
iter 200 batch 10 egitim (&#39;AUC&#39;, 0.7526888032672676)
iter 200 batch 10 test (&#39;AUC&#39;, 0.7491333333333333)
iter 200 batch 100 egitim (&#39;AUC&#39;, 0.7591727089662824)
iter 200 batch 100 test (&#39;AUC&#39;, 0.757576)</code></pre>
<p>Hazır bir SVM kodu scikit-learn kütüphanesi karşılaştıralım,</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">&#39;linear&#39;</span>,tol<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>clf.fit(np.array(data_train),np.array(label_train))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> clf.predict(data_train)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;egitim&#39;</span>,show_auc(np.array(label_train.T)[<span class="dv">0</span>], pred))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> clf.predict(data_test)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;test&#39;</span>,show_auc(np.array(label_test.T)[<span class="dv">0</span>], pred))</span></code></pre></div>
<pre class="text"><code>egitim (&#39;AUC&#39;, 0.7690303271156629)
test (&#39;AUC&#39;, 0.7533333333333333)</code></pre>
<p>Kaynaklar</p>
<p>[1] Blondel, <a
href="https://gist.github.com/mblondel/586753">https://gist.github.com/mblondel/586753</a></p>
<p>[2] Jebara, T., <em>Machine Learning Lecture, Columbia
University</em></p>
<p>[3] Song, et al., <em>Stochastic gradient descent with differentially
private updates</em></p>
<p>[4] Harrington, <em>Machine Learning in Action</em></p>
<p>[5] Stanford, <em>Stanford, CS246: Mining Massive Data Sets</em>, <a
href="http://web.stanford.edu/class/cs246/">http://web.stanford.edu/class/cs246/</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
