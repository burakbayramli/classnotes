<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Destek Vektor Makinaları (Support Vector Machines)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<h1 id="destek-vektor-makinaları-support-vector-machines">Destek Vektor Makinaları (Support Vector Machines)</h1>
<p>En basit halleriyle SVM'ler risk minimize eden lineer sınıflayıcısıdırlar.</p>
<p><span class="math display">\[
R(\Theta) \leq J(\Theta) = R_{emp}(\Theta) +
\sqrt{ \frac{h \times (\log(\frac{2N}{h}) + 1) - \log(\frac{\eta}{4})}{N}}
\]</span></p>
<p>h: sınıflayıcının kapasitesi</p>
<p>N: eğitim verisinde kaç veri noktası olduğu</p>
<p>Vapnik ve Chernovenkis <span class="math inline">\(1-\eta\)</span> olasılıkla ispaladı ki üstteki denklem doğrudur. SVM algoritması hem <span class="math inline">\(h\)</span> değerini hem de sayısal, ölçümsel riski aynı anda minimize etmektedir, ve bunu sınır noktalarını noktalarını ayırmakla yapmaktadır. Türetelim,</p>
<div class="figure">
<img src="svm-planes.png" />

</div>
<p>Karar düzlemi: <span class="math inline">\(w^{T}x + b=0\)</span></p>
<p>Şöyle bir tanım yapalım:</p>
<p><span class="math display">\[
q = \min_{x}\big\|x - 0\big\|, \quad w^T x+b=0 \textrm { sartina gore }
\]</span></p>
<p><span class="math inline">\(q\)</span>, <span class="math inline">\(H^{+}\)</span> ve <span class="math inline">\(H^{-}\)</span> formüllerini ileride kullanacağız.</p>
<p>Lagrange:</p>
<p><span class="math display">\[
\min_{x}\frac{1}{2} \big\|x - 0\big\|^2+\lambda(w^{T}x+b)
\]</span></p>
<p>Gradyanı alalım ve 0 değerine eşitleyelim,</p>
<p><span class="math display">\[
\frac{\partial}{\partial x} ( \frac{1}{2} x^T x + \lambda( w^T x + b ) ) = 0
\]</span></p>
<p><span class="math display">\[ x + \lambda w = 0 \]</span></p>
<p><span class="math display">\[ x = -\lambda w \]</span></p>
<p>Üsteki sonucu <span class="math inline">\(w^T x+b=0\)</span> şartına sokalım,</p>
<p><span class="math display">\[ w^T(-\lambda w) + b = 0 \]</span></p>
<p><span class="math display">\[ \lambda = \frac{b}{w^Tw} \]</span></p>
<p>Yani çözüm</p>
<p><span class="math display">\[ \hat{x} = - \bigg( \frac{b}{w^Tw}  \bigg) w \]</span></p>
<p>O zaman <span class="math inline">\(q\)</span></p>
<p><span class="math display">\[ q = || \hat{x}-0 || = \bigg|\bigg| -  \frac{b}{w^Tw} w \bigg|\bigg|  \]</span></p>
<p><span class="math display">\[ \big|\frac{b}{w^Tw}\big| \times \sqrt{w^Tw}  \]</span></p>
<p><span class="math display">\[ q = \frac{|b|}{||w||} \]</span></p>
<p>Tanım:</p>
<p><span class="math display">\[ H^{+} = w^{T}x + b=+1 \]</span></p>
<p><span class="math display">\[ H^{-} = w^{T}x + b=-1 \]</span></p>
<p>grafikte görüldüğü gibi yani. Üstteki şekilde tanımın bir zararı yok (çünkü +1,-1 sabit durunca ayraç genişlemesi nasıl olacak diye düşünülebilir, ama bu tanım genelliği kaybetmeden yapabilabiliyor çünkü <span class="math inline">\(b,w\)</span> değerlerinde hala oynanabilir.</p>
<p><span class="math inline">\(q^{+}\)</span> ve <span class="math inline">\(q^{-}\)</span> değerlerini hesapla</p>
<p><span class="math display">\[ q^{+} = \frac{|b-1|}{||w||} \]</span></p>
<p><span class="math display">\[ q^{-} = \frac{|-b-1|}{||w||} \]</span></p>
<p>Ayraç o zaman şöyle</p>
<p><span class="math display">\[ m=q^{+}+q^{-} =
\frac{|b-1-b-1|}{||w||} = \frac{|-2|}{||w||} = \frac{2}{||w||} \]</span></p>
<p>Ayraçların olabildiğince ayırmasını istiyorsak <span class="math inline">\(m\)</span>'i arttırız (yani <span class="math inline">\(\frac{2}{||w||}\)</span>'i maksimize ederiz), ya da <span class="math inline">\(||w||\)</span> değerini minimize ederiz.</p>
<p>Sınırlar</p>
<p>Veri noktalarını öyle sınıflamak istiyoruz ki + ve - noktalar hiperdüzlemlerin doğru noktalarında kalsınlar.</p>
<p><span class="math display">\[ w^{T}x+b \geq +1, \quad \forall y_{i}=+1   \]</span></p>
<p><span class="math display">\[ w^{T}x+b \leq -1, \quad \forall y_{i}=-1  \]</span></p>
<p>Bu iki denklemi birleştirelim</p>
<p><span class="math display">\[ y_{i}(w^{T}x+b)-1 \geq 0  \]</span></p>
<p>Her şeyi biraraya koyalım</p>
<p><span class="math display">\[ \min_w \frac{1}{2}{||w||^2}, \quad y_{i}(w^Tx_{i}+b)-1 \ge 0
\textrm{ olsun. }\]</span></p>
<p>Bu form tanıdık geliyor mu? Bu qp ile çözülebilecek karesel (quadratic) bir formül, programdır!</p>
<p>qp</p>
<p>Python dilinde cvxopt paketi vardır Matlab Optimization Toolbox'da qp() var. QP fonksiyonları problemleri genelde</p>
<p><span class="math display">\[\frac{1}{2}x^{T}Px+q^{T}x\]</span></p>
<p>formunda görmek isterler. Biraz önce elde ettiğimiz denklemi bu istenen formata doğru &quot;masajlayabiliriz''</p>
<p>İkiz (dual)</p>
<p>SVM ihtiyaçları için ikiz formül (dual) ile çalışmak daha rahattır Lagrange (tekrar) oluşturalım, türevi alalım, ve sıfıra eşitleyelim. Bunun sonucunda elimize KKT noktaları geçecektir</p>
<p><span class="math display">\[
L_{p} = \frac{1}{2}||w||^{2}-\sum_{i}\alpha_{i}(y_{i}(w^{T}x_{i}+b)-1)  
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial w} L_{p} = w-\sum_{i}\alpha_{i}y_{i}x_{i}=0  \]</span></p>
<p><span class="math display">\[
w = \sum_{i}\alpha_{i}y_{i}x_{i} 
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial b} L_{p} = -\sum_{i}\alpha_{i}y_{i}=0  
\]</span></p>
<p>Üstteki iki denklemi asal (primal) denkleme koyduğumuz zaman</p>
<p><span class="math display">\[
\textrm{ Maksimize et } L_{D}= \sum_{i}\alpha_{i}-\frac{1}{2}
\sum_{i} \sum_{j} \alpha_{i} \alpha_{j} y_{i}y_{j}x_{i}^{T}x_{j} 
\]</span></p>
<p>sınırlar</p>
<p><span class="math display">\[ \sum_{i}\alpha_{i}y_{i}=0  \]</span></p>
<p><span class="math display">\[ \alpha_{i} \geq 0  \]</span></p>
<p>qp</p>
<p>Bu yine qp() formunda bir problem! Sadece bu sefer çözeceğimiz değişkenler <span class="math inline">\(\alpha_i\)</span>'lar, <span class="math inline">\(x\)</span>'lar değil. Üstteki denklem şu forma <span class="math inline">\(\frac{1}{2}x^{T}Px+q^{T}x\)</span> masajlanabilir Bunun yapmak için <span class="math inline">\(P_{i,j}\)</span>'ye <span class="math inline">\(-y_{i}y_{j}x_{i}^{T}x_{j}\)</span> değerini atarız. Ve qp'yi çağırırız Sonuç bir <span class="math inline">\(\alpha\)</span>'lar listesi olacaktır.</p>
<p><span class="math inline">\(b\)</span> değerini hesaplamak</p>
<p>KKT koşulunun sebebiyle sıfır olmayan her <span class="math inline">\(\alpha_{i}\)</span> için ana problemde ona tekabül eden kısıtlayıcı şart şıkıdır (tight), yani bir eşitliktir. O zaman sıfır olmayan her <span class="math inline">\(\alpha_{i}\)</span> için <span class="math inline">\(b\)</span>'yi <span class="math inline">\(w^{T}x_{i}+b = y_{i}\)</span> ifadesini kullanarak hesaplarız. Sıfır olmayan her <span class="math inline">\(\alpha_{i}\)</span>'dan gelen <span class="math inline">\(b\)</span> yaklaşık olarak diğer other <span class="math inline">\(b\)</span>'lere eşit olacaktır. Final <span class="math inline">\(b\)</span>'yi hesaplamak için tüm <span class="math inline">\(b\)</span>'lerin ortalamasını almak sayısal (numeric) olarak daha garantidir.</p>
<p>Sınıflayıcı Tamamlandı</p>
<p>Her yeni <span class="math inline">\(x\)</span> noktası için artık <span class="math inline">\(sign(x^{T}w+b)\)</span> ibaresini sınıflayıcımız olarak kullanabiliriz. <span class="math inline">\(-1\)</span> ya da <span class="math inline">\(+1\)</span> olarak geri gelecek sonuç bize yeni noktanın hangi sınıfa ait olduğunu söyleyecektir.</p>
<p>Örnek Çıktı</p>
<div class="figure">
<img src="svmlinear.png" />

</div>
<p>Çekirdekler (Kernels)</p>
<p>Şimdiye kadar lineer ayraçlardan bahsettik. SVM'ler lineer olmayan ayraçlarla da çalışabilir. Çok basit: Bir temel fonksiyon kullanarak girdiyi daha yüksek boyuta doğru bir önişlemden geçirirsek bunu başarabiliriz. Algoritmanın geri kalanı değişmeden kalacaktır.</p>
<p>Gayri Lineer Çekirdek</p>
<div class="figure">
<img src="svmpoly.png" />

</div>
<p>Esneme Payı Bazen bir problem ayrılmaya müsait olmayabilir. Çok üç noktalardaki bazı noktalar sınıflayıcının çalışmasını imkansız hale getirebilir Bunun çözümü için sınıflayıcıya &quot;esneme payı&quot; dahil edebiliriz. Mesela <span class="math inline">\(y_{i}=+1\)</span> için verinin yanlış tarafa düşmesini şu durumda izin verebiliriz: <span class="math inline">\(w^{T}+b \geq -0.03\)</span> Fakat eklemek gerekir ki bu tür noktaların &quot;çok fazla'' olmasını da istemiyoruz, bu sebeple bu &quot;yanlış&quot; noktaların sayısına da bir ceza getirebiliriz.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> numpy <span class="im">import</span> linalg
<span class="im">import</span> cvxopt
<span class="im">import</span> cvxopt.solvers

<span class="kw">def</span> svm(X, y):
    n_samples, n_features <span class="op">=</span> X.shape

    <span class="co"># Gram matrix</span>
    K <span class="op">=</span> np.zeros((n_samples, n_samples))
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):
        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_samples):
            K[i,j] <span class="op">=</span> np.dot(X[i], X[j])

    P <span class="op">=</span> cvxopt.matrix(np.outer(y,y) <span class="op">*</span> K)
    q <span class="op">=</span> cvxopt.matrix(np.ones(n_samples) <span class="op">*</span> <span class="dv">-1</span>)
    A <span class="op">=</span> cvxopt.matrix(y, (<span class="dv">1</span>,n_samples))
    b <span class="op">=</span> cvxopt.matrix(<span class="fl">0.0</span>)

    G <span class="op">=</span> cvxopt.matrix(np.diag(np.ones(n_samples) <span class="op">*</span> <span class="dv">-1</span>))
    h <span class="op">=</span> cvxopt.matrix(np.zeros(n_samples))

    <span class="co"># solve QP problem</span>
    solution <span class="op">=</span> cvxopt.solvers.qp(P, q, G, h, A, b)

    <span class="bu">print</span> solution
    
    <span class="co"># Lagrange multipliers</span>
    a <span class="op">=</span> np.ravel(solution[<span class="st">&#39;x&#39;</span>])
    
    <span class="bu">print</span> <span class="st">&quot;a&quot;</span>, a

    <span class="co"># Support vectors have non zero lagrange multipliers</span>
    ssv <span class="op">=</span> a <span class="op">&gt;</span> <span class="fl">1e-5</span>
    ind <span class="op">=</span> np.arange(<span class="bu">len</span>(a))[ssv]
    a <span class="op">=</span> a[ssv]
    sv <span class="op">=</span> X[ssv]
    sv_y <span class="op">=</span> y[ssv]
    <span class="bu">print</span> <span class="st">&quot;</span><span class="sc">%d</span><span class="st"> support vectors out of </span><span class="sc">%d</span><span class="st"> points&quot;</span> <span class="op">%</span> (<span class="bu">len</span>(a), n_samples)
    <span class="bu">print</span> <span class="st">&quot;sv&quot;</span>, sv
    <span class="bu">print</span> <span class="st">&quot;sv_y&quot;</span>, sv_y

    <span class="co"># Intercept</span>
    b <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(a)):
        b <span class="op">+=</span> sv_y[n]
        b <span class="op">-=</span> np.<span class="bu">sum</span>(a <span class="op">*</span> sv_y <span class="op">*</span> K[ind[n],ssv])
    b <span class="op">/=</span> <span class="bu">len</span>(a)
        
    <span class="co"># Weight vector</span>
    w <span class="op">=</span> np.zeros(n_features)
    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(a)):
        w <span class="op">+=</span> a[n] <span class="op">*</span> sv_y[n] <span class="op">*</span> sv[n]

    <span class="bu">print</span> <span class="st">&quot;a&quot;</span>, a
    <span class="cf">return</span> w, b, sv_y, sv, a

X <span class="op">=</span> np.array([[<span class="fl">3.</span>,<span class="fl">3.</span>],[<span class="fl">4.</span>,<span class="fl">4.</span>],[<span class="fl">7.</span>,<span class="fl">7.</span>],[<span class="fl">8.</span>,<span class="fl">8.</span>]])
y <span class="op">=</span> np.array([<span class="fl">1.</span>,<span class="fl">1.</span>,<span class="op">-</span><span class="fl">1.</span>,<span class="op">-</span><span class="fl">1.</span>])
w, b, sv_y, sv, a <span class="op">=</span> svm(X, y)
<span class="bu">print</span> <span class="st">&quot;w&quot;</span>, w
<span class="bu">print</span> <span class="st">&quot;b&quot;</span>, b
<span class="bu">print</span> <span class="st">&#39;test points&#39;</span>
<span class="bu">print</span> np.dot([<span class="fl">2.</span>,<span class="fl">2.</span>], w) <span class="op">+</span> b <span class="co"># &gt; 1</span>
<span class="bu">print</span> np.dot([<span class="fl">9.</span>,<span class="fl">9.</span>], w) <span class="op">+</span> b <span class="co"># &lt; -1</span></code></pre></div>
<pre><code>     pcost       dcost       gap    pres   dres
 0: -2.9061e-01 -5.0286e-01  6e+00  2e+00  1e+00
 1: -3.6857e-02 -3.0976e-01  3e-01  4e-16  1e-15
 2: -1.0255e-01 -1.2816e-01  3e-02  3e-17  7e-16
 3: -1.1074e-01 -1.1128e-01  5e-04  3e-17  7e-16
 4: -1.1111e-01 -1.1111e-01  5e-06  4e-17  7e-16
 5: -1.1111e-01 -1.1111e-01  5e-08  1e-17  6e-16
Optimal solution found.
{&#39;status&#39;: &#39;optimal&#39;, &#39;dual slack&#39;: 7.403425105865883e-08, &#39;iterations&#39;: 5, &#39;relative gap&#39;: 4.79718822391507e-07, &#39;dual objective&#39;: -0.11111112756316754, &#39;gap&#39;: 5.330207369918724e-08, &#39;primal objective&#39;: -0.11111107426109389, &#39;primal slack&#39;: 2.7637512517768505e-08, &#39;s&#39;: &lt;4x1 matrix, tc=&#39;d&#39;&gt;, &#39;primal infeasibility&#39;: 1.077377601559697e-17, &#39;dual infeasibility&#39;: 6.043668397566901e-16, &#39;y&#39;: &lt;1x1 matrix, tc=&#39;d&#39;&gt;, &#39;x&#39;: &lt;4x1 matrix, tc=&#39;d&#39;&gt;, &#39;z&#39;: &lt;4x1 matrix, tc=&#39;d&#39;&gt;}
a [  2.76375125e-08   1.11111073e-01   1.11111073e-01   2.76375125e-08]
2 support vectors out of 4 points
sv [[ 4.  4.]
 [ 7.  7.]]
sv_y [ 1. -1.]
a [ 0.11111107  0.11111107]
w [-0.33333322 -0.33333322]
b 3.66666541806
test points
2.33333253877
-2.33333253877</code></pre>
<p>Not: İkizdeki <span class="math inline">\(L_d\)</span>'yi maksimize ediyoruz, fakat hala <code>qp()</code>'deki minimize ediciyi çağırıyoruz. Bu sebeple tüm <span class="math inline">\(\alpha\)</span>'ların toplamını temsil eden <span class="math inline">\(q\)</span>'ların negatifini alıyoruz, <code>np.ones(n_samples) *-1</code> işleminde görüldüğü gibi. Formüldeki karesel kısım içinde zaten <span class="math inline">\(-\frac{1}{2}\)</span> negatif ibaresi var, böylece geri kalan formülün değişmesine gerek yok.</p>
<p>Dayanaklı Kayıp Fonksiyonu ile SVM, Pegasos</p>
<p>SVM problemi alttaki fonksiyonu çözmek anlamına geliyordu,</p>
<p><span class="math display">\[ \min_w \frac{1}{2}{||w||^2}, \textrm{ s.t. } \quad y_{i}(w^Tx_{i}+b)-1 \ge 0 \]</span></p>
<p>ki bu bir karesel program idi ve <code>cvxopt</code> paketindeki <code>qp</code> ile çözülebiliyordu. Bazıları <span class="math inline">\(b\)</span> terimini de atıyorlar, ve</p>
<p><span class="math display">\[  \min_w \frac{1}{2}{||w||^2} + \sum \max \{ 0, 1-y_i (w^T x_i) \}  \]</span></p>
<p>olarak yazıyorlar. Ayrıca regülarizasyonu kontrol etmek için bir <span class="math inline">\(\lambda\)</span> sabiti de ekleniyor, yani üstte <span class="math inline">\(\lambda ||w||^2 / 2\)</span> kullanılması lazım. Regülarize işlemi <span class="math inline">\(w\)</span>'nin norm'unun küçük olmasını tercih eder, ki bu bazı <span class="math inline">\(w\)</span> değerlerinin sıfıra gitmesini zorlar, yani bir tür özellik seçme işi bu şekilde gerçekleşmiş olur. Toplam işleminin içindeki fonksiyona &quot;kayıp fonksiyonu (loss function)'' ismi de verilir, eğer bu kayıp fonksiyonu tam üstteki gibi ise ona dayanaklı kayıp (hinge loss) denir. Üstte görülen <span class="math inline">\(\max\)</span> ifadesi suna eşittir,</p>
<p><span class="math display">\[
Loss(w,x_i,y_i) = 
\left\{ \begin{array}{ll}
1-y_i \cdot (w \cdot x_i) &amp; \textrm{ eğer } y_i \cdot (w \cdot x_i) &lt; 1 \\
0 &amp; \textrm { diğer }
\end{array} \right.
\]</span></p>
<p>Eğer kayıp fonksiyonunun gradyanını alırsak,</p>
<p><span class="math display">\[
\nabla L = \frac{\partial Loss(w,x_i,y_i)}{\partial w} =
\left\{ \begin{array}{ll}
-y_i  x_i &amp; \textrm{ eğer } y_i \cdot (w \cdot x_i) &lt; 1 \\
0 &amp; \textrm { diğer }
\end{array} \right.
\]</span></p>
<p>Böylece bir rasgele gradyan iniş (stochastic gradient descent) yaklaşımını kodlayabiliriz.</p>
<p><span class="math display">\[ w_{t+1} = w_t - \eta (\lambda w_t + \nabla L )\]</span></p>
<p>ki <span class="math inline">\(\eta\)</span> gradyanın ne kadar güncellenme yapacağını kontrol eden bir sabittir.</p>
<p>Ufak Toptan Parçalar (Minibatching)</p>
<p>Güncelleme işlemi tüm veri üzerinde, her veri noktası için yapılabilir, ya da gradyan güncellemeleri toparlanarak belli sayıda adım sonrası bir toplam güncelleme yapılır. <span class="math inline">\(b\)</span> büyüklüğündeki ufak parça <span class="math inline">\(B_t\)</span> de rasgele seçilir, ve <span class="math inline">\(w\)</span>'ye uygulanır [3].</p>
<p><span class="math display">\[
w_{t+1} = w_t - \eta \bigg(
\lambda w_t + \frac{1}{b} \sum_{x_i,y_i \in B_t} \nabla L
\bigg)
\]</span></p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd
        
<span class="kw">def</span> predict(w, x):
    <span class="cf">return</span> np.dot(w.reshape((<span class="bu">len</span>(x),<span class="dv">1</span>)).T,x)

<span class="kw">def</span> train_sgd(data, labels, lam, <span class="bu">iter</span>, batch_size):
    m,n <span class="op">=</span> data.shape<span class="op">;</span> w <span class="op">=</span> np.zeros(n)
    idx <span class="op">=</span> <span class="bu">range</span>(m)
    eta <span class="op">=</span> <span class="fl">0.0001</span>
    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">iter</span>):
        w_delta <span class="op">=</span> np.zeros(n)
        np.random.shuffle(idx)
        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(batch_size):
            i <span class="op">=</span> idx[j]
            p <span class="op">=</span> predict(w, data[i,:])
            <span class="cf">if</span> labels[i]<span class="op">*</span>p <span class="op">&lt;</span> <span class="fl">1.</span>:
                w_delta <span class="op">+=</span> labels[i]<span class="op">*</span>data[i,:]
        w <span class="op">=</span> (<span class="fl">1.0</span> <span class="op">-</span> eta<span class="op">*</span>lam)<span class="op">*</span>w <span class="op">+</span> (eta<span class="op">/</span>batch_size)<span class="op">*</span>w_delta
    <span class="cf">return</span> w</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np, pandas <span class="im">as</span> pd, pegasos, zipfile

<span class="cf">with</span> zipfile.ZipFile(<span class="st">&#39;svmdata.zip&#39;</span>, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> z:
    df <span class="op">=</span>  pd.read_csv(z.<span class="bu">open</span>(<span class="st">&#39;features.txt&#39;</span>),sep<span class="op">=</span><span class="st">&#39;,&#39;</span>)
    labels <span class="op">=</span>  pd.read_csv(z.<span class="bu">open</span>(<span class="st">&#39;target.txt&#39;</span>))
    
<span class="bu">print</span> df.shape, labels.shape

data_train <span class="op">=</span> df.head(<span class="dv">5413</span>)
data_test <span class="op">=</span> df.tail(<span class="dv">1000</span>)
label_train <span class="op">=</span> labels.head(<span class="dv">5413</span>)
label_test <span class="op">=</span> labels.tail(<span class="dv">1000</span>)

<span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve, auc
<span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score

<span class="kw">def</span> show_auc(d1, d2):
    fpr, tpr, thresholds <span class="op">=</span> roc_curve(d1,d2)
    roc_auc <span class="op">=</span> auc(fpr, tpr)
    <span class="cf">return</span> <span class="st">&#39;AUC&#39;</span>, roc_auc</code></pre></div>
<pre><code>(6413, 122) (6413, 1)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">np.random.seed(<span class="dv">0</span>)
  
<span class="cf">for</span> epoch <span class="kw">in</span> [<span class="dv">10</span>,<span class="dv">50</span>,<span class="dv">100</span>,<span class="dv">200</span>]:
    <span class="cf">for</span> batch_size <span class="kw">in</span> [<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">100</span>]:
        w <span class="op">=</span> pegasos.train_sgd(np.array(data_train),labels<span class="op">=</span>np.array(label_train),
                              lam<span class="op">=</span><span class="dv">1</span>, <span class="bu">iter</span><span class="op">=</span>epoch,batch_size<span class="op">=</span>batch_size)
        pred <span class="op">=</span> pegasos.predict(w, data_train.T)
        score <span class="op">=</span> show_auc(np.array(label_train.T)[<span class="dv">0</span>], pred[<span class="dv">0</span>])
        <span class="bu">print</span> <span class="st">&#39;iter&#39;</span>, epoch, <span class="st">&#39;batch&#39;</span>, batch_size, <span class="st">&#39;egitim&#39;</span>, score
        pred <span class="op">=</span> pegasos.predict(w, data_test.T)
        score <span class="op">=</span> show_auc(np.array(label_test.T)[<span class="dv">0</span>], pred[<span class="dv">0</span>])
        <span class="bu">print</span> <span class="st">&#39;iter&#39;</span>, epoch, <span class="st">&#39;batch&#39;</span>, batch_size, <span class="st">&#39;test&#39;</span>, score</code></pre></div>
<pre><code>iter 10 batch 1 egitim (&#39;AUC&#39;, 0.80632699788480933)
iter 10 batch 1 test (&#39;AUC&#39;, 0.79744266666666663)
iter 10 batch 10 egitim (&#39;AUC&#39;, 0.78954806549498469)
iter 10 batch 10 test (&#39;AUC&#39;, 0.78614666666666666)
iter 10 batch 100 egitim (&#39;AUC&#39;, 0.76682726584846694)
iter 10 batch 100 test (&#39;AUC&#39;, 0.76497599999999999)
iter 50 batch 1 egitim (&#39;AUC&#39;, 0.75623733098567281)
iter 50 batch 1 test (&#39;AUC&#39;, 0.76376266666666659)
iter 50 batch 10 egitim (&#39;AUC&#39;, 0.79475937530208407)
iter 50 batch 10 test (&#39;AUC&#39;, 0.7964026666666667)
iter 50 batch 100 egitim (&#39;AUC&#39;, 0.75772752003431121)
iter 50 batch 100 test (&#39;AUC&#39;, 0.75512000000000001)
iter 100 batch 1 egitim (&#39;AUC&#39;, 0.78479444205966464)
iter 100 batch 1 test (&#39;AUC&#39;, 0.7882906666666667)
iter 100 batch 10 egitim (&#39;AUC&#39;, 0.77260941046884191)
iter 100 batch 10 test (&#39;AUC&#39;, 0.77070400000000006)
iter 100 batch 100 egitim (&#39;AUC&#39;, 0.75931456118589935)
iter 100 batch 100 test (&#39;AUC&#39;, 0.75702400000000003)
iter 200 batch 1 egitim (&#39;AUC&#39;, 0.71345805340976809)
iter 200 batch 1 test (&#39;AUC&#39;, 0.71764000000000006)
iter 200 batch 10 egitim (&#39;AUC&#39;, 0.75268880326726773)
iter 200 batch 10 test (&#39;AUC&#39;, 0.74913333333333343)
iter 200 batch 100 egitim (&#39;AUC&#39;, 0.75917270896628253)
iter 200 batch 100 test (&#39;AUC&#39;, 0.75757600000000003)</code></pre>
<p>Hazır bir SVM kodu scikit-learn kütüphanesi karşılaştıralım,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC
clf <span class="op">=</span> SVC(kernel<span class="op">=</span><span class="st">&#39;linear&#39;</span>,tol<span class="op">=</span><span class="fl">0.1</span>)
clf.fit(np.array(data_train),np.array(label_train))
pred <span class="op">=</span> clf.predict(data_train)
<span class="bu">print</span> <span class="st">&#39;egitim&#39;</span>,show_auc(np.array(label_train.T)[<span class="dv">0</span>], pred)
pred <span class="op">=</span> clf.predict(data_test)
<span class="bu">print</span> <span class="st">&#39;test&#39;</span>,show_auc(np.array(label_test.T)[<span class="dv">0</span>], pred)</code></pre></div>
<pre><code>egitim (&#39;AUC&#39;, 0.76903032711566288)
test (&#39;AUC&#39;, 0.7533333333333333)</code></pre>
<p>Kaynaklar</p>
<p>[1] Blondel, <a href="https://gist.github.com/mblondel/586753" class="uri">https://gist.github.com/mblondel/586753</a></p>
<p>[2] Jebara, T., <em>Machine Learning Lecture, Columbia University</em></p>
<p>[3] Song, et al., <em>Stochastic gradient descent with differentially private updates</em></p>
<p>[4] Harrington, <em>Machine Learning in Action</em></p>
<p>[5] Stanford, <em>Stanford, CS246: Mining Massive Data Sets</em>, <a href="http://web.stanford.edu/class/cs246/" class="uri">http://web.stanford.edu/class/cs246/</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
