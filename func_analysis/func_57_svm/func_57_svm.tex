\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Destek Vektor Makinalarý (Support Vector Machines)

En basit halleriyle SVM'ler risk minimize eden lineer sýnýflayýcýsýdýrlar. 

$$
R(\Theta) \leq J(\Theta) = R_{emp}(\Theta) +
\sqrt{ \frac{h \times (\log(\frac{2N}{h}) + 1) - \log(\frac{\eta}{4})}{N}}
$$

h: sýnýflayýcýnýn kapasitesi 

N: eðitim verisinde kaç veri noktasý olduðu

Vapnik ve Chernovenkis $1-\eta$ olasýlýkla ispaladý ki üstteki denklem doðrudur. 
SVM algoritmasý hem $h$ deðerini hem de sayýsal, ölçümsel riski ayný
anda minimize etmektedir, ve bunu sýnýr noktalarýný noktalarýný
ayýrmakla yapmaktadýr. Türetelim, 

\includegraphics[height=6cm]{svm-planes.png}

Karar düzlemi: $w^{T}x + b=0$ 

Þöyle bir taným yapalým:

$$
q = \min_{x}\big\|x - 0\big\|, \quad w^T x+b=0 \textrm { sartina gore }
$$

$q$, $H^{+}$ ve $H^{-}$ formüllerini ileride kullanacaðýz.

Lagrange:

$$
\min_{x}\frac{1}{2} \big\|x - 0\big\|^2+\lambda(w^{T}x+b)
$$

Gradyaný alalým ve 0 deðerine eþitleyelim,

$$
\frac{\partial}{\partial x} ( \frac{1}{2} x^T x + \lambda( w^T x + b ) ) = 0
$$

$$ x + \lambda w = 0 $$

$$ x = -\lambda w $$

Üsteki sonucu $w^T x+b=0$ þartýna sokalým,

$$ w^T(-\lambda w) + b = 0 $$

$$ \lambda = \frac{b}{w^Tw} $$

Yani çözüm

$$ \hat{x} = - \bigg( \frac{b}{w^Tw}  \bigg) w $$

O zaman $q$

$$ q = || \hat{x}-0 || = \bigg|\bigg| -  \frac{b}{w^Tw} w \bigg|\bigg|  $$

$$ \big|\frac{b}{w^Tw}\big| \times \sqrt{w^Tw}  $$

$$ q = \frac{|b|}{||w||} $$

Taným:

$$ H^{+} = w^{T}x + b=+1 $$

$$ H^{-} = w^{T}x + b=-1 $$

grafikte görüldüðü gibi yani. Üstteki þekilde tanýmýn bir zararý yok (çünkü
+1,-1 sabit durunca ayraç geniþlemesi nasýl olacak diye düþünülebilir, ama bu
taným genelliði kaybetmeden yapabilabiliyor çünkü $b,w$ deðerlerinde hala
oynanabilir.

$q^{+}$ ve $q^{-}$ deðerlerini hesapla

$$ q^{+} = \frac{|b-1|}{||w||} $$

$$ q^{-} = \frac{|-b-1|}{||w||} $$

Ayraç o zaman þöyle 

$$ m=q^{+}+q^{-} =
\frac{|b-1-b-1|}{||w||} = \frac{|-2|}{||w||} = \frac{2}{||w||} $$

Ayraçlarýn olabildiðince ayýrmasýný istiyorsak $m$'i arttýrýz (yani
$\frac{2}{||w||}$'i maksimize ederiz), ya da $||w||$ deðerini minimize
ederiz. 

Sýnýrlar

Veri noktalarýný öyle sýnýflamak istiyoruz ki + ve - noktalar
hiperdüzlemlerin doðru noktalarýnda kalsýnlar. 

$$ w^{T}x+b \geq +1, \quad \forall y_{i}=+1   $$

$$ w^{T}x+b \leq -1, \quad \forall y_{i}=-1  $$

Bu iki denklemi birleþtirelim

$$ y_{i}(w^{T}x+b)-1 \geq 0  $$

Her þeyi biraraya koyalým

$$ \min_w \frac{1}{2}{||w||^2}, \quad y_{i}(w^Tx_{i}+b)-1 \ge 0
\textrm{ olsun. }$$


Bu form tanýdýk geliyor mu? Bu qp ile çözülebilecek karesel (quadratic)
bir formül, programdýr!

qp

Python dilinde cvxopt paketi vardýr Matlab Optimization Toolbox'da qp() var. QP
fonksiyonlarý problemleri genelde

$$\frac{1}{2}x^{T}Px+q^{T}x$$

formunda görmek isterler. Biraz önce elde ettiðimiz denklemi bu istenen formata
doðru ``masajlayabiliriz''

Ýkiz (dual)

SVM ihtiyaçlarý için ikiz formül (dual) ile çalýþmak daha rahattýr
Lagrange (tekrar) oluþturalým, türevi alalým, ve sýfýra eþitleyelim.
Bunun sonucunda elimize KKT noktalarý geçecektir

$$
L_{p} = \frac{1}{2}||w||^{2}-\sum_{i}\alpha_{i}(y_{i}(w^{T}x_{i}+b)-1)  
$$

$$
\frac{\partial}{\partial w} L_{p} = w-\sum_{i}\alpha_{i}y_{i}x_{i}=0  $$

$$
w = \sum_{i}\alpha_{i}y_{i}x_{i} 
$$

$$
\frac{\partial}{\partial b} L_{p} = -\sum_{i}\alpha_{i}y_{i}=0  
$$

Üstteki iki denklemi asal (primal) denkleme koyduðumuz zaman

$$
\textrm{ Maksimize et } L_{D}= \sum_{i}\alpha_{i}-\frac{1}{2}
\sum_{i} \sum_{j} \alpha_{i} \alpha_{j} y_{i}y_{j}x_{i}^{T}x_{j} 
$$

sýnýrlar

$$ \sum_{i}\alpha_{i}y_{i}=0  $$

$$ \alpha_{i} \geq 0  $$ 

qp

Bu yine qp() formunda bir problem! Sadece bu sefer çözeceðimiz deðiþkenler
$\alpha_i$'lar, $x$'lar deðil.  Üstteki denklem þu forma
$\frac{1}{2}x^{T}Px+q^{T}x$ masajlanabilir Bunun yapmak için $P_{i,j}$'ye
$-y_{i}y_{j}x_{i}^{T}x_{j}$ deðerini atarýz.  Ve qp'yi çaðýrýrýz Sonuç bir
$\alpha$'lar listesi olacaktýr.

$b$ deðerini hesaplamak

KKT koþulunun sebebiyle sýfýr olmayan her $\alpha_{i}$ için ana problemde ona
tekabül eden kýsýtlayýcý þart þýkýdýr (tight), yani bir eþitliktir.  O zaman
sýfýr olmayan her $\alpha_{i}$ için $b$'yi $w^{T}x_{i}+b = y_{i}$ ifadesini
kullanarak hesaplarýz.  Sýfýr olmayan her $\alpha_{i}$'dan gelen $b$ yaklaþýk
olarak diðer other $b$'lere eþit olacaktýr. Final $b$'yi hesaplamak için tüm
$b$'lerin ortalamasýný almak sayýsal (numeric) olarak daha garantidir.

Sýnýflayýcý Tamamlandý

Her yeni $x$ noktasý için artýk $sign(x^{T}w+b)$ ibaresini sýnýflayýcýmýz olarak
kullanabiliriz. $-1$ ya da $+1$ olarak geri gelecek sonuç bize yeni noktanýn
hangi sýnýfa ait olduðunu söyleyecektir.

Örnek Çýktý

\includegraphics[height=4cm]{svmlinear.png}

Çekirdekler (Kernels)

Þimdiye kadar lineer ayraçlardan bahsettik.  SVM'ler lineer olmayan
ayraçlarla da çalýþabilir.  Çok basit: Bir temel fonksiyon kullanarak
girdiyi daha yüksek boyuta doðru bir öniþlemden geçirirsek bunu
baþarabiliriz.  Algoritmanýn geri kalaný deðiþmeden kalacaktýr.

Gayri Lineer Çekirdek

\includegraphics[height=4cm]{svmpoly.png}

Esneme Payý Bazen bir problem ayrýlmaya müsait olmayabilir.  Çok üç
noktalardaki bazý noktalar sýnýflayýcýnýn çalýþmasýný imkansýz hale
getirebilir Bunun çözümü için sýnýflayýcýya "esneme payý" dahil
edebiliriz.  Mesela $y_{i}=+1$ için verinin yanlýþ tarafa düþmesini þu
durumda izin verebiliriz: $w^{T}+b \geq -0.03$ Fakat eklemek gerekir
ki bu tür noktalarýn ``çok fazla'' olmasýný da istemiyoruz, bu sebeple
bu "yanlýþ" noktalarýn sayýsýna da bir ceza getirebiliriz.

\begin{minted}[fontsize=\footnotesize]{python}
from numpy import linalg
import cvxopt
import cvxopt.solvers

def svm(X, y):
    n_samples, n_features = X.shape

    # Gram matrix
    K = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(n_samples):
            K[i,j] = np.dot(X[i], X[j])

    P = cvxopt.matrix(np.outer(y,y) * K)
    q = cvxopt.matrix(np.ones(n_samples) * -1)
    A = cvxopt.matrix(y, (1,n_samples))
    b = cvxopt.matrix(0.0)

    G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))
    h = cvxopt.matrix(np.zeros(n_samples))

    # solve QP problem
    solution = cvxopt.solvers.qp(P, q, G, h, A, b)

    print solution
    
    # Lagrange multipliers
    a = np.ravel(solution['x'])
    
    print "a", a

    # Support vectors have non zero lagrange multipliers
    ssv = a > 1e-5
    ind = np.arange(len(a))[ssv]
    a = a[ssv]
    sv = X[ssv]
    sv_y = y[ssv]
    print "%d support vectors out of %d points" % (len(a), n_samples)
    print "sv", sv
    print "sv_y", sv_y

    # Intercept
    b = 0
    for n in range(len(a)):
        b += sv_y[n]
        b -= np.sum(a * sv_y * K[ind[n],ssv])
    b /= len(a)
        
    # Weight vector
    w = np.zeros(n_features)
    for n in range(len(a)):
        w += a[n] * sv_y[n] * sv[n]

    print "a", a
    return w, b, sv_y, sv, a

X = np.array([[3.,3.],[4.,4.],[7.,7.],[8.,8.]])
y = np.array([1.,1.,-1.,-1.])
w, b, sv_y, sv, a = svm(X, y)
print "w", w
print "b", b
print 'test points'
print np.dot([2.,2.], w) + b # > 1
print np.dot([9.,9.], w) + b # < -1
\end{minted}

\begin{verbatim}
     pcost       dcost       gap    pres   dres
 0: -2.9061e-01 -5.0286e-01  6e+00  2e+00  1e+00
 1: -3.6857e-02 -3.0976e-01  3e-01  4e-16  1e-15
 2: -1.0255e-01 -1.2816e-01  3e-02  3e-17  7e-16
 3: -1.1074e-01 -1.1128e-01  5e-04  3e-17  7e-16
 4: -1.1111e-01 -1.1111e-01  5e-06  4e-17  7e-16
 5: -1.1111e-01 -1.1111e-01  5e-08  1e-17  6e-16
Optimal solution found.
{'status': 'optimal', 'dual slack': 7.403425105865883e-08, 'iterations': 5, 'relative gap': 4.79718822391507e-07, 'dual objective': -0.11111112756316754, 'gap': 5.330207369918724e-08, 'primal objective': -0.11111107426109389, 'primal slack': 2.7637512517768505e-08, 's': <4x1 matrix, tc='d'>, 'primal infeasibility': 1.077377601559697e-17, 'dual infeasibility': 6.043668397566901e-16, 'y': <1x1 matrix, tc='d'>, 'x': <4x1 matrix, tc='d'>, 'z': <4x1 matrix, tc='d'>}
a [  2.76375125e-08   1.11111073e-01   1.11111073e-01   2.76375125e-08]
2 support vectors out of 4 points
sv [[ 4.  4.]
 [ 7.  7.]]
sv_y [ 1. -1.]
a [ 0.11111107  0.11111107]
w [-0.33333322 -0.33333322]
b 3.66666541806
test points
2.33333253877
-2.33333253877
\end{verbatim}

Not: Ýkizdeki $L_d$'yi maksimize ediyoruz, fakat hala \verb!qp()!'deki
minimize ediciyi çaðýrýyoruz. Bu sebeple tüm $\alpha$'larýn toplamýný
temsil eden $q$'larýn negatifini alýyoruz, \verb!np.ones(n_samples) *-1!
iþleminde görüldüðü gibi. Formüldeki karesel kýsým içinde zaten
$-\frac{1}{2}$ negatif ibaresi var, böylece geri kalan formülün deðiþmesine
gerek yok.

Dayanaklý Kayýp Fonksiyonu ile SVM, Pegasos

SVM problemi alttaki fonksiyonu çözmek anlamýna geliyordu,

$$ \min_w \frac{1}{2}{||w||^2}, \textrm{ s.t. } \quad y_{i}(w^Tx_{i}+b)-1 \ge 0 $$

ki bu bir karesel program idi ve \verb!cvxopt! paketindeki \verb!qp! ile
çözülebiliyordu. Bazýlarý $b$ terimini de atýyorlar, ve 

$$  \min_w \frac{1}{2}{||w||^2} + \sum \max \{ 0, 1-y_i (w^T x_i) \}  $$

olarak yazýyorlar. Ayrýca regülarizasyonu kontrol etmek için bir $\lambda$
sabiti de ekleniyor, yani üstte $\lambda ||w||^2 / 2$ kullanýlmasý
lazým. Regülarize iþlemi $w$'nin norm'unun küçük olmasýný tercih eder, ki bu
bazý $w$ deðerlerinin sýfýra gitmesini zorlar, yani bir tür özellik seçme iþi bu
þekilde gerçekleþmiþ olur. Toplam iþleminin içindeki fonksiyona ``kayýp
fonksiyonu (loss function)'' ismi de verilir, eðer bu kayýp fonksiyonu tam
üstteki gibi ise ona dayanaklý kayýp (hinge loss) denir. Üstte görülen $\max$
ifadesi suna eþittir,

$$
Loss(w,x_i,y_i) = 
\left\{ \begin{array}{ll}
1-y_i \cdot (w \cdot x_i) & \textrm{ eðer } y_i \cdot (w \cdot x_i) < 1 \\
0 & \textrm { diðer }
\end{array} \right.
$$

Eðer kayýp fonksiyonunun gradyanýný alýrsak,

$$
\nabla L = \frac{\partial Loss(w,x_i,y_i)}{\partial w} =
\left\{ \begin{array}{ll}
-y_i  x_i & \textrm{ eðer } y_i \cdot (w \cdot x_i) < 1 \\
0 & \textrm { diðer }
\end{array} \right.
$$

Böylece bir rasgele gradyan iniþ (stochastic gradient descent) yaklaþýmýný
kodlayabiliriz. 

$$ w_{t+1} = w_t - \eta (\lambda w_t + \nabla L )$$

ki $\eta$ gradyanýn ne kadar güncellenme yapacaðýný kontrol eden bir sabittir.

Ufak Toptan Parçalar (Minibatching)

Güncelleme iþlemi tüm veri üzerinde, her veri noktasý için yapýlabilir, ya da
gradyan güncellemeleri toparlanarak belli sayýda adým sonrasý bir toplam
güncelleme yapýlýr. $b$ büyüklüðündeki ufak parça $B_t$ de rasgele seçilir, ve
$w$'ye uygulanýr [3].

$$
w_{t+1} = w_t - \eta \bigg(
\lambda w_t + \frac{1}{b} \sum _{x_i,y_i \in B_t} \nabla L
\bigg)
$$

\inputminted[fontsize=\footnotesize]{python}{pegasos.py}

\begin{minted}[fontsize=\footnotesize]{python}
import numpy as np, pandas as pd, pegasos, zipfile

with zipfile.ZipFile('svmdata.zip', 'r') as z:
    df =  pd.read_csv(z.open('features.txt'),sep=',')
    labels =  pd.read_csv(z.open('target.txt'))
    
print df.shape, labels.shape

data_train = df.head(5413)
data_test = df.tail(1000)
label_train = labels.head(5413)
label_test = labels.tail(1000)

from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score

def show_auc(d1, d2):
    fpr, tpr, thresholds = roc_curve(d1,d2)
    roc_auc = auc(fpr, tpr)
    return 'AUC', roc_auc
\end{minted}

\begin{verbatim}
(6413, 122) (6413, 1)
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
np.random.seed(0)
  
for epoch in [10,50,100,200]:
    for batch_size in [1,10,100]:
        w = pegasos.train_sgd(np.array(data_train),labels=np.array(label_train),
                              lam=1, iter=epoch,batch_size=batch_size)
        pred = pegasos.predict(w, data_train.T)
        score = show_auc(np.array(label_train.T)[0], pred[0])
        print 'iter', epoch, 'batch', batch_size, 'egitim', score
        pred = pegasos.predict(w, data_test.T)
        score = show_auc(np.array(label_test.T)[0], pred[0])
        print 'iter', epoch, 'batch', batch_size, 'test', score
\end{minted}

\begin{verbatim}
iter 10 batch 1 egitim ('AUC', 0.80632699788480933)
iter 10 batch 1 test ('AUC', 0.79744266666666663)
iter 10 batch 10 egitim ('AUC', 0.78954806549498469)
iter 10 batch 10 test ('AUC', 0.78614666666666666)
iter 10 batch 100 egitim ('AUC', 0.76682726584846694)
iter 10 batch 100 test ('AUC', 0.76497599999999999)
iter 50 batch 1 egitim ('AUC', 0.75623733098567281)
iter 50 batch 1 test ('AUC', 0.76376266666666659)
iter 50 batch 10 egitim ('AUC', 0.79475937530208407)
iter 50 batch 10 test ('AUC', 0.7964026666666667)
iter 50 batch 100 egitim ('AUC', 0.75772752003431121)
iter 50 batch 100 test ('AUC', 0.75512000000000001)
iter 100 batch 1 egitim ('AUC', 0.78479444205966464)
iter 100 batch 1 test ('AUC', 0.7882906666666667)
iter 100 batch 10 egitim ('AUC', 0.77260941046884191)
iter 100 batch 10 test ('AUC', 0.77070400000000006)
iter 100 batch 100 egitim ('AUC', 0.75931456118589935)
iter 100 batch 100 test ('AUC', 0.75702400000000003)
iter 200 batch 1 egitim ('AUC', 0.71345805340976809)
iter 200 batch 1 test ('AUC', 0.71764000000000006)
iter 200 batch 10 egitim ('AUC', 0.75268880326726773)
iter 200 batch 10 test ('AUC', 0.74913333333333343)
iter 200 batch 100 egitim ('AUC', 0.75917270896628253)
iter 200 batch 100 test ('AUC', 0.75757600000000003)
\end{verbatim}

Hazýr bir SVM kodu scikit-learn kütüphanesi karþýlaþtýralým, 

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.svm import SVC
clf = SVC(kernel='linear',tol=0.1)
clf.fit(np.array(data_train),np.array(label_train))
pred = clf.predict(data_train)
print 'egitim',show_auc(np.array(label_train.T)[0], pred)
pred = clf.predict(data_test)
print 'test',show_auc(np.array(label_test.T)[0], pred)
\end{minted}

\begin{verbatim}
egitim ('AUC', 0.76903032711566288)
test ('AUC', 0.7533333333333333)
\end{verbatim}

Kaynaklar

[1] Blondel, \url{https://gist.github.com/mblondel/586753}

[2] Jebara, T., {\em Machine Learning Lecture, Columbia University}

[3] Song, et al., {\em Stochastic gradient descent with differentially private updates}

[4] Harrington, {\em Machine Learning in Action}

[5] Stanford, {\em Stanford, CS246: Mining Massive Data Sets}, \url{http://web.stanford.edu/class/cs246/}

\end{document}
