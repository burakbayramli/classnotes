<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>

  <title>Dışbükey Optimizasyonuna (Convex Optimization) Giriş</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="dışbükey-optimizasyonuna-convex-optimization-giriş">Dışbükey Optimizasyonuna (Convex Optimization) Giriş</h1>
<p>Yapay öğrenme (machine learning) ve optimizasyonda sürekli optimizasyonu görürüz. Diğer disiplenlerde de görülür tabii ama bu ikisi benim ana konularım o yüzden o konulardan bu derste daha fazla bahsedeceğiz. Derste belirli bir amaç için gereken optimizasyon problemini çözmekten çok optimizasyon mekanizmasının detaylarını inceleyeceğiz. Optimallik şartlarına bakmak, varılan çözümün niteliğine bakmak bu detaylardan bazıları.</p>
<p>Şimdi aklınıza gelen bazı optimizasyon örneklerini verin bana [öğrenciler söylüyor]</p>
<ol style="list-style-type: decimal">
<li>Regresyon - En Az Kareler. Evet. Hata karelerinin toplamı minimize edilir burada, bir hedef <span class="math inline">\(y\)</span> vardır, onu bir formül üzerinden katsayıları olan bir denklem vardır, ve model uyum iyiliğini hata kare toplamı üzerinden ölçeriz.</li>
</ol>
<p><span class="math display">\[
\min_\beta \sum (y_i - x_i^T \beta)^2 
\]</span></p>
<p>Başka ne tür regresyon şekilleri var?</p>
<ol start="2" style="list-style-type: decimal">
<li>Regülarize Edilmiş Regresyon - Lasso. Burada yine hata karelerin toplamı var, ama üstüne katsayıların L1 norm'unu minimize etmeye çalışırız. Yani</li>
</ol>
<p><span class="math display">\[
\min_\beta \sum (y_i - x_i^T \beta)^2 \quad \textrm{oyle ki}
\]</span> <span class="math display">\[
\sum |\beta| \le t
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>En Az Mutlak Sapma Regresyonu (Least Absolute Deviations) - bu da benden. Bu tür regresyon ile kare yerine mutlak değer operasyonu kullanılıyor [1, 14:35].</li>
</ol>
<p><span class="math display">\[
\min_\beta \sum |y_i - x_i^T \beta |
\]</span></p>
<p>BU tür regresyon ile aykırı (outlier) değerlere daha az önem verilmiş olur. Fakat mutlak değer hesabı kullanınca optimizasyon zorlaşıyor çünkü üstteki formül artık pürüzsüz değil.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Sınıflama - Lojistik Regresyon. LR ile <span class="math inline">\(y_i\)</span> ikisel olur, 0 ya da 1. LR formülizasyonu normal regresyona benziyor,</p></li>
<li><p>Bilgisayar Bilim - Seyahet Eden Satış Görevlisi Problemi (TSP), Planlama, Ayrıksal Optimizasyon. Bu ders bloklarının sonunda Tam Sayı Programlama (İnteğer Programming) konusuna bakacağız, bu tür konulara orada daha çok yaklaşmış olacağız.</p></li>
<li><p>İstastistik - Maksimum Olurluk. MO istatistikte pek çok yaptığımız işin mihenk taşıdır. Hatta LR, En Az Kareler, vs aslinda MO'nun özel, spesifik halleridir. Burada vurgu içbükey olurluk elde etmek, ki bir içbükey fonksiyonu maksimize etmiş olalım, bu bir dışbükey fonksiyonu minimize etmek ile aynı şey.</p></li>
</ol>
<p>Böyle devam edebilirdik, optimizasyon örnekleri sayfalar doldurabilirdik. Optimizasyon her yerde. Ama belki de neyin optimizasyon olmadığına da bakmak iyi olur. Mesela istatistikte optimizasyon olmayan problemler nedir?</p>
<p>Hipotez test etmek, p-değerleri. Ya da takviyelemek (boosting), önemli bir konu ama optimizasyon değil. Rasgele Ormanlar (Random Forests), değil. Önyükleyiciler (bootstrap), çapraz-sağlama (cross-validation), yine değil [1, 22:09].</p>
<p>Ve iddiam şu ki optimizasyon olmayan konular hakkında olanlara kıyasla daha fazla teorik bilgimiz var. Üstteki teknikler çoğunlukla prosedürsel. Ama mesela Lasso diyelim, bu bir dışbükey optimizasyonun çıktısı olduğu için optimalite şartları üzerinden onun çözümünün özellikleri hakkında konuşmak kolaylaşıyor.</p>
<p>Peki biz niye bu dersteki konuyu öğrenmek isteriz, isteyebiliriz? Sonuçta Lasso'yu birisi bulmuş onun kodunu çağırırız, iş biter. Üç sebep var. Birincisi farklı algoritmalar duruma göre daha iyi performans gösterebilir, durum derken veriden bahsediyorum. Bu sebeple her algoritmanin özünü anlamak çok önemli. İkincisi herhangi bir alandaki problemi çözen optimizasyonun temelini bilmek bize alan hakkında ek görüş kazandırabilir.</p>
<p>Üçüncü sebep optimizasyon hızlı hareket eden bir alan, eğlenceli! Mesela optimizasyon alanındaki NIPS Çalıştayına (Workshop) bakarsanız, her sene değişiyor! Birkaç sene önce dışbükey olmayan optimizasyon büyük konuydu, tabii o zaman bu dersi işlerken utanır gibi oluyorduk çünkü bizim konu dışbükey optimizasyon ve yapay öğrenimdeki en büyük konferansta dışbükey olmayan konular işleniyor.. Fakat o zamanki odağın sebebi o zamanlarda bir sürü yeni dışbükey olmayan ve yakınsadığı ispat edilen metotların bulunmuş olmasıydı. Ama bir sonraki sene rasgele (stochastic) optimizasyon geri dönüş yapmıştı, rasgele gradyan inişi vs. Böyle her sene değişim oluyor, bu güzel bir şey demek ki hala ilerleme için oldukça alan var.</p>
<p>Ornekler</p>
<p>Bu orneklerin cogu tam varyasyon gurultu yoketmek (denoising) etrafinda, bunun bir diger ismi kaynasmis (fused) lasso. Elimizde iki boyutlu izgara halinde bir veri var, bir goruntu, <span class="math inline">\(i,j\)</span> kordinatlarinda bir renk degeri var, 3 ile 7 arasindaki renkler.</p>
<div class="figure">
<img src="func_19_intro_01.png" />

</div>
<p>En soldaki gerçek resim. Ortadaki ise onun gürültülü hali, bizim elimizdeki veri bu diyelim. Görüntüyü <span class="math inline">\(y\)</span> vektörü olarak temil edeceğiz, bu tek boyutlu ama düşünün ki görüntüdeki iki boyutu alıp düzleştirdik, tek vektör yaptık, alt alta satır satırları yanyana koyduk mesela, vs. Bu resim hakkında şunu biliyoruz, görüntü parçasal olarak sabit, yani yanyana hücreler birbirinden çok farklı değil. Bazı yerlerde olabilir mesela mavi arka plandan kırmızı objeye geçiş yapılan yerlerde, ama diğer yerlerde benzerlik var. Biz gürültülü resimden gürültüsüz resmi çıkartmak istiyoruz.</p>
<p>Gürültü yoketme alanında pek çok yöntem var. Fakat gürültü yoketme problemine optimizasyon açısından yaklaşabiliriz. Mesela, hedef kriteri şu haldeki bir optimizasyon problemi,</p>
<p><span class="math display">\[
\min_{\beta \in \mathbb{R}^n} 
\frac{1}{2} \sum_{i=1}^{n} (y_i - \beta_i)^2 + 
\lambda \sum_{(i,j) \in E)}  |\beta_i - \beta_j|
\]</span></p>
<p>İlk terimde aradığımız ideal resim ile gerçek resim arasındaki karesel kayıp hesabı var, yani her hücredeki <span class="math inline">\(\theta_i\)</span>'in olabildiği kadar <span class="math inline">\(y_i\)</span> verisine yakın olmasını istiyoruz. İkinci terimdeki <span class="math inline">\(\lambda\)</span> bizim dışarıdan atadığımız bir parametre, iki terim arasındaki dengeyi kuruyor. Bu parametrenin çarptığı ikinci terim bir ceza terimi. Yanyana olan her <span class="math inline">\(i,j\)</span>'ye bakıyor, sağda solda altta üstte olsun, bu hücrelerin renk farkını cezalandırıyor, yani farkın daha az olmasını zorluyor çünkü resimde genel olarak bir süreklilik olmasını istiyoruz. Oldukça sofistike bir işlem aslında, ama optimizasyon formülasyonu açısından oldukca basit. İki terim var, o kadar.</p>
<p>Çözüm resimde en sağdaki resimde görülüyor. <span class="math inline">\(\lambda=25\)</span> seçtim onun için, ve çözdüm. <span class="math inline">\(\lambda\)</span>'yi arttırdıkça resmin daha kaba görüntülü olmaya başladığını görebilirdiniz, mesela kırmızı ile pembe bölgeler birbiri içine geçmeye başlayabilirdi. <span class="math inline">\(\lambda=\infty\)</span> için ne olur? Her şey tek bir renk olur, o renk <span class="math inline">\(y\)</span>'nin ortalaması olurdu. <span class="math inline">\(\lambda=0\)</span> için gürültülü verinin aynısını elde ederiz.</p>
<p>Çözümü nasıl elde ettim? Üstteki sonucu ADMM ile elde ettim. Bu ders bloğunun sonunda bu algoritmayi göreceğiz. Bu problemde ADMM'in spesifik bir versiyonunu kullandım, bu versiyonun bu problemde iyi işleyeceğini biliyordum. 300x200 boyutunda bir resimdi, 20 döngü sonrası sonucu elde ettim, her döngüde lineer zaman harcadı. Tüm işleyişi bir saniyenin ufak bir parçasıydı.</p>
<p>Proksimal gradyan inişi ile 1000 kere döndük, sonuç fena değil ama bazı renkler tam birleşmedi. Eğer 10000 kere döndürseydim ADMM sonucuna yaklaşırdı. Bu metot ile de her döngüde lineer zaman harcanıyor, ama algoritmanın tamamı daha yavaş yakınsadı. Yani, amaç için doğru araç diyemeyiz.</p>
<p>Sonra kordina iniş adında çok popüler bir diğer metot işlettim, 10000 kere döndü, adımlar lineer zaman, ama yakınsama olmadı. Hatta sonuç oldukca kötüydü. Kesinlikle amaç için yanlış araç. Yani iyi ile kötü metot arasında boyutsal fark var (order of magnitude), işlem hızı bakımından 1, 2, daha kötü değil, 10, 100 kat daha kötüden bahsediyoruz, ve kalite iyi değil.</p>
<p>Bu arada kordinat inişini öğrenince üstteki kriteri nasıl kullandığım kafa karıştırabilir, cevap algoritmayi kriterin ikizi üzeride işlettim. Dersimizde ilerledikçe bunun anlamını öğreneceğiz. Bir problemin ikizini almak ve bu ikize algoritmaları nasıl uygulanacağını görmek.. bunları hep göreceğiz.</p>
<p>Mesajım ne? ADMM her yerde çok iyi işler demek mi? Hayır. ADMM bazı yerlerde daha kötü işler. Diğer yerlerde proksimal gradyan daha iyidir. Bu sebeple tüm seçenek yelpazesinin bilmek, her algoritmanin özelliklerini anlamak faydalıdır.</p>
<p>Bir diger ornek [1, 42:53]. Tam varyasyon gurultu yoketme yapiliyor yine ama burada iki boyuta bakmak yerine tek boyuta bakiyoruz, yani bazi acilardan bu problem daha kolay. Veri yine <span class="math inline">\(y_1,..,y_n\)</span> ama duzlestirilmis goruntu yerine tek bir eksende veri. Ayrica verinin ortalamasi parcasal sabit, yani tek duz cizgi.</p>
<p><span class="math display">\[
\min_\theta \frac{1}{2} (y_i-\theta_i)^2 + 
\lambda \sum_{i=1}^{n-1} |\theta_i - \theta_{i+1}|
\]</span></p>
<p>Burada ceza teriminde yanyana olan iki <span class="math inline">\(\theta\)</span>'nin farkini cezalandiriyoruz, yani yanyana verinin benzer olmasini istiyoruz.</p>
<div class="figure">
<img src="func_19_intro_02.png" />

</div>
<p>Veriye bakarsak iki bolge var, bir bolgede ortalama sabit digerinde de (baska) bir sabit. Ama algoritma bunu bilmiyor tabii onu kesfetmesi gerekecek. Eger <span class="math inline">\(\lambda\)</span> buyukse global ortalama ortaya cikiyor, tek cizgi. Goruntu orneginde soyledigimiz oluyor yani ama tek boyutta. <span class="math inline">\(\lambda\)</span> kuculdukce farkli ortalama bloklarinin ortaya cikmasini sagliyoruz. Ortadaki sonuc oldukca iyi. 3. resimde <span class="math inline">\(\lambda\)</span> biraz daha kucultuldu, burada bakiyoruz algoritma basta ufak bir blok daha yaratmayi secti. Bloklarin arasindaki noktaya &quot;degisim noktasi (changepoints)'' denir.</p>
<p>Bir değişim noktası elde edince, şimdi kendimize bir istatistiki soru sorabiliriz. Bu değişim noktalarının istatistiki önemi (significance) nedir? Görsel olarak ben bakınca diyorum ki 3. resimde sağdaki değişim noktası önemli ama o baştaki ufak değişim değil. O yapma (spurious) bir değişim herhalde. Tabii <span class="math inline">\(\lambda\)</span>'yi daha da ufaltsam daha da fazla uyduruk değişim noktaları elde ederdim. Optimizasyon probleminin özü böyle, ayar değişkeni <span class="math inline">\(\lambda\)</span> elde edilen sonuçlara, neye ne kadar ağırlık verildiğini kontrol ediyor. Fakat istatistiki öneme dönersek bu tür soruları sadece tam varyasyonu iyi anladığımız takdirde cevaplandırabiliriz.</p>
<div class="figure">
<img src="func_19_intro_03.png" />

</div>
<p>Çünkü istatistiki önem hesabı için mesela 1. blok ile 2. bloktaki noktaların ortalamasının farkına bakılır, ve bir Normal dağılım referans alınarak sıfır hipotezi test edilir, ve bu hipotez neredeyse her seferinde rededilecektir (yani test bloklar farklıdır diyor ama biz olmadığını görüyoruz). Niye böyle oldu? Çünkü optimizasyonun kriterine bakarsak biz orada aktif olarak ortalama farkını fazlalaştırmaya uğraşıyoruz. Ve tabii ki uğraştığımız şeyi test edince farklılık olduğunu buluyoruz. Bu doğru değil! Eğer optimizasyonun ne yaptığını bilmesek bu sonuca varamazdık.</p>
<p>Devam edelim; Bu dersin merkezi kavramı dışbükeylik. Tarihsel olarak ilk başta lineer programlar vardı, çok ciddi bir şekilde araştırıldı bu konu, koca dersler bu konuya harcandı. O zamanlar düşünülüyordu ki lineer olan ve olmayan ayrımı optimizasyonda en önemli ayrımdır. Bir tarafta çözebildiğimiz LP'ler var, diğer tarafta daha zor, çözülmez LP olmayan problemler.</p>
<p>Ama sonradan anlaşıldı ki bazı LP olmayan problemler aslında o kadar çözülemez değil. Mesela biraz önceki 1D lasso problemi LP değil ama çözülebiliyor. Ama tabii bazı LP olmayan ve çok çetin problemler de var.. Devam eden araştırmayla ortaya çıktı ki esas ayrım LP/olmayan değil, dışbükey / olmayan arasında. Çünkü dışbükey problemler ve olmayan problemler çok çok farklı mahlukatlar. Dişbukey problemlerde genel algoritmalardan bahsedebiliyoruz, bu algoritmalar bazı şartlarda iyi, kötü işleyebilir ama hepsinin ispatlanabilir yakınsanabilirliği var. Elimizde KKT optimallik şartları ve ikizlik gibi teorik araçlar var bu sayede dışbükeylikte elde edilen sonuçların özelliğini anlamamıza yardım ediyor.</p>
<p>Teoriye giriş yapalım artık.</p>
<p>Dışbukey Kümeler ve Fonksiyonlar</p>
<p>Dışbukey küme <span class="math inline">\(C \subseteq \mathbb{R}^n\)</span>, öyle ki</p>
<p><span class="math display">\[
x,y \in C \Rightarrow tx + (1-t) y \in C, \quad \forall 0 \le t \le 1
\]</span></p>
<p>Yani dışbükey küme <span class="math inline">\(C\)</span> de seceğim herhangi iki nokta arasında çekeceğim düz çizgi o küme içinde kalmalıdır [1, 1:00:24].</p>
<div class="figure">
<img src="func_19_intro_04.png" />

</div>
<p>Üstteki resimde soldaki küme dışbükey değil, sağdaki dışbükey.</p>
<p>Dışbükey fonksiyon <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, ki <span class="math inline">\(\mathrm{dom}{(f)} \subseteq \mathbb{R}^n\)</span> dışbükey olacak şekilde, ve</p>
<p><span class="math display">\[
f( tx + (1-t) y ) \le t f(x) + (1-t) f(y), \quad 0 \le t \le 1 \textrm{ için.} 
\]</span></p>
<p>Üstteki diyor ki dışbükey fonksiyonun tanım kümesi, alanı dışbükey küme olmalı, ki <span class="math inline">\(\mathbb{R}^n\)</span> öyledir, ve bu fonksiyonu herhangi iki noktada hesaplayınca elde ettiğim değer o iki nokta arasında çektiğim düz çizgi altında kalmalı.</p>
<div class="figure">
<img src="func_19_intro_05.png" />

</div>
<p>Tipik problem</p>
<p><span class="math display">\[
\min_{x \in D} f(x), \quad \textrm{öyle ki}
\]</span> <span class="math display">\[
g_i(x) \le 0, \quad i=1,..,m
\]</span> <span class="math display">\[
h_j(x) \le 0, \quad j=1,..,r
\]</span></p>
<p>ki <span class="math inline">\(D\)</span> her <span class="math inline">\(f,g,h\)</span> fonksiyonunun ortak tanım kümesi. Dişbukey optimizasyon probleminde <span class="math inline">\(f,g\)</span> dışbükey ve <span class="math inline">\(h\)</span> ilgin (affine) olmalıdır. <span class="math inline">\(f,g\)</span> üzerinden gösterilen şartlara uyan değerler olurlu (feasible) değerler olarak bilinir.</p>
<p>Dişbukey problemler için yerel minimum [1, 1:06:03] global minimumdur. Yani tek başına diğerlerinden izole bir yerel minima diye bir şey yoktur. Bu demektir ki eğer optimizasyon sırasında bir alt noktaya varırsanız, bu nokta global çözümdür.</p>
<p>Formel şekilde, bir <span class="math inline">\(x\)</span> noktası yerel minimumdur, eğer</p>
<p><span class="math display">\[
f(x) \le f(y) \quad ||x-y||_2 \le \rho \textrm{ ve her olurlu } y \textrm{ icin}
\]</span></p>
<p>doğru ise, yani alt noktadayım ve <span class="math inline">\(\rho\)</span> büyüklüğünde bir top içinde olurlu değerler üzerinden etrafa bakınca <span class="math inline">\(f(x)\)</span>'den daha ufak bir değer görmüyorum.</p>
<p>Dişbukey problemlerde</p>
<p><span class="math display">\[
f(x) \le f(y) \textrm{ her olurlu } y \textrm{ icin}
\]</span></p>
<p>ifadesi doğrudur, yani <span class="math inline">\(\rho\)</span> sonsuzluktur. Minimuma geldik, ne kadar uzağa bakarsak bakalım, sonsuz büyüklükte top içinde her yerde en minimum biziz.</p>
<p>İspatlayalım. Bunu çelişki ile ispat üzerinden yapacağız. Diyelim ki elimizde olurlu bir nokta <span class="math inline">\(z\)</span> var, yani <span class="math inline">\(\exists z \in D\)</span> ve öyle ki <span class="math inline">\(f(z) &lt; f(x)\)</span>. Bu <span class="math inline">\(z\)</span> noktası <span class="math inline">\(x\)</span>'den daha minimal. O zaman <span class="math inline">\(||z-x||_2 &gt; \rho\)</span> olmali, yerel optimal <span class="math inline">\(x\)</span>'in etrafındaki <span class="math inline">\(\rho\)</span> topunun dışındayım.</p>
<p>Şimdi <span class="math inline">\(x\)</span> ve <span class="math inline">\(z\)</span> arasındaki <span class="math inline">\(y\)</span> noktalarına bakalım,</p>
<p><span class="math display">\[
y = tx + (1-t) z, \quad 0 \le t \le 1
\]</span></p>
<p><span class="math inline">\(y\)</span> hakkında neler biliyoruz?</p>
<ul>
<li><p><span class="math inline">\(y \in D\)</span>? <span class="math inline">\(y\)</span> ortak küme içinde mi? <span class="math inline">\(x,y\)</span> küme içinde onların kesiştiği <span class="math inline">\(y\)</span> kümesi tabi ki <span class="math inline">\(D\)</span> içinde.</p></li>
<li><p><span class="math inline">\(y\)</span> olurlu mu? Evet.</p></li>
</ul>
<p><span class="math display">\[
g_i(tx + (1-t) z) \le t g_i(x) + (1-t) g_i(z)
\]</span></p>
<p><span class="math display">\[
\le 0
\]</span></p>
<p>Ayrıca, bunu ödev olarak kontrol edin,</p>
<p><span class="math display">\[
h_i(tx + (1-t) z) = 0
\]</span></p>
<p>çünkü <span class="math inline">\(h\)</span> lineer.</p>
<p>Yani <span class="math inline">\(y\)</span> olurlu, her kısıtlamaya uygun.</p>
<p>Ayrıca yeterince büyük (1'e yakın) <span class="math inline">\(t\)</span> için</p>
<p><span class="math display">\[
||x-y||_2 \le \rho
\]</span></p>
<p>demek istiyoruz ki <span class="math inline">\(x\)</span>'den <span class="math inline">\(z\)</span>'ye bir çizgi çekiyorum ve yeterince <span class="math inline">\(z\)</span>'ye yakın bir notkada <span class="math inline">\(\rho\)</span> topunun dışına çıkmış oluyorum.</p>
<div class="figure">
<img src="func_19_intro_06.png" />

</div>
<p>Güzel. Top içinde olurlu bir noktam var, tanım kümesi içinde, <span class="math inline">\(y\)</span> de orada. <span class="math inline">\(f(y)\)</span> hakkında ne söyleyebilirim?</p>
<p><span class="math display">\[
f( t x + (1-t) z) 
\]</span></p>
<p><span class="math inline">\(f\)</span> dışbükey değil mi? O zaman üsttekini dışbükeylik üzerinden açarsam,</p>
<p><span class="math display">\[
\le t f(x) + (1-t) f(z) 
\]</span></p>
<p>Ve biliyorum ki <span class="math inline">\(f(z) &lt; f(x)\)</span>, yani harfiyen küçüklük var, çünkü daha önce söylemiştik, <span class="math inline">\(x\)</span> global minimum değil, kriterler ışığında <span class="math inline">\(z\)</span> ondan daha iyi. Ayrıca üstte &quot;yeterince büyük <span class="math inline">\(t\)</span>'' dedik, bunun için, topun dışına çıkıyoruz, <span class="math inline">\(z\)</span>'ye yakınız ama tam <span class="math inline">\(z\)</span> değiliz. O zaman üstteki formül</p>
<p><span class="math display">\[
&lt; f(x)
\]</span></p>
<p>olacaktır. Şimdi çelişkiye geldik, top içinde öyle bir <span class="math inline">\(y\)</span> noktası bulduk bu nokta harfiyen <span class="math inline">\(f(x)\)</span>'den küçük ama bunu yapınca yerel minimum / optimumluk faraziyesini ihlal etmiş olduk.</p>
<p>Kaynaklar</p>
<p>[1] Tibshirani, <em>Convex Optimization, Lecture Video 7</em>, <a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg" class="uri">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a></p>
</body>
</html>
