\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Altgradyanlar (Subgradients) 

Altgradyanlar aslýnda bir algoritma deðil, bir matematiksel kavram [1,
40:29], ve hem optimizasyon, hem analiz, hem de pratik baðlamda çok faydalý
bir kavram.  Hatýrlarsak dýþbükey ve türevi alýnabilir bir $f$ için

$$
f(y) \ge f(x) + \nabla f(x)^T (y-x)  \quad \forall x,y
$$

gerekli ve yeterli bir þart. Yani fonksiyonuma herhangi bir noktada
oluþturacaðým teðet eðri, lineer yaklaþýksallýk, fonksiyonum için bir
global eksik / az tahmin edici (underestimator) olacaktýr, yani hep ondan
küçük kalacaktýr.

Altgradyan nedir? Altgradyan üstteki gradyanin yerini alabilecek herhangi
bir $g$ vektörüdür, yerine alabilecek derken üstteki ifade her $y$ için
hala doðru olacak þekilde. Dýþbükey fonksiyon $f$'nin $x$ noktasýnda
altgradyaný herhangi bir $g \in \mathbb{R}^n$'dir öyle ki

$$
f(y) \ge f(x) + g^T (y-x) \quad \forall y
$$

Teðet çizgi hakkýnda: görsel olaral hayal edersek kap þeklinde, yani
dýþbükey olan bir fonksiyona nerede teðet çizgi çekersem çekeyim
fonksiyonun kendisi hep o çizginin üstünde kalýr. Eðer fonksiyonum kap
olmasaydý, habire aþaðý yukarý inip çýkýyor olsaydý bir noktada o çizginin
altýna düþülebilirdi. Eðer $f$ türevi alýnabilir ise dýþbükey olmasýnýn
þartý üstteki ifadenin doðru olmasý.

Dýþbükey fonksiyonlar için 

1) $g$ her zaman mevcuttur (dýþbükey olmayan fonksiyonlar için $g$'nin
mevcudiyeti þart deðildir). Bu güzel bir özellik. 

2) Eðer $x$ noktasýnda $f$'in türevi alýnabilir ise, tek bir altgradyan
vardýr, o da türevin kendisidir [1, 43:12], $g = \nabla f(x)$.

Aslýnda \#2 kalemi dýþbükey olmayan bir $f$ için bile geçerli, eðer $g$
varsa. Bu durumlarda illa altgradyan olmasý gerekmiyor, hatta türevi
alýnabilir dýþbükey olmayan $f$ için bile $g$ olmayabiliyor. 

Dýþbükey olmayan (pürüzsüz) ve altgradyaný olmayan bir fonksiyon örneði
nedir? Alttaki,

\includegraphics[width=12em]{func_42_subgrad_01.png}

Bu fonksiyonun hiçbir yerde altgradyaný yok. Eðri üzerinde bir nokta
arýyorum öyle ki oradan geçen bir çizgi tüm fonksiyonu üstte
býraksýn.. böyle bir çizgi çizilemez. Altgradyan yok [1,
43:54]. Bazýlarýmýz itiraz edebilir, ``üstteki bir içbükey fonksiyon,
dýþbükeyin ters çevrilmiþ hali''. O zaman $x^3$ diyelim, pürüzsüz, ve
altgradyaný yok.

Altgradyaný mevcut fonksiyonlar görelim, mesela mutlak deðer fonksiyonu
$f(x) = |x|$.

\includegraphics[width=15em]{func_42_subgrad_02.png}

Altgradyanlar için farklý þartlarý görelim.

$x>0$ için tek bir altgradyan var, o da $g = 1$, yani fonksiyonun eðiminin
ta kendisi, eðim=1. Ayný þekilde $x<0$ için, o zaman $g=-1$. Bu sonuç
``eðer $f$'in $x$'te türevi alýnabilir ise o noktada $g=\nabla f$''
açýklamasý ile uyuyor. $x=0$ noktasý için birçok seçenek var, herhangi bir
$[-1,1]$ öðesi için, yani -1 ve +1 arasýndaki herhangi bir sayý olabilir,
çizgili noktalar seçeneklerden ikisi.

Boyut atlayalým, $f(x) = ||x||_2$ fonksiyonunu görelim, $x$'in L2
norm'u. Ýki boyutta [1, 45:51],

\includegraphics[width=15em]{func_42_subgrad_03.png}

Eðer $x \ne 0$ ise bu fonksiyonun türevi alýnabilir (yoksa alýnamaz, bir
yaygýn görüþe göre $x=0$'da problem yok, ama var) ve altgradyaný onun
mevcut gradyaný, $x / ||x||_2$. $x=0$ noktasýnda altgradyan $g$
$\{ z: ||z||_2 < 1\}$ kümesinin herhangi bir öðesi.

Þimdi $f(x) = ||x||_1$'e bakalým,

\includegraphics[width=15em]{func_42_subgrad_04.png}

Bu fonksiyonun $x=0$'da türevi alýnamaz, aynen tek boyutlu (mutlak deðer
fonksiyonu) versiyonunda olduðu gibi. Ayrýca bu fonksiyonun herhangi bir
eksende sýfýr deðer olduðu zamanda da türevi alýnamaz. Altgradyan için öðe öðe
yaklaþmak lazým, eðer bir öðe $x_i \ne 0$ ise $g_i = \sign(x_i)$, eðer
$x_i = 0$ ise $g_i \in [-1,+1]$.

En son örnek [1, 48:35] iki dýþbükey fonksiyonun maksimumu olaný, yani
$f(x) = \max \{f_1(x),f_2(x) \}$ ki $f_1,f_2$ dýþbükey ve türevi alýnabilir
olmak üzere, ve $f(x)$ bu iki fonksiyonun her $x$ noktasýnda $f_1(x)$ ve
$f_2(x)$'den hangisi büyükse o. Bu tür bir maks fonksiyonunun sonucunun
dýþbükey olduðunu önceki derslerden biliyoruz.

\includegraphics[width=15em]{func_42_subgrad_05.png}

Altgradyan yine farklý þartlara göre deðiþik oluyor. Eðer $f_1(x) > f_2(x)$
o zaman altgradyan özgün, $g = \nabla f_1(x)$. Eðer $f_2(x) > f_1(x)$ ise
altgradyan özgün, $g = \nabla f_2(x)$. 

Kabaca çizersek birbirlerini kesen $f_1$ ve $f_2$ düþünelim, 

\includegraphics[width=15em]{func_42_subgrad_06.png}

onlarýn maks halleri yeþil ile [çok kabaca benim eklediðim] çizgi, yani
kesiþmenin solunda $f_2$ saðýnda $f_1$. Tabii ki sol tarafta $f_2$ aktif o
zaman onun gradyaný geçerli, sað tarafta $f_1$. Kesiþme noktasý, $f_1=f_2$
ilginç, $g = \alpha \nabla f_1(x) + (1-\alpha) f_2(x)$, yani $f_1,f_2$'nin
herhangi bir dýþbükey kombinasyonu, ki iki üstteki resimde görülen iki
kesikli çizgiler bazý örnekler. 

Altdiferansiyel (Subdifferential)

Diþbükey $f$'in tüm altgradyanlarýna altdiferansiyel denir [1,
52:35]. Çoðunlukla kýsmi türev için kullanýlan ayný sembolle gösterilir,
$\partial$ ile.

$$
\partial f(x) = \{ 
g \in \mathbb{R}^n: \quad g, f\textrm{'in altgradyanýdýr}
\}
$$

Yani $x$ noktasýndaki tüm mümkün altgradyanlarýn kümesi altdiferansiyel
oluyor. 

1) $\partial f(x)$ kapalý ve dýþbükey bir kümedir. Ýþin ilginç tarafý bu
dýþbükey olmayan $f$'ler için bile geçerlidir. Niye olduðuna bakalým,
$\partial f(x)$ $x$'te $f(x)$'in tüm altgradyanlarýdýr. Diyelim ki
$g_1,g_2$ altgradyanlarý bu altdiferansiyel kümesinde, $g_1 \in \partial
f(x)$ ve $g_2 \in \partial f(x)$. Simdi $\alpha g_1 + (1-\alpha) g_2$
nerededir ona bakalým [1, 53:59]. Bu deðerin $y-x$ ile iþ çarpýmýný alýrsak
ve ona $f(x)$ eklersek acaba $f(y)$'den büyük bir deðer elde eder miyiz?

$$
(\alpha g_1 + (1-\alpha) g_2)^T (y-x) + f(x) 
\underbrace{\le}_{?} f(y) \quad \forall y
\mlabel{1}
$$

Üsttekini ispatlayabilirsek $\partial f(x)$'in bir dýþbükey küme olduðunu
ispatlayabilirim, çünkü iki geçerli altgradyanýn herhangi bir dýþbükey
kombinasyonunu almýþým ve hala küme içindeysem o küme dýþbükey küme
demektir. 

Alttaki iki ifadenin doðru olduðunu biliyoruz, 

$$
 g_1^T (y-x) + f(x) \le f(y) 
$$

$$
 g_2^T (y-x) + f(x) \le f(y) 
$$

Eðer iki üstteki ifadeyi $\alpha$ ile bir üstteki ifadeyi $1-\alpha$ ile
çarparsam ve toplarsam, basitleþtirme sonrasý (1)'i elde ederim. Ýspat
böylece tamamlanýr [1, 55:11].

Dikkat edersek $f$'nin dýþbükey olup olmadýðýndan bahsetmedik bile. 

2) Boþ Olmamak: eðer $f$ dýþbükey ise $\partial f(x)$ boþ deðildir.

3) Tek Altgradyan: önceden bahsettik ama eðer $f$ $x$ noktasýnda türevi
alýnabilir ise altdiferansiyelde tek bir öðe vardýr o da o noktadaki
gradyandýr, $\partial f(x) = \{ \nabla f(x) \}$. 

4) Üstteki özelliðe tersten bakarsak, eðer $\partial f(x) = \{ g \}$, yani
altdiferansiyelde tek bir öðe var ise, o zaman $f$ o noktada türevi
alýnabilir demektir ve o noktadaki gradyan $g$'dir. 

[disbukey geometri baglantisi atlanti]

Altdiferansiyel Calculus

Altgradyanlarýn kendine has bir Calculus'u var, aynen gradyanlarý,
vs. içeren Çok Deðiþkenli Calculus'ta olduðu gibi [1, 59:53]. Birazdan
göstereceklerimizden daha fazlasý ama alttakiler en faydalý olanlarý [1,
1:00:00]. Diþbukey $f$ fonksiyonlarý için alttakiler geçerlidir,

Ölçekleme: $\partial (af) = a \cdot \partial f$, $a$ sabit ise ve $a > 0$
olacak þekilde

Toplama: $\partial (f_1 + f_2) = \partial f_1 + \partial f_2$

Doðrusal Bileþim: Eðer $g(x) = f(Ax + b)$ ise o zaman
$\partial g(x) = A^T \partial f(Ax + b)$. Bu altgradyanlar için bir tür
Zincirleme Kanunu gibi. Hatta eðer $f$ türevi alýnabilir ise, bu ifade tamý
tamýna Zincirleme Kanunu olurdu. 

Noktasal Sonlu Maksimum: Eðer $f(x) = \max_{i=1,..,m} f_i(x)$ ise, o zaman 

$$
\partial f(x) = conv \left( 
\bigcup_{i: f_i(x)=f(x)} \partial f_i(x)
\right)
$$

Biraz karmaþýk duruyor ama daha önce iki fonksiyon maksimumu üzerinden
gördüðümüz kavrama benziyor. Her noktada maks olan $f_i$'leri alýyoruz, ve
bu fonksiyonlarýn altgradyanlarýný hesaplýyoruz. Ama bu altgradyanlarýn
birleþimi her zaman bir dýþbükey küme oluþturmayabilir, ve
altdiferansiyelin bir dýþbükey küme olmasý gerekir, o zaman için elimizde
olan altgradyanlarýn $conv$ ile dýþbükey zarfýna (convex hull)
bakarýz. Yani sadece birleþim $\cup$ ile elde ettiðim kümeyi bir iþlemden
daha geçirerek onun dýþbükey küme halini alýyorum.

[atlandi, norm, 1:09:00]

Niye Altgradyanlar? 

1) Optimizasyon: Önemli bir sebep [1, 1:12:00]. Bir dýþbükey fonksiyonun
altgradyanýný hesaplamak her zaman mümkündür, o zaman her dýþbükey
fonksiyonu minimize edebilirim. Bazý durumlarda bu yavaþ olabilir ama en
azýndan minimizasyon mümkün olur.

2) Dýþbükey Analizi: Her $f$ için, dýþbükey olsun olmasýn, 

$$
f(x^*) = \min_x f(x) \iff 0 \in \partial f(x^*)
$$

Yani $x^*$ bir minimize edicidir sadece ve sadece 0 deðeri $f$'in $x^*$
noktasýnda bir altgradyaný ise. Bu özelliðe çoðunlukla ``altgradyan
optimalliði'' adý veriliyor. Ýspatý basit. Eðer $g$ vektörü $x^*$
noktasýndaki altgradyan ise o zaman alttaki ifade her $y$ için doðrudur,

$$
f(y) \ge f(x^*) + 0^T (y-x^*) = f(x^*)
$$

$$
f(y) \ge f(x^*) \quad \forall y
$$

Üstteki ifade $x^*$ bir minimize edicidir diyor, o zaman sýfýr bir
altgradyandýr. 

Bazen üstteki ifadenin dýþbükey olmayan fonksiyonlar için bile geçerli
olduðunu unutanlar oluyor [1:14:32]. Bu her $f$ için doðru diyorum bazen
bana þaþýrmýþ þekilde bakýyorlar. Söylenen biraz sürpriz edici,
evet. Ýkizlik ve KKT þartlarý hakkýnda konuþurken benzer þaþýrtýcý ifadeler
olacak.

Tabii eklemek gerekir bazen dýþbükey olmayan fonksiyonlar için altgradyan
hesaplanamaz, ya da mevcut deðillerdir. Her problemi çözmek için bir yemek
tarifi deðil bu. Mesela baþta gördüðümüz içbükey fonksiyon,

\includegraphics[width=15em]{func_42_subgrad_01.png}

Altgradyaný yok (ama tabii minimize edicisi de yok). 

Altgradyanlarla devam edelim [2, 01:11], onlar bir dýþbükey fonksiyonun
gradyaný kavramýnýn genelleþtirilmiþ hali idi. 

Bir dikkat edilmesi gereken durum var ama, altgradyanlar bir dýþbükey
fonksiyon için her zaman mevcuttur, ama bunu spesifik olarak ``taným
kümesinin nispeten iç bölgelerinde olacak þekilde'' diye vurgulamak
gerekir. Mesela gösterge fonksiyonu $I$'nin uç noktalarýnda mevcut deðildir.

Þimdi altgradyan yönteminin gücüne bir örnek görelim. Derslerimizin baþýnda
1. derece optimallik þartýný görmüþtük [2, 05:30],

$$
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
\mlabel{3}
$$

problemini çözmek istiyoruz, diyelim $f$ dýþbükey ve türevi alýnabilir. Bu
problem için $x$'in çözüm olmasýnýn þartý 

$$
\nabla f(x)^T (y-x) \ge 0 \quad \forall y \in C
$$

eþitsizliðinin doðru olmasýdýr. Yani 1. derece minimallik gradyan sýfýrý
verir, o zaman herhangi bir $\nabla f(x)^T(y-x)$ yönünde adým atmak bizi
her zaman bu minimallikten uzaklaþtýrmalýdýr. Bu durum her olurlu $y \in C$
için doðru ise minimal yerdeyiz demektir [2, 05:50]. Ya da þöyle anlatalým,
$x$ noktasýndayýz, $y$ noktasýna gitmeyi düþünüyoruz. O zaman $y-x$
vektörünü oluþturuyoruz, ve su soruyu soruyoruz, ``kriter fonksiyonunun
gradyaný ayný çizgi de mi?''. Eðer ayný yönde ise o yönde hareket etmek
kriter $f(x)$'i arttýrýr. Yani eðer gradyan her mümkün olurlu yön ile aþaðý
yukarý ayný yönü gösteriyorsa (azaltma / çoðaltma, -90/+90 derece
baðlamýnda) o zaman minimum noktadayýz demektir. 

\includegraphics[width=10em]{func_42_subgrad_07.png}

Ýþte bunu altgradyan perspektifinden ispatlayabiliriz [2, 06:33]. 

Üsttekini altgradyan perspektifinden ispatlayabiliriz. Önce problemimizi
sýnýrsýz bir formatta tekrar tanýmlayacaðýz.  Sýnýrlamayý bir gösterge $I_C$
haline getirerek bunu yapabiliriz,

$$
\min_x f(x) + I_C(x)
\mlabel{2}
$$

ki $I_C(x) = 0$ eðer $x$, $C$ kümesi içindeyse, dýþýndaysa sonsuzluk. Þimdi
üstteki fonksiyona altgradyan optimalliði uygulayalým, eðer üstteki
fonksiyonu minimize eden bir nokta varsa elimde, bunun tercümesi sýfýrýn o
noktada fonksiyonun altgradyaný olmasý. Fonksiyonun altgradyanýný
hesaplayalým, kurallarýmýza göre iki dýþbükey fonksiyon toplamýnýn
altgradyani o fonksiyonlarýn ayrý ayrý altgradyanlarýnýn toplamý. $f$
dýþbükey, $I_C$ dýþbükey (çünkü $C$ kümesi dýþbükey küme). $f$ pürüzsüz, o
zaman $x$'te onun altgradyan kümesi sadece o noktadaki gradyan. $I_C$'nin
altgradyaný normal koni $N_C$. O zaman 

$$
0 \in \partial ( f(x) + I_C(x) ) 
$$

$$
\iff 0 \in \nabla f(x) + N_C(x) 
$$


olmalý, ya da

$$
\iff - \nabla f(x) \in N_C(x) 
$$

olmalý. Þimdi normal koniyi hatýrlayalým, tanýmý

$$
N_C(x) = \{ g \in \mathbb{R}^n: g^T x \ge g^T y \quad \forall y \in C
$$

buna göre iki üstteki $N_C$, $g=-\nabla f$ üzerinden

$$
\iff -\nabla f(x)^T x \ge -\nabla f(x)^T y \quad \forall y \in C
$$

olarak açýlabilir. Ya da

$$
\iff \nabla f(x)^T(y-x) \ge 0 \quad \forall y \in C
$$

Üstteki 1. derece optimallik þartýna benziyor zaten. $-\nabla f(x)$ üstteki
tanýmýn bir öðesidir, o zaman 0 altgradyan kümesinin öðesidir. 

Ýþte gayet temiz bir þekilde optimallik ispatý yapmýþ olduk. Bu arada
sýnýrlama içeren optimizasiyon problemi için alttaki taným

$$
0 \in \partial f(x) + N_c
$$

ifadesi her nasýlsa tamamen genel, yani dýþbükey bir problem tanýmý için
gerekli ve yeterli bir þart çünkü hatýrlarsak bahsettik ki tüm dýþbükey
problemleri (2) ya da (3) formunda öne sürmek mümkün. Tabii üstteki formlea
iþ yapmak kolay deðildir, çünkü $N_C$ ile çalýþmak zor. Eðer $C$ çetrefil
bir küme ise, mesela

$$
C = \{ x: g_i(x) \le 0, Ax = b \}
$$

gibi, o zaman normal koniyi oluþturmak zor olacaktýr. Yani iki üstteki
tanýmýn her zaman faydalý olduðunu söyleyemeyiz, ama her dýþbükey problem
için gerekli ve yeterli þart olduðunu söyleyebiliyoruz.

Sonradan optimalliði tanýmlamanýn farklý bir yolunu göreceðiz. Sýnýrlama
ifadeleri olduðu zaman problemler daha az çetin / çözülür hale gelir,
problemler sýnýrsýz-sýnýrlý halde birbirine eþit þekilde tanýmlanabilirler,
ama sýnýrlý tanýmlarý çözmek daha kolay. KKT koþullarý burada devreye
girecek.  Yani her þeyi kritere týkmak, gösterge vs ile uðraþmak,
altgradyan almak yerine bu tür tanýmla çalýþmak daha rahat oluyor [2, 12:00].

Altgradyan optimalliðinin bazý diðer örneklerini görelim, mesela Lasso
için altgradyan optimalliði. Bazýlarýnýn bilebileceði üzere Lasso
problemini parametrize etmenin iki yolu vardýr, birisi katsayýlar üzerinde
bir L1 norm kýsýtlamasý tanýmlamak, diðeri ise alttaki gibi onu kritere
dahil etmek, 

$$
\min_\beta \frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1
\mlabel{5}
$$

ki $\lambda \ge 0$. Altgradyan optimalliðinde sadece ve sadece alttaki þart
geçerliyse elimizde bir çözüm var diyebiliyoruz, bu þart, 

$$
0 \in \big( 
\frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1
\big)
$$

yani eðer 0 kriterimin altgradyan kümesinde ise. Üstteki altgradyanýn
uygulandýðý toplam iþaretinin iki tarafý da dýþbükey o zaman onlarý
altgradyanlarýn toplamý olarak açabilirim, ayrýca soldaki terim bir de
pürüzsüz olduðu için tek altgradyan normal gradyandýr,

$$
\iff 0 \in -X^T (y - X\beta) + \lambda \partial ||\beta||_1
$$

$$
\iff X^T (y - X\beta) = \lambda v 
\mlabel{4}
$$

herhangi bir $v \in \partial ||\beta||_1$ için. L1 norm'un altgradyaný için
daha önce gördüðümüz üzere bileþen bileþen bakmak gerekiyor, ve farklý
þartlara göre parçalý bir fonksiyon elde edeceðiz, 

$$
v_i \in
\left\{ \begin{array}{ll}
\{1\} & \textrm{eðer } \beta_i > 0 \\
\{-1\} & \textrm{eðer } \beta_i < 0 \\
{[} -1,+1{]} & \textrm{eger } \beta_i = 0 
\end{array} \right.
$$

Yeni öyle bir $\beta$ arýyorum ki herhangi bir $v$ vektörü için (4)'u
tatmin edecek ve bu $v$ geçerli bir altgradyan olacak, yani üstteki
þartlara uyacak. O zaman çözüme eriþmiþim demektir. Çözümü þu anda
vermiyoruz, bunlar çözüm için uyulmasý gereken optimallik þartlarý [2,
15:24].

Her $\beta_i$ için üstteki denklemin nasýl oluþacaðýný görmek istersek, ve
$X_1,..,X_p$ deðerleri $X$ matrisinin kolonlarý olacak þekilde

$$
\left[\begin{array}{ccc}
\uparrow & \uparrow & \\
&& \\
X_1 & X_2 & \dots \\
&& \\
\downarrow & \downarrow & 
\end{array}\right]^T
\left[\begin{array}{c}
y_1 \\ \vdots \\ y_p 
\end{array}\right] 
- 
\left[\begin{array}{ccc}
\uparrow & \uparrow & \\
&& \\
X_1 & X_2 & \dots \\
&& \\
\downarrow & \downarrow & 
\end{array}\right]
\left[\begin{array}{c}
\beta_1 \\ \vdots \\ \beta_p 
\end{array}\right]  =
\lambda
\left[\begin{array}{c}
v_1 \\ \vdots \\ v_p 
\end{array}\right] 
$$

O zaman bunu her $v_i$ olasýlýðý için yazarsak, $\beta_i$'in sýfýr olup
olmadýðý üzerinden bir parçalý fonksiyon ortaya çýkartabiliriz. Altgradyan
optimallik þartý,

$$
\left\{ \begin{array}{ll}
X_i^T (y-X\beta) = \lambda \cdot \sign(\beta_i) & \textrm{eger } \beta_i \ne 0 \\
| X_i^T (y-X\beta)  | \le \lambda  & \textrm{eger } \beta_i = 0 
\end{array} \right.
$$

haline geldi. Ýkinci satýrý nasýl elde ettik? Eðer $\beta_i=0$ ise bu bana
$\lambda v$ ifadesi $-\lambda$ ve $+\lambda$ arasýnda herhangi bir yerde
olabilir diyor (çünkü $v_i$ parçalý fonksiyonunda $\beta_i=0$ ise $v_i$ -1
ve +1 arasý herhangi bir deðer dedik), ve $-\lambda$ ve $+\lambda$ arasý
olma durumunu son satýrdaki mutlak deðer ifadesine tercüme edebiliriz.

Dikkat, üstteki ifade optimalliðe bakma / kontrol etmek için bir
yöntem. Birisi size bir vektör veriyor [2, 16:57], sonra soruyor ``bu
vektör Lasso kriterine göre optimal midir?'' Öyle olup olmadýðýna bakmak
için vektörün her ögesine bakýyoruz, ve üstteki kontrolü iþletiyoruz. Eðer
her öge optimal ise evet diyoruz, tek bir öðe bile optimal deðilse hayýr
diyoruz.

Üstteki parçalý formüldeki ikinci bölümü ilginç bir þekilde
kullanabiliriz. Diyelim ki 100 deðiþkenlik modeli Lasso ile veriye
uydurduk, ve $\beta$ katsayýlarý elde ettik, bir regresyon yaptýk
yani. Diyelim ki çözümden sonra birisi geliyor size 101. kolon veriyor,
acaba tüm uydurma iþlemini baþtan tekrar mý yapmak lazým? Belki hayýr, 
$| X_{101}^T (y-X\beta)  | \le \lambda$ kontrolünü yaparýz, eðer koþul
doðru ise o zaman $\beta_{100} = 0$ demektir, ve bu katsayýya gerek
yoktur, modelin geri kalaný deðiþmeden kalýr [2, 20:42]. 

Bir diðer ilginç uygulama Lasso'nun basitleþtirilmiþ hali; $X = I$ yani
birim matrisi olduðu durum. Bu yaklaþýmla bazýlarýnýn gürültü silme
(denoising) dediði iþlemi yapabilmiþ oluyoruz. $X=I$ deyince Lasso'da geri
kalan, 

$$
\min_\beta \frac{1}{2} || y - \beta ||_2^2 + \lambda ||\beta||_1
$$

Bu problem ifadesi diyor ki ``öyle bir $\beta$ vektörü bul ki $y$'ye
olabildiði kadar yakýn olsun ve $\beta$ üzerinde bir L1 cezasý
olsun''. Yana bana içinde bir sürü gözlem noktasý taþýyan bir $y$ veriliyor
ve ben bu gözlemleri en iyi þekilde yaklaþýklayan $\beta$'yi arýyorum ve bu
$\beta$'nin seyrek olmasýný [2, 24:23] tercih ediyorum (L1 cezasý ile bu
oluyor, büyük deðerler cezalandýrýlýnca çözü katsayýnýn sýfýra yakýn
olmasýný özendirmiþ oluruz). Artýk biliyoruz ki üstteki problemi altgradyan
optimalliði ile çözmek mümkün.

Daha önce gördüðümüz Lasso altgradyan optimalliðini $X=I$  için tekrar
yazarsak

$$
\left\{ \begin{array}{ll}
(y-\beta_i) = \lambda \cdot \sign(\beta_i) & \textrm{eger } \beta_i \ne 0 \\
| |y-\beta_i|  | \le \lambda  & \textrm{eger } \beta_i = 0 
\end{array} \right.
$$

Çözüm $\beta = S_\lambda(y)$, ki $S_\lambda(y)$'ye yumuþak eþikleme
(soft-threshold) operatörü deniyor. Üstteki optimalliðe uyan bir çözüm,
hatta tek çözüm, budur.

$$
[ S_\lambda(y) ]_i = 
\left\{ \begin{array}{ll}
y_i - \lambda & \textrm{eðer } y_i > \lambda_i \\
0 & \textrm{eðer } -\lambda \ge y_i \ge \lambda, \quad i=1,..,n \\
y_i + \lambda & \textrm{eðer } y_i < -\lambda_i 
\end{array} \right.
$$

Çözümün optimallik þartlarýna uyup uymadýðý rahatça kontrol
edilebilir. Formülde $\beta=S_\lambda(y)$ diyerek alttakilerin doðru olup
olmadýðýna bakarýz, 

Eðer $y_i > \lambda, \beta_i = y_i - \lambda > 0$ ise $y_i-\beta_i =
\lambda = \lambda \cdot 1$ mý?

Eðer $y_i < -\lambda$ ise benzer þekilde

Eðer $|y_i| \ge \lambda, \beta_i=0$ ise $|y_i-\beta_i| = |y_i| \ge \lambda$ mi?

[distance to convex set örneði atlandý]

Daha önce gradyan iniþi (gradient descent) algoritmasýný görmüþtük, bu
algoritma çok basittir. Þimdi iþleri biraz daha zorlaþtýracaðýz [2,
48:00]. Bu metotun bir dezavantajý optimize edilen $f$'nin türevi
alýnabilir bir fonksiyon olma zorunluluðu. Diðer bir dezavantaj
yakýnsamanýn uzun zaman alabilmesi.

Altgradyan Metodu

Gradyan iniþi yapýsýna benziyor, $f$'in dýþbükey olmasý ve
$\dom(f) = \mathbb{R}^n$ olmasý lazým, ama $f$'nin pürüzsüz olma
zorunluluðu yok.

Gradyan iniþi gibi özyineli bir þekilde, $x^{(0)}$'dan baþlýyoruz, ve

$$
x^{(k)} = x^{(k-1)} - t_k \cdot g^{(k-1)}, \quad k=1,2,3,..
$$

adým atarak ilerliyoruz, öyle ki $g^{(k-1)} \in \partial f(x^{(k-1)})$
yani $f$'nin $x^{(k-1)}$ noktasýndaki herhangi bir altgradyaný. Adým ata
ata gidiyorum, her adýmda mevcut altgradyanlara bakýyorum, herhangi birini
seçiyorum, ona $g^{(k-1)}$ diyelim, ve $x$'i bu yönde olacak þekilde bir
$t_k$'ye oranlý olarak güncelliyorum [2, 49:50]. 

Altgradyan metotunun ilginç özelliklerinden biri her adýmda iniþ yapmanýn
garanti olmamasý (herhalde onun için ``altgradyan iniþi'' yerine
``altgradyan metotu'' ismi verilmiþ). Bu sebeple adým atarken o ana kadar,
yani $x^{(0)},.., x^{(k)}$ içinde olan en iyi (best) (en minimal) noktayý
hatýrlamak gerekiyor, $x_{best}^{(k)}$, ki 

$$
f(x_{best}^{(k)}) = \min_{i=0,..,k} f(x^{(i)})
$$

Eðer mesela altgradyan metotunun 100,000 adým iþletmiþsem eriþilmiþ minimal
nokta olarak bu hatýrlanan en iyi noktayý sonuç olarak rapor ederim.

Adým büyüklüðü nasýl seçilir?

Sabit adým büyüklüðü seçmek bir seçenek. Küçükçe seçilen böyle bir
büyüklük iþler. 

Çokça kullanýlan bir diðer seçenek ``gittikçe yokolan'' adým büyüklüðü. Bu
tür adým seçimi için kullanabilecek pek çok kural var, aranan bir nitelik
sýfýra gidilmesi ama çok hýzlý gidilmemesi. Mesela $t_k = 1/k$ uygun.
Altgradyanlarda geriye iz sürmenin karþýlýðý yok.

Yakýnsama analizi

Altgradyan metotunun yakýnsama analizi gradyan iniþinin analizinden biraz
farklý [2, 54:15]. Diyelim ki elimizde bir dýþbükey fonksiyon $f$ var ve
taným kümesi herþey, $\dom(f) = \mathbb{R}^n$. Fonksiyon ayrýca Lipschitz
sürekli, fonksiyonun sürekli olduðunun söylemiyoruz dikkat, fonksiyon sabit
$G > 0$ üzerinden Lipschitz sürekli, yani

$$
|f(x) - f(y)| \ge G || x-y ||_2 \quad \forall x,y
$$

Bunu baz alarak iki tane teori öne sürebiliriz, birisi sabit adým
büyüklüðü, diðeri azalan adým büyüklüðü için.

Yokolan adým büyüklüðü için iki þart tanýmlayalým, ``kare toplanabilir ama
toplanabilir deðil'', yani þu iki þart,

$$
\sum  _{k=1}^{\infty} t_k^2 < \infty, \quad \sum  _{k=1}^{\infty} t_k - \infty
$$

Þimdi iki teoriyi tanýmlayabiliriz,

Teori 1

Sabitlenmiþ $t$ için altgradyan metotu alttaki þartý tanýmlar, 

$$
\lim_{k \to \infty} f(x_{best}^{(k)} ) \le f^* + G^2 t / 2
$$

Üstteki ifade diyor ki eðer altgradyan metotunu sonsuza kadar iþletirsek
eldeki en iyi noktadan elde edilecek fonksiyon deðeri gerçek optimum artý
Lipschitz sabitinin karesi çarpý $t/2$'dan küçük olacaktýr, ki $t$
sabitlenmiþ adým büyüklüðü. Ama üstteki yine de çözüm için bir limit
vermiyor. Onun için alttaki lazým,

Teori 2

Yokolan adým büyüklükleri için

$$
\lim_{k \to \infty} f(x_{best}^{(k)} ) = f^*
$$

[ek detaylar atlandý]

Altgradyan metotuna bir örnek olarak [2, 1:01:36] regülarize edilmiþ
lojistik regresyona bakabiliriz. $\beta$ katsayýlarýný bulmaya uðraþýyoruz,
ve veriye uydurma baðlamýnda bir kayýp fonksiyonunu minimize etmeye
uðracaðýz. Önce normal regresyon,

$$
f(\beta) = \sum _{i=1}^{n} \left(-y_ix_i^T\beta + \log( 1 + \exp (x_i^T\beta) \right) 
$$

Üstteki pürüzsüz ve dýþbükey bir fonksiyon. 

$$
\nabla f(\beta) = \sum _{i=1}^{n} (y_i - p_i(\beta)) x_i
$$

ki $p_i(x) = \exp(x_i^T\beta) / (1+ \exp(x_i^T\beta))$, $i=1,..,n$. 

Regülarize edilmiþ lojistik regresyon

$$
\min_\beta f(\beta) + \lambda \cdot P(\beta)
$$

ki $P(\beta) = ||\beta||_2^2$ olabilir (Ridge cezasý) ya da
$P(\beta) = ||\beta|_1|$ (Lasso cezasý). Bu cezalardan ilki pürüzsüz,
diðeri deðil. Böylece birinde gradyan iniþi diðerinde altgradyan metotu
kullanmak zorunda olacaðýz.

Ekler

Alttaki örnek [3]'ten,

$$
\min_w F(w) = \frac{1}{2} \sum _{i=1}^{N} (x_i^T w - y_i)^2 + \lambda ||w||_1
$$

gibi bir Lasso örneði var. 

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def subgrad(w):
    return X.T*(X*w-y) + lamda*np.sign(w)

def obj(w):
    r = X*w-y;
    return np.sum(np.multiply(r,r))/2 + lamda * np.sum(np.abs(w))

N = 40
dim = 10
max_iter = 200
lamda = 1/np.sqrt(N);
np.random.seed(50)
w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T
X = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim), size = N))
y = X*w

w = np.matrix([0.0]*dim).T
obj_SD = []
gamma = 0.01
for t in range(0, max_iter):
    obj_val = obj(w)
    w = w - gamma * subgrad(w)/np.sqrt(t+1)
    obj_SD.append(obj_val.item())
    if (t%5==0): print('iter= {},\tobjective= {:3f}'.format(t, obj_val.item()))

print (w)
\end{minted}

\begin{verbatim}
iter= 0,	objective= 169.279279
iter= 5,	objective= 24.721959
iter= 10,	objective= 13.195682
iter= 15,	objective= 8.739994
iter= 20,	objective= 6.419780
iter= 25,	objective= 5.034598
iter= 30,	objective= 4.138547
iter= 35,	objective= 3.526977
iter= 40,	objective= 3.093047
iter= 45,	objective= 2.775883
iter= 50,	objective= 2.538530
iter= 55,	objective= 2.357443
iter= 60,	objective= 2.217031
iter= 65,	objective= 2.106647
iter= 70,	objective= 2.018829
iter= 75,	objective= 1.948229
iter= 80,	objective= 1.890943
iter= 85,	objective= 1.844077
iter= 90,	objective= 1.805447
iter= 95,	objective= 1.773392
iter= 100,	objective= 1.746629
iter= 105,	objective= 1.724158
iter= 110,	objective= 1.705193
iter= 115,	objective= 1.689110
iter= 120,	objective= 1.675411
iter= 125,	objective= 1.663694
iter= 130,	objective= 1.653633
iter= 135,	objective= 1.644963
iter= 140,	objective= 1.637466
iter= 145,	objective= 1.630964
iter= 150,	objective= 1.625306
iter= 155,	objective= 1.620369
iter= 160,	objective= 1.616050
iter= 165,	objective= 1.612262
iter= 170,	objective= 1.608931
iter= 175,	objective= 1.605996
iter= 180,	objective= 1.603403
iter= 185,	objective= 1.601108
iter= 190,	objective= 1.599073
iter= 195,	objective= 1.597264
[[-1.53942055]
 [-0.02366012]
 [-0.61081721]
 [-1.43597808]
 [ 1.3626909 ]
 [-0.47342589]
 [-0.78826118]
 [ 1.04965236]
 [-1.27159815]
 [-1.32969646]]
\end{verbatim}

Bir diðer örnek [4]'ten, burada kodlanan (5)'teki formül aslýnda, kodun
sembollerini kullanýrsak,  

$$
\min_x \frac{1}{2} || Ax - b||_2 + \lambda ||x||_1
$$

L1 normu $||x||_1$ gradyan iniþi için problemli o sebeple altgradyan metotu
kullanacaðýz. Veri olarak [5]'te görülen diyabet verisini alabiliriz,

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.io as sio
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import numpy.linalg as lin

def subgrad_func(A,b,lam):
    n2 = A.shape[1]
    x = np.zeros((n2,1))
    k=1
    g = np.ones((n2,1))
    t = 0.01
    f = []
    while True:
        if k>3: 
            crit = np.abs(f[k-2]-f[k-3])/f[k-2]
            if crit < 1e-5: break
        tmp = 0.5*lin.norm(np.dot(A,x)-b,2)**2+lam*lin.norm(x,1);        
        if k%10==0: print (tmp)
        f.append(tmp)
        s = x.copy()
        s[x>0]=1
        s[x<0]=-1
        if len(s[x==0])>0: 
            s[x==0] = -2*np.random.rand(len(x==0))+1
        g = np.dot(A.T,np.dot(A,x)-b) + lam*s
        x = x - t*g
        k = k+1
    return x
                
diabetes = pd.read_csv("../../stat/stat_120_regular/diabetes.csv",sep=';')
y = np.array(diabetes['response'].astype(float)).reshape(442,1)
A = np.array(diabetes.drop("response",axis=1))

lam = 0.1;
x = subgrad_func(A,y,lam);
print ('x')
print (x)
\end{minted}

\begin{verbatim}
6167530.253256479
6017359.332614086
5936043.378499364
5887955.958919385
5856837.417105291
5835069.931049544
5818932.590413047
5806489.16976122
5796653.745550204
5788760.388544399
5782366.953999686
5777155.717300746
5772889.40103409
5769385.329280358
5766499.620202982
5764117.531938736
5762146.838816163
5760513.025122407
5759155.660694774
5758025.5987459235
5757082.775086282
5756294.462375693
5755633.876252789
x
[[   5.34807751]
 [-200.87752927]
 [ 490.9420537 ]
 [ 304.14877939]
 [ -43.03872491]
 [-107.97583692]
 [-207.47086585]
 [ 127.15527296]
 [ 410.70880916]
 [ 115.41867007]]
\end{verbatim}

Bir dýþbükey kümesi üzerinden tanýmlý ama pürüzsüz olmayabilecek bir
dýþbükey fonksiyonu minimize etmek için yansýtýlan altgradyan (projected
subgradient) metotu adlý bir metot ta kullanýlabilir [2, 22:04].  

Dýþbükey $f$'yi dýþbükey küme $C$ üzerinden optimize etmek için 

$$
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
$$

$$
x^{(k)} = P_C \left( x^{(k-1)} - t_k g_k^{(k-1)} \right)
$$

Bu metot normal altgradyan metotu gibi, tek fark parantez içinde görülen
altgradyan adýmý atýldýktan sonra elde edilen sonucun $C$ kümesine geri
yansýtýlmasý (projection), çünkü atýlan adým sonucunda olurlu bir sonuç
elde etmemiþ olabiliriz. 

Yakýnsama analizi normal altgraydan metotuna benziyor, bu metotla da benzer
yakýnsama garantisi elde edilebiliyor, yakýnsama oraný da buna dahil.

Yansýtma adýmý bazen zor olabilir, hangi durumlarda kolay olduðunun listesi
aþaðýda [2, 23:53]. Hangi kümelere yansýtmak kolaydýr?

1) Doðrusal görüntüler: $\{  Ax + b: x \in \mathbb{R}^n  \}$

2) $\{ x: Ax = b \}$ sisteminin çözüm kümesi 

3) Negatif olmayan bölge: $\mathbb{R}_{+}^n = \{x: x \ge 0 \}$. Verilen
vektörün negatif deðerlerinin sýfýr yapmak, pozitifleri tutmak bize o
vektörün negatif olmayan bölge karþýlýðýný veriyor. 

4) Bazý norm toplarý $\{ x: ||x||_p \le 1 \}$, $p=1,2,..,\infty$ için. Daha
önce 2-norm topuna yansýtmayý gördük, vektörü alýp normalize edersek bu
kümeye yansýtma yapmýþ oluyoruz aslýnda. Bu yansýtma bizi o vektörün 2-norm
topundaki en yakýn diðer vektöre götürüyor. Sonsuz norm kolay, bir kutuya
yansýtma yapmýþ oluyoruz, bunun ne demek olduðunu düþünmeyi size
býrakýyorum, 1-norm en zoru. 

5) Bazý çokyüzlüler (polyhedra) ve basit koniler.

Bir uyarýda bulunalým, tanýmý basit duran kümeler ortaya çýkartmak
kolaydýr, fakat bu kümelere yansýtma yapan operatörler çok zor
olabilir. Mesela geliþigüzel bir çokyüzlü $C = \{ x: Ax \le b \}$ kümesine
yansýtma yapmak zordur. Bu problemin kendisi apayrý bir optimizasyon
problemi aslýnda, bir QP. 

[stochastic subgradient method atlandý]

Altgradyanlarýn iyi tarafý genel uygulanabilirlik. Altgradyan metotlarý
dýþbükey, Lipschitz fonksiyonlarý üzerinde bir anlamda optimaldir. Ünlü
bilimci Nesterov'un bu baðlamda bir teorisi vardýr [2, 45:12], pürüzsüz
durumlarda ve 1. derece yöntemlerde altgradyanlarýn lineer kombinasyonlarý
üzerinden güncellemenin sonucu olan adýmlarýn baþarýsýna bir alt sýnýr
tanýmlar. 


Kaynaklar

[1] Tibshirani, {\em Convex Optimization, Lecture Video 6}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

[2] Tibshirani, {\em Convex Optimization, Lecture Video 7}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

[3] He, {\em IE 598 - BIG DATA OPTIMIZATION},  
    \url{http://niaohe.ise.illinois.edu/IE598_2016/}

[4] Feng, {\em Lasso}, 
    \url{https://github.com/fengcls/Lasso}

[5] Bayramlý, {\em Istatistik, Regresyon, Ridge, Lasso, Çapraz Saðlama, Regülarize Etmek

\end{document}
