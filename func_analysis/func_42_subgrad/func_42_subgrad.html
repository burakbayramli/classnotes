<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>

  <title>Altgradyanlar (Subgradients)</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="altgradyanlar-subgradients">Altgradyanlar (Subgradients)</h1>
<p>Altgradyanlar aslında bir algoritma değil, bir matematiksel kavram [1, 40:29], ve hem optimizasyon, hem analiz, hem de pratik bağlamda çok faydalı bir kavram. Hatırlarsak dışbükey ve türevi alınabilir bir <span class="math inline">\(f\)</span> için</p>
<p><span class="math display">\[
f(y) \ge f(x) + \nabla f(x)^T (y-x)  \quad \forall x,y
\]</span></p>
<p>gerekli ve yeterli bir şart. Yani fonksiyonuma herhangi bir noktada oluşturacağım teğet eğri, lineer yaklaşıksallık, fonksiyonum için bir global eksik / az tahmin edici (underestimator) olacaktır, yani hep ondan küçük kalacaktır.</p>
<p>Altgradyan nedir? Altgradyan üstteki gradyanin yerini alabilecek herhangi bir <span class="math inline">\(g\)</span> vektörüdür, yerine alabilecek derken üstteki ifade her <span class="math inline">\(y\)</span> için hala doğru olacak şekilde. Dışbükey fonksiyon <span class="math inline">\(f\)</span>'nin <span class="math inline">\(x\)</span> noktasında altgradyanı herhangi bir <span class="math inline">\(g \in \mathbb{R}^n\)</span>'dir öyle ki</p>
<p><span class="math display">\[
f(y) \ge f(x) + g^T (y-x) \quad \forall y
\]</span></p>
<p>Teğet çizgi hakkında: görsel olaral hayal edersek kap şeklinde, yani dışbükey olan bir fonksiyona nerede teğet çizgi çekersem çekeyim fonksiyonun kendisi hep o çizginin üstünde kalır. Eğer fonksiyonum kap olmasaydı, habire aşağı yukarı inip çıkıyor olsaydı bir noktada o çizginin altına düşülebilirdi. Eğer <span class="math inline">\(f\)</span> türevi alınabilir ise dışbükey olmasının şartı üstteki ifadenin doğru olması.</p>
<p>Dışbükey fonksiyonlar için</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(g\)</span> her zaman mevcuttur (dışbükey olmayan fonksiyonlar için <span class="math inline">\(g\)</span>'nin mevcudiyeti şart değildir). Bu güzel bir özellik.</p></li>
<li><p>Eğer <span class="math inline">\(x\)</span> noktasında <span class="math inline">\(f\)</span>'in türevi alınabilir ise, tek bir altgradyan vardır, o da türevin kendisidir [1, 43:12], <span class="math inline">\(g = \nabla f(x)\)</span>.</p></li>
</ol>
<p>Aslında #2 kalemi dışbükey olmayan bir <span class="math inline">\(f\)</span> için bile geçerli, eğer <span class="math inline">\(g\)</span> varsa. Bu durumlarda illa altgradyan olması gerekmiyor, hatta türevi alınabilir dışbükey olmayan <span class="math inline">\(f\)</span> için bile <span class="math inline">\(g\)</span> olmayabiliyor.</p>
<p>Dışbükey olmayan (pürüzsüz) ve altgradyanı olmayan bir fonksiyon örneği nedir? Alttaki,</p>
<div class="figure">
<img src="func_42_subgrad_01.png" />

</div>
<p>Bu fonksiyonun hiçbir yerde altgradyanı yok. Eğri üzerinde bir nokta arıyorum öyle ki oradan geçen bir çizgi tüm fonksiyonu üstte bıraksın.. böyle bir çizgi çizilemez. Altgradyan yok [1, 43:54]. Bazılarımız itiraz edebilir, &quot;üstteki bir içbükey fonksiyon, dışbükeyin ters çevrilmiş hali''. O zaman <span class="math inline">\(x^3\)</span> diyelim, pürüzsüz, ve altgradyanı yok.</p>
<p>Altgradyanı mevcut fonksiyonlar görelim, mesela mutlak değer fonksiyonu <span class="math inline">\(f(x) = |x|\)</span>.</p>
<div class="figure">
<img src="func_42_subgrad_02.png" />

</div>
<p>Altgradyanlar için farklı şartları görelim.</p>
<p><span class="math inline">\(x&gt;0\)</span> için tek bir altgradyan var, o da <span class="math inline">\(g = 1\)</span>, yani fonksiyonun eğiminin ta kendisi, eğim=1. Aynı şekilde <span class="math inline">\(x&lt;0\)</span> için, o zaman <span class="math inline">\(g=-1\)</span>. Bu sonuç &quot;eğer <span class="math inline">\(f\)</span>'in <span class="math inline">\(x\)</span>'te türevi alınabilir ise o noktada <span class="math inline">\(g=\nabla f\)</span>'' açıklaması ile uyuyor. <span class="math inline">\(x=0\)</span> noktası için birçok seçenek var, herhangi bir <span class="math inline">\([-1,1]\)</span> öğesi için, yani -1 ve +1 arasındaki herhangi bir sayı olabilir, çizgili noktalar seçeneklerden ikisi.</p>
<p>Boyut atlayalım, <span class="math inline">\(f(x) = ||x||_2\)</span> fonksiyonunu görelim, <span class="math inline">\(x\)</span>'in L2 norm'u. İki boyutta [1, 45:51],</p>
<div class="figure">
<img src="func_42_subgrad_03.png" />

</div>
<p>Eğer <span class="math inline">\(x \ne 0\)</span> ise bu fonksiyonun türevi alınabilir (yoksa alınamaz, bir yaygın görüşe göre <span class="math inline">\(x=0\)</span>'da problem yok, ama var) ve altgradyanı onun mevcut gradyanı, <span class="math inline">\(x / ||x||_2\)</span>. <span class="math inline">\(x=0\)</span> noktasında altgradyan <span class="math inline">\(g\)</span> <span class="math inline">\(\{ z: ||z||_2 &lt; 1\}\)</span> kümesinin herhangi bir öğesi.</p>
<p>Şimdi <span class="math inline">\(f(x) = ||x||_1\)</span>'e bakalım,</p>
<div class="figure">
<img src="func_42_subgrad_04.png" />

</div>
<p>Bu fonksiyonun <span class="math inline">\(x=0\)</span>'da türevi alınamaz, aynen tek boyutlu (mutlak değer fonksiyonu) versiyonunda olduğu gibi. Ayrıca bu fonksiyonun herhangi bir eksende sıfır değer olduğu zamanda da türevi alınamaz. Altgradyan için öğe öğe yaklaşmak lazım, eğer bir öğe <span class="math inline">\(x_i \ne 0\)</span> ise <span class="math inline">\(g_i = \mathrm{sign}(x_i)\)</span>, eğer <span class="math inline">\(x_i = 0\)</span> ise <span class="math inline">\(g_i \in [-1,+1]\)</span>.</p>
<p>En son örnek [1, 48:35] iki dışbükey fonksiyonun maksimumu olanı, yani <span class="math inline">\(f(x) = \max \{f_1(x),f_2(x) \}\)</span> ki <span class="math inline">\(f_1,f_2\)</span> dışbükey ve türevi alınabilir olmak üzere, ve <span class="math inline">\(f(x)\)</span> bu iki fonksiyonun her <span class="math inline">\(x\)</span> noktasında <span class="math inline">\(f_1(x)\)</span> ve <span class="math inline">\(f_2(x)\)</span>'den hangisi büyükse o. Bu tür bir maks fonksiyonunun sonucunun dışbükey olduğunu önceki derslerden biliyoruz.</p>
<div class="figure">
<img src="func_42_subgrad_05.png" />

</div>
<p>Altgradyan yine farklı şartlara göre değişik oluyor. Eğer <span class="math inline">\(f_1(x) &gt; f_2(x)\)</span> o zaman altgradyan özgün, <span class="math inline">\(g = \nabla f_1(x)\)</span>. Eğer <span class="math inline">\(f_2(x) &gt; f_1(x)\)</span> ise altgradyan özgün, <span class="math inline">\(g = \nabla f_2(x)\)</span>.</p>
<p>Kabaca çizersek birbirlerini kesen <span class="math inline">\(f_1\)</span> ve <span class="math inline">\(f_2\)</span> düşünelim,</p>
<div class="figure">
<img src="func_42_subgrad_06.png" />

</div>
<p>onların maks halleri yeşil ile [çok kabaca benim eklediğim] çizgi, yani kesişmenin solunda <span class="math inline">\(f_2\)</span> sağında <span class="math inline">\(f_1\)</span>. Tabii ki sol tarafta <span class="math inline">\(f_2\)</span> aktif o zaman onun gradyanı geçerli, sağ tarafta <span class="math inline">\(f_1\)</span>. Kesişme noktası, <span class="math inline">\(f_1=f_2\)</span> ilginç, <span class="math inline">\(g = \alpha \nabla f_1(x) + (1-\alpha) f_2(x)\)</span>, yani <span class="math inline">\(f_1,f_2\)</span>'nin herhangi bir dışbükey kombinasyonu, ki iki üstteki resimde görülen iki kesikli çizgiler bazı örnekler.</p>
<p>Altdiferansiyel (Subdifferential)</p>
<p>Dişbükey <span class="math inline">\(f\)</span>'in tüm altgradyanlarına altdiferansiyel denir [1, 52:35]. Çoğunlukla kısmi türev için kullanılan aynı sembolle gösterilir, <span class="math inline">\(\partial\)</span> ile.</p>
<p><span class="math display">\[
\partial f(x) = \{ 
g \in \mathbb{R}^n: \quad g, f\textrm{&#39;in altgradyanıdır}
\}
\]</span></p>
<p>Yani <span class="math inline">\(x\)</span> noktasındaki tüm mümkün altgradyanların kümesi altdiferansiyel oluyor.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\partial f(x)\)</span> kapalı ve dışbükey bir kümedir. İşin ilginç tarafı bu dışbükey olmayan <span class="math inline">\(f\)</span>'ler için bile geçerlidir. Niye olduğuna bakalım, <span class="math inline">\(\partial f(x)\)</span> <span class="math inline">\(x\)</span>'te <span class="math inline">\(f(x)\)</span>'in tüm altgradyanlarıdır. Diyelim ki <span class="math inline">\(g_1,g_2\)</span> altgradyanları bu altdiferansiyel kümesinde, <span class="math inline">\(g_1 \in \partial f(x)\)</span> ve <span class="math inline">\(g_2 \in \partial f(x)\)</span>. Simdi <span class="math inline">\(\alpha g_1 + (1-\alpha) g_2\)</span> nerededir ona bakalım [1, 53:59]. Bu değerin <span class="math inline">\(y-x\)</span> ile iş çarpımını alırsak ve ona <span class="math inline">\(f(x)\)</span> eklersek acaba <span class="math inline">\(f(y)\)</span>'den büyük bir değer elde eder miyiz?</li>
</ol>
<p><span class="math display">\[
(\alpha g_1 + (1-\alpha) g_2)^T (y-x) + f(x) 
\underbrace{\le}_{?} f(y) \quad \forall y
\qquad (1)
\]</span></p>
<p>Üsttekini ispatlayabilirsek <span class="math inline">\(\partial f(x)\)</span>'in bir dışbükey küme olduğunu ispatlayabilirim, çünkü iki geçerli altgradyanın herhangi bir dışbükey kombinasyonunu almışım ve hala küme içindeysem o küme dışbükey küme demektir.</p>
<p>Alttaki iki ifadenin doğru olduğunu biliyoruz,</p>
<p><span class="math display">\[
 g_1^T (y-x) + f(x) \le f(y) 
\]</span></p>
<p><span class="math display">\[
 g_2^T (y-x) + f(x) \le f(y) 
\]</span></p>
<p>Eğer iki üstteki ifadeyi <span class="math inline">\(\alpha\)</span> ile bir üstteki ifadeyi <span class="math inline">\(1-\alpha\)</span> ile çarparsam ve toplarsam, basitleştirme sonrası (1)'i elde ederim. İspat böylece tamamlanır [1, 55:11].</p>
<p>Dikkat edersek <span class="math inline">\(f\)</span>'nin dışbükey olup olmadığından bahsetmedik bile.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Boş Olmamak: eğer <span class="math inline">\(f\)</span> dışbükey ise <span class="math inline">\(\partial f(x)\)</span> boş değildir.</p></li>
<li><p>Tek Altgradyan: önceden bahsettik ama eğer <span class="math inline">\(f\)</span> <span class="math inline">\(x\)</span> noktasında türevi alınabilir ise altdiferansiyelde tek bir öğe vardır o da o noktadaki gradyandır, <span class="math inline">\(\partial f(x) = \{ \nabla f(x) \}\)</span>.</p></li>
<li><p>Üstteki özelliğe tersten bakarsak, eğer <span class="math inline">\(\partial f(x) = \{ g \}\)</span>, yani altdiferansiyelde tek bir öğe var ise, o zaman <span class="math inline">\(f\)</span> o noktada türevi alınabilir demektir ve o noktadaki gradyan <span class="math inline">\(g\)</span>'dir.</p></li>
</ol>
<p>[disbukey geometri baglantisi atlanti]</p>
<p>Altdiferansiyel Calculus</p>
<p>Altgradyanların kendine has bir Calculus'u var, aynen gradyanları, vs. içeren Çok Değişkenli Calculus'ta olduğu gibi [1, 59:53]. Birazdan göstereceklerimizden daha fazlası ama alttakiler en faydalı olanları [1, 1:00:00]. Dişbukey <span class="math inline">\(f\)</span> fonksiyonları için alttakiler geçerlidir,</p>
<p>Ölçekleme: <span class="math inline">\(\partial (af) = a \cdot \partial f\)</span>, <span class="math inline">\(a\)</span> sabit ise ve <span class="math inline">\(a &gt; 0\)</span> olacak şekilde</p>
<p>Toplama: <span class="math inline">\(\partial (f_1 + f_2) = \partial f_1 + \partial f_2\)</span></p>
<p>Doğrusal Bileşim: Eğer <span class="math inline">\(g(x) = f(Ax + b)\)</span> ise o zaman <span class="math inline">\(\partial g(x) = A^T \partial f(Ax + b)\)</span>. Bu altgradyanlar için bir tür Zincirleme Kanunu gibi. Hatta eğer <span class="math inline">\(f\)</span> türevi alınabilir ise, bu ifade tamı tamına Zincirleme Kanunu olurdu.</p>
<p>Noktasal Sonlu Maksimum: Eğer <span class="math inline">\(f(x) = \max_{i=1,..,m} f_i(x)\)</span> ise, o zaman</p>
<p><span class="math display">\[
\partial f(x) = conv \left( 
\bigcup_{i: f_i(x)=f(x)} \partial f_i(x)
\right)
\]</span></p>
<p>Biraz karmaşık duruyor ama daha önce iki fonksiyon maksimumu üzerinden gördüğümüz kavrama benziyor. Her noktada maks olan <span class="math inline">\(f_i\)</span>'leri alıyoruz, ve bu fonksiyonların altgradyanlarını hesaplıyoruz. Ama bu altgradyanların birleşimi her zaman bir dışbükey küme oluşturmayabilir, ve altdiferansiyelin bir dışbükey küme olması gerekir, o zaman için elimizde olan altgradyanların <span class="math inline">\(conv\)</span> ile dışbükey zarfına (convex hull) bakarız. Yani sadece birleşim <span class="math inline">\(\cup\)</span> ile elde ettiğim kümeyi bir işlemden daha geçirerek onun dışbükey küme halini alıyorum.</p>
<p>[atlandi, norm, 1:09:00]</p>
<p>Niye Altgradyanlar?</p>
<ol style="list-style-type: decimal">
<li><p>Optimizasyon: Önemli bir sebep [1, 1:12:00]. Bir dışbükey fonksiyonun altgradyanını hesaplamak her zaman mümkündür, o zaman her dışbükey fonksiyonu minimize edebilirim. Bazı durumlarda bu yavaş olabilir ama en azından minimizasyon mümkün olur.</p></li>
<li><p>Dışbükey Analizi: Her <span class="math inline">\(f\)</span> için, dışbükey olsun olmasın,</p></li>
</ol>
<p><span class="math display">\[
f(x^\ast) = \min_x f(x) \iff 0 \in \partial f(x^\ast)
\]</span></p>
<p>Yani <span class="math inline">\(x^\ast\)</span> bir minimize edicidir sadece ve sadece 0 değeri <span class="math inline">\(f\)</span>'in <span class="math inline">\(x^\ast\)</span> noktasında bir altgradyanı ise. Bu özelliğe çoğunlukla &quot;altgradyan optimalliği'' adı veriliyor. İspatı basit. Eğer <span class="math inline">\(g\)</span> vektörü <span class="math inline">\(x^\ast\)</span> noktasındaki altgradyan ise o zaman alttaki ifade her <span class="math inline">\(y\)</span> için doğrudur,</p>
<p><span class="math display">\[
f(y) \ge f(x^\ast) + 0^T (y-x^\ast) = f(x^\ast)
\]</span></p>
<p><span class="math display">\[
f(y) \ge f(x^\ast) \quad \forall y
\]</span></p>
<p>Üstteki ifade <span class="math inline">\(x^\ast\)</span> bir minimize edicidir diyor, o zaman sıfır bir altgradyandır.</p>
<p>Bazen üstteki ifadenin dışbükey olmayan fonksiyonlar için bile geçerli olduğunu unutanlar oluyor [1:14:32]. Bu her <span class="math inline">\(f\)</span> için doğru diyorum bazen bana şaşırmış şekilde bakıyorlar. Söylenen biraz sürpriz edici, evet. İkizlik ve KKT şartları hakkında konuşurken benzer şaşırtıcı ifadeler olacak.</p>
<p>Tabii eklemek gerekir bazen dışbükey olmayan fonksiyonlar için altgradyan hesaplanamaz, ya da mevcut değillerdir. Her problemi çözmek için bir yemek tarifi değil bu. Mesela başta gördüğümüz içbükey fonksiyon,</p>
<div class="figure">
<img src="func_42_subgrad_01.png" />

</div>
<p>Altgradyanı yok (ama tabii minimize edicisi de yok).</p>
<p>Altgradyanlarla devam edelim [2, 01:11], onlar bir dışbükey fonksiyonun gradyanı kavramının genelleştirilmiş hali idi.</p>
<p>Bir dikkat edilmesi gereken durum var ama, altgradyanlar bir dışbükey fonksiyon için her zaman mevcuttur, ama bunu spesifik olarak &quot;tanım kümesinin nispeten iç bölgelerinde olacak şekilde'' diye vurgulamak gerekir. Mesela gösterge fonksiyonu <span class="math inline">\(I\)</span>'nin uç noktalarında mevcut değildir.</p>
<p>Şimdi altgradyan yönteminin gücüne bir örnek görelim. Derslerimizin başında 1. derece optimallik şartını görmüştük [2, 05:30],</p>
<p><span class="math display">\[
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
\qquad (3)
\]</span></p>
<p>problemini çözmek istiyoruz, diyelim <span class="math inline">\(f\)</span> dışbükey ve türevi alınabilir. Bu problem için <span class="math inline">\(x\)</span>'in çözüm olmasının şartı</p>
<p><span class="math display">\[
\nabla f(x)^T (y-x) \ge 0 \quad \forall y \in C
\]</span></p>
<p>eşitsizliğinin doğru olmasıdır. Yani 1. derece minimallik gradyan sıfırı verir, o zaman herhangi bir <span class="math inline">\(\nabla f(x)^T(y-x)\)</span> yönünde adım atmak bizi her zaman bu minimallikten uzaklaştırmalıdır. Bu durum her olurlu <span class="math inline">\(y \in C\)</span> için doğru ise minimal yerdeyiz demektir [2, 05:50]. Ya da şöyle anlatalım, <span class="math inline">\(x\)</span> noktasındayız, <span class="math inline">\(y\)</span> noktasına gitmeyi düşünüyoruz. O zaman <span class="math inline">\(y-x\)</span> vektörünü oluşturuyoruz, ve su soruyu soruyoruz, &quot;kriter fonksiyonunun gradyanı aynı çizgi de mi?''. Eğer aynı yönde ise o yönde hareket etmek kriter <span class="math inline">\(f(x)\)</span>'i arttırır. Yani eğer gradyan her mümkün olurlu yön ile aşağı yukarı aynı yönü gösteriyorsa (azaltma / çoğaltma, -90/+90 derece bağlamında) o zaman minimum noktadayız demektir.</p>
<div class="figure">
<img src="func_42_subgrad_07.png" />

</div>
<p>İşte bunu altgradyan perspektifinden ispatlayabiliriz [2, 06:33].</p>
<p>Üsttekini altgradyan perspektifinden ispatlayabiliriz. Önce problemimizi sınırsız bir formatta tekrar tanımlayacağız. Sınırlamayı bir gösterge <span class="math inline">\(I_C\)</span> haline getirerek bunu yapabiliriz,</p>
<p><span class="math display">\[
\min_x f(x) + I_C(x)
\qquad (2)
\]</span></p>
<p>ki <span class="math inline">\(I_C(x) = 0\)</span> eğer <span class="math inline">\(x\)</span>, <span class="math inline">\(C\)</span> kümesi içindeyse, dışındaysa sonsuzluk. Şimdi üstteki fonksiyona altgradyan optimalliği uygulayalım, eğer üstteki fonksiyonu minimize eden bir nokta varsa elimde, bunun tercümesi sıfırın o noktada fonksiyonun altgradyanı olması. Fonksiyonun altgradyanını hesaplayalım, kurallarımıza göre iki dışbükey fonksiyon toplamının altgradyani o fonksiyonların ayrı ayrı altgradyanlarının toplamı. <span class="math inline">\(f\)</span> dışbükey, <span class="math inline">\(I_C\)</span> dışbükey (çünkü <span class="math inline">\(C\)</span> kümesi dışbükey küme). <span class="math inline">\(f\)</span> pürüzsüz, o zaman <span class="math inline">\(x\)</span>'te onun altgradyan kümesi sadece o noktadaki gradyan. <span class="math inline">\(I_C\)</span>'nin altgradyanı normal koni <span class="math inline">\(N_C\)</span>. O zaman</p>
<p><span class="math display">\[
0 \in \partial ( f(x) + I_C(x) ) 
\]</span></p>
<p><span class="math display">\[
\iff 0 \in \nabla f(x) + N_C(x) 
\]</span></p>
<p>olmalı, ya da</p>
<p><span class="math display">\[
\iff - \nabla f(x) \in N_C(x) 
\]</span></p>
<p>olmalı. Şimdi normal koniyi hatırlayalım, tanımı</p>
<p><span class="math display">\[
N_C(x) = \{ g \in \mathbb{R}^n: g^T x \ge g^T y \quad \forall y \in C
\]</span></p>
<p>buna göre iki üstteki <span class="math inline">\(N_C\)</span>, <span class="math inline">\(g=-\nabla f\)</span> üzerinden</p>
<p><span class="math display">\[
\iff -\nabla f(x)^T x \ge -\nabla f(x)^T y \quad \forall y \in C
\]</span></p>
<p>olarak açılabilir. Ya da</p>
<p><span class="math display">\[
\iff \nabla f(x)^T(y-x) \ge 0 \quad \forall y \in C
\]</span></p>
<p>Üstteki 1. derece optimallik şartına benziyor zaten. <span class="math inline">\(-\nabla f(x)\)</span> üstteki tanımın bir öğesidir, o zaman 0 altgradyan kümesinin öğesidir.</p>
<p>İşte gayet temiz bir şekilde optimallik ispatı yapmış olduk. Bu arada sınırlama içeren optimizasiyon problemi için alttaki tanım</p>
<p><span class="math display">\[
0 \in \partial f(x) + N_c
\]</span></p>
<p>ifadesi her nasılsa tamamen genel, yani dışbükey bir problem tanımı için gerekli ve yeterli bir şart çünkü hatırlarsak bahsettik ki tüm dışbükey problemleri (2) ya da (3) formunda öne sürmek mümkün. Tabii üstteki formlea iş yapmak kolay değildir, çünkü <span class="math inline">\(N_C\)</span> ile çalışmak zor. Eğer <span class="math inline">\(C\)</span> çetrefil bir küme ise, mesela</p>
<p><span class="math display">\[
C = \{ x: g_i(x) \le 0, Ax = b \}
\]</span></p>
<p>gibi, o zaman normal koniyi oluşturmak zor olacaktır. Yani iki üstteki tanımın her zaman faydalı olduğunu söyleyemeyiz, ama her dışbükey problem için gerekli ve yeterli şart olduğunu söyleyebiliyoruz.</p>
<p>Sonradan optimalliği tanımlamanın farklı bir yolunu göreceğiz. Sınırlama ifadeleri olduğu zaman problemler daha az çetin / çözülür hale gelir, problemler sınırsız-sınırlı halde birbirine eşit şekilde tanımlanabilirler, ama sınırlı tanımları çözmek daha kolay. KKT koşulları burada devreye girecek. Yani her şeyi kritere tıkmak, gösterge vs ile uğraşmak, altgradyan almak yerine bu tür tanımla çalışmak daha rahat oluyor [2, 12:00].</p>
<p>Altgradyan optimalliğinin bazı diğer örneklerini görelim, mesela Lasso için altgradyan optimalliği. Bazılarının bilebileceği üzere Lasso problemini parametrize etmenin iki yolu vardır, birisi katsayılar üzerinde bir L1 norm kısıtlaması tanımlamak, diğeri ise alttaki gibi onu kritere dahil etmek,</p>
<p><span class="math display">\[
\min_\beta \frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1
\qquad (5)
\]</span></p>
<p>ki <span class="math inline">\(\lambda \ge 0\)</span>. Altgradyan optimalliğinde sadece ve sadece alttaki şart geçerliyse elimizde bir çözüm var diyebiliyoruz, bu şart,</p>
<p><span class="math display">\[
0 \in \big( 
\frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1
\big)
\]</span></p>
<p>yani eğer 0 kriterimin altgradyan kümesinde ise. Üstteki altgradyanın uygulandığı toplam işaretinin iki tarafı da dışbükey o zaman onları altgradyanların toplamı olarak açabilirim, ayrıca soldaki terim bir de pürüzsüz olduğu için tek altgradyan normal gradyandır,</p>
<p><span class="math display">\[
\iff 0 \in -X^T (y - X\beta) + \lambda \partial ||\beta||_1
\]</span></p>
<p><span class="math display">\[
\iff X^T (y - X\beta) = \lambda v 
\qquad (4)
\]</span></p>
<p>herhangi bir <span class="math inline">\(v \in \partial ||\beta||_1\)</span> için. L1 norm'un altgradyanı için daha önce gördüğümüz üzere bileşen bileşen bakmak gerekiyor, ve farklı şartlara göre parçalı bir fonksiyon elde edeceğiz,</p>
<p><span class="math display">\[
v_i \in
\left\{ \begin{array}{ll}
\{1\} &amp; \textrm{eğer } \beta_i &gt; 0 \\
\{-1\} &amp; \textrm{eğer } \beta_i &lt; 0 \\
{[} -1,+1{]} &amp; \textrm{eger } \beta_i = 0 
\end{array} \right.
\]</span></p>
<p>Yeni öyle bir <span class="math inline">\(\beta\)</span> arıyorum ki herhangi bir <span class="math inline">\(v\)</span> vektörü için (4)'u tatmin edecek ve bu <span class="math inline">\(v\)</span> geçerli bir altgradyan olacak, yani üstteki şartlara uyacak. O zaman çözüme erişmişim demektir. Çözümü şu anda vermiyoruz, bunlar çözüm için uyulması gereken optimallik şartları [2, 15:24].</p>
<p>Her <span class="math inline">\(\beta_i\)</span> için üstteki denklemin nasıl oluşacağını görmek istersek, ve <span class="math inline">\(X_1,..,X_p\)</span> değerleri <span class="math inline">\(X\)</span> matrisinin kolonları olacak şekilde</p>
<p><span class="math display">\[
\left[\begin{array}{ccc}
\uparrow &amp; \uparrow &amp; \\
&amp;&amp; \\
X_1 &amp; X_2 &amp; \dots \\
&amp;&amp; \\
\downarrow &amp; \downarrow &amp; 
\end{array}\right]^T
\left[\begin{array}{c}
y_1 \\ \vdots \\ y_p 
\end{array}\right] 
- 
\left[\begin{array}{ccc}
\uparrow &amp; \uparrow &amp; \\
&amp;&amp; \\
X_1 &amp; X_2 &amp; \dots \\
&amp;&amp; \\
\downarrow &amp; \downarrow &amp; 
\end{array}\right]
\left[\begin{array}{c}
\beta_1 \\ \vdots \\ \beta_p 
\end{array}\right]  =
\lambda
\left[\begin{array}{c}
v_1 \\ \vdots \\ v_p 
\end{array}\right] 
\]</span></p>
<p>O zaman bunu her <span class="math inline">\(v_i\)</span> olasılığı için yazarsak, <span class="math inline">\(\beta_i\)</span>'in sıfır olup olmadığı üzerinden bir parçalı fonksiyon ortaya çıkartabiliriz. Altgradyan optimallik şartı,</p>
<p><span class="math display">\[
\left\{ \begin{array}{ll}
X_i^T (y-X\beta) = \lambda \cdot \mathrm{sign}(\beta_i) &amp; \textrm{eger } \beta_i \ne 0 \\
| X_i^T (y-X\beta)  | \le \lambda  &amp; \textrm{eger } \beta_i = 0 
\end{array} \right.
\]</span></p>
<p>haline geldi. İkinci satırı nasıl elde ettik? Eğer <span class="math inline">\(\beta_i=0\)</span> ise bu bana <span class="math inline">\(\lambda v\)</span> ifadesi <span class="math inline">\(-\lambda\)</span> ve <span class="math inline">\(+\lambda\)</span> arasında herhangi bir yerde olabilir diyor (çünkü <span class="math inline">\(v_i\)</span> parçalı fonksiyonunda <span class="math inline">\(\beta_i=0\)</span> ise <span class="math inline">\(v_i\)</span> -1 ve +1 arası herhangi bir değer dedik), ve <span class="math inline">\(-\lambda\)</span> ve <span class="math inline">\(+\lambda\)</span> arası olma durumunu son satırdaki mutlak değer ifadesine tercüme edebiliriz.</p>
<p>Dikkat, üstteki ifade optimalliğe bakma / kontrol etmek için bir yöntem. Birisi size bir vektör veriyor [2, 16:57], sonra soruyor &quot;bu vektör Lasso kriterine göre optimal midir?'' Öyle olup olmadığına bakmak için vektörün her ögesine bakıyoruz, ve üstteki kontrolü işletiyoruz. Eğer her öge optimal ise evet diyoruz, tek bir öğe bile optimal değilse hayır diyoruz.</p>
<p>Üstteki parçalı formüldeki ikinci bölümü ilginç bir şekilde kullanabiliriz. Diyelim ki 100 değişkenlik modeli Lasso ile veriye uydurduk, ve <span class="math inline">\(\beta\)</span> katsayıları elde ettik, bir regresyon yaptık yani. Diyelim ki çözümden sonra birisi geliyor size 101. kolon veriyor, acaba tüm uydurma işlemini baştan tekrar mı yapmak lazım? Belki hayır, <span class="math inline">\(| X_{101}^T (y-X\beta) | \le \lambda\)</span> kontrolünü yaparız, eğer koşul doğru ise o zaman <span class="math inline">\(\beta_{100} = 0\)</span> demektir, ve bu katsayıya gerek yoktur, modelin geri kalanı değişmeden kalır [2, 20:42].</p>
<p>Bir diğer ilginç uygulama Lasso'nun basitleştirilmiş hali; <span class="math inline">\(X = I\)</span> yani birim matrisi olduğu durum. Bu yaklaşımla bazılarının gürültü silme (denoising) dediği işlemi yapabilmiş oluyoruz. <span class="math inline">\(X=I\)</span> deyince Lasso'da geri kalan,</p>
<p><span class="math display">\[
\min_\beta \frac{1}{2} || y - \beta ||_2^2 + \lambda ||\beta||_1
\]</span></p>
<p>Bu problem ifadesi diyor ki &quot;öyle bir <span class="math inline">\(\beta\)</span> vektörü bul ki <span class="math inline">\(y\)</span>'ye olabildiği kadar yakın olsun ve <span class="math inline">\(\beta\)</span> üzerinde bir L1 cezası olsun''. Yana bana içinde bir sürü gözlem noktası taşıyan bir <span class="math inline">\(y\)</span> veriliyor ve ben bu gözlemleri en iyi şekilde yaklaşıklayan <span class="math inline">\(\beta\)</span>'yi arıyorum ve bu <span class="math inline">\(\beta\)</span>'nin seyrek olmasını [2, 24:23] tercih ediyorum (L1 cezası ile bu oluyor, büyük değerler cezalandırılınca çözü katsayının sıfıra yakın olmasını özendirmiş oluruz). Artık biliyoruz ki üstteki problemi altgradyan optimalliği ile çözmek mümkün.</p>
<p>Daha önce gördüğümüz Lasso altgradyan optimalliğini <span class="math inline">\(X=I\)</span> için tekrar yazarsak</p>
<p><span class="math display">\[
\left\{ \begin{array}{ll}
(y-\beta_i) = \lambda \cdot \mathrm{sign}(\beta_i) &amp; \textrm{eger } \beta_i \ne 0 \\
| |y-\beta_i|  | \le \lambda  &amp; \textrm{eger } \beta_i = 0 
\end{array} \right.
\]</span></p>
<p>Çözüm <span class="math inline">\(\beta = S_\lambda(y)\)</span>, ki <span class="math inline">\(S_\lambda(y)\)</span>'ye yumuşak eşikleme (soft-threshold) operatörü deniyor. Üstteki optimalliğe uyan bir çözüm, hatta tek çözüm, budur.</p>
<p><span class="math display">\[
[ S_\lambda(y) ]_i = 
\left\{ \begin{array}{ll}
y_i - \lambda &amp; \textrm{eğer } y_i &gt; \lambda_i \\
0 &amp; \textrm{eğer } -\lambda \ge y_i \ge \lambda, \quad i=1,..,n \\
y_i + \lambda &amp; \textrm{eğer } y_i &lt; -\lambda_i 
\end{array} \right.
\]</span></p>
<p>Çözümün optimallik şartlarına uyup uymadığı rahatça kontrol edilebilir. Formülde <span class="math inline">\(\beta=S_\lambda(y)\)</span> diyerek alttakilerin doğru olup olmadığına bakarız,</p>
<p>Eğer <span class="math inline">\(y_i &gt; \lambda, \beta_i = y_i - \lambda &gt; 0\)</span> ise <span class="math inline">\(y_i-\beta_i = \lambda = \lambda \cdot 1\)</span> mı?</p>
<p>Eğer <span class="math inline">\(y_i &lt; -\lambda\)</span> ise benzer şekilde</p>
<p>Eğer <span class="math inline">\(|y_i| \ge \lambda, \beta_i=0\)</span> ise <span class="math inline">\(|y_i-\beta_i| = |y_i| \ge \lambda\)</span> mi?</p>
<p>[distance to convex set örneği atlandı]</p>
<p>Daha önce gradyan inişi (gradient descent) algoritmasını görmüştük, bu algoritma çok basittir. Şimdi işleri biraz daha zorlaştıracağız [2, 48:00]. Bu metotun bir dezavantajı optimize edilen <span class="math inline">\(f\)</span>'nin türevi alınabilir bir fonksiyon olma zorunluluğu. Diğer bir dezavantaj yakınsamanın uzun zaman alabilmesi.</p>
<p>Altgradyan Metodu</p>
<p>Gradyan inişi yapısına benziyor, <span class="math inline">\(f\)</span>'in dışbükey olması ve <span class="math inline">\(\mathrm{dom}(f) = \mathbb{R}^n\)</span> olması lazım, ama <span class="math inline">\(f\)</span>'nin pürüzsüz olma zorunluluğu yok.</p>
<p>Gradyan inişi gibi özyineli bir şekilde, <span class="math inline">\(x^{(0)}\)</span>'dan başlıyoruz, ve</p>
<p><span class="math display">\[
x^{(k)} = x^{(k-1)} - t_k \cdot g^{(k-1)}, \quad k=1,2,3,..
\]</span></p>
<p>adım atarak ilerliyoruz, öyle ki <span class="math inline">\(g^{(k-1)} \in \partial f(x^{(k-1)})\)</span> yani <span class="math inline">\(f\)</span>'nin <span class="math inline">\(x^{(k-1)}\)</span> noktasındaki herhangi bir altgradyanı. Adım ata ata gidiyorum, her adımda mevcut altgradyanlara bakıyorum, herhangi birini seçiyorum, ona <span class="math inline">\(g^{(k-1)}\)</span> diyelim, ve <span class="math inline">\(x\)</span>'i bu yönde olacak şekilde bir <span class="math inline">\(t_k\)</span>'ye oranlı olarak güncelliyorum [2, 49:50].</p>
<p>Altgradyan metotunun ilginç özelliklerinden biri her adımda iniş yapmanın garanti olmaması (herhalde onun için &quot;altgradyan inişi'' yerine &quot;altgradyan metotu'' ismi verilmiş). Bu sebeple adım atarken o ana kadar, yani <span class="math inline">\(x^{(0)},.., x^{(k)}\)</span> içinde olan en iyi (best) (en minimal) noktayı hatırlamak gerekiyor, <span class="math inline">\(x_{best}^{(k)}\)</span>, ki</p>
<p><span class="math display">\[
f(x_{best}^{(k)}) = \min_{i=0,..,k} f(x^{(i)})
\]</span></p>
<p>Eğer mesela altgradyan metotunun 100,000 adım işletmişsem erişilmiş minimal nokta olarak bu hatırlanan en iyi noktayı sonuç olarak rapor ederim.</p>
<p>Adım büyüklüğü nasıl seçilir?</p>
<p>Sabit adım büyüklüğü seçmek bir seçenek. Küçükçe seçilen böyle bir büyüklük işler.</p>
<p>Çokça kullanılan bir diğer seçenek &quot;gittikçe yokolan'' adım büyüklüğü. Bu tür adım seçimi için kullanabilecek pek çok kural var, aranan bir nitelik sıfıra gidilmesi ama çok hızlı gidilmemesi. Mesela <span class="math inline">\(t_k = 1/k\)</span> uygun. Altgradyanlarda geriye iz sürmenin karşılığı yok.</p>
<p>Yakınsama analizi</p>
<p>Altgradyan metotunun yakınsama analizi gradyan inişinin analizinden biraz farklı [2, 54:15]. Diyelim ki elimizde bir dışbükey fonksiyon <span class="math inline">\(f\)</span> var ve tanım kümesi herşey, <span class="math inline">\(\mathrm{dom}(f) = \mathbb{R}^n\)</span>. Fonksiyon ayrıca Lipschitz sürekli, fonksiyonun sürekli olduğunun söylemiyoruz dikkat, fonksiyon sabit <span class="math inline">\(G &gt; 0\)</span> üzerinden Lipschitz sürekli, yani</p>
<p><span class="math display">\[
|f(x) - f(y)| \ge G || x-y ||_2 \quad \forall x,y
\]</span></p>
<p>Bunu baz alarak iki tane teori öne sürebiliriz, birisi sabit adım büyüklüğü, diğeri azalan adım büyüklüğü için.</p>
<p>Yokolan adım büyüklüğü için iki şart tanımlayalım, &quot;kare toplanabilir ama toplanabilir değil'', yani şu iki şart,</p>
<p><span class="math display">\[
\sum_{k=1}^{\infty} t_k^2 &lt; \infty, \quad \sum_{k=1}^{\infty} t_k - \infty
\]</span></p>
<p>Şimdi iki teoriyi tanımlayabiliriz,</p>
<p>Teori 1</p>
<p>Sabitlenmiş <span class="math inline">\(t\)</span> için altgradyan metotu alttaki şartı tanımlar,</p>
<p><span class="math display">\[
\lim_{k \to \infty} f(x_{best}^{(k)} ) \le f^\ast + G^2 t / 2
\]</span></p>
<p>Üstteki ifade diyor ki eğer altgradyan metotunu sonsuza kadar işletirsek eldeki en iyi noktadan elde edilecek fonksiyon değeri gerçek optimum artı Lipschitz sabitinin karesi çarpı <span class="math inline">\(t/2\)</span>'dan küçük olacaktır, ki <span class="math inline">\(t\)</span> sabitlenmiş adım büyüklüğü. Ama üstteki yine de çözüm için bir limit vermiyor. Onun için alttaki lazım,</p>
<p>Teori 2</p>
<p>Yokolan adım büyüklükleri için</p>
<p><span class="math display">\[
\lim_{k \to \infty} f(x_{best}^{(k)} ) = f^\ast 
\]</span></p>
<p>[ek detaylar atlandı]</p>
<p>Altgradyan metotuna bir örnek olarak [2, 1:01:36] regülarize edilmiş lojistik regresyona bakabiliriz. <span class="math inline">\(\beta\)</span> katsayılarını bulmaya uğraşıyoruz, ve veriye uydurma bağlamında bir kayıp fonksiyonunu minimize etmeye uğracağız. Önce normal regresyon,</p>
<p><span class="math display">\[
f(\beta) = \sum_{i=1}^{n} \left(-y_ix_i^T\beta + \log( 1 + \exp (x_i^T\beta) \right) 
\]</span></p>
<p>Üstteki pürüzsüz ve dışbükey bir fonksiyon.</p>
<p><span class="math display">\[
\nabla f(\beta) = \sum_{i=1}^{n} (y_i - p_i(\beta)) x_i
\]</span></p>
<p>ki <span class="math inline">\(p_i(x) = \exp(x_i^T\beta) / (1+ \exp(x_i^T\beta))\)</span>, <span class="math inline">\(i=1,..,n\)</span>.</p>
<p>Regülarize edilmiş lojistik regresyon</p>
<p><span class="math display">\[
\min_\beta f(\beta) + \lambda \cdot P(\beta)
\]</span></p>
<p>ki <span class="math inline">\(P(\beta) = ||\beta||_2^2\)</span> olabilir (Ridge cezası) ya da <span class="math inline">\(P(\beta) = ||\beta|_1|\)</span> (Lasso cezası). Bu cezalardan ilki pürüzsüz, diğeri değil. Böylece birinde gradyan inişi diğerinde altgradyan metotu kullanmak zorunda olacağız.</p>
<p>Ekler</p>
<p>Alttaki örnek [3]'ten,</p>
<p><span class="math display">\[
\min_w F(w) = \frac{1}{2} \sum_{i=1}^{N} (x_i^T w - y_i)^2 + \lambda ||w||_1
\]</span></p>
<p>gibi bir Lasso örneği var.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt

<span class="kw">def</span> subgrad(w):
    <span class="cf">return</span> X.T<span class="op">*</span>(X<span class="op">*</span>w<span class="op">-</span>y) <span class="op">+</span> lamda<span class="op">*</span>np.sign(w)

<span class="kw">def</span> obj(w):
    r <span class="op">=</span> X<span class="op">*</span>w<span class="op">-</span>y<span class="op">;</span>
    <span class="cf">return</span> np.<span class="bu">sum</span>(np.multiply(r,r))<span class="op">/</span><span class="dv">2</span> <span class="op">+</span> lamda <span class="op">*</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(w))

N <span class="op">=</span> <span class="dv">40</span>
dim <span class="op">=</span> <span class="dv">10</span>
max_iter <span class="op">=</span> <span class="dv">200</span>
lamda <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>np.sqrt(N)<span class="op">;</span>
np.random.seed(<span class="dv">50</span>)
w <span class="op">=</span> np.matrix(np.random.multivariate_normal([<span class="fl">0.0</span>]<span class="op">*</span>dim, np.eye(dim))).T
X <span class="op">=</span> np.matrix(np.random.multivariate_normal([<span class="fl">0.0</span>]<span class="op">*</span>dim, np.eye(dim), size <span class="op">=</span> N))
y <span class="op">=</span> X<span class="op">*</span>w

w <span class="op">=</span> np.matrix([<span class="fl">0.0</span>]<span class="op">*</span>dim).T
obj_SD <span class="op">=</span> []
gamma <span class="op">=</span> <span class="fl">0.01</span>
<span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, max_iter):
    obj_val <span class="op">=</span> obj(w)
    w <span class="op">=</span> w <span class="op">-</span> gamma <span class="op">*</span> subgrad(w)<span class="op">/</span>np.sqrt(t<span class="op">+</span><span class="dv">1</span>)
    obj_SD.append(obj_val.item())
    <span class="cf">if</span> (t<span class="op">%</span><span class="dv">5</span><span class="op">==</span><span class="dv">0</span>): <span class="bu">print</span>(<span class="st">&#39;iter= </span><span class="sc">{}</span><span class="st">,</span><span class="ch">\t</span><span class="st">objective= </span><span class="sc">{:3f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(t, obj_val.item()))

<span class="bu">print</span> (w)</code></pre></div>
<pre><code>iter= 0,    objective= 169.279279
iter= 5,    objective= 24.721959
iter= 10,   objective= 13.195682
iter= 15,   objective= 8.739994
iter= 20,   objective= 6.419780
iter= 25,   objective= 5.034598
iter= 30,   objective= 4.138547
iter= 35,   objective= 3.526977
iter= 40,   objective= 3.093047
iter= 45,   objective= 2.775883
iter= 50,   objective= 2.538530
iter= 55,   objective= 2.357443
iter= 60,   objective= 2.217031
iter= 65,   objective= 2.106647
iter= 70,   objective= 2.018829
iter= 75,   objective= 1.948229
iter= 80,   objective= 1.890943
iter= 85,   objective= 1.844077
iter= 90,   objective= 1.805447
iter= 95,   objective= 1.773392
iter= 100,  objective= 1.746629
iter= 105,  objective= 1.724158
iter= 110,  objective= 1.705193
iter= 115,  objective= 1.689110
iter= 120,  objective= 1.675411
iter= 125,  objective= 1.663694
iter= 130,  objective= 1.653633
iter= 135,  objective= 1.644963
iter= 140,  objective= 1.637466
iter= 145,  objective= 1.630964
iter= 150,  objective= 1.625306
iter= 155,  objective= 1.620369
iter= 160,  objective= 1.616050
iter= 165,  objective= 1.612262
iter= 170,  objective= 1.608931
iter= 175,  objective= 1.605996
iter= 180,  objective= 1.603403
iter= 185,  objective= 1.601108
iter= 190,  objective= 1.599073
iter= 195,  objective= 1.597264
[[-1.53942055]
 [-0.02366012]
 [-0.61081721]
 [-1.43597808]
 [ 1.3626909 ]
 [-0.47342589]
 [-0.78826118]
 [ 1.04965236]
 [-1.27159815]
 [-1.32969646]]</code></pre>
<p>Bir diğer örnek [4]'ten, burada kodlanan (5)'teki formül aslında, kodun sembollerini kullanırsak,</p>
<p><span class="math display">\[
\min_x \frac{1}{2} || Ax - b||_2 + \lambda ||x||_1
\]</span></p>
<p>L1 normu <span class="math inline">\(||x||_1\)</span> gradyan inişi için problemli o sebeple altgradyan metotu kullanacağız. Veri olarak [5]'te görülen diyabet verisini alabiliriz,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> scipy.io <span class="im">as</span> sio
<span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt
<span class="im">import</span> numpy.linalg <span class="im">as</span> lin

<span class="kw">def</span> subgrad_func(A,b,lam):
    n2 <span class="op">=</span> A.shape[<span class="dv">1</span>]
    x <span class="op">=</span> np.zeros((n2,<span class="dv">1</span>))
    k<span class="op">=</span><span class="dv">1</span>
    g <span class="op">=</span> np.ones((n2,<span class="dv">1</span>))
    t <span class="op">=</span> <span class="fl">0.01</span>
    f <span class="op">=</span> []
    <span class="cf">while</span> <span class="va">True</span>:
        <span class="cf">if</span> k<span class="op">&gt;</span><span class="dv">3</span>: 
            crit <span class="op">=</span> np.<span class="bu">abs</span>(f[k<span class="dv">-2</span>]<span class="op">-</span>f[k<span class="dv">-3</span>])<span class="op">/</span>f[k<span class="dv">-2</span>]
            <span class="cf">if</span> crit <span class="op">&lt;</span> <span class="fl">1e-5</span>: <span class="cf">break</span>
        tmp <span class="op">=</span> <span class="fl">0.5</span><span class="op">*</span>lin.norm(np.dot(A,x)<span class="op">-</span>b,<span class="dv">2</span>)<span class="op">**</span><span class="dv">2</span><span class="op">+</span>lam<span class="op">*</span>lin.norm(x,<span class="dv">1</span>)<span class="op">;</span>        
        <span class="cf">if</span> k<span class="op">%</span><span class="dv">10</span><span class="op">==</span><span class="dv">0</span>: <span class="bu">print</span> (tmp)
        f.append(tmp)
        s <span class="op">=</span> x.copy()
        s[x<span class="op">&gt;</span><span class="dv">0</span>]<span class="op">=</span><span class="dv">1</span>
        s[x<span class="op">&lt;</span><span class="dv">0</span>]<span class="op">=-</span><span class="dv">1</span>
        <span class="cf">if</span> <span class="bu">len</span>(s[x<span class="op">==</span><span class="dv">0</span>])<span class="op">&gt;</span><span class="dv">0</span>: 
            s[x<span class="op">==</span><span class="dv">0</span>] <span class="op">=</span> <span class="dv">-2</span><span class="op">*</span>np.random.rand(<span class="bu">len</span>(x<span class="op">==</span><span class="dv">0</span>))<span class="op">+</span><span class="dv">1</span>
        g <span class="op">=</span> np.dot(A.T,np.dot(A,x)<span class="op">-</span>b) <span class="op">+</span> lam<span class="op">*</span>s
        x <span class="op">=</span> x <span class="op">-</span> t<span class="op">*</span>g
        k <span class="op">=</span> k<span class="op">+</span><span class="dv">1</span>
    <span class="cf">return</span> x
                
diabetes <span class="op">=</span> pd.read_csv(<span class="st">&quot;../../stat/stat_120_regular/diabetes.csv&quot;</span>,sep<span class="op">=</span><span class="st">&#39;;&#39;</span>)
y <span class="op">=</span> np.array(diabetes[<span class="st">&#39;response&#39;</span>].astype(<span class="bu">float</span>)).reshape(<span class="dv">442</span>,<span class="dv">1</span>)
A <span class="op">=</span> np.array(diabetes.drop(<span class="st">&quot;response&quot;</span>,axis<span class="op">=</span><span class="dv">1</span>))

lam <span class="op">=</span> <span class="fl">0.1</span><span class="op">;</span>
x <span class="op">=</span> subgrad_func(A,y,lam)<span class="op">;</span>
<span class="bu">print</span> (<span class="st">&#39;x&#39;</span>)
<span class="bu">print</span> (x)</code></pre></div>
<pre><code>6167530.253256479
6017359.332614086
5936043.378499364
5887955.958919385
5856837.417105291
5835069.931049544
5818932.590413047
5806489.16976122
5796653.745550204
5788760.388544399
5782366.953999686
5777155.717300746
5772889.40103409
5769385.329280358
5766499.620202982
5764117.531938736
5762146.838816163
5760513.025122407
5759155.660694774
5758025.5987459235
5757082.775086282
5756294.462375693
5755633.876252789
x
[[   5.34807751]
 [-200.87752927]
 [ 490.9420537 ]
 [ 304.14877939]
 [ -43.03872491]
 [-107.97583692]
 [-207.47086585]
 [ 127.15527296]
 [ 410.70880916]
 [ 115.41867007]]</code></pre>
<p>Bir dışbükey kümesi üzerinden tanımlı ama pürüzsüz olmayabilecek bir dışbükey fonksiyonu minimize etmek için yansıtılan altgradyan (projected subgradient) metotu adlı bir metot ta kullanılabilir [2, 22:04].</p>
<p>Dışbükey <span class="math inline">\(f\)</span>'yi dışbükey küme <span class="math inline">\(C\)</span> üzerinden optimize etmek için</p>
<p><span class="math display">\[
\min_x f(x) \quad \textrm{öyle ki} \quad x \in C
\]</span></p>
<p><span class="math display">\[
x^{(k)} = P_C \left( x^{(k-1)} - t_k g_k^{(k-1)} \right)
\]</span></p>
<p>Bu metot normal altgradyan metotu gibi, tek fark parantez içinde görülen altgradyan adımı atıldıktan sonra elde edilen sonucun <span class="math inline">\(C\)</span> kümesine geri yansıtılması (projection), çünkü atılan adım sonucunda olurlu bir sonuç elde etmemiş olabiliriz.</p>
<p>Yakınsama analizi normal altgraydan metotuna benziyor, bu metotla da benzer yakınsama garantisi elde edilebiliyor, yakınsama oranı da buna dahil.</p>
<p>Yansıtma adımı bazen zor olabilir, hangi durumlarda kolay olduğunun listesi aşağıda [2, 23:53]. Hangi kümelere yansıtmak kolaydır?</p>
<ol style="list-style-type: decimal">
<li><p>Doğrusal görüntüler: <span class="math inline">\(\{ Ax + b: x \in \mathbb{R}^n \}\)</span></p></li>
<li><p><span class="math inline">\(\{ x: Ax = b \}\)</span> sisteminin çözüm kümesi</p></li>
<li><p>Negatif olmayan bölge: <span class="math inline">\(\mathbb{R}_{+}^n = \{x: x \ge 0 \}\)</span>. Verilen vektörün negatif değerlerinin sıfır yapmak, pozitifleri tutmak bize o vektörün negatif olmayan bölge karşılığını veriyor.</p></li>
<li><p>Bazı norm topları <span class="math inline">\(\{ x: ||x||_p \le 1 \}\)</span>, <span class="math inline">\(p=1,2,..,\infty\)</span> için. Daha önce 2-norm topuna yansıtmayı gördük, vektörü alıp normalize edersek bu kümeye yansıtma yapmış oluyoruz aslında. Bu yansıtma bizi o vektörün 2-norm topundaki en yakın diğer vektöre götürüyor. Sonsuz norm kolay, bir kutuya yansıtma yapmış oluyoruz, bunun ne demek olduğunu düşünmeyi size bırakıyorum, 1-norm en zoru.</p></li>
<li><p>Bazı çokyüzlüler (polyhedra) ve basit koniler.</p></li>
</ol>
<p>Bir uyarıda bulunalım, tanımı basit duran kümeler ortaya çıkartmak kolaydır, fakat bu kümelere yansıtma yapan operatörler çok zor olabilir. Mesela gelişigüzel bir çokyüzlü <span class="math inline">\(C = \{ x: Ax \le b \}\)</span> kümesine yansıtma yapmak zordur. Bu problemin kendisi apayrı bir optimizasyon problemi aslında, bir QP.</p>
<p>[stochastic subgradient method atlandı]</p>
<p>Altgradyanların iyi tarafı genel uygulanabilirlik. Altgradyan metotları dışbükey, Lipschitz fonksiyonları üzerinde bir anlamda optimaldir. Ünlü bilimci Nesterov'un bu bağlamda bir teorisi vardır [2, 45:12], pürüzsüz durumlarda ve 1. derece yöntemlerde altgradyanların lineer kombinasyonları üzerinden güncellemenin sonucu olan adımların başarısına bir alt sınır tanımlar.</p>
<p>Kaynaklar</p>
<p>[1] Tibshirani, <em>Convex Optimization, Lecture Video 6</em>, <a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg" class="uri">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a></p>
<p>[2] Tibshirani, <em>Convex Optimization, Lecture Video 7</em>, <a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg" class="uri">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a></p>
<p>[3] He, <em>IE 598 - BIG DATA OPTIMIZATION</em>,<br />
<a href="http://niaohe.ise.illinois.edu/IE598_2016/" class="uri">http://niaohe.ise.illinois.edu/IE598_2016/</a></p>
<p>[4] Feng, <em>Lasso</em>, <a href="https://github.com/fengcls/Lasso" class="uri">https://github.com/fengcls/Lasso</a></p>
<p>[5] Bayramlı, {</p>
</body>
</html>
