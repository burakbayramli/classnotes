\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Log-Bariyer Yöntemi

Þimdiye kadar kýsýtlanmamýþ Newton yönteminden bahsettik [1, 53:18]. Þimdi
lineer olarak kýsýtlanmýþ, $Ax=b$ ile, olan duruma bakalým, çünkü ileride
faydalý olacak.

En bariz yaklaþým $Ax=b$'nin bir doðrusal (affine) uzay yaratmasý, ve
optimizasyonun bu daha ufak uzayda iþ yapmasý. Bir deðiþken deðiþimi
yaparýz, $x = Fy + x_0$, ki $F$, $A$'nin sýfýr uzayýný kapsýyor, ardýndan
optimizasyon problemini $y$ bazlý bir probleme indirge. Bu yaklaþýma
``indirgenmiþ uzay yaklaþýmý'' deniyor. Tarif edilen gayet doðal, basit bir
yaklaþým aslýnda, fakat bir bedeli de var. Eðer orijinal problem seyrek ise
deðiþim sonrasý çok miktarda yapý kaybý (yani seyreklik) olacak.

Bir diðer seçenek eþitlikle kýsýtlanmýþ Newton (equality constrained
Newton). Bu yöntemle $x^+ = x + t v$ adýmýnda $v$ karesel
yaklaþýksallamanýn çözümü, fakat bu çözüm sýnýrlanmýþ bir problemin çözümü,
bir sonraki $x^+$'yi olurlu halde tutacak bir sýnýrlama bu.

Çözülen problem 

$$
v = \arg\min_{Az = 0} \nabla f(x)^T (z-x) + \frac{1}{2} (z-x)^T \nabla^2 f(x)(z-x)
$$

Üstteki $x^+$'i olurlu kümede tutar çünkü 

$$
Ax^+ = Ax + tAv = b + 0 = b
$$

KKT koþullarý üzerinden çözüm

$$
\left[\begin{array}{cc}
\nabla^2 f(x) & A^T \\
A & 0
\end{array}\right]
\left[\begin{array}{c}
v \\
w
\end{array}\right] = 
- 
\left[\begin{array}{c}
\nabla f(x) \\
Ax - b
\end{array}\right]
$$

ile belirtilebilir. KKT bölümünde karesel problem örneðindeki gördüðümüz
$Q$ burada $\nabla^2 f(x)$ oluyor, atýlan adým sonrasý gelinen yerin ne
olacaðý da kýsýtlama içinde görülebilir.

Olurlu noktadan basliyorsak, ozel durum $Ax = b$, tabii o zaman 

$$
\left[\begin{array}{cc}
\nabla^2 f(x) & A^T \\
A & 0
\end{array}\right]
\left[\begin{array}{c}
v \\
w
\end{array}\right] = 
- 
\left[\begin{array}{c}
\nabla f(x) \\
0
\end{array}\right]
$$

O zaman bariyer metotu nedir? Bu metotla yine Newton metotunu uzatacaðýz,
ve eþitsizlik olan problemlerle uðraþacaðýz. 

$$
\min_x f(x) \quad \textrm{öyle ki}
$$
$$
Ax = b
$$
$$
h_i(x) \le 0, \quad i=1,..,m
\mlabel{3}
$$

Kriter dýþbükey, sýnýrlamalar lineer, ve dýþbükey eþitsizlik
sýnýrlamalarý, $h_i(x) \le 0$ ile.

Bariyer metotu eþitsizliklerle baþetmenin bir yolu. Eþitsizlik içeren
programlarýn en büyük problemi sýnýrlarda ne yapýlacaðýna karar vermek,
yani olurlu kümenin sýnýrlarýnda. Baþetmek için olurlu küme 
$C \equiv \{ x: h_i(x) \le 0 , i=1,..,m\}$ 

$$
\min_x f(x) + I_C(x)
$$
$$
Ax = b
$$

ile kritere dahil edilir, ki $I_C$ göstergeç fonksiyonudur. Ana fikir sýnýr
noktalarýnda iþler zorlaþýyorsa niye oralardan uzak durmuyoruz? Fakat
göstergeç fonksiyonu ile çalýþmak zor, onu da yaklaþýksal olarak temsil
ederiz, iþte bariyer fonksiyonu budur. Öyle bir fonksiyon seçeriz ki
sýnýrlarda aþýrý büyük deðerler vererek bizi minimizasyon baðlamýnda
oralardan ``geri iter''. Sanki sýnýrlara bir manyetik alan koyuyoruz,
oralara yaklaþýnca geri itiliyoruz. 

Tabii bir yandan Newton metotu da kullanabilmek istiyoruz, ideal olarak
pürüzsüz bir metot olursa elimizde iyi olur. Log bariyer fonksiyonu böyle
bir fonksiyon

$$
\phi(x) = -\sum_{i=1}^{m} \log(-h_i(x))
\mlabel{1}
$$

Böylece ana problemi þu hale getirebiliriz,

$$
\min_x f(x) + \frac{1}{t} \phi(x) \quad \textrm{öyle ki}
$$
$$
Ax = b
$$

ki $t>0$. Görülen $1/t$ dýþarýdan bizim tanýmladýðýmýz bir parametre,
optimizasyonu ayarlamak için kullanýyoruz onu. Eðer $t$ küçükse, sýfýra
yakýnsa o zaman bariyer baskýn haldedir (daha büyüktür), tabii o zaman
sýnýrlardan kaçmak optimizasyon için daha önemli hale gelir. Eðer $t$'yi
büyütürsek bu ``orijinal kriter $f$'ye daha fazla önem ver'' demektir.

Dersin geri kalanýnda üstteki format yerine alttaki ile çalýþacaðýz,
üstteki ile ayný, 

$$
\min_x t f(x) + \phi(x) \quad \textrm{öyle ki}
$$
$$
Ax = b
\mlabel{2}
$$


Problemi yaklaþýk olarak temsil ettik. Uygun þartlarda problem (2)'nin
çözümü vardýr. Neden olduðunu görmek zor deðil, $\phi$ beni sýnýrlardan
uzak tutar, ve hedefim $f$'ye ulaþýrým, lineer diðer sýnýrlarý dikkate
alarak tabii. Gidis yolu neye benzerdi?

Merkezi gidiþ yolu, (1) için KKT koþullarý üzerinden gösterilebilir?  Bu
yol (1) probleminin $t$ üzerinden fonksiyon haline getirilmiþ
çözümüdür. Her $t$ için (1)'i çözebilirim, çözüme $x^*(t)$ derim, ve
$x^*$'ye $t$'nin bir fonksiyonu olarak bakýnca, bariyer metot çerçevesinde,
ortaya çýkan çözüm serisine merkezi gidiþ yolu denir. Her $t$ için (1)'in
çözümünü KKT koþullarý üzerinden temsil edebilirim, böylece her adýmda
$x^*(t)$'nin optimal olmasý için ne gerekir sorusunu formülize etmiþ
oluyorum.  Önce KKT ana olurluk þartlarý

$$
A x^* = b, \quad h_i(x^*(t)) \le 0
$$

Geri kalanlar, önce Lagrangian

$$
t f(x) + \phi(x) + w^T (Ax - b)
$$

Bu Lagrangian'in $x$ üzerinden gradyanýný alýp sýfýra eþitlersem duraðanlýk
þartýný elde ediyorum. 

$$
t \nabla f(x^*(t)) - \sum_{i=1}^{m} \frac{1}{h_i(x^* (t))} 
\nabla h_i(x^*(t)) + A^T w = 0
$$

Duraðanlýk için gradyan kullanmak yeterli çünkü tüm fonksiyonlar dýþbükey
ve pürüzsüz.

Üstteki problemi bazý herhangi bir $w$ için çözebilirsem o zaman merkezi
gidiþ yolundaki bir çözümü her $t$ için karakterize etmiþ olurum. Umudumuz
o ki $t$'yi sonsuzluða doðru iterken gördüðümüz KKT koþullarýnýn ima ettiði
çözümler orijinal problemdeki sonuçlara yaklaþsýn. Çünkü bariyer
fonksiyonunu $1/t$ ile çarptým ve $t$'yi büyüttükçe çarpým gösterge
fonksiyonuna yaklaþtýðýný biliyorum, o zaman optimizasyonu yaparken $t$'yi
büyüttükçe orijinal problemin gerçek sonucuna yaklaþmýþ olurum.

Þimdi bir lineer program üzerinden merkezi gidiþ yoluna bakalým. Basit bir
LP, eþitlik þartlarýný attým, 

$$
\min_x c^T x, \quad \textrm{öyle ki}
$$
$$
Dx \le e
$$

Bariyer olarak

$$
\min_x t c^T x - \sum _{i=1}^{m} \log(e_i - d_i^T x)
$$

Bu problemi belli bir $t$ için çözersem, sonra $t$'yi büyütürsem ve o yeni
$t$ için çözersem, bunu ardý ardýna yaparsam, umudum o ki orijinal LP'imin
sonucuna yaklaþacaðým.

\includegraphics[width=20em]{func_59_barr_01.png}

Resimde görüldüðü gibi, ortadan baþlýyoruz, $t=0$'da diyelim, ve $t$'yi
büyüttükçe yolda ilerliyoruz, ve sonuca eriþiyoruz. Gidiþ pürüzsüz, ve
LP'lerin karakterinden biliyoruz ki nihai sonuç çokyüzlümün (polyhedra)
ekstrem noktalarýnýn birinde olmalý. Yarý yolda $t=10$'daki bir nokta
egösteriliyor, nihai sonuç belki $t=100$'da [2, 26:59]

KKT koþulu üzerinden duraðanlýðý temiz bir þekilde gösterebiliyoruz, ya da
iç nokta ve ortada, merkezde bir yol takip edilmesini zorlama baðlamýnda,
merkezlik þartý da deniyor buna, gradyan alýnýnca

$$
= tc - \sum _{i=1}^{m} \frac{1}{e_i - d_t ^T x^(t)} d_i
$$

Bu demektir ki gradyan $\nabla (x^*(t))$, $-c$'ye paralel olmalýdýr, ya da
$\{ x: c^T x = c^T x^*(t) \}$ hiper düzlemi $\phi$'nin $x^*(t)$'deki
konturuna teðet durmalýdýr.

Birazdan merkezi yoldan ikiz noktalar alabileceðimizi göreceðiz. Bu çok
faydalý olacak çünkü bu ikiz noktalarý bir ikiz boþluðu hesaplamak için
kullanacaðýz. 

Merkezi yoldayken bu yoldaki noktalar $x*(t)$'leri kullanarak olurlu ikiz
noktalar hesaplayabiliriz. Orijinal problem (3)'e tekrar bakarsak, bu
problem için ikiz deðiþkenleri elde etmek için her eþitsizlik için bir
$u_i$'ye, her eþitlik þartý için bir $v_i$'ya ihtiyacým var. Onlarý nasýl
tanýmlarým? Merkezi yol üzerindeki çözümler üzerinden,

$$
u_i^*(t) = \frac{1}{t h_i(x^*(t))}, \quad i=1,..,m, \quad v^*(t) = w/t
$$

$w$ bariyer problemi için KKT koþullarýný çözerken elde ettiðim deðiþken
idi. 

Niye üsttekiler orijinal problem için olurlu? Bunu görmek kolay, ilk önce,
$u_i^*(t)$'nin her ögesi harfiyen pozitif, çünkü $h_i(x^*(t))$'nin her
ögesi harfiyen negatif. Bu bariyer probleminin ana olurluk þartýndan
geliyor. Ayrýca $(u^*(t),v^*(t))$ Lagrange ikiz fonksiyonu $g(u,v)$'nin
taným kümesinde (domain). Hatýrlarsak Lagrange ikizi formülize ettiðimizde
taným kümesinde bazý dolaylý sýnýrlamalar elde ediyorduk. Tarif itibariyle

$$
\nabla f(x^*(t)) + \sum _{i=1}^{m} u_i (x^*(t)) \nabla h_i(x^*(t)) + 
A^T v^*(t) = 0
$$


Yani $x^*(t)$, Lagrangian $L(x,u^*(t),v^*(t))$'i tüm $x$'ler üzerinden
minimize edeceði için $g(u^*(t),v^*(t)) > -\infty$. Bu direk duraðanlýk
þartýndan geliyor iþte. O kadar bariz birþey ki aslýnda bazen kafa
karýþtýrýyor. Merkezi yol probleminden çözdüðümüz duraðanlýk koþulu
þöyleydi,

$$
t \nabla f(x^*(t)) - \sum \frac{1}{h_i(x^*(t))} \nabla h_i(x^*(t)) + A^T w= 0
$$

Bir $x^*$ çözümü ve $w$ olduðunu farz ediyoruz. Tüm formülü $t$ ile
bölersem,

$$
\nabla f(x^*(t)) - \sum \frac{1}{t h_i(x^*(t))} \nabla h_i(x^*(t)) + A^T \frac{w}{t}= 0
$$

Tek yaptýðýmýz ``üstteki orijinal problemdeki duraðanlýk þartýna çok
benziyor'' demek, deðil mi, çünkü 

$$
\nabla f(x^*(t)) +
\sum \underbrace{\frac{-1}{t h_i(x^*(t))}}_{u_i} \nabla h_i(x^*(t)) + 
A^T \underbrace{\frac{w}{t}}_{v}= 0 
\mlabel{4}
$$

desem, orijinal problemin duraðanlýk þartýna benzeyen bir ifade elde etmiþ
olurum [2, 33:48]. Demiþtik ki üstteki $u_i,v$ tanýmlarý üzerinden orijinal
problem için olurlu ikiz noktalarý alabiliyoruz. 

Soru: niye orijinal problem için optimal noktalarý elde etmedim? Biraz önce
gördük, duraðanlýk koþulunu tatmin ettim, ana, ikiz olurluk
var.. ama.. tamamlayýcý gevþeklik tatmin edilmedi. Çok önemli. Onun yerine
ne var? $u_i h_i (x) = 0$ olmasý lazým, onun yerine ne var? Taným
itibariyle $u_i = -1/t ..$ var. O zaman sýfýra yakýnsak çok yakýnsak bölüm
büyür, dolaylý olarak $t$'yi büyüttükçe orijial problemin KKT koþullarýný
yaklaþýksallamýþ oluruz. O zaman log bariyer problemini çözmüþ olmamýza
raðmen belli bir $t$ deðer için orijinal problem için çözüm olmamasýnýn
sebebi tamamlayýcý gevþekliðin tatmin edilmiyor olmasý.

Ama elimizdekiler hala çok faydalý, çünkü herhangi bir anda merkez yol
üzerinde $t$'nin fonksiyonu olarak ne kadar alt optimal olduðumuzu
sýnýrlamak mümkün oluyor. Bunun için sadece ikiz boþluðunu hesaplýyoruz, o
kadar. O zaman $u^*,v^*$'da Lagrange ikiz fonksiyonu hesaplýyorum, ve $f^*$
ile bu Lagrange farkýný buluyorum, ve ikiz boþluðu hesaplanmýþ oluyor. 
(4)'te gördük ki $x^*$ Lagrangian'i $u^*,v^*$'da minimize eder, o zaman
ikizi alttaki gibi hesaplayarak 

$$
g(u^*(t),v^*(t)) = 
f(x^*(t)) + \sum_{i=1}^{m} u_i^*(t) h_i(x^*(t)) + v^*(t)^T (Ax^*(t) - b)
$$

Büyük toplamdaki ikinci terim sýfýr, çünkü merkezi yolda $Ax^*$ her zaman
$b$'ye eþittir. Birince terimde, $u_i$'i $-1/t$'ye eþitledik, ve bu $m$
kere toplanacak, sonuç

$$
= f(x^*(t)) - m/t
$$

Yani göstermiþ olduk ki merkezi yolun optimallikten olan uzaklýðý en fazla
$m/t$ olacaktýr,

$$
f(x^*(t)) - f^* \le m/t
$$

Üstteki bariyer metorun iþlediðine dair ispata en yakýn sonuç, bize diyor
ki eðer herhangi bir $t$ için bariyer problemini çözersem optimalliðe
yakýnlýk her zaman $m/t$'den küçük olur. $t$'yi isteðe baðlý olarak
büyüttükçe o ölçüde optimalliðe yaklaþmýþ olurum. 

Merkezi yolu yorumlamanýn bir diðer yolu ``sarsýma uðratýlmýþ KKT
koþullarý'' denen bir teknik üzerinden. Þimdiye kadar gördük ki merkez yol
ve ona tekabül eden ikiz deðerler (4)'teki duraðanlýk þartýný çözüyor. 
$u_i \ge 0$, $h_i(x) \le 0$, ve $Ax = b$. Tamamlayýcý gevþeklik haricinde tüm
koþullar tatmin. Esas KKT koþullarýnda 

$$
u_i^*(t) \cdot h_i^*(t) = 0, \quad i=1,..,m
$$

olurdu, biz onun yerine 

$$
u_i^*(t) \cdot h_i^*(t) = -1/t, \quad i=1,..,m
$$

dedik. Yani bir anlamda log bariyer fonksiyonunu unutuyoruz, onun yerine þu
probleme bakýyoruz,

$$
\nabla f(x^*(t)) + \sum_{i=1}^{m} u_i(x^*(t))\nabla h_i(x^*(t)) + A^T v^*(t) = 0
$$

$$
u_i^*(t) \cdot h_i^*(t) = -1/t, \quad i=1,..,m
$$

$$
h_i(x^*(t)) \le 0, \quad i=1,..,m, \quad Ax^*(t) = b
$$

$$
u_i(x^*(t)) \ge 0
$$

Ve $t$'yi büyüterek üstteki problemi çözüyorum. Yani olurluðu, duraðanlýðý
tam olarak, tamamlayýcý gevþekliði ise yaklaþýk olarak çözmüþ oluyorum, ve
yaklaþýksallýðý gittikçe büyüyen $t$'ler üzerinden daha sýký hale getiriyorum. 
Yani log bariyer tekniði ile sarsýma uðratýlmýþ KKT koþullarý sýnýrlý
problemleri çözmenin iki yolu. 

Ekler

Esitsizlik Sýnýrlamalarý 

Optimizasyon problemimizde 

$$
\min_x f(x), 
\quad 
\textrm{oyle ki}, 
\quad 
c_i(x) \ge 0, 
\quad i=1,2,..,m
$$

$c_i$ ile gösterilen eþitsizlik içeren (üstte büyüklük türünden)
kýsýtlamalar olduðunu düþünelim. Bu problemi nasýl çözeriz?

Bir fikir, problemin eþitizliklerini bir gösterge (indicator)
fonksiyonu üzerinden, Lagrange yönteminde olduðu gibi, ana hedef
fonksiyonuna dahil etmek, ve elde edilen yeni hedefi kýsýtlanmamýþ bir
problem gibi çözmek. Yani üstteki yerine, alttaki problemi çözmek,

$$
\min_x f(x) + \sum_{i=1}^{m} I(c_i(x))
$$

ki $I$ pozitif reel fonksiyonlar için göstergeç fonksiyonu,

$$
I(u) = 
\left\{ \begin{array}{ll}
0 & u \le 0 \\
\infty & u > 0
\end{array} \right.
$$

Bu yaklaþýmýn nasýl iþleyeceðini kabaca tahmin edebiliriz. $I$ fonksiyonu
0'dan büyük deðerler için müthiþ büyük deðerler veriyor, bu sebeple
optimizasyon sýrasýnda o deðerlerden tabii ki kaçýnýlacak, ve arayýþ
istediðimiz noktalara doðru kayacak. Tabii $x_1 > 3$ gibi bir þart varsa
onu $x_1 - 3 > 0$ þartýna deðiþtiriyoruz ki üstteki göstergeci
kullanabilelim. Bu yaklaþýma "bariyer metotu" ismi veriliyor çünkü $I$ ile
bir bariyer yaratýlmýþ oluyor.

Fakat bir problem var, göstergeç fonksiyonunun türevini almak, ve pürüzsüz
rahat kullanýlabilen bir yeni fonksiyon elde etmek kolay deðil. Acaba $I$
yerine onu yaklaþýk temsil edebilen bir baþka sürekli fonksiyon kullanamaz
mýyýz?

Log fonksiyonunu kullanabiliriz. 
O zaman eldeki tüm $c_i(x) \ge 0$ kýsýtlamalarýný

$$
- \sum_{i=1}^{m} \log c_i(x)
$$

ile hedef fonksiyonuna dahil edebiliriz, yeni birleþik fonksiyon,

$$
P(x;\mu) = f(x) - \mu \sum_{i=1}^{m} \log c_i(x) 
$$

olur. Böylece elde edilen yaklaþým log-bariyer yaklaþýmý
olacaktýr. Mýnýmizasyon sýrasýnda hem baþta bariyerden kaçýnilmiþ olunacak,
hem de $\mu$ küçükdükçe hedefin geri kalanýnda istenilen minimal deðerlere
doðru kayýlmýþ olunacak. 

Algoritma olarak optimizasyon þu þekilde gider;

1) Bir $x$ ve $\mu$ deðerinden baþla.

2) Newton metotu ile birkaç adým at (durma kriteri yaklaþýma göre deðisebilir)

3) $\mu$'yu küçült

4) Ana durma kriterine bak, tamamsa dur. Yoksa baþa dön

Bu yaklaþýmýn dýþbükey (convex) problemler için global minimuma
gittiði ispatlanmýþtýr [4, sf. 504].

Örnek

$\min (x_1 + 0.5)^2 + (x_2 - 0.5)^2$ problemini çöz, $x_1 \in [0,1]$ ve $x_2 \in
[0,1]$ kriterine göre.

Üstteki fonksiyon için log-bariyer,

$$
P(x;\mu) = (x_1 + 0.5)^2 + (x_2-0.5)^2 -
\mu 
\big[
\log x_1 + \log (1-x_1) + \log x_2 + \log (1-x_2)
\big]
$$

Bu formülasyonu nasýl elde ettiðimiz bariz herhalde, $x_1 \ge 0$ ve
$x_1 \le 1$ kýsýtlamalarý var mesela, ikinci ifadeyi büyüktür
iþaretine çevirmek için eksi ile çarptýk, $-x_1 \ge 1$, ya da $1-x_1
\ge 0$ böylece $\log(1-x_1)$ oldu.

Artýk Newton yöntemini kullanarak sanki elimizde bir kýsýtlanmasý
olmayan fonksiyon varmýþ gibi kodlama yapabiliriz, $P$'yi minimize
edebiliriz. Newton yönü $d$ için gereken Hessian ve Jacobian
matrislerini otomatik türevle hesaplayacaðýz, belli bir noktadan
baþlayacaðýz, ve her adýmda $d = -H(x)^{-1} \nabla f(x)$ yönünde adým
atacaðýz.

\begin{minted}[fontsize=\footnotesize]{python}
from autograd import numpy as anp, grad, hessian, jacobian
import numpy.linalg as lin

x = np.array([0.8,0.2])
mu = 2.0
for i in range(10):
    def P(x):
    	x1,x2=x[0],x[1]
    	return (x1+0.5)**2 + (x2-0.5)**2 - mu * \
	   (anp.log(x1) + anp.log(1-x1) + anp.log(x2)+anp.log(1-x2))

    h = hessian(P)
    j = jacobian(P)
    J = j(np.array(x))
    H = h(np.array(x))
    d = np.dot(-lin.inv(H), J)
    x = x + d
    print (i, x, np.round(mu,5))
    mu = mu*0.1
\end{minted}

\begin{verbatim}
0 [0.61678005 0.34693878] 2.0
1 [-0.00858974  0.486471  ] 0.2
2 [-0.02078755  0.49999853] 0.02
3 [-0.18014768  0.5       ] 0.002
4 [-0.49963245  0.5       ] 0.0002
5 [-0.50002667  0.5       ] 2e-05
6 [-0.50000267  0.5       ] 0.0
7 [-0.50000027  0.5       ] 0.0
8 [-0.50000003  0.5       ] 0.0
9 [-0.5  0.5] 0.0
\end{verbatim}

Görüldüðü gibi 5. adýmda optimal noktaya gelindi, o noktada $\mu$
oldukca küçük, ve bariyerle tanýmladýðýmýz yerlerden uzak duruldu,
optimal nokta $x_1=-0.5,x_2=0.5$ bulundu.

Kaynaklar

[1] Tibshirani, {\em Convex Optimization, Lecture Video 14}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}   

[2] Tibshirani, {\em Convex Optimization, Lecture Video 15, Part 1}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}   

[3] Tibshirani, {\em Convex Optimization, Lecture Video 15, Part 2}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}   

[4] Nocedal, {\em Numerical Optimization}

\end{document}



