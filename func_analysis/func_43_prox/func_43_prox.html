<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="proksimal-yakınsal-gradyan-metotu-proximal-gradient-method">Proksimal / Yakınsal Gradyan Metotu (Proximal Gradient Method)</h1>
<p>Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uğraşmak yerine belli bir yapıya uyan çetrefil fonksiyonları optimize etmeye uğraşır. Bu yapı</p>
<p><span class="math display">\[
f(x) = g(x) + h(x) 
\qquad (1)
\]</span></p>
<p>formundadır [1, 45:59]. <span class="math inline">\(g,h\)</span>'in ikisi de dışbükey. <span class="math inline">\(g\)</span>'nin pürüzsüz, türevi alınabilir olduğu farz edilir (çoğunlukla oldukca çetrefil olabilir) ve <span class="math inline">\(\mathrm{dom}(g) = \mathbb{R}^n\)</span>, <span class="math inline">\(h\)</span> ise pürüzsüz olmayabilir. Tabii pürüzsüz artı pürüzsüz olmayan iki fonksısyon toplamı kriterin tamamını pürüzsüz olmayan hale çevirir. Ama bu toplam daha basittir denebilir, onun üzerinde proksimal operatörü uygulanabilir.</p>
<p>Hatırlarsak eğer <span class="math inline">\(f\)</span> türevi alınabilir olsaydı gradyan iniş güncellemesi</p>
<p><span class="math display">\[
x^{+} = x - t \cdot \nabla f(x)
\]</span></p>
<p>Bu formüle erişmenin bir yöntemi karesel yaklaşıklama üzerinden idi, <span class="math inline">\(f\)</span>'nin <span class="math inline">\(x\)</span> etrafındaki yaklaşıklamasında <span class="math inline">\(\nabla^2f(x)\)</span> yerine <span class="math inline">\(\frac{1}{t} I\)</span> koyunca,</p>
<p><span class="math display">\[
x^{+} = \arg\min_x f(x) + 
\underbrace{
  \nabla f(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 
}_{f_t(z)}
\]</span></p>
<p>elde ediliyordu. Yani gradyan inişi sanki ardı ardına geldiği her noktada bir karesel yaklaşıklama yapıyor, ve onu adım atarak minimize etmeye uğraşıyor.</p>
<p>Ama (1)'deki <span class="math inline">\(f\)</span> pürüzsüz değil, o sebeple üstteki mantık ise yaramayacak. Fakat, belki de karesel yaklaşıklamanın bir kısmını hala kullanabiliriz, ve minimizasyonu sadece <span class="math inline">\(g\)</span>'ye uygularız çünkü pürüzsüz olan kısım o. Yani niye <span class="math inline">\(g\)</span>'nin yerine karesel yaklaşıklama koymayalım? Şimdi bunu yapacağız [1, 50:10].</p>
<p><span class="math display">\[
x^{+} = \arg\min_z \tilde{g}_t(z) + h(z)
\]</span></p>
<p>ki <span class="math inline">\(\tilde{g}_t(z)\)</span> <span class="math inline">\(g\)</span>'nin <span class="math inline">\(x\)</span> noktası etrafındaki karesel yaklaşıklaması oluyor. Eğer elimizde <span class="math inline">\(h\)</span> olmasaydı üstteki sadece bir gradyan güncellemesine indirgenebilirdi. Neyse açılımı yaparsak</p>
<p><span class="math display">\[
= \arg\min_z g(x) + \nabla g(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 + h(z)
\]</span></p>
<p>Daha uygun bir formda yazabiliriz, <span class="math inline">\(z\)</span>'nin <span class="math inline">\(g\)</span> üzerindeki değişikliğe karesel uzaklığı olarak,</p>
<p><span class="math display">\[
= \arg\min_z \frac{1}{2t} 
\big|\big| z - \big( x-t\nabla g(x) \big) \big|\big|_2^2 + 
h(z)
\]</span></p>
<p>Yani söylenmek istenen, hem sadece <span class="math inline">\(g\)</span> olsaydı atacağımız gradyan adımına yakın durmaya çalışmak, hem de <span class="math inline">\(h\)</span>'nin kendisini ufak tutmak. Dışarıdan ayarlanan <span class="math inline">\(t\)</span> parametresi <span class="math inline">\(g\)</span> gradyan adımı ile <span class="math inline">\(h\)</span> arasında bir denge kurmak gibi görülebilir, eğer <span class="math inline">\(t\)</span> ufak ise o zaman gradyan adımına yakın durmaya daha fazla önem atfetmiş olacağız, <span class="math inline">\(h\)</span>'nin ufak tutulmasına daha az. <span class="math inline">\(t\)</span> çok büyük ise tam tersi olacak.</p>
<p>Bu aslında kabaca Proksimal Gradyan İnişi yöntemini tarif etmiş oluyor. Şimdi proksimal eşlemesi operatörünü göstereceğiz, ki bu algoritmaları daha temiz olarak yazmamıza yardımcı olacak.</p>
<p>Bir <span class="math inline">\(h\)</span> fonksiyonu için bir <span class="math inline">\(\mathrm{prox}\)</span> operatörü tanımlıyoruz,</p>
<p><span class="math display">\[
\mathrm{prox}_{t} (x) = \arg\min_z \frac{1}{2t} ||x-z||_2^2 + h(z)
\qquad (3)
\]</span></p>
<p>Üstteki <span class="math inline">\(x\)</span>'in bir fonksiyonu olarak <span class="math inline">\(\arg\min\)</span>'de gösterilen kriteri minimize eden <span class="math inline">\(z\)</span>'yi buluyor. Operatör <span class="math inline">\(t\)</span>'ye bağlı, dışarıdan verilen parametre. Tabii ki <span class="math inline">\(h\)</span>'ye de bağlı, ki bazen üstteki operatörü <span class="math inline">\(\mathrm{prox}_{h,t}\)</span> olarak gösteren de oluyor.</p>
<p>Üstteki ifadenin eşleme olduğunu kontrol edelim. Ciddi tanımlı bir fonksiyon üstteki değil mi? Ona bir <span class="math inline">\(x\)</span> veriyorsunuz, o da size tek bir sonuç döndürüyor. Ayrıca bu eşleme / fonksiyonun kendisi bir minimizasyon. Peki bu minimizasyon problemi dışbükey mi? Evet. Peki bu problemin özgün bir sonucu var mıdır? Vardır çünkü üstteki problem harfiyen dışbükey. Değil mi? Eğer <span class="math inline">\(h\)</span> harfiyen dışbükey olmasa bile ifadenin tümü harfiyen dışbükey olurdu çünkü <span class="math inline">\(\frac{1}{2t} ||x-z||_2^2\)</span> harfiyen dışbükey [1, 55:09], <span class="math inline">\(z-x\)</span>'in karesi var [ayrıca <span class="math inline">\(x\)</span> değişkeni <span class="math inline">\(h\)</span>'ye geçilmiyor].</p>
<p>Yani harfiyen dışbükey, o zaman özgün sonuç var. Biz de bu özgün sonucu alarak bir iniş algoritması yazıyoruz [1, 56:28],</p>
<p><span class="math display">\[
x^{(k)} = \mathrm{prox}_{t_k} \big( 
x^{(k-1)} - t_k \nabla g(x^{(k-1)})
\big), \quad k=1,2,...
\qquad (2)
\]</span></p>
<p>Güncelleme adımını tanıdık şekilde yazmak için</p>
<p><span class="math display">\[
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k} (x^{(k-1)})
\]</span></p>
<p>ki <span class="math inline">\(G_t\)</span>'ye <span class="math inline">\(f\)</span>'nin genelleştirilmiş gradyanı denebilir,</p>
<p><span class="math display">\[
G_t(x) = \frac{x - \mathrm{prox}_{t}(x - t \nabla g(x))}{t}
\]</span></p>
<p><span class="math inline">\(G\)</span>'nin dışbükey fonksiyonların alışılageldik gradyanlarına benzer pek çok özelliği vardır, ki bu özellikler proksimal metotların yakınsadığıyla alakalı ispatlarda kullanılabilir.</p>
<p>Şimdiye kadar anlattıklarımıza bakanlara bu komik bir hikaye gibi gelebilir. Bir <span class="math inline">\(g + h\)</span> toplamını minimize etmek istiyordum, bunu yapabilmek için (2) formunda adımlar atacağım, bir prox operatörüm var, ama bu operatör bir sonuç döndürüyor aslında, ve bu sonuç bir başka minimizasyondan geliyor. Yani <span class="math inline">\(g + h\)</span> türü bir toplam minimizasyonu yerine her adımda, bir sürü minimizasyonları koymuş oldum. Bu nasıl daha iyi bir sonuç verecek ki?</p>
<p>Şunu belirtmek lazım, sadece eğer proksimal gradyanları analitik, ya da hızlı bir şekilde hesaplayabiliyorsak onları çözüm için düşünürüz. Yani her ne kadar her adımda (3) turu optimizasiyonlar yapıyorsak ta, bunu pürüzsüz olan kısım <span class="math inline">\(h\)</span> yeterince basit olduğu zaman yapıyoruz ki tüm (3) için analitik ya da hızlı hesapsal çözüm olsun [1, 58:54].</p>
<p>Diğer noktalar, dikkat edersek proksimal operatör <span class="math inline">\(g\)</span>'ye bağlı değil, tamamen <span class="math inline">\(h\)</span> bazlı. Eğer <span class="math inline">\(h\)</span> basit ise ama <span class="math inline">\(g\)</span> müthiş çetrefil ise bu proksimal hesaplarını çok zorlaştırmıyor. Eğer o çok çetrefil (ve pürüzsüz) <span class="math inline">\(g\)</span> için gradyan hesaplanabiliyorsa, durumu kurtardık demektir.</p>
<p>Tekrar bir Lasso problemi göreceğiz. Bu Lasso için gördüğümüz ikinci algoritma, ve belirtmek gerekir ki Proksimal metot altgradyan metotuna göre çok daha verimlidir, hızlıdır.</p>
<p>Verili bir <span class="math inline">\(y \in \mathbb{R}^n\)</span>, <span class="math inline">\(X \in \mathbb{R}^{n \times p}\)</span> için Lasso kriterini hatırlarsak,</p>
<p><span class="math display">\[
f(\beta) = \frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1 
\]</span></p>
<p>Kriterdeki ilk terim en az kareler kayıp fonksiyonu, ikinci terim bir ayar parametresi üzerinden katsayıların 1. normu. Bu kriteri pürüzsüz, ve pürüzsüz olmayan ama basitçe olan iki kısma ayıracağız, yani zaten oldukca bariz, pürüzsüz kısım 1. terim, <span class="math inline">\(g(\beta)\)</span> diyelim, olmayan 2. terim, <span class="math inline">\(h(\beta)\)</span> diyelim. Proksimal gradyan inişi için bize iki şey gerekiyor, birincisi <span class="math inline">\(g\)</span>'nin gradyanı, ikincisi <span class="math inline">\(h\)</span> için prox operatörünü hesaplayabilmek.</p>
<p><span class="math inline">\(g\)</span>'nin gradyanı oldukca basit, onu bu noktada uykumuzda bile bulabiliyor olmamız lazım. <span class="math inline">\(h\)</span>'nin prox operatörü,</p>
<p><span class="math display">\[
\mathrm{prox}_t(\beta) = \arg\min_z \frac{1}{2t} ||\beta-z||_2^2 + \lambda ||z||_1
\]</span></p>
<p>Her şeyi <span class="math inline">\(t\)</span> ile çarparsam,</p>
<p><span class="math display">\[
\mathrm{prox}_t(\beta) = \arg\min_z \frac{1}{2} ||\beta-z||_2^2 + \lambda t ||z||_1
\]</span></p>
<p>Üstteki minimizasyonun çözümünü daha önce altgradyanlar üzerinden görmüştük,</p>
<p><span class="math display">\[
= S_{\lambda t}(\beta)
\]</span></p>
<p>Yine yumuşak eşikleme (soft-threshold) operatörüne gelmiş olduk,</p>
<p><span class="math display">\[
[ S_{\lambda} (\beta) ]_i = 
\left\{ \begin{array}{ll}
\beta_i - \lambda &amp; \textrm{eğer } \beta_i &gt; \lambda_i \\
0 &amp; \textrm{eğer } -\lambda \ge \beta_i \ge \lambda, \quad i=1,..,n \\
\beta_i + \lambda &amp; \textrm{eğer } \beta_i &lt; -\lambda_i 
\end{array} \right.
\]</span></p>
<p>Tüm algoritma neye benziyor? Önce <span class="math inline">\(g\)</span>'ye göre bir graydan güncellemesi yaparım, gradyan</p>
<p><span class="math display">\[
\nabla g(\beta) = -X^T (y-X\beta)
\]</span></p>
<p>O zaman güncelleme</p>
<p><span class="math display">\[\beta + t X^T (y-X\beta)\]</span></p>
<p>olur. Buna prox uygularsak,</p>
<p><span class="math display">\[
\beta^+ = S_{\lambda t}(\beta + t X^T (y-X\beta))
\]</span></p>
<p>Yumuşak eşikleme ne yapar? Her ögeye teker teker bakar, eğer mutlak değeri çok ufaksa onu ya sıfıra eşitler, ya da onu <span class="math inline">\(\lambda \cdot t\)</span> kadar sıfıra yaklaştırır.</p>
<p>Üstteki Lasso algoritmasina ISTA adı da verilir.</p>
<p>Geriye Çizgisel İz Sürme (Backtracking line search)</p>
<p>Graydan inişinde görmüştük ki adım büyüklüklerini dinamik olarak seçebiliyorduk, her adımdaki duruma adapte olabiliyorduk, tipik olarak Lipschitz sabitini bilmiyoruz çünkü. O sebeple geriye iz sürme pratikte iyi işlemesiyle beraber, teorik olarak yakınsamayı da garantiliyordu.</p>
<p>Proksimal gradyanları için benzer bir kavram geçerli. Ayrıca proksimal durumda geriye doğru iz sürmenin birden fazla yolu var [1, 1:10:23]. Ben sadece <span class="math inline">\(g\)</span> üzerinde işlem yapan bir yöntem seçtim, çünkü bu metotu hatırlaması daha kolay. Gradyan inişi için iz sürmeyi hatırlarsak, <span class="math inline">\(x\)</span> noktasındayız diyelim ve <span class="math inline">\(x-t \nabla g\)</span> yönünde gitmek istiyoruz, o zaman alttakinin doğru olup olmadığını kontrol ediyorduk,</p>
<p><span class="math display">\[
f(x - t\nabla f(x) ) &gt; f(x) - \frac{t}{2} ||\nabla f||_2^2 
\]</span></p>
<p>Proksimal gradyan için benzer bir yöntem [1:11:57], ve <span class="math inline">\(G\)</span> üzerinde işlem yapıyoruz dikkat, <span class="math inline">\(h\)</span> değil, yani <span class="math inline">\(G\)</span> üzerinden yeterince &quot;iniş'' yapmaya uğraşacağız, <span class="math inline">\(g+h\)</span> değil. Normal iz sürmede üsttekinin doğruluğunu kontrol ediyoruz, doğru ise <span class="math inline">\(t\)</span>'yi belli bir ölçüde ufaltıyoruz. Doğru değilse yani yeterince iniş yaptıysak, o zaman eldeki değerlerle güncellemeyi yapıyoruz.</p>
<p><span class="math display">\[
g(x - t G_t (x) ) &gt; g(x) - t \nabla g(x)^T G_x(x) + \frac{t}{2} ||G_x(x)||_2^2
\]</span></p>
<p>Yani gradyan güncellemesi 1. derece Taylor güncellemesinden geldi. Üstteki ifadede de eğime ek olarak 1. derece Taylor güncellemesine göre de yeterince iniş yapmış olmak istiyorum. Dikkat edersek eğer <span class="math inline">\(G\)</span> yerine <span class="math inline">\(g\)</span>'nin gradyanını koyarsam, iki üstteki formüle benzer bir formül elde ederim.</p>
<p>Geriye iz sürme genel algoritmasi şöyle, bir <span class="math inline">\(0 &lt; \beta &lt; 1\)</span> paremetresi var (dışarıdan ayarlanan bir parametre). <span class="math inline">\(t=1\)</span> ile başlıyoruz, ve üstteki formülü işletiyoruz, eğer gerekiyorsa <span class="math inline">\(t = \beta t\)</span> ile küçültme yapıyoruz. İz sürme bitince bulduğumuz <span class="math inline">\(t\)</span> ile güncelleme yapıyoruz [3, 07:35].</p>
<p>[bazi ek detaylar atlandi]</p>
<p>Matris Tamamlamasi</p>
<p>Şimdi prox operatörü sofistike olan bir örnek görelim [3, 11:38]. Buradan çıkan algoritma ilginç olacak.</p>
<p>Bize bir <span class="math inline">\(Y \in \mathbb{R}^{m \times n}\)</span> matrisi veriliyor ama biz sadece bu matrisin bazı öğelerini görebiliyoruz, bu öğeler <span class="math inline">\(Y_{i,j}, (i,j) \in \omega\)</span> ile belirtiliyor ki <span class="math inline">\(\omega\)</span> belli bir indis kümesidir. Bu problem bir tavsiye sistemi olabilir, matrisin tamamı bir ideal müşteri / ürün eşlemesidir, biz sadece bu matrisin belli bir kısmını görüyoruz (tipik olarak mevcut müşterilerin tarihi veride yaptığı alımlar, ürünler üzerindeki beğendi/beğenmedi yorumları matrisin &quot;görünen'' kısmını temsil edebilir).</p>
<p>Bu tür problemleri iz norm regülarizasyonu (trace norm regularization) problemi olarak görmenin iyi işlediği görülmüştür. Bu problem aslında daha önce gördüğümüz Lasso problemine benzer, onun matrisler için olan formudur bir açıdan. Problem,</p>
<p><span class="math display">\[
\min_B \frac{1}{2} \sum_{(i,j) \in \omega} (Y_{ij} - B_{ij})^2 + \lambda ||B||_{tr}
\qquad (4)
\]</span></p>
<p>ki <span class="math inline">\(||B||_{tr}\)</span> iz (ya da nükleer) normudur,</p>
<p><span class="math display">\[
||B||_{tr} = \sum_{i=1}^{r} \sigma_i(B)
\]</span></p>
<p>ile gösterilir, <span class="math inline">\(r\)</span> kertedir, <span class="math inline">\(r = \mathrm{rank}(B)\)</span>, ve herhangi bir <span class="math inline">\(X\)</span> matrisi için <span class="math inline">\(\sigma_1(X) \ge .. \ge \sigma_r(X) \ge 0\)</span>, <span class="math inline">\(X\)</span>'in eşsiz (singular) değerleridir.</p>
<p>Minimizasyon ifadesindeki ilk toplam, <span class="math inline">\(B\)</span>'deki değerleri zaten görülen, bildiğimiz değerlere <span class="math inline">\(Y\)</span>'ye karesel kayıp yakın tut diyor, ve ona bir ayar parametresi üzerinden <span class="math inline">\(B\)</span>'nin iz normunu ekliyoruz, bu istatistiki bağlamda bir tür regülarizasyon yapmış oluyor. Eğer bu ek olmsaydı problem kötü konumlanmış (ill-posed) olurdu [3, 15:06]. Eğer o terim olmasaydı ve <span class="math inline">\(\lambda\)</span> sıfır olsaydı o zaman optimizasyon <span class="math inline">\(B\)</span>'yi <span class="math inline">\(Y\)</span>'ye eşitlerdik, oldu bitti derdik, ama o zaman hiçbir iş yapmamış olurduk. Sağdaki terim ekiyle yapmaya uğraştığımız <span class="math inline">\(Y\)</span>'ye olabildiğince yakın bir <span class="math inline">\(B\)</span> seçmek ve bu <span class="math inline">\(B\)</span>'nin düşük kerte olmasını zorlamak. Bu bir regülarizasyon yöntemi, diyelim <span class="math inline">\(B\)</span> şu şekilde</p>
<p><span class="math display">\[
\underbrace{B}_{m \times n} = 
\underbrace{U}_{m \times k}
\underbrace{V^T}_{k \times n}
\]</span></p>
<p>yani <span class="math inline">\(B\)</span>'yi güya oluşturan birer <span class="math inline">\(U,V\)</span>'nin boyutlarındaki <span class="math inline">\(k\)</span>'nin olabildiğince düşük olmasını istiyoruz.</p>
<p>Bir diğer bakış açısı, daha önceki Lasso'yla ilintilendirmek bağlamında, iz normu L1-normun matrisler için olan versiyonu olarak görmek. Eğer elimde bir köşegen matris olsaydı iz köşegendeki öğelerin (tek öğeler onlar) toplamı olurdu, ki bu bir vektörün 1-norm'unu almak gibi değil mi? Eğer köşegeni bir vektör gibi görürsem, ve matriste bu vektörden başka bir şey yoksa.. bağlantıyı görüyoruz herhalde.</p>
<p>Bir anlamda <span class="math inline">\(||B||_{tr}\)</span> <span class="math inline">\(B\)</span> matrisinin kertesini yaklaşıksal olarak temsil ediyor çünkü kerte bir matrisin sıfır olmayan eşsiz değerlerinin sayısıdır. <span class="math inline">\(||B||_{tr}\)</span> tabii ki eşsiz değerlerinin sayısı değil onların kendilerinin toplamı, ama yaklaşıksal olarak kullanabileceğimiz bir şey bu y [3, 17:49] , çünkü değer toplamı dışbükey [altgradyan, türevi alınabildiği için bu yaklaşıklık seçilmiş herhalde]. (4) problemi tamamen dışbükey bu arada, ilk terim dışbükey, ve ikinci terim düzgün bir norm'dur o zaman dışbükeydir.</p>
<p>Bilimciler proksimal gradyan kullanmadan önce bu problem zor bir problemdi. Optimizasiyon problemi bir yarı kesin (semidefinite) program olarak tanımlanır. Eskiden bu tür problemleri iç nokta teknikleri ile çözüyorduk, ve bu teknikler üstteki gibi problemler üzerinde oldukca yavaştır.</p>
<p>Devam edelim, problemi şöyle hazırlarız, bir yansıtma operatörü <span class="math inline">\(P_\Omega\)</span> tanımlayalım, gözlenen kümeye yansıtma yapacağız,</p>
<p><span class="math display">\[
[P_\Omega (B) ]_{ij} = 
\left\{ \begin{array}{ll}
B_{ij} &amp; (i,j) \in \Omega \\
0 &amp; (i,j) \notin \Omega 
\end{array} \right.
\]</span></p>
<p>Bu operatörün yaptığı verilen bir matrisin gözlem olmayan her öge için değeri sıfır yapmak, yoksa olduğu gibi bırakmak. Simdi kriteri yazabiliriz,</p>
<p><span class="math display">\[
f(B) = 
\underbrace{\frac{1}{2} || P_\Omega (Y) - P_\Omega(B) ||_F^2}_{g(B)} + 
\underbrace{\lambda ||B||_{tr}}_{h(B)}
\]</span></p>
<p>Problemi proksimalin beklediği <span class="math inline">\(g + h\)</span> formuna soktuk. Hatırlatalım, <span class="math inline">\(g\)</span> dışbükey ama pürüzsüz değil. Bu yaklaşım için gerekenler neydi? Pürüzsüz kısım için gradyan hesaplayabilmek, ve pürüzsüz olmayan kısım için prox operatörünü hesaplayabilmek. Gradyan oldukca basit,</p>
<p><span class="math display">\[
\nabla g(B) = -(  P_\Omega (Y) - P_\Omega(B) ) 
\qquad (8)
\]</span></p>
<p>Sadece işaretleri biraz değiştirdim çünkü onu sonra gradyandan çıkartacağız, o sebeple parantez dışında bir eksi işareti olması faydalı. Şimdi ikinci terim için prox operatörünü görelim.</p>
<p><span class="math display">\[
\mathrm{prox}_t(B) = \arg\min_Z \frac{1}{2t} || B - Z ||_F^2 + \lambda ||Z||_{tr}
\qquad (5)
\]</span></p>
<p>Tüm mümkün <span class="math inline">\(Z\)</span> matrisleri üzerinden minimizasyon yapıyoruz, ve <span class="math inline">\(B_i\)</span> ile <span class="math inline">\(Z_i\)</span> arasındaki tüm farkların kare toplamlarını alıyoruz, artı bir sabit çarpı <span class="math inline">\(Z\)</span>'nin iz normu. Yani optimizasyon hedefi <span class="math inline">\(Z\)</span>, ve bu minimizasyonu prox operatörünü <span class="math inline">\(B\)</span> üzerinde uygulayarak elde ediyoruz [3, 22:15].</p>
<p>Üstteki ifadeyi hesaplamak için altgradyan matematiğine inmek gerekiyor biraz.</p>
<p><span class="math display">\[
\mathrm{prox}_t(B) = S_{\lambda t} (B)
\]</span></p>
<p>Yani prox'u <span class="math inline">\(\lambda\)</span> seviyesinde bir matriste yumuşak eşikleme olarak görmek mümkün. Yani aynen 1-norm'un prox operatörünün vektörsel yumuşak eşikleme olduğu gibi burada matris seviyesinde bir eşikleme var [3, 22:46]. Bu demektir ki herhangi bir matris <span class="math inline">\(B\)</span>'nin prox operatörü için <span class="math inline">\(B\)</span>'nin SVD'sini alıyoruz, yani <span class="math inline">\(B = U\Sigma V^T\)</span> diyoruz, ve köşegen matris <span class="math inline">\(\Sigma\)</span> üzerinde yumuşak eşikleme işletiyoruz, bunu yapmak için ya köşegendeki her ögeden <span class="math inline">\(\lambda_i\)</span> çıkartıyoruz ve eğer pozitifse o değeri kullanıyoruz, ya da sıfır alıyoruz.</p>
<p><span class="math display">\[
S_\lambda (B) = U \Sigma_\lambda V^T
\]</span></p>
<p><span class="math display">\[
(\Sigma_\lambda)_{ii}  = \max \{ \Sigma_{ii} - \lambda_i 0 \}
\]</span></p>
<p><span class="math inline">\(S_\lambda (B)\)</span> normal <span class="math inline">\(B\)</span>'den daha düşük kerteli olacaktır, çünkü üstteki işlem ile <span class="math inline">\(\Sigma_\lambda\)</span>'nin köşegenindeki bazı değerler sıfırlanır, ve bu matris ile hesaplanan <span class="math inline">\(U \Sigma_\lambda V^T\)</span>, çarpım sırasında <span class="math inline">\(U,V\)</span>'deki bazı satırlar, kolonları da sıfırlar [3, 26:00].</p>
<p>Peki <span class="math inline">\(\mathrm{prox}_t(B) = S_{\lambda t} (B)\)</span> nereden geliyor? Prox operatörünü oluşturan kriterin (5) altgradyanını alırız, sıfıra eşitleriz, ve ortaya çıkan formülü doğrulayan <span class="math inline">\(Z\)</span>'yi buluruz. Cevabın ne olacağını biliyoruz, sadece tarif edilen altgradyan vs sonrası aynı formülü bulup bulmayacağımızı kontrol edeceğiz.</p>
<p>(5)'teki ilk terim puruzsuz, direk gradyan alinir,</p>
<p><span class="math display">\[
0 \in Z - B + \lambda t \cdot \partial ||Z||_{tr} 
\qquad (7)
\]</span></p>
<p>İz normunun altgradyanını gösterelim,</p>
<p><span class="math display">\[
\partial ||Z||_{tr} = \{  UV^T + W : ||W||_{op} &lt; 1, U^TW = 0, WV = 0 \}
\qquad (6)
\]</span></p>
<p>Üstteki notasyon diyor ki aradığımız <span class="math inline">\(W\)</span>'ler en büyük eşsiz değeri 1'den küçük olan ve <span class="math inline">\(U\)</span>'nun kolonlarına ve <span class="math inline">\(W\)</span>'nin satırlarına dikgen olmalı. Yıne sonradan doğrulama ile ispat yapılabilir, üstteki tanımı baz alarak <span class="math inline">\(Z = S_{\lambda t}(B)\)</span> olarak formüle sokarsak sıfır sonucu aldığımızı göreceğiz. Ek bazı bilgiler, iz normu ve op normu birbirinin ikizidir. Yani</p>
<p><span class="math display">\[
||Z||_{tr} = \max_{||Y||_{op} \le 1}  Z \cdot Y
\]</span></p>
<p>Üstteki <span class="math inline">\(\cdot\)</span> işareti öğesel çarpım ve bu çarpımların toplamı anlamında. <span class="math inline">\(||Y||_{op} \le 1\)</span> şartına uyan tüm <span class="math inline">\(Y\)</span>'lere bakıyorum, ve bu <span class="math inline">\(Y\)</span>'lerle <span class="math inline">\(Z\)</span> arasında ögesel çarpım yapıyorum topluyorum bu bana <span class="math inline">\(Z\)</span>'nin iz normunu veriyor.</p>
<p>Ya da</p>
<p><span class="math display">\[
= \max_{||Y||_{op} \le 1}  \mathrm{tr} (Z^TY)
\]</span></p>
<p>işlemi de aynı kapıya çıkar, bahsettiğimiz ikizlik sayesinde bu mümkün oluyor. Devam edersek, üstteki bir <span class="math inline">\(\max\)</span> ifadesi ve bu ifadeleri altgradyanını almayı gördük, üstteki ifade üzerinden, <span class="math inline">\(||Y||_{op} \le 1\)</span> şartına uyan tüm <span class="math inline">\(Y\)</span>'ler içinden <span class="math inline">\(\mathrm{tr}{Z^TY}\)</span> maksimumunu gerçekleştiren, eşsiz değerlerin toplamını / iz normunu veren <span class="math inline">\(Y\)</span>'leri istiyoruz. Ve (6) içindeki tüm <span class="math inline">\(U,V\)</span>'lerin bunu yapabileceğini kontrol edebilirsiniz [3, 31:56], <span class="math inline">\(Z\)</span> SVD'sinden gelen <span class="math inline">\(U,V\)</span>'lerin herhangi bir öğesel çarpımı size <span class="math inline">\(Z\)</span>'nin eşsiz değerlerinin toplamını verecektir, ve tasarım itibariyle onların operatör normu en fazla 1'dır. $ UV^T$'un operatör normu en fazla 1'dır, <span class="math inline">\(W\)</span> için aynı şekilde, ve her iki terim birbirine dikgen olacak şekilde tasarlanmıştır. Tüm bu matrislerin $||Z||_{tr} $'in altgradyani olduğunu ispatlamanın yolu budur.</p>
<p>Neyse üsttekilerin bize altgradyan verdiğini ispatladıktan sonra <span class="math inline">\(Z = S_{\lambda t}(B)\)</span>'i (7)'ye sokup ispatı tamamlıyoruz.</p>
<p>Bu oldukca zor bir prox operatörüydü tabii. Hesaplamanabilir olduğu anlaşıldığında pek çok araştırmacı sevinmişti, pek çok yerde kullanıldı, vs. İspatı size ödev olarak veriyorum ama rutin bir ödev sorusu olmadığını belirtmek isterim.</p>
<p>Artık algoritmayı oluşturabiliriz, <span class="math inline">\(g\)</span> gradyanının negatif yönünde adım atıyoruz. Gradyanı (8)'de görmüştük.</p>
<p><span class="math display">\[
B^+ = S_{\lambda t} \bigg( 
B + t (P_\Omega(Y) - P_\Omega(B) )
\bigg)
\]</span></p>
<p>Büyük parantez içini gradyan ile hesaplıyoruz sonra elde edileni yumuşak eşikten geçiriyoruz. Bu ne demekti? Parantez içindekilerin SVD'sini al, eşsiz değeri <span class="math inline">\(\lambda t\)</span>'den küçük olan tüm değerleri sıfırla, ve kalanlardan <span class="math inline">\(\lambda t\)</span> çıkar, ve bu yeni değerlerle yeni bir matris yarat. Prox operatörünü uygulamak bu demek.</p>
<p>Bu arada pürüzsüz kısmımızdaki gradyan <span class="math inline">\(\nabla g(B)\)</span>, <span class="math inline">\(L=1\)</span> ile Lipschitz süreklidir. (8)'deki ifade lineer, sabit 1 üzerinden tabii ki Lipschitz. Yakınsama analiz bilgimiz bize diyor ki proksimal gradyan inişinde adım büyüklüğü en fazla <span class="math inline">\(1/L\)</span> olabilir, ki burada 1 sadece. Bu problemin güzel taraflarından biri bu, olabilecek en büyük adım büyüklüğünü kullanabiliyoruz ve onu hesaplaması kolay, sadece 1.</p>
<p>Ve <span class="math inline">\(t=1\)</span>'i üstteki formüle sokuyoruz,</p>
<p><span class="math display">\[
B^+ = S_{\lambda t} \bigg( B + P_\Omega(Y) - P_\Omega(B)  \bigg)
\]</span></p>
<p>elde ediyoruz. Formüldeki <span class="math inline">\(B - P_\Omega(B)\)</span> bize görülmeyen kümedeki herşeyi veriyor değil mi? <span class="math inline">\(B\)</span>'de herşey var, <span class="math inline">\(P_\Omega(B)\)</span>'de görülen kümedeki <span class="math inline">\(B\)</span> öğeleri var (geri kalanlar sıfır), o zaman bu çıkartmayı yapınca görülmeyen kümedeki herşeyi elde ederiz.</p>
<p><span class="math display">\[
= S_{\lambda t} \bigg( P_\Omega(Y) + P_{\Omega^C}(B) \bigg)
\]</span></p>
<p>olarak yazabiliriz, <span class="math inline">\(P_{\Omega^C}(B)\)</span>, <span class="math inline">\(P_{\Omega}(Y)\)</span>'in tamamlayıcısı (complement), görülmüş öğeler için sıfır, görülmemiş <span class="math inline">\(ij\)</span> öğeleri için <span class="math inline">\(B_{ij}\)</span>.</p>
<p>Algoritmanın mantığı çok doğal, her adımda tahminim <span class="math inline">\(P_{\Omega^C}(B)\)</span>'a bakıyorum, burada olanların görülmemiş kümede ne yapacağımı kontrol etmesine izin veriyorum, artı <span class="math inline">\(P_\Omega(Y)\)</span> ile orijinal veride görülen öğeleri alıyorum.</p>
<p>Yani bir matris oluşturuyorum, görülen yerlerde <span class="math inline">\(Y_{ij}\)</span> var görülmeyen yerlerde <span class="math inline">\(B_{ij}\)</span> var. Sonra bu matrisi düşük kerteli hale getiriyorum, SVD'sini alıyorum, eşsiz değerlerini işleyerek bazılarını sıfırlayıp tekrar matrisi oluşturuyorum yani. Ve bu tarif edilenleri ardı ardına yapıyorum. Elde edileni yeni <span class="math inline">\(B\)</span> yap, mevcut olmayan ogeler icin bu matrisi kullan, vs.</p>
<p>Bu algoritmaya yumuşak atfetme (soft-impute) algoritmasi da deniyor, çünkü her adımda kayıp değerleri &quot;yüklüyoruz'', yaklaşık olarak atıyoruz, ve bunu yumuşak bağlamda yapıyoruz, eşikleme ile yavaş yavaş kerteye düşürüyoruz.</p>
<p>Bu arada üstteki yaklaşım ilk bulunduğunda proksimal gradyan olduğu bilinmiyordu. Bilimci yaklaşımı buluyor, hakkında makale yazıyor, sonra sonra makaleyi revize ederken birdenbire anlıyor ki bu metot proksimal gradyan. Daha önce gördüğümüz İSTA da biraz böyle aslında, değil mi? Kendi başına durabilecek, gayet doğal bir mantığı olan bir algoritma, sanki proksimal yapısı dışında da bulunabilecek bir şey.</p>
<p>Devam edelim, algoritmadaki prox operatörü ne kadar pahalı? Her adımda SVD işletmemiz lazım, bu İSTA'dan farklı. SVD oldukca pahalı bir işlemdir, özellikle büyük matrisler için.</p>
<p>[atlandi]</p>
<p>Matris tamamlama böyle. Şimdi proksimal gradyan inişine kategorik olarak bakalım, PGİ gradyan inişinin genelleştirilmiş halidir dedik. <span class="math inline">\(f = g + h\)</span> kriterini çözüyorsak,</p>
<p><span class="math inline">\(h=0\)</span> ise normal gradyan inişini elde ederiz.</p>
<p><span class="math inline">\(h = I_C\)</span> ise yansıtılan gradyan inişini elde ederiz.</p>
<p><span class="math inline">\(g=0\)</span> ise proksimal minimizasyon algoritmasini elde ediyoruz. Bu durumda kriterde sadece pürüzsüz olmayan bir fonksiyon var, bu algoritma altgradyan inişine bir alternatif olarak kullanılabiliyor.</p>
<p>Tabii üstteki tüm özel durumların da yakınsama oranı <span class="math inline">\(O(1/\epsilon)\)</span>. Bunu biliyoruz çünkü proksimal gradyanin teorik yakınsama oranı öyle [3, 48:30].</p>
<p>Ekler</p>
<p>Örnek kod, Lasso problem çözümü [2], pür proksimal gradyan inişi, iz sürme yok</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
diabetes <span class="op">=</span> pd.read_csv(<span class="st">&quot;../../stat/stat_120_regular/diabetes.csv&quot;</span>,sep<span class="op">=</span><span class="st">&#39;;&#39;</span>)
y <span class="op">=</span> np.array(diabetes[<span class="st">&#39;response&#39;</span>].astype(<span class="bu">float</span>)).reshape(<span class="dv">442</span>,<span class="dv">1</span>)
X <span class="op">=</span> np.array(diabetes.drop(<span class="st">&quot;response&quot;</span>,axis<span class="op">=</span><span class="dv">1</span>))
N,dim <span class="op">=</span> X.shape
<span class="bu">print</span> (N,dim)

lam <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>np.sqrt(N)<span class="op">;</span>
w <span class="op">=</span> np.matrix(np.random.multivariate_normal([<span class="fl">0.0</span>]<span class="op">*</span>dim, np.eye(dim))).T

L <span class="op">=</span> (np.linalg.svd(X)[<span class="dv">1</span>][<span class="dv">0</span>])<span class="op">**</span><span class="dv">2</span>
<span class="bu">print</span>(L)
max_iter <span class="op">=</span> <span class="dv">500</span>

<span class="kw">def</span> obj(w):
    r <span class="op">=</span> X<span class="op">*</span>w<span class="op">-</span>y<span class="op">;</span>
    <span class="cf">return</span> np.<span class="bu">sum</span>(np.multiply(r,r))<span class="op">/</span><span class="dv">2</span> <span class="op">+</span>  lam <span class="op">*</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(w))

<span class="kw">def</span> f_grad(w):
    <span class="cf">return</span>  X.T<span class="op">*</span>(X<span class="op">*</span>w<span class="op">-</span>y) 

<span class="kw">def</span> soft_threshod(w,mu):
    <span class="cf">return</span> np.multiply(np.sign(w), np.maximum(np.<span class="bu">abs</span>(w)<span class="op">-</span>mu,<span class="dv">0</span>))  

w <span class="op">=</span> np.matrix([<span class="fl">0.0</span>]<span class="op">*</span>dim).T
<span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, max_iter):
    obj_val <span class="op">=</span> obj(w)
    w <span class="op">=</span> w <span class="op">-</span> (<span class="dv">1</span><span class="op">/</span>L)<span class="op">*</span> f_grad(w)
    w<span class="op">=</span> soft_threshod(w,lam<span class="op">/</span>L)    
    <span class="cf">if</span> (t <span class="op">%</span> <span class="dv">50</span><span class="op">==</span><span class="dv">0</span>):
        <span class="bu">print</span>(<span class="st">&#39;iter= </span><span class="sc">{}</span><span class="st">,</span><span class="ch">\t</span><span class="st">objective= </span><span class="sc">{:3f}</span><span class="st">&#39;</span>.<span class="bu">format</span>(t, obj_val.item()))

<span class="bu">print</span> (w)</code></pre></div>
<pre><code>442 10
4.0242141761466925
iter= 0,    objective= 6425460.500000
iter= 50,   objective= 5751070.568959
iter= 100,  objective= 5750285.357193
iter= 150,  objective= 5749670.506866
iter= 200,  objective= 5749177.635558
iter= 250,  objective= 5748779.527464
iter= 300,  objective= 5748457.810485
iter= 350,  objective= 5748197.804952
iter= 400,  objective= 5747987.670443
iter= 450,  objective= 5747817.840900
[[  -8.71913404]
 [-238.35531517]
 [ 522.93302022]
 [ 323.11825944]
 [-526.09642955]
 [ 265.58097894]
 [ -17.84381222]
 [ 143.15165377]
 [ 652.14114865]
 [  68.55685031]]</code></pre>
<p>Kaynaklar</p>
<p>[1] Tibshirani, <em>Convex Optimization, Lecture Video 8</em>, <a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg" class="uri">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a></p>
<p>[2] He, <em>IE 598 - Big Data Optimization</em>,<br />
<a href="http://niaohe.ise.illinois.edu/IE598_2016/" class="uri">http://niaohe.ise.illinois.edu/IE598_2016/</a></p>
<p>[3] Tibshirani, <em>Convex Optimization, Lecture Video 9</em>, <a href="https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg" class="uri">https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg</a></p>
</body>
</html>
