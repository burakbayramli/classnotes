\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Proksimal / Yakýnsal Gradyan Metotu (Proximal Gradient Method) 

Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uðraþmak
yerine belli bir yapýya uyan çetrefil fonksiyonlarý optimize etmeye
uðraþýr. Bu yapý

$$
f(x) = g(x) + h(x) 
\mlabel{1}
$$

formundadýr [1, 45:59]. $g,h$'in ikisi de dýþbükey. $g$'nin pürüzsüz,
türevi alýnabilir olduðu farz edilir (çoðunlukla oldukca çetrefil olabilir)
ve $\dom(g) = \mathbb{R}^n$, $h$ ise pürüzsüz olmayabilir. Tabii pürüzsüz
artý pürüzsüz olmayan iki fonksýsyon toplamý kriterin tamamýný pürüzsüz
olmayan hale çevirir. Ama bu toplam daha basittir denebilir, onun üzerinde
proksimal operatörü uygulanabilir.

Hatýrlarsak eðer $f$ türevi alýnabilir olsaydý gradyan iniþ güncellemesi 

$$
x^{+} = x - t \cdot \nabla f(x)
$$

Bu formüle eriþmenin bir yöntemi karesel yaklaþýklama üzerinden idi,
$f$'nin $x$ etrafýndaki yaklaþýklamasýnda $\nabla^2f(x)$ yerine
$\frac{1}{t} I$ koyunca,

$$
x^{+} = \arg\min_x f(x) + 
\underbrace{
  \nabla f(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 
}_{f_t(z)}
$$

elde ediliyordu. Yani gradyan iniþi sanki ardý ardýna geldiði her noktada
bir karesel yaklaþýklama yapýyor, ve onu adým atarak minimize etmeye
uðraþýyor.

Ama (1)'deki $f$ pürüzsüz deðil, o sebeple üstteki mantýk ise
yaramayacak. Fakat, belki de karesel yaklaþýklamanýn bir kýsmýný hala
kullanabiliriz, ve minimizasyonu sadece $g$'ye uygularýz çünkü pürüzsüz
olan kýsým o. Yani niye $g$'nin yerine karesel yaklaþýklama koymayalým?
Þimdi bunu yapacaðýz [1, 50:10]. 

$$
x^{+} = \arg\min_z \tilde{g}_t(z) + h(z)
$$

ki $\tilde{g}_t(z)$ $g$'nin $x$ noktasý etrafýndaki karesel yaklaþýklamasý
oluyor.  Eðer elimizde $h$ olmasaydý üstteki sadece bir gradyan
güncellemesine indirgenebilirdi. Neyse açýlýmý yaparsak

$$
= \arg\min_z g(x) + \nabla g(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 + h(z)
$$

Daha uygun bir formda yazabiliriz, $z$'nin $g$ üzerindeki deðiþikliðe
karesel uzaklýðý olarak,

$$
= \arg\min_z \frac{1}{2t} 
\big|\big| z - \big( x-t\nabla g(x) \big) \big|\big|_2^2 + 
h(z)
$$

Yani söylenmek istenen, hem sadece $g$ olsaydý atacaðýmýz gradyan adýmýna
yakýn durmaya çalýþmak, hem de $h$'nin kendisini ufak tutmak. Dýþarýdan
ayarlanan $t$ parametresi $g$ gradyan adýmý ile $h$ arasýnda bir denge
kurmak gibi görülebilir, eðer $t$ ufak ise o zaman gradyan adýmýna yakýn
durmaya daha fazla önem atfetmiþ olacaðýz, $h$'nin ufak tutulmasýna daha
az. $t$ çok büyük ise tam tersi olacak. 

Bu aslýnda kabaca Proksimal Gradyan Ýniþi yöntemini tarif etmiþ
oluyor. Þimdi proksimal eþlemesi operatörünü göstereceðiz, ki bu
algoritmalarý daha temiz olarak yazmamýza yardýmcý olacak. 

Bir $h$ fonksiyonu için bir $\prox$ operatörü tanýmlýyoruz, 

$$
\prox_{t} (x) = \arg\min_z \frac{1}{2t} ||x-z||_2^2 + h(z)
\mlabel{3}
$$

Üstteki $x$'in bir fonksiyonu olarak $\arg\min$'de gösterilen kriteri
minimize eden $z$'yi buluyor. Operatör $t$'ye baðlý, dýþarýdan verilen
parametre. Tabii ki $h$'ye de baðlý, ki bazen üstteki operatörü
$\prox_{h,t}$ olarak gösteren de oluyor. 

Üstteki ifadenin eþleme olduðunu kontrol edelim. Ciddi tanýmlý bir
fonksiyon üstteki deðil mi? Ona bir $x$ veriyorsunuz, o da size tek bir
sonuç döndürüyor. Ayrýca bu eþleme / fonksiyonun kendisi bir
minimizasyon. Peki bu minimizasyon problemi dýþbükey mi? Evet. Peki bu
problemin özgün bir sonucu var mýdýr? Vardýr çünkü üstteki problem harfiyen
dýþbükey. Deðil mi? Eðer $h$ harfiyen dýþbükey olmasa bile ifadenin tümü
harfiyen dýþbükey olurdu çünkü $\frac{1}{2t} ||x-z||_2^2 $ harfiyen
dýþbükey [1, 55:09], $z-x$'in karesi var [ayrýca $x$ deðiþkeni $h$'ye
geçilmiyor].

Yani harfiyen dýþbükey, o zaman özgün sonuç var. Biz de bu özgün sonucu
alarak bir iniþ algoritmasý yazýyoruz [1, 56:28], 

$$
x^{(k)} = \prox_{t_k} \big( 
x^{(k-1)} - t_k \nabla g(x^{(k-1)})
\big), \quad k=1,2,...
\mlabel{2}
$$

Güncelleme adýmýný tanýdýk þekilde yazmak için 

$$
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k} (x^{(k-1)})
$$

ki $G_t$'ye $f$'nin genelleþtirilmiþ gradyaný denebilir,

$$
G_t(x) = \frac{x - \prox_{t}(x - t \nabla g(x))}{t}
$$

$G$'nin dýþbükey fonksiyonlarýn alýþýlageldik gradyanlarýna benzer pek çok
özelliði vardýr, ki bu özellikler proksimal metotlarýn yakýnsadýðýyla
alakalý ispatlarda kullanýlabilir.

Þimdiye kadar anlattýklarýmýza bakanlara bu komik bir hikaye gibi
gelebilir. Bir $g + h$ toplamýný minimize etmek istiyordum, bunu yapabilmek
için (2) formunda adýmlar atacaðým, bir $\prox$ operatörüm var, ama bu
operatör bir sonuç döndürüyor aslýnda, ve bu sonuç bir baþka
minimizasyondan geliyor. Yani $g + h$ türü bir toplam minimizasyonu yerine
her adýmda, bir sürü minimizasyonlarý koymuþ oldum. Bu nasýl daha iyi bir
sonuç verecek ki?

Þunu belirtmek lazým, sadece eðer proksimal gradyanlarý analitik, ya da
hýzlý bir þekilde hesaplayabiliyorsak onlarý çözüm için düþünürüz. Yani her
ne kadar her adýmda (3) turu optimizasiyonlar yapýyorsak ta, bunu pürüzsüz
olan kýsým $h$ yeterince basit olduðu zaman yapýyoruz ki tüm (3) için
analitik  ya da hýzlý hesapsal çözüm olsun [1, 58:54]. 

Diðer noktalar, dikkat edersek proksimal operatör $g$'ye baðlý deðil,
tamamen $h$ bazlý. Eðer $h$ basit ise ama $g$ müthiþ çetrefil ise bu
proksimal hesaplarýný çok zorlaþtýrmýyor. Eðer o çok çetrefil (ve pürüzsüz)
$g$ için gradyan hesaplanabiliyorsa, durumu kurtardýk demektir. 

Tekrar bir Lasso problemi göreceðiz. Bu Lasso için gördüðümüz ikinci
algoritma, ve belirtmek gerekir ki Proksimal metot altgradyan metotuna göre
çok daha verimlidir, hýzlýdýr.

Verili bir $y \in \mathbb{R}^n$, $X \in \mathbb{R}^{n \times p}$ için Lasso
kriterini hatýrlarsak, 

$$
f(\beta) = \frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1 
$$

Kriterdeki ilk terim en az kareler kayýp fonksiyonu, ikinci terim bir ayar
parametresi üzerinden katsayýlarýn 1. normu. Bu kriteri pürüzsüz, ve
pürüzsüz olmayan ama basitçe olan iki kýsma ayýracaðýz, yani zaten oldukca
bariz, pürüzsüz kýsým 1. terim, $g(\beta)$ diyelim, olmayan 2. terim,
$h(\beta)$ diyelim. Proksimal gradyan iniþi için bize iki þey gerekiyor,
birincisi $g$'nin gradyaný, ikincisi $h$ için $\prox$ operatörünü
hesaplayabilmek.

$g$'nin gradyaný oldukca basit, onu bu noktada uykumuzda bile bulabiliyor
olmamýz lazým. $h$'nin $\prox$ operatörü,

$$
\prox_t(\beta) = \arg\min_z \frac{1}{2t} ||\beta-z||_2^2 + \lambda ||z||_1
$$

Her þeyi $t$ ile çarparsam,

$$
\prox_t(\beta) = \arg\min_z \frac{1}{2} ||\beta-z||_2^2 + \lambda t ||z||_1
$$

Üstteki minimizasyonun çözümünü daha önce altgradyanlar üzerinden
görmüþtük, 

$$
= S_{\lambda t}(\beta)
$$

Yine yumuþak eþikleme (soft-threshold) operatörüne gelmiþ olduk, 

$$
[ S_{\lambda} (\beta) ]_i = 
\left\{ \begin{array}{ll}
\beta_i - \lambda & \textrm{eðer } \beta_i > \lambda_i \\
0 & \textrm{eðer } -\lambda \ge \beta_i \ge \lambda, \quad i=1,..,n \\
\beta_i + \lambda & \textrm{eðer } \beta_i < -\lambda_i 
\end{array} \right.
$$

Tüm algoritma neye benziyor? Önce $g$'ye göre bir graydan güncellemesi
yaparým, gradyan

$$
\nabla g(\beta) = -X^T (y-X\beta)
$$

O zaman güncelleme 

$$\beta + t X^T (y-X\beta)$$ 

olur. Buna $\prox$ uygularsak,

$$
\beta^+ = S_{\lambda t}(\beta + t X^T (y-X\beta))
$$

Yumuþak eþikleme ne yapar? Her ögeye teker teker bakar, eðer mutlak deðeri
çok ufaksa onu ya sýfýra eþitler, ya da onu $\lambda \cdot t$ kadar sýfýra
yaklaþtýrýr. 

Üstteki Lasso algoritmasina ISTA adý da verilir. 

Geriye Çizgisel Ýz Sürme  (Backtracking line search)

Graydan iniþinde görmüþtük ki adým büyüklüklerini dinamik olarak
seçebiliyorduk, her adýmdaki duruma adapte olabiliyorduk, tipik olarak
Lipschitz sabitini bilmiyoruz çünkü. O sebeple geriye iz sürme pratikte
iyi iþlemesiyle beraber, teorik olarak yakýnsamayý da garantiliyordu. 

Proksimal gradyanlarý için benzer bir kavram geçerli. Ayrýca proksimal
durumda geriye doðru iz sürmenin birden fazla yolu var [1, 1:10:23]. Ben
sadece $g$ üzerinde iþlem yapan bir yöntem seçtim, çünkü bu metotu
hatýrlamasý daha kolay. Gradyan iniþi için iz sürmeyi hatýrlarsak, $x$
noktasýndayýz diyelim ve $x-t \nabla g$ yönünde gitmek istiyoruz, o zaman
alttakinin doðru olup olmadýðýný kontrol ediyorduk, 

$$
f(x - t\nabla f(x) ) > f(x) - \frac{t}{2} ||\nabla f||_2^2 
$$

Proksimal gradyan için benzer bir yöntem, ve $G$ üzerinde iþlem yapýyoruz
dikkat, $h$ deðil, yani $G$ üzerinden yeterince ``iniþ'' yapmaya
uðraþacaðýz, $g+h$ deðil. Normal iz sürmede üsttekinin doðruluðunu kontrol
ediyoruz, doðru ise $t$'yi belli bir ölçüde ufaltýyoruz. Doðru deðilse yani
yeterince iniþ yaptýysak, o zaman eldeki deðerlerle güncellemeyi
yapýyoruz. 





























Örnek kod, Lasso problem çözümü [2]

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
diabetes = pd.read_csv("../../stat/stat_120_regular/diabetes.csv",sep=';')
y = np.array(diabetes['response'].astype(float)).reshape(442,1)
X = np.array(diabetes.drop("response",axis=1))
N,dim = X.shape
print (N,dim)

lam = 1/np.sqrt(N);
w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T

L = (np.linalg.svd(X)[1][0])**2
print(L)
max_iter = 500

def obj(w):
    r = X*w-y;
    return np.sum(np.multiply(r,r))/2 +  lam * np.sum(np.abs(w))

def f_grad(w):
    return  X.T*(X*w-y) 

def soft_threshod(w,mu):
    return np.multiply(np.sign(w), np.maximum(np.abs(w)-mu,0))  

w = np.matrix([0.0]*dim).T
for t in range(0, max_iter):
    obj_val = obj(w)
    w = w - (1/L)* f_grad(w)
    w= soft_threshod(w,lam/L)    
    if (t % 50==0):
        print('iter= {},\tobjective= {:3f}'.format(t, obj_val.item()))

print (w)
\end{minted}

\begin{verbatim}
442 10
4.0242141761466925
iter= 0,	objective= 6425460.500000
iter= 50,	objective= 5751070.568959
iter= 100,	objective= 5750285.357193
iter= 150,	objective= 5749670.506866
iter= 200,	objective= 5749177.635558
iter= 250,	objective= 5748779.527464
iter= 300,	objective= 5748457.810485
iter= 350,	objective= 5748197.804952
iter= 400,	objective= 5747987.670443
iter= 450,	objective= 5747817.840900
[[  -8.71913404]
 [-238.35531517]
 [ 522.93302022]
 [ 323.11825944]
 [-526.09642955]
 [ 265.58097894]
 [ -17.84381222]
 [ 143.15165377]
 [ 652.14114865]
 [  68.55685031]]
\end{verbatim}

Kaynaklar

[1] Tibshirani, {\em Convex Optimization, Lecture Video 8}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

[2] He, {\em IE 598 - Big Data Optimization},  
    \url{http://niaohe.ise.illinois.edu/IE598_2016/}

[3] Tibshirani, {\em Convex Optimization, Lecture Video 9}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}


\end{document}




