\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Proksimal / Yakýnsal Gradyan Metotu (Proximal Gradient Method) 

Bu metot, herhangi bir pürüzsüz olmayan fonksiyonu optimize etmeye uðraþmak
yerine belli bir yapýya uyan çetrefil fonksiyonlarý optimize etmeye
uðraþýr. Bu yapý

$$
f(x) = g(x) + h(x) 
\mlabel{1}
$$

formundadýr [1, 45:59]. $g,h$'in ikisi de dýþbükey. $g$'nin pürüzsüz,
türevi alýnabilir olduðu farz edilir (çoðunlukla oldukca çetrefil olabilir)
ve $\dom(g) = \mathbb{R}^n$, $h$ ise pürüzsüz olmayabilir. Tabii pürüzsüz
artý pürüzsüz olmayan iki fonksýsyon toplamý kriterin tamamýný pürüzsüz
olmayan hale çevirir. Ama bu toplam daha basittir denebilir, onun üzerinde
proksimal operatörü uygulanabilir.

Hatýrlarsak eðer $f$ türevi alýnabilir olsaydý gradyan iniþ güncellemesi 

$$
x^{+} = x - t \cdot \nabla f(x)
$$

Bu formüle eriþmenin bir yöntemi karesel yaklaþýklama üzerinden idi,
$f$'nin $x$ etrafýndaki yaklaþýklamasýnda $\nabla^2f(x)$ yerine
$\frac{1}{t} I$ koyunca,

$$
x^{+} = \arg\min_x f(x) + 
\underbrace{
  \nabla f(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 
}_{f_t(z)}
$$

elde ediliyordu. Yani gradyan iniþi sanki ardý ardýna geldiði her noktada
bir karesel yaklaþýklama yapýyor, ve onu adým atarak minimize etmeye
uðraþýyor.

Ama (1)'deki $f$ pürüzsüz deðil, o sebeple üstteki mantýk ise
yaramayacak. Fakat, belki de karesel yaklaþýklamanýn bir kýsmýný hala
kullanabiliriz, ve minimizasyonu sadece $g$'ye uygularýz çünkü pürüzsüz
olan kýsým o. Yani niye $g$'nin yerine karesel yaklaþýklama koymayalým?
Þimdi bunu yapacaðýz [1, 50:10]. 

$$
x^{+} = \arg\min_z \tilde{g}_t(z) + h(z)
$$

ki $\tilde{g}_t(z)$ $g$'nin $x$ noktasý etrafýndaki karesel yaklaþýklamasý
oluyor.  Eðer elimizde $h$ olmasaydý üstteki sadece bir gradyan
güncellemesine indirgenebilirdi. Neyse açýlýmý yaparsak

$$
= \arg\min_z g(x) + \nabla g(x)^T (z-x) + \frac{1}{2t} ||z-x||_2^2 + h(z)
$$

Daha uygun bir formda yazabiliriz, $z$'nin $g$ üzerindeki deðiþikliðe
karesel uzaklýðý olarak,

$$
= \arg\min_z \frac{1}{2t} 
\big|\big| z - \big( x-t\nabla g(x) \big) \big|\big|_2^2 + 
h(z)
$$

Yani söylenmek istenen, hem sadece $g$ olsaydý atacaðýmýz gradyan adýmýna
yakýn durmaya çalýþmak, hem de $h$'nin kendisini ufak tutmak. Dýþarýdan
ayarlanan $t$ parametresi $g$ gradyan adýmý ile $h$ arasýnda bir denge
kurmak gibi görülebilir, eðer $t$ ufak ise o zaman gradyan adýmýna yakýn
durmaya daha fazla önem atfetmiþ olacaðýz, $h$'nin ufak tutulmasýna daha
az. $t$ çok büyük ise tam tersi olacak. 

Bu aslýnda kabaca Proksimal Gradyan Ýniþi yöntemini tarif etmiþ
oluyor. Þimdi proksimal eþlemesi operatörünü göstereceðiz, ki bu
algoritmalarý daha temiz olarak yazmamýza yardýmcý olacak. 

Bir $h$ fonksiyonu için bir $\prox$ operatörü tanýmlýyoruz, 

$$
\prox_{t} (x) = \arg\min_z \frac{1}{2t} ||x-z||_2^2 + h(z)
\mlabel{3}
$$

Üstteki $x$'in bir fonksiyonu olarak $\arg\min$'de gösterilen kriteri
minimize eden $z$'yi buluyor. Operatör $t$'ye baðlý, dýþarýdan verilen
parametre. Tabii ki $h$'ye de baðlý, ki bazen üstteki operatörü
$\prox_{h,t}$ olarak gösteren de oluyor.

Üstteki ifadenin eþleme olduðunu kontrol edelim. Ciddi tanýmlý bir
fonksiyon üstteki deðil mi? Ona bir $x$ veriyorsunuz, o da size tek bir
sonuç döndürüyor. Ayrýca bu eþleme / fonksiyonun kendisi bir
minimizasyon. Peki bu minimizasyon problemi dýþbükey mi? Evet. Peki bu
problemin özgün bir sonucu var mýdýr? Vardýr çünkü üstteki problem harfiyen
dýþbükey. Deðil mi? Eðer $h$ harfiyen dýþbükey olmasa bile ifadenin tümü
harfiyen dýþbükey olurdu çünkü $\frac{1}{2t} ||x-z||_2^2 $ harfiyen
dýþbükey [1, 55:09], $z-x$'in karesi var [ayrýca $x$ deðiþkeni $h$'ye
geçilmiyor].

Yani harfiyen dýþbükey, o zaman özgün sonuç var. Biz de bu özgün sonucu
alarak bir iniþ algoritmasý yazýyoruz [1, 56:28], 

$$
x^{(k)} = \prox_{t_k} \big( 
x^{(k-1)} - t_k \nabla g(x^{(k-1)})
\big), \quad k=1,2,...
\mlabel{2}
$$

Güncelleme adýmýný tanýdýk þekilde yazmak için 

$$
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k} (x^{(k-1)})
$$

ki $G_t$'ye $f$'nin genelleþtirilmiþ gradyaný denebilir,

$$
G_t(x) = \frac{x - \prox_{t}(x - t \nabla g(x))}{t}
$$

$G$'nin dýþbükey fonksiyonlarýn alýþýlageldik gradyanlarýna benzer pek çok
özelliði vardýr, ki bu özellikler proksimal metotlarýn yakýnsadýðýyla
alakalý ispatlarda kullanýlabilir.

Þimdiye kadar anlattýklarýmýza bakanlara bu komik bir hikaye gibi
gelebilir. Bir $g + h$ toplamýný minimize etmek istiyordum, bunu yapabilmek
için (2) formunda adýmlar atacaðým, bir prox operatörüm var, ama bu
operatör bir sonuç döndürüyor aslýnda, ve bu sonuç bir baþka
minimizasyondan geliyor. Yani $g + h$ türü bir toplam minimizasyonu yerine
her adýmda, bir sürü minimizasyonlarý koymuþ oldum. Bu nasýl daha iyi bir
sonuç verecek ki?

Þunu belirtmek lazým, sadece eðer proksimal gradyanlarý analitik, ya da
hýzlý bir þekilde hesaplayabiliyorsak onlarý çözüm için düþünürüz. Yani her
ne kadar her adýmda (3) turu optimizasiyonlar yapýyorsak ta, bunu pürüzsüz
olan kýsým $h$ yeterince basit olduðu zaman yapýyoruz ki tüm (3) için
analitik  ya da hýzlý hesapsal çözüm olsun [1, 58:54]. 

Diðer noktalar, dikkat edersek proksimal operatör $g$'ye baðlý deðil,
tamamen $h$ bazlý. Eðer $h$ basit ise ama $g$ müthiþ çetrefil ise bu
proksimal hesaplarýný çok zorlaþtýrmýyor. Eðer o çok çetrefil (ve pürüzsüz)
$g$ için gradyan hesaplanabiliyorsa, durumu kurtardýk demektir. 

Tekrar bir Lasso problemi göreceðiz. Bu Lasso için gördüðümüz ikinci
algoritma, ve belirtmek gerekir ki Proksimal metot altgradyan metotuna göre
çok daha verimlidir, hýzlýdýr.

Verili bir $y \in \mathbb{R}^n$, $X \in \mathbb{R}^{n \times p}$ için Lasso
kriterini hatýrlarsak, 

$$
f(\beta) = \frac{1}{2} || y - X \beta ||_2^2 + \lambda ||\beta||_1 
$$

Kriterdeki ilk terim en az kareler kayýp fonksiyonu, ikinci terim bir ayar
parametresi üzerinden katsayýlarýn 1. normu. Bu kriteri pürüzsüz, ve
pürüzsüz olmayan ama basitçe olan iki kýsma ayýracaðýz, yani zaten oldukca
bariz, pürüzsüz kýsým 1. terim, $g(\beta)$ diyelim, olmayan 2. terim,
$h(\beta)$ diyelim. Proksimal gradyan iniþi için bize iki þey gerekiyor,
birincisi $g$'nin gradyaný, ikincisi $h$ için prox operatörünü
hesaplayabilmek.

$g$'nin gradyaný oldukca basit, onu bu noktada uykumuzda bile bulabiliyor
olmamýz lazým. $h$'nin prox operatörü,

$$
\prox_t(\beta) = \arg\min_z \frac{1}{2t} ||\beta-z||_2^2 + \lambda ||z||_1
$$

Her þeyi $t$ ile çarparsam,

$$
\prox_t(\beta) = \arg\min_z \frac{1}{2} ||\beta-z||_2^2 + \lambda t ||z||_1
$$

Üstteki minimizasyonun çözümünü daha önce altgradyanlar üzerinden
görmüþtük, 

$$
= S_{\lambda t}(\beta)
$$

Yine yumuþak eþikleme (soft-threshold) operatörüne gelmiþ olduk, 

$$
[ S_{\lambda} (\beta) ]_i = 
\left\{ \begin{array}{ll}
\beta_i - \lambda & \textrm{eðer } \beta_i > \lambda_i \\
0 & \textrm{eðer } -\lambda \ge \beta_i \ge \lambda, \quad i=1,..,n \\
\beta_i + \lambda & \textrm{eðer } \beta_i < -\lambda_i 
\end{array} \right.
$$

Tüm algoritma neye benziyor? Önce $g$'ye göre bir graydan güncellemesi
yaparým, gradyan

$$
\nabla g(\beta) = -X^T (y-X\beta)
$$

O zaman güncelleme 

$$\beta + t X^T (y-X\beta)$$ 

olur. Buna prox uygularsak,

$$
\beta^+ = S_{\lambda t}(\beta + t X^T (y-X\beta))
$$

Yumuþak eþikleme ne yapar? Her ögeye teker teker bakar, eðer mutlak deðeri
çok ufaksa onu ya sýfýra eþitler, ya da onu $\lambda \cdot t$ kadar sýfýra
yaklaþtýrýr. 

Üstteki Lasso algoritmasina ISTA adý da verilir. 

Geriye Çizgisel Ýz Sürme  (Backtracking line search)

Graydan iniþinde görmüþtük ki adým büyüklüklerini dinamik olarak
seçebiliyorduk, her adýmdaki duruma adapte olabiliyorduk, tipik olarak
Lipschitz sabitini bilmiyoruz çünkü. O sebeple geriye iz sürme pratikte
iyi iþlemesiyle beraber, teorik olarak yakýnsamayý da garantiliyordu. 

Proksimal gradyanlarý için benzer bir kavram geçerli. Ayrýca proksimal
durumda geriye doðru iz sürmenin birden fazla yolu var [1, 1:10:23]. Ben
sadece $g$ üzerinde iþlem yapan bir yöntem seçtim, çünkü bu metotu
hatýrlamasý daha kolay. Gradyan iniþi için iz sürmeyi hatýrlarsak, $x$
noktasýndayýz diyelim ve $x-t \nabla g$ yönünde gitmek istiyoruz, o zaman
alttakinin doðru olup olmadýðýný kontrol ediyorduk, 

$$
f(x - t\nabla f(x) ) > f(x) - \frac{t}{2} ||\nabla f||_2^2 
$$

Proksimal gradyan için benzer bir yöntem [1:11:57], ve $G$ üzerinde iþlem
yapýyoruz dikkat, $h$ deðil, yani $G$ üzerinden yeterince ``iniþ'' yapmaya
uðraþacaðýz, $g+h$ deðil. Normal iz sürmede üsttekinin doðruluðunu kontrol
ediyoruz, doðru ise $t$'yi belli bir ölçüde ufaltýyoruz. Doðru deðilse yani
yeterince iniþ yaptýysak, o zaman eldeki deðerlerle güncellemeyi yapýyoruz.

$$
g(x - t G_t (x) ) > g(x) - t \nabla g(x)^T G_x(x) + \frac{t}{2} ||G_x(x)||_2^2
$$

Yani gradyan güncellemesi 1. derece Taylor güncellemesinden geldi. Üstteki
ifadede de eðime ek olarak 1. derece Taylor güncellemesine göre de
yeterince iniþ yapmýþ olmak istiyorum. Dikkat edersek eðer $G$ yerine
$g$'nin gradyanýný koyarsam, iki üstteki formüle benzer bir formül elde
ederim.

Geriye iz sürme genel algoritmasi þöyle, bir $0 < \beta < 1$ paremetresi
var (dýþarýdan ayarlanan bir parametre). $t=1$ ile baþlýyoruz, ve üstteki
formülü iþletiyoruz, eðer gerekiyorsa $t = \beta t$ ile küçültme
yapýyoruz. Ýz sürme bitince bulduðumuz $t$ ile güncelleme yapýyoruz [3,
07:35].

[bazi ek detaylar atlandi]

Matris Tamamlamasi

Þimdi prox operatörü sofistike olan bir örnek görelim [3, 11:38]. Buradan
çýkan algoritma ilginç olacak. 

Bize bir $Y \in \mathbb{R}^{m \times n}$ matrisi veriliyor ama biz sadece
bu matrisin bazý öðelerini görebiliyoruz, bu öðeler
$Y_{i,j}, (i,j) \in \omega$ ile belirtiliyor ki $\omega$ belli bir indis
kümesidir. Bu problem bir tavsiye sistemi olabilir, matrisin tamamý bir
ideal müþteri / ürün eþlemesidir, biz sadece bu matrisin belli bir kýsmýný
görüyoruz (tipik olarak mevcut müþterilerin tarihi veride yaptýðý alýmlar,
ürünler üzerindeki beðendi/beðenmedi yorumlarý matrisin ``görünen'' kýsmýný
temsil edebilir). 

Bu tür problemleri iz norm regülarizasyonu (trace norm regularization)
problemi olarak görmenin iyi iþlediði görülmüþtür. Bu problem aslýnda daha
önce gördüðümüz Lasso problemine benzer, onun matrisler için olan formudur
bir açýdan. Problem,

$$
\min_B \frac{1}{2} \sum _{(i,j) \in \omega} (Y_{ij} - B_{ij})^2 + \lambda ||B||_{tr}
\mlabel{4}
$$

ki $||B||_{tr}$ iz (ya da nükleer) normudur, 

$$
||B||_{tr} = \sum _{i=1}^{r} \sigma_i(B)
$$

ile gösterilir, $r$ kertedir, $r = \rank(B)$, ve herhangi bir $X$ matrisi
için $\sigma_1(X) \ge .. \ge \sigma_r(X) \ge 0$, $X$'in eþsiz (singular)
deðerleridir.

Minimizasyon ifadesindeki ilk toplam, $B$'deki deðerleri zaten görülen,
bildiðimiz deðerlere $Y$'ye karesel kayýp yakýn tut diyor, ve ona bir ayar
parametresi üzerinden $B$'nin iz normunu ekliyoruz, bu istatistiki baðlamda
bir tür regülarizasyon yapmýþ oluyor. Eðer bu ek olmsaydý problem kötü
konumlanmýþ (ill-posed) olurdu [3, 15:06]. Eðer o terim olmasaydý ve
$\lambda$ sýfýr olsaydý o zaman optimizasyon $B$'yi $Y$'ye eþitlerdik, oldu
bitti derdik, ama o zaman hiçbir iþ yapmamýþ olurduk. Saðdaki terim ekiyle
yapmaya uðraþtýðýmýz $Y$'ye olabildiðince yakýn bir $B$ seçmek ve bu
$B$'nin düþük kerte olmasýný zorlamak. Bu bir regülarizasyon yöntemi,
diyelim $B$ þu þekilde 

$$
\underbrace{B}_{m \times n} = 
\underbrace{U}_{m \times k}
\underbrace{V^T}_{k \times n}
$$

yani $B$'yi güya oluþturan birer $U,V$'nin boyutlarýndaki $k$'nin
olabildiðince düþük olmasýný istiyoruz. 

Bir diðer bakýþ açýsý, daha önceki Lasso'yla ilintilendirmek baðlamýnda, iz
normu L1-normun matrisler için olan versiyonu olarak görmek. Eðer elimde
bir köþegen matris olsaydý iz köþegendeki öðelerin (tek öðeler onlar)
toplamý olurdu, ki bu bir vektörün 1-norm'unu almak gibi deðil mi? Eðer
köþegeni bir vektör gibi görürsem, ve matriste bu vektörden baþka bir þey
yoksa.. baðlantýyý görüyoruz herhalde. 

Bir anlamda $||B||_{tr}$ $B$ matrisinin kertesini yaklaþýksal olarak temsil
ediyor çünkü kerte bir matrisin sýfýr olmayan eþsiz deðerlerinin
sayýsýdýr. $||B||_{tr}$ tabii ki eþsiz deðerlerinin sayýsý deðil onlarýn
kendilerinin toplamý, ama yaklaþýksal olarak kullanabileceðimiz bir þey bu
y [3, 17:49] , çünkü deðer toplamý dýþbükey [altgradyan, türevi
alýnabildiði için bu yaklaþýklýk seçilmiþ herhalde].  (4) problemi tamamen
dýþbükey bu arada, ilk terim dýþbükey, ve ikinci terim düzgün bir norm'dur
o zaman dýþbükeydir.

Bilimciler proksimal gradyan kullanmadan önce bu problem zor bir
problemdi. Optimizasiyon problemi bir yarý kesin (semidefinite) program
olarak tanýmlanýr. Eskiden bu tür problemleri iç nokta teknikleri ile
çözüyorduk, ve bu teknikler üstteki gibi problemler üzerinde oldukca
yavaþtýr. 

Devam edelim, problemi þöyle hazýrlarýz, bir yansýtma operatörü $P_\Omega$
tanýmlayalým, gözlenen kümeye yansýtma yapacaðýz, 

$$
[P_\Omega (B) ]_{ij} = 
\left\{ \begin{array}{ll}
B_{ij} & (i,j) \in \Omega \\
0 & (i,j) \notin \Omega 
\end{array} \right.
$$

Bu operatörün yaptýðý verilen bir matrisin gözlem olmayan her öge için
deðeri sýfýr yapmak, yoksa olduðu gibi býrakmak. Simdi kriteri yazabiliriz, 

$$
f(B) = 
\underbrace{\frac{1}{2} || P_\Omega (Y) - P_\Omega(B) ||_F^2}_{g(B)} + 
\underbrace{\lambda ||B||_{tr}}_{h(B)}
$$

Problemi proksimalin beklediði $g + h$ formuna soktuk. Hatýrlatalým, $g$
dýþbükey ama pürüzsüz deðil. Bu yaklaþým için gerekenler neydi? Pürüzsüz
kýsým için gradyan hesaplayabilmek, ve pürüzsüz olmayan kýsým için prox
operatörünü hesaplayabilmek. Gradyan oldukca basit,

$$
\nabla g(B) = -(  P_\Omega (Y) - P_\Omega(B) ) 
\mlabel{8}
$$

Sadece iþaretleri biraz deðiþtirdim çünkü onu sonra gradyandan
çýkartacaðýz, o sebeple parantez dýþýnda bir eksi iþareti olmasý
faydalý. Þimdi ikinci terim için prox operatörünü görelim. 

$$
\prox_t(B) = \arg\min_Z \frac{1}{2t} || B - Z ||_F^2 + \lambda ||Z||_{tr}
\mlabel{5}
$$

Tüm mümkün $Z$ matrisleri üzerinden minimizasyon yapýyoruz, ve $B_i$ ile
$Z_i$ arasýndaki tüm farklarýn kare toplamlarýný alýyoruz, artý bir sabit
çarpý $Z$'nin iz normu. Yani optimizasyon hedefi $Z$, ve bu minimizasyonu
prox operatörünü $B$ üzerinde uygulayarak elde ediyoruz [3, 22:15]. 

Üstteki ifadeyi hesaplamak için altgradyan matematiðine inmek gerekiyor
biraz. 

$$
\prox_t(B) = S_{\lambda t} (B)
$$

Yani prox'u $\lambda$ seviyesinde bir matriste yumuþak eþikleme olarak
görmek mümkün. Yani aynen 1-norm'un prox operatörünün vektörsel yumuþak
eþikleme olduðu gibi burada matris seviyesinde bir eþikleme var [3, 22:46].
Bu demektir ki herhangi bir matris $B$'nin prox operatörü için $B$'nin
SVD'sini alýyoruz, yani $B = U\Sigma V^T$ diyoruz, ve köþegen matris
$\Sigma$ üzerinde yumuþak eþikleme iþletiyoruz, bunu yapmak için ya
köþegendeki her ögeden $\lambda_i$ çýkartýyoruz ve eðer pozitifse o deðeri
kullanýyoruz, ya da sýfýr alýyoruz. 

$$
S_\lambda (B) = U \Sigma_\lambda V^T
$$
 
$$
(\Sigma_\lambda)_{ii}  = \max \{ \Sigma_{ii} - \lambda_i 0 \}
$$

$S_\lambda (B)$ normal $B$'den daha düþük kerteli olacaktýr, çünkü üstteki
iþlem ile $\Sigma_\lambda$'nin köþegenindeki bazý deðerler sýfýrlanýr, ve
bu matris ile hesaplanan $U \Sigma_\lambda V^T$, çarpým sýrasýnda
$U,V$'deki bazý satýrlar, kolonlarý da sýfýrlar [3, 26:00].

Peki $\prox_t(B) = S_{\lambda t} (B)$ nereden geliyor? Prox operatörünü
oluþturan kriterin (5) altgradyanýný alýrýz, sýfýra eþitleriz, ve ortaya
çýkan formülü doðrulayan $Z$'yi buluruz. Cevabýn ne olacaðýný biliyoruz,
sadece tarif edilen altgradyan vs sonrasý ayný formülü bulup
bulmayacaðýmýzý kontrol edeceðiz. 

(5)'teki ilk terim puruzsuz, direk gradyan alinir, 

$$
0 \in Z - B + \lambda t \cdot \partial ||Z||_{tr} 
\mlabel{7}
$$

Ýz normunun altgradyanýný gösterelim, 

$$
\partial ||Z||_{tr} = \{  UV^T + W : ||W||_{op} < 1, U^TW = 0, WV = 0 \}
\mlabel{6}
$$

Üstteki notasyon diyor ki aradýðýmýz $W$'ler en büyük eþsiz deðeri 1'den
küçük olan ve $U$'nun kolonlarýna ve $W$'nin satýrlarýna dikgen
olmalý. Yýne sonradan doðrulama ile ispat yapýlabilir, üstteki tanýmý baz
alarak $Z = S_{\lambda t}(B)$ olarak formüle sokarsak sýfýr sonucu
aldýðýmýzý göreceðiz. Ek bazý bilgiler, iz normu ve op normu birbirinin
ikizidir. Yani

$$
||Z||_{tr} = \max_{||Y||_{op} \le 1}  Z \cdot Y
$$

Üstteki $\cdot$ iþareti öðesel çarpým ve bu çarpýmlarýn toplamý
anlamýnda. $||Y||_{op} \le 1$ þartýna uyan tüm $Y$'lere bakýyorum, ve bu
$Y$'lerle $Z$ arasýnda ögesel çarpým yapýyorum topluyorum bu bana $Z$'nin
iz normunu veriyor. 
 
Ya da

$$
= \max_{||Y||_{op} \le 1}  \tr (Z^TY)
$$

iþlemi de ayný kapýya çýkar, bahsettiðimiz ikizlik sayesinde bu mümkün
oluyor. Devam edersek, üstteki bir $\max$ ifadesi ve bu ifadeleri
altgradyanýný almayý gördük, üstteki ifade üzerinden, $||Y||_{op} \le 1$
þartýna uyan tüm $Y$'ler içinden $\tr{Z^TY}$ maksimumunu gerçekleþtiren,
eþsiz deðerlerin toplamýný / iz normunu veren $Y$'leri istiyoruz. Ve (6)
içindeki tüm $U,V$'lerin bunu yapabileceðini kontrol edebilirsiniz [3,
31:56], $Z$ SVD'sinden gelen $U,V$'lerin herhangi bir öðesel çarpýmý size
$Z$'nin eþsiz deðerlerinin toplamýný verecektir, ve tasarým itibariyle
onlarýn operatör normu en fazla 1'dýr. $ UV^T$'un operatör normu en fazla
1'dýr, $W$ için ayný þekilde, ve her iki terim birbirine dikgen olacak
þekilde tasarlanmýþtýr. Tüm bu matrislerin $||Z||_{tr} $'in altgradyani
olduðunu ispatlamanýn yolu budur. 

Neyse üsttekilerin bize altgradyan verdiðini ispatladýktan sonra
$Z = S_{\lambda t}(B)$'i (7)'ye sokup ispatý tamamlýyoruz.  

Bu oldukca zor bir prox operatörüydü tabii. Hesaplamanabilir olduðu
anlaþýldýðýnda pek çok araþtýrmacý sevinmiþti, pek çok yerde kullanýldý,
vs. Ýspatý size ödev olarak veriyorum ama rutin bir ödev sorusu olmadýðýný
belirtmek isterim. 

Artýk algoritmayý oluþturabiliriz, $g$ gradyanýnýn negatif yönünde adým
atýyoruz. Gradyaný (8)'de görmüþtük. 

$$
B^+ = S_{\lambda t} \bigg( 
B + t (P_\Omega(Y) - P_\Omega(B) )
\bigg)
$$

Büyük parantez içini gradyan ile hesaplýyoruz sonra elde edileni yumuþak
eþikten geçiriyoruz. Bu ne demekti? Parantez içindekilerin SVD'sini al,
eþsiz deðeri $\lambda t$'den küçük olan tüm deðerleri sýfýrla, ve
kalanlardan $\lambda t$ çýkar, ve bu yeni deðerlerle yeni bir matris
yarat. Prox operatörünü uygulamak bu demek.

Bu arada pürüzsüz kýsmýmýzdaki gradyan $\nabla g(B)$, $L=1$ ile Lipschitz
süreklidir. (8)'deki ifade lineer, sabit 1 üzerinden tabii ki
Lipschitz. Yakýnsama analiz bilgimiz bize diyor ki proksimal gradyan
iniþinde adým büyüklüðü en fazla $1/L$ olabilir, ki burada 1 sadece. Bu
problemin güzel taraflarýndan biri bu, olabilecek en büyük adým büyüklüðünü
kullanabiliyoruz ve onu hesaplamasý kolay, sadece 1.

Ve $t=1$'i üstteki formüle sokuyoruz, 

$$
B^+ = S_{\lambda t} \bigg( B + P_\Omega(Y) - P_\Omega(B)  \bigg)
$$

elde ediyoruz. Formüldeki $B - P_\Omega(B)$  bize görülmeyen kümedeki
herþeyi veriyor deðil mi? $B$'de herþey var, $P_\Omega(B)$'de görülen
kümedeki $B$ öðeleri var (geri kalanlar sýfýr), o zaman bu çýkartmayý
yapýnca görülmeyen kümedeki herþeyi elde ederiz. 

$$
= S_{\lambda t} \bigg( P_\Omega(Y) + P_{\Omega^C}(B) \bigg)
$$
  
olarak yazabiliriz, $P_{\Omega^C}(B)$, $P_{\Omega}(Y)$'in tamamlayýcýsý
(complement), görülmüþ öðeler için sýfýr, görülmemiþ $ij$ öðeleri için
$B_{ij}$.  

Algoritmanýn mantýðý çok doðal, her adýmda tahminim $P_{\Omega^C}(B)$'a
bakýyorum, burada olanlarýn görülmemiþ kümede ne yapacaðýmý kontrol
etmesine izin veriyorum, artý $P_\Omega(Y)$ ile orijinal veride görülen
öðeleri alýyorum.

Yani bir matris oluþturuyorum, görülen yerlerde $Y_{ij}$ var görülmeyen
yerlerde $B_{ij}$ var. Sonra bu matrisi düþük kerteli hale getiriyorum,
SVD'sini alýyorum, eþsiz deðerlerini iþleyerek bazýlarýný sýfýrlayýp tekrar
matrisi oluþturuyorum yani. Ve bu tarif edilenleri ardý ardýna
yapýyorum. Elde edileni yeni $B$ yap, mevcut olmayan ogeler icin bu matrisi
kullan, vs. 

Bu algoritmaya yumuþak atfetme (soft-impute) algoritmasi da deniyor, çünkü
her adýmda kayýp deðerleri ``yüklüyoruz'',  yaklaþýk olarak atýyoruz, ve
bunu yumuþak baðlamda yapýyoruz, eþikleme ile yavaþ yavaþ kerteye
düþürüyoruz. 

Bu arada üstteki yaklaþým ilk bulunduðunda proksimal gradyan olduðu
bilinmiyordu. Bilimci yaklaþýmý buluyor, hakkýnda makale yazýyor, sonra
sonra makaleyi revize ederken birdenbire anlýyor ki bu metot proksimal
gradyan.  Daha önce gördüðümüz ÝSTA da biraz böyle aslýnda, deðil mi? Kendi
baþýna durabilecek, gayet doðal bir mantýðý olan bir algoritma, sanki
proksimal yapýsý dýþýnda da bulunabilecek bir þey.

Devam edelim, algoritmadaki prox operatörü ne kadar pahalý? Her adýmda SVD
iþletmemiz lazým, bu ÝSTA'dan farklý. SVD oldukca pahalý bir iþlemdir,
özellikle büyük matrisler için. 

[atlandi]

Matris tamamlama böyle. Þimdi proksimal gradyan iniþine kategorik olarak
bakalým, PGÝ gradyan iniþinin genelleþtirilmiþ halidir dedik. $f = g + h$
kriterini çözüyorsak, 

$h=0$ ise normal gradyan iniþini elde ederiz. 

$h = I_C$ ise yansýtýlan gradyan iniþini elde ederiz. 

$g=0$ ise proksimal minimizasyon algoritmasini elde ediyoruz. Bu durumda
kriterde sadece pürüzsüz olmayan bir fonksiyon var, bu algoritma altgradyan
iniþine bir alternatif olarak kullanýlabiliyor. 

Tabii üstteki tüm özel durumlarýn da yakýnsama oraný $O(1/\epsilon)$. Bunu
biliyoruz çünkü proksimal gradyanin teorik yakýnsama oraný öyle [3,
48:30]. 









[devam edecek]

Ekler

Örnek kod, Lasso problem çözümü [2], pür proksimal gradyan iniþi, iz sürme yok

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
diabetes = pd.read_csv("../../stat/stat_120_regular/diabetes.csv",sep=';')
y = np.array(diabetes['response'].astype(float)).reshape(442,1)
X = np.array(diabetes.drop("response",axis=1))
N,dim = X.shape
print (N,dim)

lam = 1/np.sqrt(N);
w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T

L = (np.linalg.svd(X)[1][0])**2
print(L)
max_iter = 500

def obj(w):
    r = X*w-y;
    return np.sum(np.multiply(r,r))/2 +  lam * np.sum(np.abs(w))

def f_grad(w):
    return  X.T*(X*w-y) 

def soft_threshod(w,mu):
    return np.multiply(np.sign(w), np.maximum(np.abs(w)-mu,0))  

w = np.matrix([0.0]*dim).T
for t in range(0, max_iter):
    obj_val = obj(w)
    w = w - (1/L)* f_grad(w)
    w= soft_threshod(w,lam/L)    
    if (t % 50==0):
        print('iter= {},\tobjective= {:3f}'.format(t, obj_val.item()))

print (w)
\end{minted}

\begin{verbatim}
442 10
4.0242141761466925
iter= 0,	objective= 6425460.500000
iter= 50,	objective= 5751070.568959
iter= 100,	objective= 5750285.357193
iter= 150,	objective= 5749670.506866
iter= 200,	objective= 5749177.635558
iter= 250,	objective= 5748779.527464
iter= 300,	objective= 5748457.810485
iter= 350,	objective= 5748197.804952
iter= 400,	objective= 5747987.670443
iter= 450,	objective= 5747817.840900
[[  -8.71913404]
 [-238.35531517]
 [ 522.93302022]
 [ 323.11825944]
 [-526.09642955]
 [ 265.58097894]
 [ -17.84381222]
 [ 143.15165377]
 [ 652.14114865]
 [  68.55685031]]
\end{verbatim}

Kaynaklar

[1] Tibshirani, {\em Convex Optimization, Lecture Video 8}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}

[2] He, {\em IE 598 - Big Data Optimization},  
    \url{http://niaohe.ise.illinois.edu/IE598_2016/}

[3] Tibshirani, {\em Convex Optimization, Lecture Video 9}, 
\url{https://www.youtube.com/channel/UCIvaLZcfz3ikJ1cD-zMpIXg}


\end{document}




