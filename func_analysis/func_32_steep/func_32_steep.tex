\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
En Dik Ýniþ (Steepest Descent)

Daha önce gradyan iniþi konusunda iþlediðimiz üzere bir $f$ fonksiyonu için
hesaplanan $-\nabla f(x)$ gradyaný $x$ noktasýnda fonksiyon için en yüksek
iniþ (descent) olacak yönü gösteriyordu [1, sf. 151]. Fakat dikkat, {\em
  yön} kelimesini kullandýk, o yönde ne kadar adým atýlacaðýný
belirtmedik. Gradyanýn temel hesabý türeve dayalý olduðu için ve türev
hesapladýðý noktaya yakýn bir yerde doðru bir yaklaþýklama olacaðý için o
yönde atýlan adýmýn büyüklüðüne göre minimizasyon iyi ya da kötü sonuçlar
verebilir. Bu sebeple gradyan iniþi algoritmalarý, ki

$$
x^{x+1} = x^k + \alpha_k \nabla f(x^k)
$$

ile kodlanýrlar, çoðunlukla ufak ve pek çok adým atarlar, yani $\alpha_k$
sabitleri ufak seçilir. En Dik Ýniþ (SD) algoritmasi bu noktada bir
ilerleme. Her $\alpha$, yani $\alpha_k$ öyle seçilir ki
$\phi(\alpha) \equiv f(x^k - \alpha \nabla f(x^k))$ kesinlikle minimize
edilsin / belli bir yöndeki en minimum noktaya vardýracak büyüklükte adým
atýlsýn. Ya da

$$
\alpha_k = \arg\min_{\alpha \ge 0} f(x^k - \alpha \nabla f(x^k))
$$

Yani gradyanýn iþaret ettiði yönde bir tür ``arama'' yapmýþ oluyoruz, adým
büyüklüðünü öyle seçiyoruz ki fonksiyon o yönde o kadar adým atýldýðýnda en
fazla iniþi gerçekleþtirmiþ olsun. Bu sebeple bu metota çizgi aramasý (line
search) metotu deniyor.

Tabii arama derken akla ikinci bir döngü içinde yine ufak ufak adýmlar
atarak çizgi üzerinde gelinen yere bakýp büyüklük hesabýný böyle yapmak
gelebilir, bu sonuçsal olarak, kabaca doðru, ama asýl adým hesabý bazý
cebirsel temellerle, ya da onu çözen yaklaþýksal þekilde yapýlýyor. 

En basiti atýlan adým $\alpha$'yi pür cebirsel olarak çözmek, altta bir
örnek [3, sf. 101].

Soru

$f(x) = 9x_1^2 + 4x_1x_2 + 7x_2^2$ fonksiyonunun optimal noktasýný bul.

Çözüm

Gradyanýn öðeleri

$\frac{\partial f}{\partial x_1} = 18 x_1 + 4x_2$ ve 
$\frac{\partial  f}{\partial x_2} = 4 x_1 + 14 x_2$. Þimdi SD yöntemini
uygulayalým, baþlangýç noktasý $x^0 = [\begin{array}{cc} 1 & 1 \end{array}]^T$
olsun. Bu durumda $f(x^0) = 20$, ve $\nabla f(x_0) = [\begin{array}{cc} 22 & 18 \end{array}]^T$. 
Adým denklemine göre, 

$$
x^1 = x^0 - \alpha_0 \nabla f(x^0)
$$

ya da 

$$
\left[\begin{array}{c}
x_1 \\ x_2
\end{array}\right]
= 
\left[\begin{array}{c}
1 \\ 1
\end{array}\right]
\alpha_0 
\left[\begin{array}{c}
22 \\ 18
\end{array}\right]
$$

Þimdi öyle bir $\alpha_0$ seçmeliyiz ki $f(x^1)$ minimum olsun. Üstteki
deðerlerin bize verdiði $x_1$ ve $x_2$ deðerleri (ki $\alpha_0$ bazlý
olacaklar) ana formüle yeni $x$ olarak sokarsak, $\alpha_o$ bazlý bir
denklem edeceðiz,

$$
f(\alpha_0) = 20 - 808 \alpha_0 + 8208 (\alpha_0)^2
$$

$\frac{\ud f(\alpha_0)}{\ud \alpha_0} = 0$ üzerinden $\alpha_0$'nun optimum
deðeri $0.05$'týr. Yani adýmý þu þekilde atmalýyýz,

$$
x^1 = 
\left[\begin{array}{c}
x_1 \\ x_2
\end{array}\right]
= 
\left[\begin{array}{c}
1 \\ 1
\end{array}\right]
0.05
\left[\begin{array}{c}
22 \\ 18
\end{array}\right]
$$

ki bu hesap bize $f(x^1) = 0.12$ verir. Bu þekilde özyineli döngüye devam
edersek nihai optimum noktayý buluruz.

Sekant Yöntemi

Basit cebirsel numaralar ile üstte adýmý bulduk. Daha çetrefil durumlar
için sekant yöntemini kullanabiliriz. Bu yöntemi [2]'de iþledik, ayrýca bkz
[1, sf. 120]. Sonuçta aradýðýmýz $d$ yönündeki minimum

$$
\phi_k(\alpha) = f(x^k + \alpha d^k)
$$

deðerini bulmaktýr. Üstteki formülün $\alpha$ üzerinden türevi

$$
\phi_k'(\alpha) = {d^k}^T \nabla f(x^k + \alpha d^k) 
$$

O zaman minimum $\alpha$ icin 

$$
0 = {d^k}^T \nabla f(x^k + \alpha d^k) 
$$

denklemini çözen $\alpha$ gerekli. Bu bir kök bulma problemi ve sekant
yöntemini kullanabiliriz. 

\begin{minted}[fontsize=\footnotesize]{python}
def linesearch_secant(grad, d, x):
    epsilon=10**(-8)
    max = 500
    alpha_curr=0
    alpha=10**-8
    dphi_zero=np.dot(np.array(grad(x)).T,d)

    dphi_curr=dphi_zero
    i=0;
    while np.abs(dphi_curr)>epsilon*np.abs(dphi_zero):
        alpha_old=alpha_curr
        alpha_curr=alpha
        dphi_old=dphi_curr        
        dphi_curr=np.dot(np.array(grad(x+alpha_curr*d)).T,d)
        alpha=(dphi_curr*alpha_old-dphi_old*alpha_curr)/(dphi_curr-dphi_old);
        i += 1
        if (i >= max) and (np.abs(dphi_curr)>epsilon*np.abs(dphi_zero)):
            print('Line search terminating with number of iterations:')
            print(i)
            print(alpha)
            break
        
    return alpha
\end{minted}

Örnek

$f(x_1,x_2,x_3) = (x_1 - 4)^4 + (x_2 - 3)^2 + 4(x_3 + 5)^4$ fonksiyonunun
minimize edicisini bul.

Baþlangýç noktamýz $\left[\begin{array}{ccc} 4 & 2 & -1 \end{array}\right]^T$
olacak. 

Üstteki fonksiyonun gradyaný

$$
\nabla f(x) = \left[\begin{array}{ccc}  
4(x_1-4)^3 & 2(x_2-3) & 16(x_3+5)^3
\end{array}\right]^T
$$

Kod olarak,

\begin{minted}[fontsize=\footnotesize]{python}
def g(x): return np.array([4*(x[0]-4)**3, 2*(x[1]-3), 16*(x[2]+5)**3])
\end{minted}

$x^1$ hesaplamak için 

$$
\alpha_0 = \arg\min_{\alpha \ge 0} f(x^0 - \alpha \nabla f(x^0))
$$

lazým, tam açýlmýþ haliyle, 

$$
= \arg\min_{\alpha \ge 0} (0 + (2+2\alpha-3)^2 + 4(-1-1024\alpha+5)^4
$$

Ama üstteki cebirle boðuþmaya gerek yok, gradyan fonksiyonu ve gidiþ yönü
üzerinden kök bulup bize döndürecek üstteki çizgi aramasý kodunu
kullanabiliriz,

\begin{minted}[fontsize=\footnotesize]{python}
x0 = np.array([4,2,-1])
print (g(x0))
d0 = -g(x0)
alpha0 = linesearch_secant(g, d0, x0)
alpha0 = np.round(alpha0, 5)
print ('alpha0 =',alpha0)
x1 = x0 - alpha0*g(x0)
print ('x1',x1)
\end{minted}

\begin{verbatim}
[   0   -2 1024]
alpha0 = 0.00397
x1 [ 4.       2.00794 -5.06528]
\end{verbatim}

Arka arkaya iki adým daha atarsak,

\begin{minted}[fontsize=\footnotesize]{python}
print ('g1',g(x1))
d1 = -g(x1)
alpha1 = linesearch_secant(g, d1, x1)
print (alpha1)
x2 = x1 - alpha1*g(x1)
print ('x2',x2)
print ('\n')
print ('g2',g(x2))
d2 = -g(x2)
alpha3 = linesearch_secant(g, d2, x2)
print (alpha3)
x3 = x2 - alpha3*g(x2)
print ('x3',x3)
\end{minted}

\begin{verbatim}
g1 [ 0.         -1.98412    -0.00445103]
0.5000022675782785
x2 [ 4.          3.0000045  -5.06305448]


g2 [ 0.00000000e+00  8.99829483e-06 -4.01113920e-03]
14.894217818923421
x3 [ 4.          2.99987048 -5.00331169]
\end{verbatim}

Optimal noktaya eriþmiþ olduk.

Duruþ Þartlarý 

Optimizasyonda minimum varlýðý için birinci-derecen gerekli þart
(first-order necessary condition -FONC-) minimumda $\nabla f(x) = 0$
olmasý. Eðer böyle bir noktaya eriþmiþsek, diyelim $x^k$ için
$\nabla f(x^k) = 0$ olmuþ, bu nokta FONC'yi tatmin eder çünkü o zaman
$x^{k+1} = x^k$ olur, ve minimumdayýz demektir. Bu teorik bilgiyi
algoritmamýzýn ne zaman duracaðýný anlamasý için bir þart olarak kullanamaz
mýyýz?

Ne yazýk ki sayýsal hesaplarda, yani pratikte $\nabla f(x^k) = 0$ hesabý
nadiren ortaya çýkar. Bir çözüm gradyanýn normu $|| \nabla f(x) ||$ sýfýr
olmasýna bakmak. 

Ya da $| f(x^{k+1}) - f(x^k) |$ mutlak deðerine bakmak, yani hedef
fonksiyonun iki nokta arasýndaki farkýnýn mutlak deðerine, bu deðer eðer
daha önceden belirlenmiþ bir eþik deðeri $\epsilon$'un altýna düþmüþse
durmak. Ayný þeyi $x^{n+1}$ ve $x^n$ deðerlerinin kendisi için de
yapabiliriz.

Fakat bu yöntemler ölçek açýsýndan problemli olabilir. Mesela 1 ve 1000
arasýnda gidip gelen $f(x)$'lerle 0 ve 1 arasýnda gidip gelen $f(x)$'lerin
kullanacaðý $\epsilon$ farklý olabilir. Bir tanesi için $\epsilon = 100$
iyidir, diðeri için belki $\epsilon = 0.001$. Bu sebeple izafi bir hesap
daha faydalý olur, mesela

$$
\frac{|f(x^{k+1} - f(x^k))|}{|f(x^k)|} < \epsilon
$$

ya da 

$$
\frac{||x^{k+1} - x^k||}{||x^k||} < \epsilon
$$


Üstteki yaklaþým ``ölçekten baðýmsýz'' olduðu için daha tercih edilir
yaklaþým, bir problemden diðerine geçtiðimizde farklý bir $\epsilon$
kullanmamýz gerekmez.

Uygulama

Gradyan Ýniþi ve Model Uydurmak

Pek çok farklý probleme çözüm saðlayan bir teknik gradyan iniþidir. Ne yazýk ki
bilgisayar bilim lisans seviyesinde bu teknik genellikle öðretilmiyor. Bu yazýda
GÝ'nin hepimizin bildiði bir problemi, lineer regresyonu çözmek için nasýl
kullanýlacaðýný anlatacaðým [1].

Teorik seviyede GÝ bir fonksiyonu minimize etmeye yarar. Elde bazý parametreler
üzerinden tanýmlý bir fonksiyon vardýr, ve GÝ bir baþlangýç deðerinden
baþlayarak azar azar o parametreleri deðiþtirerek fonksiyonun minimal olduðu
yeri bulmaya uðraþýr. Bu azar azar, adým atýlarak yapýlan minimizasyon Calculus
sayesindedir, fonksiyonun gradyanýnýn negatif yönünde adým atýlarak mümkün
olur. Bazen bu matematiksel açýklamanýn pratik kullanýmý nasýl olur görmek zor
oluyor; Örnek olarak bir veriye lineer bir çizgi / model uyduralým.

Basit bir taným yaparsak lineer regresyonun amacý eldeki bir veri kümesine düz
çizgi uydurmaktýr. Veri alttaki gibi olabilir,

\begin{minted}[fontsize=\footnotesize]{python}
points = np.genfromtxt("data.csv", delimiter=",")
plt.scatter(points[:,0],points[:,1])
plt.savefig('vision_90fitting_04.png')
\end{minted}

\includegraphics[height=6cm]{vision_90fitting_04.png}

Üstteki veriyi düz çizgi olarak modellemek istiyoruz, bunun için lise
matematiðinden bilinen $y = mx + b$ formülünü kullanacaðýz, $m$ eðim (slope),
$b$ ise kesi (intercept), yani y-ekseninin kesildiði yer. Veriye uyan en iyi
çizgiyi bulmak demek en iyi $m,b$ deðerlerini bulmak demek.

Bunu yapmanýnýn standart yolu bir hata fonksiyonu tanýmlamak (bazen bedel
fonksiyonu da deniyor). Hata fonksiyonu bir çizginin ne kadar ``iyi'' olduðunu
ölçebilen bir fonksiyondur, bir $m,b$ çiftini alacak, veriye bakacak, ve bize
uyumun ne kadar iyi olduðunu bir hata deðeri üzerinden raporlayacak. Hata deðeri
hesabý için elimizdeki verideki tüm $x,y$ deðerlerine bakacaðýz, ve bunu
yaparken her veri $y$ deðeri ile, yine veri $x$'i üzerinden hesapladýðýmýz
$mx+b$ deðeri arasýndaki farka bakacaðýz; daha doðrusu farkýn karesini alacaðýz,
ve her veri noktasý için hesaplanan tüm bu kare hesaplarýný toplayacaðýz. Kare
alýnýyor, çünkü bu hatayý pozitif hale çevirmemizi saðlýyor, bir diðer fayda
tabii kare fonksiyonun türevi alýnabilir olmasý (kýyasla mutlak deðer fonksiyonu
iþleri daha karýþtýrýrdý). Pozitif bir hata yeterli, çünkü hata yapýlmýþsa
alttan mý üstten mi olduðu bizi ilgilendirmiyor. Hata $E$ hesabý þöyle,

Matematiksel olarak

$$ E_{(m,b)} = \frac{1}{N} \sum _{i=1}^{N} (y_i - (mx_i + b))^2$$

\begin{minted}[fontsize=\footnotesize]{python}
# y = mx + b
# m is slope, b is y-intercept
def compute_error_for_line_given_points(b, m, points):
    totalError = 0
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        totalError += (y - (m * x + b)) ** 2
    return totalError / float(len(points))
\end{minted}

Veriye daha iyi uyan çizgiler (ki ``daha iyi''nin ne olduðu hata fonksiyonumuz
üzerinden tanýmlý) daha az hata deðerleri anlamýna gelecektir. O zaman, eðer
hata fonksiyonunu minimize edersek, veriye uyan iyi çizgiyi bulacaðýz
demektir. Hata fonksiyonumuz iki parametreli olduðu için onu iki boyutlu bir
yüzey olarak grafikleyebiliriz,

\includegraphics[height=6cm]{vision_90fitting_06.png}

Bu iki boyutlu yüzey üzerindeki her nokta deðiþik bir çizgiyi temsil
ediyor. Yüzeyin alt düzlemden olan yüksekliði o çizgiye tekabül eden
hata. Gördüðümüz gibi bazý çizgiler bazýlarýndan daha az hataya sahip (yani
veriye daha iyi uymuþ). Gradyan iniþi ile arama yaptýðýmýz zaman bu yüzeyin
herhangi bir noktasýndan baþlayacaðýz, ve yokuþ aþaðý inerek hatasý en az olan
çizgiyi bulacaðýz.

Hata fonksiyonu üzerinde GÝ iþletmek için önce fonksiyonun gradyanýný
hesaplamamýz lazým. Gradyan bizim için nerede olursak olalým her zaman dip
noktasýný gösteren bir pusula görevini görüyor. Gradyan hesabý için hata
fonksiyonunun türevi alýnmalý. Hata fonksiyonunun $m,b$ adýnda iki tane
parametresi olduðuna göre bu iki parametrenin her biri için ayrý ayrý kýsmi
türev almamýz lazým. Bu türevler,

$$ 
\frac{\partial E}{\partial m} =
\frac{2}{N} \sum _{i=1}^{N} -x_i (y_i - (mx_i+b))
$$

$$ 
\frac{\partial E}{\partial b} =
\frac{2}{N} \sum _{i=1}^{N} -(y_i - (mx_i+b))
$$

Artýk GÝ iþletmek için gerekli tüm araçlara sahibiz. Aramayý herhangi bir $m,b$
noktasýndan (herhangi bir çizgi) baþlatýrýz, ve GÝ yokuþ aþaðý en iyi çizgi
parametrelerine doðru gider. Her döngü $m,b$ deðerlerini bu iniþe göre günceller
(dikkat inen {\em parametreler} deðil, hatada inilirken bu iniþe tekabül eden
$m,b$ deðerleri), ki bu sayede döngünün bir sonraki adýmýndaki hata bir öncekine
göre azalmýþ olur.

Matematiðe biraz daha yakýndan bakalým [2]. Türev almak, türeve göre adým atmak
bir fonksiyonunun minimum noktasýný bulmamýzý nasýl saðlýyor? Basit bir
fonksiyon $f(x)$'i düþünelim, 

\includegraphics[height=4cm]{vision_90fitting_05.png}

Gradyan, ya da belli bir $x$ noktasýndaki deðiþim oraný $\oslash y / \oslash x$
ile yaklaþýksallanabilir (çoðunlukla literatur $\Delta$ sembolünü kullanýr, [2]
$\oslash$ kullanmýþ, önemli deðil). Ya da bu yaklaþýksallýðý þöyle yazabiliriz,

$$ 
\frac{\partial f}{\partial x} =
\lim_{\oslash \to 0} \frac{\oslash y}{\oslash x} =
\lim_{\oslash \to 0} \frac{f(x + \oslash x) - f(x)}{\oslash x}
$$

ki bu ifade $f(x)$'in $x$'e göre kýsmi türevi olarak bilinir. Üstteki yöntem ile
sembolik olarak pek çok ifadenin türevini almayý biliyoruz, mesela $ax^2$ için
$2ax$, vs.

Þimdi elimizde bir $f(x)$ olduðunu düþünelim, ve $x$'i öyle bir þekilde
deðiþtirmek istiyoruz ki $f(x)$ minimize olsun. Ne yapacaðýmýz $f(x)$'in
gradyanýnýn ne olduðuna baðlý. Üç tane mümkün durum var:

Eðer $\frac{\partial f}{\partial x} > 0$ ise $x$ artarken $f(x)$ artar, o zaman
$x$'i azaltmalýyýz.

Eðer $\frac{\partial f}{\partial x} < 0$ ise $x$ artarken $f(x)$ azalýr, o zaman
$x$'i arttýrmalýyýz.

Eðer $\frac{\partial f}{\partial x} = 0$ ise $f(x)$ ya minimum ya da maksimum
noktasýndadýr, o zaman $x$'i olduðu gibi býrakmalýyýz.

Özet olarak $x$'i alttaki miktar kadar azaltýrsak $f(x)$'i de azaltabiliriz,

$$ \oslash x = x_{yeni} - x_{eski} = -\eta  \frac{\partial f}{\partial x}$$

ki $\eta$ ufak bir pozitif sabittir, $x$'i deðiþtirirken bu atýlan adýmýn
büyüklüðünü dýþarýdan ayarlayabilmemizi saðlar, deðiþimin hangi yönde olacaðýný
$\frac{\partial f}{\partial x}$ belirtiyor zaten. Bu formülü ardý ardýna
kullanýrsak, $f(x)$ yavaþ yavaþ minimum noktasýna doðru ``inecektir'', bu
yönteme gradyan iniþi minimizasyonu adý verilmesinin sebebi de budur. 

Örneðimize dönelim, 

\begin{minted}[fontsize=\footnotesize]{python}
def step_gradient(b_current, m_current, points, eta):
    b_gradient = 0
    m_gradient = 0
    N = float(len(points))
    for i in range(0, len(points)):
        x = points[i, 0]
        y = points[i, 1]
        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))
        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))
    new_b = b_current - (eta * b_gradient)
    new_m = m_current - (eta * m_gradient)
    return [new_b, new_m]

eta = 0.0001
initial_b = 0 # initial y-intercept guess
initial_m = 0 # initial slope guess
num_iterations = 8
print "Starting gradient descent at b = {0}, m = {1}, error = {2}".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points))
print "Running..."
b = initial_b
m = initial_m
xx = np.linspace(np.min(points[:,0]),np.max(points[:,0]), 100)
for i in range(num_iterations):
    b, m = step_gradient(b, m, np.array(points), eta)
    if i % 2 == 0: 
        print i, b,m
        yy = m * xx + b
        plt.scatter(points[:,0],points[:,1])
        plt.hold(True)
        plt.scatter(xx,yy)
        plt.hold(False)
        plt.savefig('grad_desc_%d' % i)
print "After {0} iterations b = {1}, m = {2}, error = {3}".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points))    
\end{minted}

\begin{verbatim}
Starting gradient descent at b = 0, m = 0, error = 5565.10783448
Running...
0 0.0145470101107 0.737070297359
2 0.0255792243213 1.29225466491
4 0.0284450719817 1.43194723238
6 0.029256114126 1.46709461772
After 8 iterations b = 0.0294319691638, m = 1.47298329822, error = 112.737981876
\end{verbatim}

\includegraphics[height=6cm]{grad_desc_0.png}
\includegraphics[height=6cm]{grad_desc_2.png}
\includegraphics[height=6cm]{grad_desc_4.png}
\includegraphics[height=6cm]{grad_desc_6.png}

Optimal $m,b$ deðerleri bulundu. $m=-1, b=0$'da baþladýk ve optimal sonucu
bulduk. Deðiþken \verb!eta! (yani $\eta$) adým büyüklüðü demiþtik, dikkat eðer
adým çok büyük seçilirse minimum ``atlanabilir'', yani varýþ noktasý
kaçýrýlabilir. Eðer $\eta$ çok küçük ise minimuma eriþmek için çok vakit
geçebilir. Ayrýca GÝ'nin doðru iþlediðini anlamanýn iyi yollarýndan birisi her
döngüde hatanýn azalýp azalmadýðýna bakmaktýr.

Bu basit bir örnekti, fakat bir bedel fonksiyonunu minimize edecek parametre
deðiþimlerini yapma kavramý yüksek dereceli polinomlarda, ya da diðer Yapay
Öðrenim problemlerinde de iþe yarýyor.

GÝ ile akýlda tutulmasý gereken bazý konular:

1) Dýþbükeylik (Convexity): Üstteki problemde sadece bir tane minimum vardý,
hata yüzeyi dýþbükeydi. Nereden baþlarsak baþlayalým, adým atarak minimuma
eriþecektik. Çoðunlukla durum böyle olmaz. Bazý problemlerde yerel minimumda
takýlý kalmak mümkün olabiliyor, bu problemleri aþmak için farklý çözümler var,
mesela Rasgele Gradyan Ýniþi (Stochastic Gradient Descent) kullanmak gibi.

2) Performans: Örnekte basit bir GÝ yaklaþýmý kullandýk, çizgi arama (line
search) gibi yaklaþýmlarla döngü sayýsýnýn azaltmak mümkün olabiliyor.

3) Yakýnsama (Convergence): Aramanýn bittiðinin kararlaþtýrýlmasýný kodlamadýk,
bu çoðunlukla hata döngüsündeki deðiþimlere bakýlarak yapýlýr; eðer hatadaki
deðiþim belli bir eþik deðerinden daha küçük ise, gradyanýn sýfýr olduðu yere
yaklaþýlmýþ demektir, ve arama durdurulabilir.

Not: Lineer regresyon tabii ki direk, tek bir adýmda çözülebilen bir
problem. GÝ'yi burada bir örnek amaçlý kullandýk. 

Kaynaklar 

[1] Zak, {\em An Introduction to Optimization, 4th Edition}

[2] Bayramli, {\em Diferansiyel Denklemler, Kök Bulmak, Karesel Formül (Root Finding, Quadratic Formula)}

[3] Dutta, {\em Optimization in Chemical Engineering}

\end{document}



