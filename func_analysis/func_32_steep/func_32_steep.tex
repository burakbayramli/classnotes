\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
En Dik Ýniþ (Steepest Descent)

Daha önce gradyan iniþi konusunda iþlediðimiz üzere bir $f$ fonksiyonu için
hesaplanan $-\nabla f(x)$ gradyaný $x$ noktasýnda fonksiyon için en yüksek
iniþ (descent) olacak yönü gösterir [1, sf. 151]. Dikkat, {\em yön}
kelimesini kullandýk, o yönde ne kadar adým atýlacaðýný
belirtmedik. Gradyanýn temel hesabý türeve dayalý olduðu için ve türev
hesapladýðý noktaya yakýn bir yerde doðru bir yaklaþýklama olacaðý için o
yönde atýlan adýmýn büyüklüðüne göre minimizasyon iyi ya da kötü sonuçlar
verebilir. Bu sebeple gradyan iniþi algoritmalarý, ki

$$
x^{x+1} = x^k + \alpha_k \nabla f(x^k)
$$

ile kodlanýrlar, çoðunlukla ufak ve pek çok adým atarlar, yani $\alpha_k$
sabitleri ufak seçilir. En Dik Ýniþ (SD) algoritmasi bu noktada bir
ilerleme. Her $\alpha$, yani $\alpha_k$ öyle seçilir ki
$\phi(\alpha) \equiv f(x^k - \alpha \nabla f(x^k))$ kesinlikle minimize
edilsin / belli bir yöndeki en minimum noktaya vardýracak büyüklükte adým
atýlsýn. Ya da

$$
\alpha_k = \arg\min_{\alpha \ge 0} f(x^k - \alpha \nabla f(x^k))
$$

Yani gradyanýn iþaret ettiði yönde bir tür ``arama'' yapmýþ oluyoruz, adým
büyüklüðünü öyle seçiyoruz ki fonksiyon o yönde o kadar adým atýldýðýnda en
fazla iniþi gerçekleþtirmiþ olsun. Bu sebeple bu metota çizgi aramasý (line
search) metotu deniyor.

Tabii arama derken akla ikinci bir döngü içinde yine ufak ufak adýmlar
atarak çizgi üzerinde gelinen yere bakýp büyüklük hesabýný böyle yapmak
gelebilir, bu sonuçsal olarak, kabaca doðru, ama asýl adým hesabý bazý
cebirsel temellerle, ya da onu çözen yaklaþýksal þekilde yapýlýyor. 

En basiti atýlan adým $\alpha$'yi pür cebirsel olarak çözmek, altta bir
örnek [3, sf. 101].

Soru

$f(x) = 9x_1^2 + 4x_1x_2 + 7x_2^2$ fonksiyonunun optimal noktasýný bul.

Çözüm

Gradyanýn öðeleri

$\frac{\partial f}{\partial x_1} = 18 x_1 + 4x_2$ ve 
$\frac{\partial  f}{\partial x_2} = 4 x_1 + 14 x_2$. Þimdi SD yöntemini
uygulayalým, baþlangýç noktasý $x^0 = \left[\begin{array}{cc} 1 & 1 \end{array}\right]^T$
olsun. Bu durumda $f(x^0) = 20$, ve $\nabla f(x_0) = \left[\begin{array}{cc} 22 18 \end{array}\right]^T$. 
Adým denklemine göre, 

$$
x^1 = x^0 - \alpha_0 \nabla f(x^0)
$$

ya da 

$$
\left[\begin{array}{c}
x_1 \\ x_2
\end{array}\right]
= 
\left[\begin{array}{c}
1 \\ 1
\end{array}\right]
\alpha_0 
\left[\begin{array}{c}
22 \\ 18
\end{array}\right]
$$

Þimdi öyle bir $\alpha_0$ seçmeliyiz ki $f(x^1)$ minimum olsun. Üstteki
deðerlerin bize verdiði $x_1$ ve $x_2$ deðerleri (ki $\alpha_0$ bazlý
olacaklar) ana formüle yeni $x$ olarak sokarsak, $\alpha_o$ bazlý bir
denklem edeceðiz,

$$
f(\alpha_0) = 20 - 808 \alpha_0 + 8208 (\alpha_0)^2
$$

$\frac{\ud f(\alpha_0)}{\ud \alpha_0} = 0$ üzerinden $\alpha_0$'nun optimum
deðeri $0.05$'týr. Yani adýmý þu þekilde atmalýyýz,

$$
x^1 = 
\left[\begin{array}{c}
x_1 \\ x_2
\end{array}\right]
= 
\left[\begin{array}{c}
1 \\ 1
\end{array}\right]
0.05
\left[\begin{array}{c}
22 \\ 18
\end{array}\right]
$$

ki bu hesap bize $f(x^1) = 0.12$ verir. Bu þekilde özyineli döngüye devam
edersek nihai optimum noktayý buluruz.

Sekant Yöntemi

Basit cebirsel numaralar ile üstte adýmý bulduk. Daha çetrefil durumlar
için sekant yöntemini kullanabiliriz. Bu yöntemi [2]'de iþledik, ayrýca bkz
[1, sf. 120]. Sonuçta aradýðýmýz $d$ yönündeki minimum

$$
\phi_k(\alpha) = f(x^k + \alpha d^k)
$$

deðerini bulmaktýr. Üstteki formülün $\alpha$ üzerinden türevi

$$
\phi_k'(\alpha) = {d^k}^T \nabla f(x^k + \alpha d^k) 
$$

O zaman minimum $\alpha$ icin 

$$
0 = {d^k}^T \nabla f(x^k + \alpha d^k) 
$$

denklemini çözen $\alpha$ gerekli. Bu bir kök bulma problemi ve sekant
yöntemini kullanabiliriz. 

\begin{minted}[fontsize=\footnotesize]{python}
def linesearch_secant(grad, d, x):
    epsilon=10**(-8)
    max = 500
    alpha_curr=0
    alpha=10**-8
    dphi_zero=np.dot(np.array(grad(x)).T,d)

    dphi_curr=dphi_zero
    i=0;
    while np.abs(dphi_curr)>epsilon*np.abs(dphi_zero):
        alpha_old=alpha_curr
        alpha_curr=alpha
        dphi_old=dphi_curr        
        dphi_curr=np.dot(np.array(grad(x+alpha_curr*d)).T,d)
        alpha=(dphi_curr*alpha_old-dphi_old*alpha_curr)/(dphi_curr-dphi_old);
        i += 1
        if (i >= max) and (np.abs(dphi_curr)>epsilon*np.abs(dphi_zero)):
            print('Line search terminating with number of iterations:')
            print(i)
            print(alpha)
            break
        
    return alpha
\end{minted}

Örnek

$f(x_1,x_2,x_3) = (x_1 - 4)^4 + (x_2 - 3)^2 + 4(x_3 + 5)^4$ fonksiyonunun
minimize edicisini bul.

Baþlangýç noktamýz $\left[\begin{array}{ccc} 4 & 2 & -1 \end{array}\right]^T$
olacak. 

Genel gradyan formülünü

$$
\nabla f(x) = \left[\begin{array}{ccc}  
4(x_1-4)^3 & 2(x_2-3) & 16(x_3+5)^3
\end{array}\right]^T
$$

olarak bulduk.

\begin{minted}[fontsize=\footnotesize]{python}
def g(x): return np.array([4*(x[0]-4)**3, 2*(x[1]-3), 16*(x[2]+5)**3])
\end{minted}

$x^1$ hesaplamak için 

$$
\alpha_0 = \arg\min_{\alpha \ge 0} f(x^0 - \alpha \nabla f(x^0))
$$

lazým, tam açýlmýþ haliyle, 

$$
= \arg\min_{\alpha \ge 0} (0 + (2+2\alpha-3)^2 + 4(-1-1024\alpha+5)^4
$$

Ama üstteki cebirle boðuþmaya gerek yok, gradyan fonksiyonu ve gidiþ yönü
üzerinden kök bulup bize döndürecek üstteki hesabý kullanabiliriz,

\begin{minted}[fontsize=\footnotesize]{python}
x0 = np.array([4,2,-1])
print (g(x0))
d0 = -g(x0)
alpha0 = linesearch_secant(g, d0, x0)
alpha0 = np.round(alpha0, 5)
print ('alpha0 =',alpha0)
x1 = x0 - alpha0*g(x0)
print ('x1',x1)
\end{minted}

\begin{verbatim}
[   0   -2 1024]
alpha0 = 0.00397
x1 [ 4.       2.00794 -5.06528]
\end{verbatim}

Arka arkaya iki adým daha atarsak,

\begin{minted}[fontsize=\footnotesize]{python}
print ('g1',g(x1))
d1 = -g(x1)
alpha1 = linesearch_secant(g, d1, x1)
print (alpha1)
x2 = x1 - alpha1*g(x1)
print ('x2',x2)
print ('\n')
print ('g2',g(x2))
d2 = -g(x2)
alpha3 = linesearch_secant(g, d2, x2)
print (alpha3)
x3 = x2 - alpha3*g(x2)
print ('x3',x3)
\end{minted}

\begin{verbatim}
g1 [ 0.         -1.98412    -0.00445103]
0.5000022675782785
x2 [ 4.          3.0000045  -5.06305448]


g2 [ 0.00000000e+00  8.99829483e-06 -4.01113920e-03]
14.894217818923421
x3 [ 4.          2.99987048 -5.00331169]
\end{verbatim}

Optimal noktaya eriþmiþ olduk.

Duruþ Þartlarý 

Optimizasyonda minimum varlýðý için birinci-derecen gerekli þart
(first-order necessary condition -FONC-) minimumda $\nabla f(x) = 0$
olmasý. Eðer böyle bir noktaya eriþmiþsek, diyelim $x^k$ için
$\nabla f(x^k) = 0$ olmuþ, bu nokta FONC'yi tatmin eder çünkü o zaman
$x^{k+1} = x^k$ olur, ve minimumdayýz demektir. Bu teorik bilgiyi
algoritmamizin ne zaman duracaðýný anlamasý için bir þart olarak kullanamaz
mýyýz?

Ne yazýk ki sayýsal hesaplarda, yani pratikte $\nabla f(x^k) = 0$ hesabý
nadiren ortaya çýkar. Bir çözüm gradyanýn normu $|| \nabla f(x) ||$ sýfýr
olmasýna bakmak. Ya da $| f(x^{k+1}) - f(x^k) |$ mutlak deðerine bakmak,
yani hedef fonksiyonun iki nokta arasýndaki farkýnýn mutlak deðerine, bu
deðer eðer daha önceden belirlenmiþ bir eþik deðeri $\epsilon$'un altýna
düþmüþse durmak. Ayný þeyi $x^{n+1}$ ve $x^n$ deðerlerinin kendisi için de
yapabiliriz.

Fakat bu yöntemler ölçek açýsýndan problemli olabilir. Mesela 1 ve 1000
arasýnda gidip gelen $f(x)$'lerle 0 ve 1 arasýnda tanýmlý $f(x)$'lerin
kullanacaðý $\epsilon$ farklý olabilir. Bir tanesi için $\epsilon = 100$
iyidir, diðeri için belki $\epsilon = 0.001$. Bu sebeple izafi bir hesap
daha faydalý olur, mesela 

$$
\frac{|f(x^{k+1} - f(x^k))|}{|f(x^k)|} < \epsilon
$$

ya da 

$$
\frac{||x^{k+1} - x^k||}{||x^k||} < \epsilon
$$


Üstteki yaklaþým ``ölçekten baðýmsýz'' olduðu için daha tercih edilir
yaklaþýmdýr, bir problemden diðerine geçtiðimizde farklý bir $\epsilon$
kullanmamýz gerekmez.  

Kaynaklar 

[1] Zak, {\em An Introduction to Optimization, 4th Edition}

[2] Bayramli, {\em Diferansiyel Denklemler, Kök Bulmak, Karesel Formül (Root Finding, Quadratic Formula)}

[3] Dutta, {\em Optimization in Chemical Engineering}

\end{document}
