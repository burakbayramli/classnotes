\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Uzaklýklar, Norm, Benzerlik

Literatürdeki anlatým norm ve uzaklýk konusu etrafýnda biraz kafa karýþýklýðý
yaratabiliyor, bu yazýda biraz açýklýk getirmeye çalýþalým. Norm bir büyüklük
ölçüsüdür. Vektör uzaylarý ile olan alakasýný görmek için {\em Fonksiyonel
  Analiz} notlarýna bakýlabilir. Büyüklük derken bir $x$ vektörünün
büyüklüðünden bahsediyoruz, ki bu çoðunlukla $||x||$ gibi bir kullanýmda
görülür, eðer altsimge yok ise, o zaman 2 kabul edilir, yani $||x||_2$. Bu ifade
bir L2 norm'unu ifade eder. $||x||_1$ varsa L1 norm'ü olurdu.

L1,L2 normalarý, ya da genel olarak $p$ üzerinden $L_p$ normlarý þöyle gösterilir,

$$ ||x||_p = (\sum_i |x_i|^p)^{1/p} $$

ki $x_i$, $x$ vektörü içindeki öðelerdir. Eðer $p=2$ ise, L2 norm

$$ ||x||_2 = \bigg(\sum_i |x_i|^2 \bigg)^{1/2} $$

Üstel olarak $1/2$'nin karekök demek olduðunu hatýrlayalým, yani 

$$ ||x||_2 = \sqrt{\sum |x_i|^2} $$

Bu norm ayrýca Öklitsel (Euclidian) norm olarak ta bilinir, tabii ki bunun
Öklitsel uzaklýk ile yakýn baðlantýsý var (iki vektörü birbirinden çýkartýp
Öklit normunu alýrsak Öklit uzaklýðýný hesaplamýþ oluruz).

Eðer $p=1$ olsaydý, yani L1 norm, o zaman üstel olarak $1/1$ olur, yani hiçbir
üstel / köksel iþlem yapýlmasýna gerek yoktur, iptal olurlar,

$$ ||x||_1 = \sum |x_i|^2 $$

Örnek

$$ 
a = \left[\begin{array}{r}
3 \\ -2 \\ 1
\end{array}\right]
 $$

$$ ||a|| = \sqrt{3^2+(-2)^2+1^2} = 3.742 $$

Örnekte altsimge yok, demek ki L2 norm. 

Ek Notasyon, Ýþlemler

L1 normu için yapýlan iþlemi düþünelim, vektör öðeleri kendileri ile
çarpýlýyor ve sonuçlar toplanýyor. Bu iþlem

$||x||_1 = x^Tx$

olarak ta gösterilemez mi? Ya da $x \cdot x$ olarak ki bu noktasal çarpýmdýr.

Bazen de yapay öðrenim literatüründe $||x||^2$ þekilde bir kullaným
görebiliyorsunuz. Burada neler oluyor? Altsimge yok, demek ki L2
norm. Sonra L2 normun karesi alýnmýþ, fakat L2 normu tanýmýna göre bir
karekök almýyor muydu? Evet, fakat o zaman kare iþlemi karekökü iptal eder,
demek ki L2 normunun karesini almak bizi L1 normuna döndürür! Eh bu normu
da $x^Tx$ olarak hesaplayabildiðimize göre hemen o notasyona geçebiliriz,
demek ki $||x||^2 = x^Tx = x \cdot x$. 

Ýkisel Vektörlerde Benzerlik

Diðer ilginç bir kullaným ikisel deðerler içeren iki vektör arasýnda
çakýþan 1 deðerlerinin toplamýný bulmak. Mesela 

\begin{minted}[fontsize=\footnotesize]{python}
a = np.array([1,0,0,1,0,0,1,1])
b = np.array([0,0,1,1,0,1,1,0])
\end{minted}

Bu iki vektör arasýndaki 1 uyusumunu bulmak için noktasal çarpým yeterli,
çünkü 1 ve 0, 0 ve 1, 0 ve 0 çarpýmý sýfýr verir, ama 1 çarpý 1 = 1
sonucunu verir. O zaman L1 norm bize ikisel iki vektör arasýnda kabaca bir
benzerlik fikri verebilir.

\begin{minted}[fontsize=\footnotesize]{python}
print np.dot(a,b)
\end{minted}

\begin{verbatim}
2
\end{verbatim}

Matris Normlarý

Vektörlerin norm'ü hesaplanabildiði gibi matris norm'ü da hesaplanabilir. Bir
$A$ matrisi için matris norm'ü

$$ || A || = \sup \{ ||Ax|| : x \in \mathbb{R}^n, ||x||=1 \textrm{ olacak þekilde } \} $$

Bazen þöyle de gösterilir,

$$  || A || = \sup_{||x||=1} \{ ||Ax|| \} $$

ya da

$$ || A || = \sup \{ \frac{||Ax||}{||x||} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak þekilde } \} $$

Daha genel formda p-norm'u

$$ || A || = \sup
\bigg\{
\frac{||Ax||_p}{||x||_p} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak þekilde }
\bigg\} $$

Özel durum $p=2$ için ki bu yine, vektörler için olduðu gibi, Öklitsel norm
olarak biliniyor. Bu durumda $A$'nýn normu $A$'nýn en büyük eþsiz
deðeridir. Yaklaþýk olarak hesaplama açýsýndan þunu da verelim,

$$ ||A||_1 = \max_{1 \le j \le n} \sum _{i=1}^{m} |a_{ij}| $$

Yani tüm matris kolonlarýnýn hücrelerinin mutlak deðerleri toplanýyor, bu
toplamlar arasýnda en büyük sayýyý veren kolonun toplamý normun yaklaþýk
deðeridir.

Spektral (Operatör) ve Ýz (Trace) Norm

Bu normlar sýrasýyla matrisin en büyük eþsiz (singular) deðeri, ve tüm
eþsiz deðerlerinin toplamýyla hesaplanýr. Bir matris $X$ için operatör norm

$$
||X||_{op} = \sigma_1(X)
$$

Ýz normu

$$
||X||_{tr} = \sigma _{i=1}^{r} \sigma_i(X)
$$


\newpage

$$ (x-v)^TA(x-v) < 1 $$

Üstteki formülde $x$ yerine $Px$ geçirirsek, ki $P$ herhangi bir matris,
eþitsizliðin sol tarafýna ne olur?

$$ (P(x-v))^T A (P(x-v))$$

$$ (x-v)^T P^T A P (x-v)  $$

Bu formüle bir þekilde ulaþmamýz lazým. Ama nasýl? Basitleþtirme amaçlý olarak
$w = x-v$ tanýmlayalým, ki $x \ne v$ olacak þekilde. $X = \frac{1}{||w||^2} I$
tanýmlayalým, bu bir köþegen matris, köþegeninde $1/||w||^2$ deðerleri var. Bu
sayede

$$ w^T A W < 1  \Rightarrow w^T A W < w^T X w  $$

1 yerine üstteki en saðdaki terimi kullanmýþ olduk. Herhangi bir $x$ için
üstteki eþitsizlik her $w$ için doðru olacaktýr. Bu da $A - X$ negatif kesin
demektir (pozitif kesinliðin tersi), o zaman þunu da söyleyebiliriz,

$$ A - X < 0 \Rightarrow P^T(A-x)P < 0 \Rightarrow P^T AP < P^TXP $$

Soldan ve saðdan $w^T,w$ ile çarparsak,

$$ w^T P^T AP w < w^T P^TXP w = \frac{1}{||w||^2} w^T P^T P w = (Pu)^T Pu$$

ki $u = \frac{w}{||w||}$ $x-v$ yönünü gösteren birim vektördür. 

Þimdi matris normunun ne olduðunu hatýrlayalým,

$$ ||P|| = \sup_{||u||=1} || Pu || $$

O zaman emin bir þekilde diyebiliriz ki 

$$ (x-v)^TA(x-v) < 1 \Rightarrow (x-v)^T P^T A P (x-v) < ||P||^2 $$


\newpage 

Sherley-Morrison Formülü

Bu formülün temeli þu eþitlikten baþlýyor [1, sf. 124],

$$
(I+cd^T)^{-1} = I - \frac{cd^T}{1+d^Tc}
\mlabel{1}
$$

ki $c,d$ birer vektör, ve $1+d^Tc \ne 0$ olacak þekilde, üstteki eþitliðin
doðru olduðunu kontrol için iki tarafý $(I+cd^T)$ ile çarpabiliriz, eðer
sað tarafta birim matrisi elde edersek eþitlik doðru demektir,

$$
I + cd^T - \frac{cd^T (I + cd^T)}{1+d^Tc}
$$

$$
= I + cd^T - \frac{I cd^T (1+cd^T)}{1+d^Tc}
$$

$$
= I + cd^T - cd^T = I
$$

Eðer sýfýrdan baþlayarak türetmek istesek, öyle bir $\alpha$ arýyoruz ki
$(I + cd^T)$ ifadesini $(I + \alpha cd^T)$ ile çarpýnca bize birim matrisi
versin. Çarpýmý yaparsak,

$$
(I + cd^T) (I + \alpha cd^T) = I + cd^T + \alpha cd^T + \alpha cd^Tcd^T 
$$

$$
= I + (1 + \alpha + \alpha d^T c) cd^T
$$

Üsttekinin birim matrisi $I$ olmasý için $1 + \alpha + \alpha d^T c$ sýfýr
olmalý, onun sýfýr olmasý için de

$$\alpha = \frac{-1}{1 + d^Tc}$$

doðru olmalý. $\alpha$'yi yerine koyarsak, 

$$
(I + \alpha cd^T) = I - \frac{cd^T}{1+d^Tc}
$$

elde ederiz, yani $(I + \alpha cd^T)^{-1}$ açýlýmý budur. 

Sherman-Morrison formülü 

$$
(A + cd^T)^{-1} = A^{-1} - \frac{A^{-1} cd^T A^{-1} }{1 + d^TA^{-1}c}
$$

Bu formüle eriþmek için $(A + cd^T)^{-1}$ ile baþlayalým, $A$'yi parantez
dýþýna çekersek,

$$
(A + cd^T)^{-1} = \big( A ( I + A ^{-1} cd^T ) \big)^{-1} 
$$

$$
= ( I + A ^{-1} cd^T )^{-1} A^{-1} 
$$

Parantez içinin (1)'in sol tarafýna benzediðini görebiliriz, $b = A^{-1}c$ 
desek,  $( I + bd^T )^{-1} $ açýlýmýni yapýyor olurduk, 

$$
( I + bd^T )^{-1} 
= I - \frac{bd^T}{1+d^Tb} 
= I - \frac{A^{-1}cd^T}{1+d^TA^{-1}c}
$$

Bu sonucu iki üstteki parantez içindeki $A ( I + A ^{-1} cd^T$ yerine
koyarsak,

$$
(A + cd^T)^{-1} = I - \frac{A^{-1}cd^T}{1+d^TA^{-1}c} A^{-1} 
$$

sonucuna eriþmiþ oluyoruz. 

Sherman-Morrison-Woodburry

Bu son formül Sherman-Morrison formülünün daha genelleþtirilmiþ hali
[3]. Diyelim ki $A \in \mathbb{R}^{n \times n}$ eþsiz deðil, ve
$U,V \in \mathbb{R}^{n \times p}$ öyle ki 

$$
U + V^T A^{-1} U \in \mathbb{R}^{p \times p}
$$

O zaman 

$$B = A + UV^T$$

eþsiz deðildir, ve 

$$
B^{-1} = A^{-1} - A^{-1} U ( I + V^T A^{-1} U)^{-1} V^T A^{-1} 
$$

Kaynaklar 

[1] Meyer, {\em Matrix Analysis and Applied Linear Algebra}

[2] Gadzinski, {How to Derive the Sherman-Morrison Base Formula, Math Stackexchange Sorusuna Cevap}, 
    \url{https://math.stackexchange.com/a/3462542/6786}

[3] Gockenbach, {\em Numerical Optimization MA 5630, Globalizing Newton's method: Descent Directions (II)}
    \url{https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures.html}

[4] Marmer, {\em Economics 627 Econometric Theory II, Vector and Matrix Differentiation}, 
    \url{http://faculty.arts.ubc.ca/vmarmer/econ627/}
        
[5] Duda, Hart, {\em Pattern Classification}

[6] Bishop, {\em Pattern Recognition and Machine Learning}

[7] Wikipedia, {\em Matrix norm}, 
    \url{https://en.wikipedia.org/wiki/Matrix_norm}

[8] Thibshirani, {\em Convex Optimization}, 
    \url{https://www.stat.cmu.edu/~ryantibs/convexopt}

\newpage 

Yunan Harfleri

\includegraphics[width=30em]{../../algs/algs_999_zapp/letters.png}

\end{document}





