<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>

  <title>Uzaklıklar, Norm, Benzerlik</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="uzaklıklar-norm-benzerlik">Uzaklıklar, Norm, Benzerlik</h1>
<p>Literatürdeki anlatım norm ve uzaklık konusu etrafında biraz kafa karışıklığı yaratabiliyor, bu yazıda biraz açıklık getirmeye çalışalım. Norm bir büyüklük ölçüsüdür. Vektör uzayları ile olan alakasını görmek için {} notlarına bakılabilir. Büyüklük derken bir <span class="math inline">\(x\)</span> vektörünün büyüklüğünden bahsediyoruz, ki bu çoğunlukla <span class="math inline">\(||x||\)</span> gibi bir kullanımda görülür, eğer altsimge yok ise, o zaman 2 kabul edilir, yani <span class="math inline">\(||x||_2\)</span>. Bu ifade bir L2 norm'unu ifade eder. <span class="math inline">\(||x||_1\)</span> varsa L1 norm'ü olurdu.</p>
<p>L1,L2 normaları, ya da genel olarak <span class="math inline">\(p\)</span> üzerinden <span class="math inline">\(L_p\)</span> normları şöyle gösterilir,</p>
<p><span class="math display">\[ ||x||_p = (\sum_i |x_i|^p)^{1/p} \]</span></p>
<p>ki <span class="math inline">\(x_i\)</span>, <span class="math inline">\(x\)</span> vektörü içindeki öğelerdir. Eğer <span class="math inline">\(p=2\)</span> ise, L2 norm</p>
<p><span class="math display">\[ ||x||_2 = \bigg(\sum_i |x_i|^2 \bigg)^{1/2} \]</span></p>
<p>Üstel olarak <span class="math inline">\(1/2\)</span>'nin karekök demek olduğunu hatırlayalım, yani</p>
<p><span class="math display">\[ ||x||_2 = \sqrt{\sum |x_i|^2} \]</span></p>
<p>Bu norm ayrıca Öklitsel (Euclidian) norm olarak ta bilinir, tabii ki bunun Öklitsel uzaklık ile yakın bağlantısı var (iki vektörü birbirinden çıkartıp Öklit normunu alırsak Öklit uzaklığını hesaplamış oluruz).</p>
<p>Eğer <span class="math inline">\(p=1\)</span> olsaydı, yani L1 norm, o zaman üstel olarak <span class="math inline">\(1/1\)</span> olur, yani hiçbir üstel / köksel işlem yapılmasına gerek yoktur, iptal olurlar,</p>
<p><span class="math display">\[ ||x||_1 = \sum |x_i|^2 \]</span></p>
<p>Örnek</p>
<p><span class="math display">\[ 
a = \left[\begin{array}{r}
3 \\ -2 \\ 1
\end{array}\right]
 \]</span></p>
<p><span class="math display">\[ ||a|| = \sqrt{3^2+(-2)^2+1^2} = 3.742 \]</span></p>
<p>Örnekte altsimge yok, demek ki L2 norm.</p>
<p>Ek Notasyon, İşlemler</p>
<p>L1 normu için yapılan işlemi düşünelim, vektör öğeleri kendileri ile çarpılıyor ve sonuçlar toplanıyor. Bu işlem</p>
<p><span class="math inline">\(||x||_1 = x^Tx\)</span></p>
<p>olarak ta gösterilemez mi? Ya da <span class="math inline">\(x \cdot x\)</span> olarak ki bu noktasal çarpımdır.</p>
<p>Bazen de yapay öğrenim literatüründe <span class="math inline">\(||x||^2\)</span> şekilde bir kullanım görebiliyorsunuz. Burada neler oluyor? Altsimge yok, demek ki L2 norm. Sonra L2 normun karesi alınmış, fakat L2 normu tanımına göre bir karekök almıyor muydu? Evet, fakat o zaman kare işlemi karekökü iptal eder, demek ki L2 normunun karesini almak bizi L1 normuna döndürür! Eh bu normu da <span class="math inline">\(x^Tx\)</span> olarak hesaplayabildiğimize göre hemen o notasyona geçebiliriz, demek ki <span class="math inline">\(||x||^2 = x^Tx = x \cdot x\)</span>.</p>
<p>İkisel Vektörlerde Benzerlik</p>
<p>Diğer ilginç bir kullanım ikisel değerler içeren iki vektör arasında çakışan 1 değerlerinin toplamını bulmak. Mesela</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">a <span class="op">=</span> np.array([<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>])
b <span class="op">=</span> np.array([<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>])</code></pre></div>
<p>Bu iki vektör arasındaki 1 uyusumunu bulmak için noktasal çarpım yeterli, çünkü 1 ve 0, 0 ve 1, 0 ve 0 çarpımı sıfır verir, ama 1 çarpı 1 = 1 sonucunu verir. O zaman L1 norm bize ikisel iki vektör arasında kabaca bir benzerlik fikri verebilir.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> np.dot(a,b)</code></pre></div>
<pre><code>2</code></pre>
<p>Matris Normları</p>
<p>Vektörlerin norm'ü hesaplanabildiği gibi matris norm'ü da hesaplanabilir. Bir <span class="math inline">\(A\)</span> matrisi için matris norm'ü</p>
<p><span class="math display">\[ || A || = \sup \{ ||Ax|| : x \in \mathbb{R}^n, ||x||=1 \textrm{ olacak şekilde } \} \]</span></p>
<p>Bazen şöyle de gösterilir,</p>
<p><span class="math display">\[  || A || = \sup_{||x||=1} \{ ||Ax|| \} \]</span></p>
<p>ya da</p>
<p><span class="math display">\[ || A || = \sup \{ \frac{||Ax||}{||x||} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak şekilde } \} \]</span></p>
<p>Daha genel formda p-norm'u</p>
<p><span class="math display">\[ || A || = \sup
\bigg\{
\frac{||Ax||_p}{||x||_p} : x \in \mathbb{R}^n, x \ne 0 \textrm{ olacak şekilde }
\bigg\} \]</span></p>
<p>Özel durum <span class="math inline">\(p=2\)</span> için ki bu yine, vektörler için olduğu gibi, Öklitsel norm olarak biliniyor. Bu durumda <span class="math inline">\(A\)</span>'nın normu <span class="math inline">\(A\)</span>'nın en büyük eşsiz değeridir. Yaklaşık olarak hesaplama açısından şunu da verelim,</p>
<p><span class="math display">\[ ||A||_1 = \max_{1 \le j \le n} \sum_{i=1}^{m} |a_{ij}| \]</span></p>
<p>Yani tüm matris kolonlarının hücrelerinin mutlak değerleri toplanıyor, bu toplamlar arasında en büyük sayıyı veren kolonun toplamı normun yaklaşık değeridir.</p>
<p>Spektral (Operatör) ve İz (Trace) Norm</p>
<p>Bu normlar sırasıyla matrisin en büyük eşsiz (singular) değeri, ve tüm eşsiz değerlerinin toplamıyla hesaplanır. Bir matris <span class="math inline">\(X\)</span> için operatör norm</p>
<p><span class="math display">\[
||X||_{op} = \sigma_1(X)
\]</span></p>
<p>İz normu</p>
<p><span class="math display">\[
||X||_{tr} = \sigma_{i=1}^{r} \sigma_i(X)
\]</span></p>
<hr>
<p><span class="math display">\[ (x-v)^TA(x-v) &lt; 1 \]</span></p>
<p>Üstteki formülde <span class="math inline">\(x\)</span> yerine <span class="math inline">\(Px\)</span> geçirirsek, ki <span class="math inline">\(P\)</span> herhangi bir matris, eşitsizliğin sol tarafına ne olur?</p>
<p><span class="math display">\[ (P(x-v))^T A (P(x-v))\]</span></p>
<p><span class="math display">\[ (x-v)^T P^T A P (x-v)  \]</span></p>
<p>Bu formüle bir şekilde ulaşmamız lazım. Ama nasıl? Basitleştirme amaçlı olarak <span class="math inline">\(w = x-v\)</span> tanımlayalım, ki <span class="math inline">\(x \ne v\)</span> olacak şekilde. <span class="math inline">\(X = \frac{1}{||w||^2} I\)</span> tanımlayalım, bu bir köşegen matris, köşegeninde <span class="math inline">\(1/||w||^2\)</span> değerleri var. Bu sayede</p>
<p><span class="math display">\[ w^T A W &lt; 1  \Rightarrow w^T A W &lt; w^T X w  \]</span></p>
<p>1 yerine üstteki en sağdaki terimi kullanmış olduk. Herhangi bir <span class="math inline">\(x\)</span> için üstteki eşitsizlik her <span class="math inline">\(w\)</span> için doğru olacaktır. Bu da <span class="math inline">\(A - X\)</span> negatif kesin demektir (pozitif kesinliğin tersi), o zaman şunu da söyleyebiliriz,</p>
<p><span class="math display">\[ A - X &lt; 0 \Rightarrow P^T(A-x)P &lt; 0 \Rightarrow P^T AP &lt; P^TXP \]</span></p>
<p>Soldan ve sağdan <span class="math inline">\(w^T,w\)</span> ile çarparsak,</p>
<p><span class="math display">\[ w^T P^T AP w &lt; w^T P^TXP w = \frac{1}{||w||^2} w^T P^T P w = (Pu)^T Pu\]</span></p>
<p>ki <span class="math inline">\(u = \frac{w}{||w||}\)</span> <span class="math inline">\(x-v\)</span> yönünü gösteren birim vektördür.</p>
<p>Şimdi matris normunun ne olduğunu hatırlayalım,</p>
<p><span class="math display">\[ ||P|| = \sup_{||u||=1} || Pu || \]</span></p>
<p>O zaman emin bir şekilde diyebiliriz ki</p>
<p><span class="math display">\[ (x-v)^TA(x-v) &lt; 1 \Rightarrow (x-v)^T P^T A P (x-v) &lt; ||P||^2 \]</span></p>
<hr>
<p>Sherley-Morrison Formülü</p>
<p>Bu formülün temeli şu eşitlikten başlıyor [1, sf. 124],</p>
<p><span class="math display">\[
(I+cd^T)^{-1} = I - \frac{cd^T}{1+d^Tc}
\qquad (1)
\]</span></p>
<p>ki <span class="math inline">\(c,d\)</span> birer vektör, ve <span class="math inline">\(1+d^Tc \ne 0\)</span> olacak şekilde, üstteki eşitliğin doğru olduğunu kontrol için iki tarafı <span class="math inline">\((I+cd^T)\)</span> ile çarpabiliriz, eğer sağ tarafta birim matrisi elde edersek eşitlik doğru demektir,</p>
<p><span class="math display">\[
I + cd^T - \frac{cd^T (I + cd^T)}{1+d^Tc}
\]</span></p>
<p><span class="math display">\[
= I + cd^T - \frac{I cd^T (1+cd^T)}{1+d^Tc}
\]</span></p>
<p><span class="math display">\[
= I + cd^T - cd^T = I
\]</span></p>
<p>Eğer sıfırdan başlayarak türetmek istesek, öyle bir <span class="math inline">\(\alpha\)</span> arıyoruz ki <span class="math inline">\((I + cd^T)\)</span> ifadesini <span class="math inline">\((I + \alpha cd^T)\)</span> ile çarpınca bize birim matrisi versin. Çarpımı yaparsak,</p>
<p><span class="math display">\[
(I + cd^T) (I + \alpha cd^T) = I + cd^T + \alpha cd^T + \alpha cd^Tcd^T 
\]</span></p>
<p><span class="math display">\[
= I + (1 + \alpha + \alpha d^T c) cd^T
\]</span></p>
<p>Üsttekinin birim matrisi <span class="math inline">\(I\)</span> olması için <span class="math inline">\(1 + \alpha + \alpha d^T c\)</span> sıfır olmalı, onun sıfır olması için de</p>
<p><span class="math display">\[\alpha = \frac{-1}{1 + d^Tc}\]</span></p>
<p>doğru olmalı. <span class="math inline">\(\alpha\)</span>'yi yerine koyarsak,</p>
<p><span class="math display">\[
(I + \alpha cd^T) = I - \frac{cd^T}{1+d^Tc}
\]</span></p>
<p>elde ederiz, yani <span class="math inline">\((I + \alpha cd^T)^{-1}\)</span> açılımı budur.</p>
<p>Sherman-Morrison formülü</p>
<p><span class="math display">\[
(A + cd^T)^{-1} = A^{-1} - \frac{A^{-1} cd^T A^{-1} }{1 + d^TA^{-1}c}
\]</span></p>
<p>Bu formüle erişmek için <span class="math inline">\((A + cd^T)^{-1}\)</span> ile başlayalım, <span class="math inline">\(A\)</span>'yi parantez dışına çekersek,</p>
<p><span class="math display">\[
(A + cd^T)^{-1} = \big( A ( I + A ^{-1} cd^T ) \big)^{-1} 
\]</span></p>
<p><span class="math display">\[
= ( I + A ^{-1} cd^T )^{-1} A^{-1} 
\]</span></p>
<p>Parantez içinin (1)'in sol tarafına benzediğini görebiliriz, <span class="math inline">\(b = A^{-1}c\)</span> desek, $( I + bd^T )^{-1} $ açılımıni yapıyor olurduk,</p>
<p><span class="math display">\[
( I + bd^T )^{-1} 
= I - \frac{bd^T}{1+d^Tb} 
= I - \frac{A^{-1}cd^T}{1+d^TA^{-1}c}
\]</span></p>
<p>Bu sonucu iki üstteki parantez içindeki <span class="math inline">\(A ( I + A ^{-1} cd^T\)</span> yerine koyarsak,</p>
<p><span class="math display">\[
(A + cd^T)^{-1} = I - \frac{A^{-1}cd^T}{1+d^TA^{-1}c} A^{-1} 
\]</span></p>
<p>sonucuna erişmiş oluyoruz.</p>
<p>Sherman-Morrison-Woodburry</p>
<p>Bu son formül Sherman-Morrison formülünün daha genelleştirilmiş hali [3]. Diyelim ki <span class="math inline">\(A \in \mathbb{R}^{n \times n}\)</span> eşsiz değil, ve <span class="math inline">\(U,V \in \mathbb{R}^{n \times p}\)</span> öyle ki</p>
<p><span class="math display">\[
U + V^T A^{-1} U \in \mathbb{R}^{p \times p}
\]</span></p>
<p>O zaman</p>
<p><span class="math display">\[B = A + UV^T\]</span></p>
<p>eşsiz değildir, ve</p>
<p><span class="math display">\[
B^{-1} = A^{-1} - A^{-1} U ( I + V^T A^{-1} U)^{-1} V^T A^{-1} 
\]</span></p>
<hr>
<p>Gereğinden Fazla Calculus</p>
<p>(MİT üniversitesi Matematik hocası Gilbert Strang'in MİT üniversitesine hitaben bir yazısından [9] alınmıştır)</p>
<p>Calculus I, Calculus II, Calculus III - öğretim sistemimizde ne kadar büyük bir dengesizlik! Matematiğin geri kalan kısmı Calculus tarafından boğuldu denebilir. Bu kadar Calculus dersinden sonra takip eden ders herhalde Türevsel Denklemler (gene Calculus), Calculus'tan önceki ders'te herhalde Calculus'a Giriş dersi idi. Arkadaşlar, bu dengesizliği düzeltmek bizim görevimiz, bunu başkasından bekleyemeyiz. Lineer cebir'in ne kadar önemli bir ders olduğunu biliyoruz. Bu ders seçmeli/rasgele alınan bir ders değil, uygulama olarak birçok öğrenciye Calculus'dan daha faydalı olacak bir ders. Artık sayısal bir dünyada yaşıyoruz. Bu konu hakkında dünyadaki hocalara öncü ve örnek olmamızı istediğim için, Lineer Cebir'in faydalarından bahsetmek istiyorum. Özetle şöyle düşünüyorum: Eğer şu ankinden daha fazla öğrenci Lineer Cebir öğreniyor ise, matematik bölümü bir şeyleri doğru yapıyor demektir. İstatistik ve Ayrıksal Matematik te lazım. Umarım bölüm başkanı ve rektör onaylar. İnanıyorum ki bu sayede öğrencilerimi için doğru şeyi yapmış olacağız.İzin verirseniz, lineer cebir ders basamaklarından bahsedeyim. Mesela dersin her aşaması belli denklemlerin çözümüne, ya da o denklemleri çözmeye temel olan fikirlere ve algoritmalara göre ayırılabilir. Bu safhalar birbirini tamamlamalıdır. Mesela 4 denklemi merkez olarak alabiliriz.</p>
<p><span class="math display">\[
Ax = b, \quad A&#39;Ax = A&#39;b, \quad Ax = \lambda x, \quad \mathrm{d} u / \mathrm{d} t = Au
\]</span></p>
<p>En önemli nokta, herhangi bir uygulama için (gerçek dünyada) bir doğrusal 'sistemi' görebilmek. Şu bizim ünlü A matrisimizi bulabilmek, tanımlayabilmek ne kadar önemli değil mi? Bunun sonrasında tabii ki o matris üzerinde işlem yapmamıza yardımcı olacak fikirler takip edecek.</p>
<ul>
<li><p>Alt-uzaylar ve bazlar, izdüşümler ve dikgenlik, özvektörler ve özdeğerler</p></li>
<li><p>Algoritmalar da çok önemli (matris çarpımı da buna dahil)</p></li>
<li><p>Ax = kolonların katışımı, A = LU ile yokedilmesi, sonra Gram-Schmidt işlemi</p></li>
</ul>
<p>En mühim konu da 'doğrusal dönüşüm'. Eğer bir problem içinde matrisin bazına ne olduğunu biliyorsak, her şeyi biliyoruz demektir. Ben örneklere odaklanabilirim, siz ispatlara odaklanabilirsiniz, ama sınıfın ne beklediğine her zaman kulağımızı açık tutalım.Tekrar ana konuya döneyim, çünkü hepimizin yardımını ve eylemini gerektiriyor. Lineer cebir hakkında çoğunlukla destek görüyoruz, ya da aldırmazlık görüyoruz. Öteki hocaların da kendi yapacak işleri var, hattâ ve hattâ üst düzey mühendisler bile lineer cebiri istenmeyen bir şey olarak görebiliyorlar. Belki de bilgisayarların işleri nasıl değiştirdiğinin farkında değiller. Fakat sonuçta öğrencileri önde tutarak doğru seçimi yapacaklardır. Öğrenciler durumu anladığında hocalarımız da doğru seçimi yaptıklarını inanacaklarına eminim. Calculus I, II ve III derslerinin kendisinin reform edilmesi bu sunumun dışında. Bu dersler de önemli, ama hayat-memat seviyesinde değil. Öğrencilerin çoğuna 'yararlı' olacak türden matematik öğretmek bizim görevimiz.</p>
<p>Kaynaklar</p>
<p>[1] Meyer, <em>Matrix Analysis and Applied Linear Algebra</em></p>
<p>[2] Gadzinski, {How to Derive the Sherman-Morrison Base Formula, Math Stackexchange Sorusuna Cevap}, <a href="https://math.stackexchange.com/a/3462542/6786" class="uri">https://math.stackexchange.com/a/3462542/6786</a></p>
<p>[3] Gockenbach, <em>Numerical Optimization MA 5630, Globalizing Newton's method: Descent Directions (II)</em> <a href="https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures.html" class="uri">https://pages.mtu.edu/~msgocken/ma5630spring2003/lectures.html</a></p>
<p>[4] Marmer, <em>Economics 627 Econometric Theory II, Vector and Matrix Differentiation</em>, <a href="http://faculty.arts.ubc.ca/vmarmer/econ627/" class="uri">http://faculty.arts.ubc.ca/vmarmer/econ627/</a></p>
<p>[5] Duda, Hart, <em>Pattern Classification</em></p>
<p>[6] Bishop, <em>Pattern Recognition and Machine Learning</em></p>
<p>[7] Wikipedia, <em>Matrix norm</em>, <a href="https://en.wikipedia.org/wiki/Matrix_norm" class="uri">https://en.wikipedia.org/wiki/Matrix_norm</a></p>
<p>[8] Thibshirani, <em>Convex Optimization</em>, <a href="https://www.stat.cmu.edu/~ryantibs/convexopt" class="uri">https://www.stat.cmu.edu/~ryantibs/convexopt</a></p>
<p>[9] Strang, <em>Too Much Calculus</em>, <a href="http://web.mit.edu/18.06/www/Essays/too-much-calculus.pdf" class="uri">http://web.mit.edu/18.06/www/Essays/too-much-calculus.pdf</a></p>
<hr>
<p>Yunan Harfleri</p>
<div class="figure">
<img src="../../algs/algs_999_zapp/letters.png" />

</div>
</body>
</html>
