<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Doküman İndekslemek, Aramak, TF-IDF</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="doküman-indekslemek-aramak-tf-idf">Doküman İndekslemek, Aramak,
TF-IDF</h1>
<p>Bir doküman arayıp bulmak için popüler tekniklerden biri onu bir
vektör olarak temsil etmek. Vektörün her hücresi bir kelimeyi temsil
eder ve bu vektörde, bu ve tüm diğer dokümanlardaki kelimelerin her
birinden o dokümanda kaç tane olduğu saptanır, hücreye o kelimeye
tekabül eden yere bu sayı yazılır. Bu doküman temsil yöntemine “kelime
çuvalı (bag-of-words)’’ yaratmak ismi de veriliyor, çünkü temsil yöntemi
kelimelerin arasındaki sırayı dikkate almıyor, sadece kelime sayılıp
vektöre yazılıyor, bilgi”çuvala” atılmış oluyor, çuval içinde herşey
birarada, sıra önemi kalmamış.</p>
<p>Altta yazar Shakespeare’in bazı eserlerini kolonda, o eserlerdeki
kelimelerin bazılarını satırda gösterirsek,</p>
<p><img src="shake.png" /></p>
<p>Hamlet eserinde (doküman) Ceasar (Sezar) kelimesi 2 kez geçiyor.
Şimdi Hamlet kolonunun tamamına bakarsak o vektörün tamamını Hamlet
dokümanını temsil eden bir vektör olarak görebiliriz, ve arama yaparken
bu vektörleri kullanabiliriz.</p>
<p>TF</p>
<p>Fakat bir dokümanda geçen kelime sayısını direk kullanmak uygun olur
mu? Mesela bir terim (kelime) bir dokümanda 10 kez geçiyor, diğerinde 1
kez geçiyorsa evet, kelimenin 10 kez geçtiği dokümanda bu terim daha
önemlidir, ama 10 kat daha mı önemlidir? Hayır. Frekansın (yani terim
sayısının) log’unu alırsak bu doğrusal, lineer alakayı / oranı
azaltabiliriz belki, o zaman <span class="math inline">\(t\)</span>
teriminin <span class="math inline">\(d\)</span> dokümanındaki önemli /
ağırlığı (weight),</p>
<p><span class="math display">\[ w_{t,d} = \left\{ \begin{array}{ll}
1 + \log tf_{t,d} &amp; \textrm{ eğer }  tf_{t,d} &gt; 0 \\
0 &amp; \textrm { diğer }
\end{array} \right. \]</span></p>
<p>olarak gösterilebilir.</p>
<p><span class="math inline">\(tf_{t,d}\)</span>: <span
class="math inline">\(t\)</span> kelimesi kaç kez <span
class="math inline">\(d\)</span> dokümanı içinde görülüyor.</p>
<p>IDF</p>
<p>Nadir görülen kelimeler sık görülen kelimelere nazaran aramakta daha
faydalıdır. Mesela içinde <em>kolonoskopi</em> kelimesi geçen bir
doküman düşünelim, bu nadir bir kelime, günlük konuşma, normal yazım
içinde çok kullanılmaz, ve eğer <em>kolonoskopi</em> kelimesiyle bir
arama yapıyorsak içinde bu kelimenin geçtiği her doküman bizi
ilgilendirir, kelime nadir olduğu için onun içinde olduğu her doküman
büyük bir ihtimalle aradığımız dokümandır. Arama sırasında bu tür nadir
kelimelere daha fazla ağırlık verilmesini isteriz. Tüm dokümanlarda sık
görülen kelimelere ise daha az ağırlık vermek daha iyi olur.</p>
<p>Bu tür bir ağırlığı tanımlamak için önce doküman frekansından
başlarız. <span class="math inline">\(N\)</span>: doküman sayısı, <span
class="math inline">\(df_t\)</span>: <span
class="math inline">\(t\)</span>’yi içeren kaç tane doküman olduğu,
<span class="math inline">\(df_t \le N\)</span> olacak şekilde. Ardından
ters doküman frekansı (inverse document frequency) lazım, çünkü nadir
kelimeler az dokümanda, sık kelimeler çok dokümanda olur, birincinin
ağırlığı fazla ikinci az olsun istiyorsak bu hesabı tersine çevirmemiz
lazım, bir bölüm işlemi ile bunu başarırız,</p>
<p><span class="math display">\[ idf_t = \log (N/df_t) \]</span></p>
<p>TF-IDF</p>
<p>Eh üstte bahsettiğimiz iki ölçütü birleştirirsek ünlü TF-İDF ölçütünü
elde ederiz.</p>
<p><span class="math display">\[ w_{t,d} = (1+\log tf_{t,d}) \cdot \log
(N/df_t)  \]</span></p>
<p>Not olarak şunu da ekleyelim; üstteki temsil sistemi, ya da birazdan
anlatacağımız arama sisteminin istatistiksel bir temeli yok. Yöntem
akıllı tahmin (heuristics) ile, deneme / yanılma, sayısal deneyler
kullanılarak kararlaştırılmış. İyi işlediği görülmüş, ve kullanılmaya
devam edilmiş. Bir açıdan yapay öğrenimdeki özellik yaratmak (feature
extraction) yapılan, ham veriyi işleyip onu daha rahat çalışılabilecek
bir hale getirmek.</p>
<p>Sorgulamak (Querying)</p>
<p>Bir dokümanı vektör olarak temsil ediyoruz. Şimdi bir sorgu var
elimizde, bu sorgudaki kelimeleri de TF-IDF ile bir vektöre
çevirebiliriz, o zaman bu sorgunun (vektörün) hangi dokümana daha yakın
olduğunu bulmak bize sorgulama yeteneğini sağlar. Uzaklık için en çok
kullanılan teknik kosinüs uzaklığı, benzerliğidir,</p>
<p><span class="math display">\[
\cos\theta = \frac{A \cdot B}{||A||||B||}
\]</span></p>
<p>Eğer <span class="math inline">\(A,B\)</span>’yi normalize edersek,
ki <span class="math inline">\(||A||=||B||=1\)</span> olacak şekilde, o
zaman</p>
<p><span class="math display">\[
\cos\theta = A \cdot B
\]</span></p>
<p>yeterli olur. <span class="math inline">\(A\)</span> tek bir vektör
olabilir, ya da içinde tüm dokümanları temsil eden vektörlerin üst üste
konulduğu bir matris olabilir. Sorgu vektörü <span
class="math inline">\(B\)</span>. Bu durumda hala tek bir çarpım
yeterli, ama o tek çarpım bu sefer bize sorgunun tüm dokümanlara olan
yakınlığını verir.</p>
<p>Altta bu kavramların kullanımını görüyoruz; daha önce [4] yazısında
kullandığımız veriler bunlar, Barack Obama ve Stephen Hawking’in
yazdıklarından alınan bölümler. Önce indeksleme, sonra bazı kelimelerle
arama yapıyoruz. Kelimeleri sayısal bir kolon indisine çevirmek için
anahtarlama numarası kullanıldı (aynı yazıda bundan da bahsediliyor);
<code>D</code> kadar kolon tanımlıyoruz, her kelime üzerinde bir hash
sayısı hesaplayıp matematik modülosunu alıyoruz (bölümden arta kalan
sayı) bu sayıyı kolon indisi yapıyoruz. Böylece <code>D</code>’den fazla
kolon olamaz, ve her kelime hala aynı / özgün kolona gider.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk, string, sys      </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.sparse <span class="im">as</span> sps</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> <span class="st">&quot;../../stat/stat_naive/data/</span><span class="sc">%s</span><span class="st">&quot;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>stemmer <span class="op">=</span> nltk.stem.porter.PorterStemmer()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stem_tokens(tokens):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [stemmer.stem(item) <span class="cf">for</span> item <span class="kw">in</span> tokens]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>docs <span class="op">=</span> [<span class="st">&#39;a1.txt&#39;</span>,<span class="st">&#39;a2.txt&#39;</span>,<span class="st">&#39;a3.txt&#39;</span>,<span class="st">&#39;a4.txt&#39;</span>,<span class="st">&#39;b1.txt&#39;</span>,<span class="st">&#39;b2.txt&#39;</span>,<span class="st">&#39;b3.txt&#39;</span>,<span class="st">&#39;b4.txt&#39;</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="bu">len</span>(docs)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> sps.csr_matrix((N,D))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> A.shape</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,f <span class="kw">in</span> <span class="bu">enumerate</span>(docs):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">file</span> <span class="op">=</span> base <span class="op">%</span> f</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    lowers <span class="op">=</span> <span class="bu">open</span>(<span class="bu">file</span>).read().decode(<span class="st">&quot;ISO-8859-1&quot;</span>).lower()</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> nltk.word_tokenize(lowers)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> stem_tokens(tokens)    </span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> i, tokens[:<span class="dv">6</span>]</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># kelimeler kolonda, dokumanlar satirda</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> tokens: A[i,<span class="bu">hash</span>(token) <span class="op">%</span> D] <span class="op">+=</span> <span class="dv">1</span> </span></code></pre></div>
<pre><code>(8, 10000)
0 [u&#39;a&#39;, u&#39;well-known&#39;, u&#39;scientist&#39;, u&#39;(&#39;, u&#39;some&#39;, u&#39;say&#39;]
1 [u&#39;kepler&#39;, u&#39;,&#39;, u&#39;and&#39;, u&#39;the&#39;, u&#39;italian&#39;, u&#39;,&#39;]
2 [u&#39;time&#39;, u&#39;in&#39;, u&#39;such&#39;, u&#39;a&#39;, u&#39;manner&#39;, u&#39;as&#39;]
3 [u&#39;of&#39;, u&#39;mass&#39;, u&#39;and&#39;, u&#39;energi&#39;, u&#39;in&#39;, u&#39;it&#39;]
4 [u&#39;page&#39;, u&#39;provid&#39;, u&#39;a&#39;, u&#39;manifesto&#39;, u&#39;for&#39;, u&#39;action&#39;]
5 [u&#39;take&#39;, u&#39;a&#39;, u&#39;deep&#39;, u&#39;breath&#39;, u&#39;.&#39;, u&#39;when&#39;]
6 [u&#39;larger&#39;, u&#39;mean&#39;, u&#39;.&#39;, u&#39;we&#39;, u&#39;lose&#39;, u&#39;elect&#39;]
7 [u&#39;so&#39;, u&#39;let\xe2\x80\x99&#39;, u&#39;be&#39;, u&#39;clear&#39;, u&#39;.&#39;, u&#39;the&#39;]</code></pre>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> normalize</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> A.shape[<span class="dv">0</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># sifirdan buyuk tum frekanslari 1 yap, boylece kelime bazinda</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># bazinda toplam alinca o kelimenin kac diger dokumanda oldugu</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># hemen hesaplanir.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>A[A <span class="op">&gt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>idf <span class="op">=</span> A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># sadece sifir olmayan ogelerin log&#39;unu al</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>idf[idf.nonzero()] <span class="op">=</span> np.log(N<span class="op">/</span>idf[idf.nonzero()])</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>tf <span class="op">=</span> A.tocoo()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>tf.data <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> np.log(tf.data)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>tfidf <span class="op">=</span> sps.csr_matrix(tf.multiply(idf))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>tfidf <span class="op">=</span> normalize(tfidf, norm<span class="op">=</span><span class="st">&#39;l2&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> search(s):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    sm <span class="op">=</span> sps.lil_matrix((<span class="dv">1</span>,D))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> nltk.word_tokenize(s.lower())</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> stem_tokens(tokens)    </span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> tokens[:<span class="dv">20</span>]</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> tokens: sm[<span class="dv">0</span>,<span class="bu">hash</span>(token) <span class="op">%</span> D] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    tfidf_new <span class="op">=</span> sm.multiply(idf)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    tfidf_new <span class="op">=</span> sps.csr_matrix(tfidf_new)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    tfidf_new[tfidf_new<span class="op">==</span><span class="fl">1.0</span>] <span class="op">=</span> <span class="fl">0.0</span>        </span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    tfidf_new <span class="op">=</span> normalize(tfidf_new, norm<span class="op">=</span><span class="st">&#39;l2&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    dist <span class="op">=</span> tfidf.dot(tfidf_new.T)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> pd.DataFrame(dist.todense(),columns<span class="op">=</span>[<span class="st">&#39;score&#39;</span>])</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    res[<span class="st">&#39;docid&#39;</span>] <span class="op">=</span> <span class="bu">range</span>(N)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.sort_values(by<span class="op">=</span><span class="st">&#39;score&#39;</span>,ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    res[<span class="st">&#39;doc&#39;</span>] <span class="op">=</span> res.<span class="bu">apply</span>(<span class="kw">lambda</span> x: docs[<span class="bu">int</span>(x[<span class="st">&#39;docid&#39;</span>])],axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> res</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>search(<span class="st">&quot;Galileo was a friend of mine&quot;</span>)    </span></code></pre></div>
<pre><code>[u&#39;galileo&#39;, u&#39;wa&#39;, u&#39;a&#39;, u&#39;friend&#39;, u&#39;of&#39;, u&#39;mine&#39;]
      score  docid     doc
1  0.087243      1  a2.txt
0  0.004841      0  a1.txt
2  0.004704      2  a3.txt
3  0.004360      3  a4.txt
7  0.003754      7  b4.txt
4  0.000000      4  b1.txt
5  0.000000      5  b2.txt
6  0.000000      6  b3.txt</code></pre>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>search(<span class="st">&quot;shortest distance between two points&quot;</span>)</span></code></pre></div>
<pre><code>[u&#39;shortest&#39;, u&#39;distanc&#39;, u&#39;between&#39;, u&#39;two&#39;, u&#39;point&#39;]
      score  docid     doc
0  0.058233      0  a1.txt
2  0.056584      2  a3.txt
3  0.029833      3  a4.txt
7  0.025684      7  b4.txt
1  0.022051      1  a2.txt
5  0.006857      5  b2.txt
4  0.001763      4  b1.txt
6  0.000000      6  b3.txt</code></pre>
<p>Kaynaklar</p>
<p>[1] Bayramlı, <em>Dil Isleme, Python - NLTK</em>, <a
href="https://burakbayramli.github.io/dersblog/sk/2016/04/dil-isleme-python-nltk.html">https://burakbayramli.github.io/dersblog/sk/2016/04/dil-isleme-python-nltk.html</a></p>
<p>[2] Manning, <em>Introduction to NLP, Lecture Notes</em>, <a
href="https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html">https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html</a></p>
<p>[3] Bayramlı, <em>Scipy Seyrek Matrisleri (Sparse Matrices)</em>, <a
href="https://burakbayramli.github.io/dersblog/sk/2016/04/scipy-seyrek-matrisler-sparse-matrices.html">https://burakbayramli.github.io/dersblog/sk/2016/04/scipy-seyrek-matrisler-sparse-matrices.html</a></p>
<p>[4] Bayramlı, İstatistik, <em>Naive Bayes</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
