<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Doküman İndekslemek, Aramak, TF-IDF</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="doküman-indekslemek-aramak-tf-idf">Doküman İndekslemek, Aramak, TF-IDF</h1>
<p>Bir doküman arayıp bulmak için popüler tekniklerden biri onu bir vektör olarak temsil etmek. Vektörün her hücresi bir kelimeyi temsil eder ve bu vektörde, bu ve tüm diğer dokümanlardaki kelimelerin her birinden o dokümanda kaç tane olduğu saptanır, hücreye o kelimeye tekabül eden yere bu sayı yazılır. Bu doküman temsil yöntemine &quot;kelime çuvalı (bag-of-words)'' yaratmak ismi de veriliyor, çünkü temsil yöntemi kelimelerin arasındaki sırayı dikkate almıyor, sadece kelime sayılıp vektöre yazılıyor, bilgi &quot;çuvala&quot; atılmış oluyor, çuval içinde herşey birarada, sıra önemi kalmamış.</p>
<p>Altta yazar Shakespeare'in bazı eserlerini kolonda, o eserlerdeki kelimelerin bazılarını satırda gösterirsek,</p>
<div class="figure">
<img src="shake.png" />

</div>
<p>Hamlet eserinde (doküman) Ceasar (Sezar) kelimesi 2 kez geçiyor. Şimdi Hamlet kolonunun tamamına bakarsak o vektörün tamamını Hamlet dokümanını temsil eden bir vektör olarak görebiliriz, ve arama yaparken bu vektörleri kullanabiliriz.</p>
<p>TF</p>
<p>Fakat bir dokümanda geçen kelime sayısını direk kullanmak uygun olur mu? Mesela bir terim (kelime) bir dokümanda 10 kez geçiyor, diğerinde 1 kez geçiyorsa evet, kelimenin 10 kez geçtiği dokümanda bu terim daha önemlidir, ama 10 kat daha mı önemlidir? Hayır. Frekansın (yani terim sayısının) log'unu alırsak bu doğrusal, lineer alakayı / oranı azaltabiliriz belki, o zaman <span class="math inline">\(t\)</span> teriminin <span class="math inline">\(d\)</span> dokümanındaki önemli / ağırlığı (weight),</p>
<p><span class="math display">\[ w_{t,d} = \left\{ \begin{array}{ll}
1 + \log tf_{t,d} &amp; \textrm{ eğer }  tf_{t,d} &gt; 0 \\
0 &amp; \textrm { diğer }
\end{array} \right. \]</span></p>
<p>olarak gösterilebilir.</p>
<p><span class="math inline">\(tf_{t,d}\)</span>: <span class="math inline">\(t\)</span> kelimesi kaç kez <span class="math inline">\(d\)</span> dokümanı içinde görülüyor.</p>
<p>IDF</p>
<p>Nadir görülen kelimeler sık görülen kelimelere nazaran aramakta daha faydalıdır. Mesela içinde <em>kolonoskopi</em> kelimesi geçen bir doküman düşünelim, bu nadir bir kelime, günlük konuşma, normal yazım içinde çok kullanılmaz, ve eğer <em>kolonoskopi</em> kelimesiyle bir arama yapıyorsak içinde bu kelimenin geçtiği her doküman bizi ilgilendirir, kelime nadir olduğu için onun içinde olduğu her doküman büyük bir ihtimalle aradığımız dokümandır. Arama sırasında bu tür nadir kelimelere daha fazla ağırlık verilmesini isteriz. Tüm dokümanlarda sık görülen kelimelere ise daha az ağırlık vermek daha iyi olur.</p>
<p>Bu tür bir ağırlığı tanımlamak için önce doküman frekansından başlarız. <span class="math inline">\(N\)</span>: doküman sayısı, <span class="math inline">\(df_t\)</span>: <span class="math inline">\(t\)</span>'yi içeren kaç tane doküman olduğu, <span class="math inline">\(df_t \le N\)</span> olacak şekilde. Ardından ters doküman frekansı (inverse document frequency) lazım, çünkü nadir kelimeler az dokümanda, sık kelimeler çok dokümanda olur, birincinin ağırlığı fazla ikinci az olsun istiyorsak bu hesabı tersine çevirmemiz lazım, bir bölüm işlemi ile bunu başarırız,</p>
<p><span class="math display">\[ idf_t = \log (N/df_t) \]</span></p>
<p>TF-IDF</p>
<p>Eh üstte bahsettiğimiz iki ölçütü birleştirirsek ünlü TF-İDF ölçütünü elde ederiz.</p>
<p><span class="math display">\[ w_{t,d} = (1+\log tf_{t,d}) \cdot \log (N/df_t)  \]</span></p>
<p>Not olarak şunu da ekleyelim; üstteki temsil sistemi, ya da birazdan anlatacağımız arama sisteminin istatistiksel bir temeli yok. Yöntem akıllı tahmin (heuristics) ile, deneme / yanılma, sayısal deneyler kullanılarak kararlaştırılmış. İyi işlediği görülmüş, ve kullanılmaya devam edilmiş. Bir açıdan yapay öğrenimdeki özellik yaratmak (feature extraction) yapılan, ham veriyi işleyip onu daha rahat çalışılabilecek bir hale getirmek.</p>
<p>Sorgulamak (Querying)</p>
<p>Bir dokümanı vektör olarak temsil ediyoruz. Şimdi bir sorgu var elimizde, bu sorgudaki kelimeleri de TF-IDF ile bir vektöre çevirebiliriz, o zaman bu sorgunun (vektörün) hangi dokümana daha yakın olduğunu bulmak bize sorgulama yeteneğini sağlar. Uzaklık için en çok kullanılan teknik kosinüs uzaklığı, benzerliğidir,</p>
<p><span class="math display">\[
\cos\theta = \frac{A \cdot B}{||A||||B||}
\]</span></p>
<p>Eğer <span class="math inline">\(A,B\)</span>'yi normalize edersek, ki <span class="math inline">\(||A||=||B||=1\)</span> olacak şekilde, o zaman</p>
<p><span class="math display">\[
\cos\theta = A \cdot B
\]</span></p>
<p>yeterli olur. <span class="math inline">\(A\)</span> tek bir vektör olabilir, ya da içinde tüm dokümanları temsil eden vektörlerin üst üste konulduğu bir matris olabilir. Sorgu vektörü <span class="math inline">\(B\)</span>. Bu durumda hala tek bir çarpım yeterli, ama o tek çarpım bu sefer bize sorgunun tüm dokümanlara olan yakınlığını verir.</p>
<p>Altta bu kavramların kullanımını görüyoruz; daha önce [4] yazısında kullandığımız veriler bunlar, Barack Obama ve Stephen Hawking'in yazdıklarından alınan bölümler. Önce indeksleme, sonra bazı kelimelerle arama yapıyoruz. Kelimeleri sayısal bir kolon indisine çevirmek için anahtarlama numarası kullanıldı (aynı yazıda bundan da bahsediliyor); <code>D</code> kadar kolon tanımlıyoruz, her kelime üzerinde bir hash sayısı hesaplayıp matematik modülosunu alıyoruz (bölümden arta kalan sayı) bu sayıyı kolon indisi yapıyoruz. Böylece <code>D</code>'den fazla kolon olamaz, ve her kelime hala aynı / özgün kolona gider.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> nltk, string, sys      
<span class="im">import</span> scipy.sparse <span class="im">as</span> sps

base <span class="op">=</span> <span class="st">&quot;../../stat/stat_naive/data/</span><span class="sc">%s</span><span class="st">&quot;</span>

stemmer <span class="op">=</span> nltk.stem.porter.PorterStemmer()

<span class="kw">def</span> stem_tokens(tokens):
    <span class="cf">return</span> [stemmer.stem(item) <span class="cf">for</span> item <span class="kw">in</span> tokens]

D <span class="op">=</span> <span class="dv">10000</span>

docs <span class="op">=</span> [<span class="st">&#39;a1.txt&#39;</span>,<span class="st">&#39;a2.txt&#39;</span>,<span class="st">&#39;a3.txt&#39;</span>,<span class="st">&#39;a4.txt&#39;</span>,<span class="st">&#39;b1.txt&#39;</span>,<span class="st">&#39;b2.txt&#39;</span>,<span class="st">&#39;b3.txt&#39;</span>,<span class="st">&#39;b4.txt&#39;</span>]
N <span class="op">=</span> <span class="bu">len</span>(docs)
A <span class="op">=</span> sps.csr_matrix((N,D))
<span class="bu">print</span> A.shape

<span class="cf">for</span> i,f <span class="kw">in</span> <span class="bu">enumerate</span>(docs):
    <span class="bu">file</span> <span class="op">=</span> base <span class="op">%</span> f
    lowers <span class="op">=</span> <span class="bu">open</span>(<span class="bu">file</span>).read().decode(<span class="st">&quot;ISO-8859-1&quot;</span>).lower()
    tokens <span class="op">=</span> nltk.word_tokenize(lowers)
    tokens <span class="op">=</span> stem_tokens(tokens)    
    <span class="bu">print</span> i, tokens[:<span class="dv">6</span>]
    <span class="co"># kelimeler kolonda, dokumanlar satirda</span>
    <span class="cf">for</span> token <span class="kw">in</span> tokens: A[i,<span class="bu">hash</span>(token) <span class="op">%</span> D] <span class="op">+=</span> <span class="dv">1</span> </code></pre></div>
<pre><code>(8, 10000)
0 [u&#39;a&#39;, u&#39;well-known&#39;, u&#39;scientist&#39;, u&#39;(&#39;, u&#39;some&#39;, u&#39;say&#39;]
1 [u&#39;kepler&#39;, u&#39;,&#39;, u&#39;and&#39;, u&#39;the&#39;, u&#39;italian&#39;, u&#39;,&#39;]
2 [u&#39;time&#39;, u&#39;in&#39;, u&#39;such&#39;, u&#39;a&#39;, u&#39;manner&#39;, u&#39;as&#39;]
3 [u&#39;of&#39;, u&#39;mass&#39;, u&#39;and&#39;, u&#39;energi&#39;, u&#39;in&#39;, u&#39;it&#39;]
4 [u&#39;page&#39;, u&#39;provid&#39;, u&#39;a&#39;, u&#39;manifesto&#39;, u&#39;for&#39;, u&#39;action&#39;]
5 [u&#39;take&#39;, u&#39;a&#39;, u&#39;deep&#39;, u&#39;breath&#39;, u&#39;.&#39;, u&#39;when&#39;]
6 [u&#39;larger&#39;, u&#39;mean&#39;, u&#39;.&#39;, u&#39;we&#39;, u&#39;lose&#39;, u&#39;elect&#39;]
7 [u&#39;so&#39;, u&#39;let\xe2\x80\x99&#39;, u&#39;be&#39;, u&#39;clear&#39;, u&#39;.&#39;, u&#39;the&#39;]</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> normalize

N <span class="op">=</span> A.shape[<span class="dv">0</span>]
<span class="co"># sifirdan buyuk tum frekanslari 1 yap, boylece kelime bazinda</span>
<span class="co"># bazinda toplam alinca o kelimenin kac diger dokumanda oldugu</span>
<span class="co"># hemen hesaplanir.</span>
A[A <span class="op">&gt;</span> <span class="dv">0</span>] <span class="op">=</span> <span class="fl">1.</span>
idf <span class="op">=</span> A.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">0</span>)
<span class="co"># sadece sifir olmayan ogelerin log&#39;unu al</span>
idf[idf.nonzero()] <span class="op">=</span> np.log(N<span class="op">/</span>idf[idf.nonzero()])

tf <span class="op">=</span> A.tocoo()
tf.data <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> np.log(tf.data)
tfidf <span class="op">=</span> sps.csr_matrix(tf.multiply(idf))
tfidf <span class="op">=</span> normalize(tfidf, norm<span class="op">=</span><span class="st">&#39;l2&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd

<span class="kw">def</span> search(s):
    sm <span class="op">=</span> sps.lil_matrix((<span class="dv">1</span>,D))
    tokens <span class="op">=</span> nltk.word_tokenize(s.lower())
    tokens <span class="op">=</span> stem_tokens(tokens)    
    <span class="bu">print</span> tokens[:<span class="dv">20</span>]
    <span class="cf">for</span> token <span class="kw">in</span> tokens: sm[<span class="dv">0</span>,<span class="bu">hash</span>(token) <span class="op">%</span> D] <span class="op">+=</span> <span class="dv">1</span>
    tfidf_new <span class="op">=</span> sm.multiply(idf)
    tfidf_new <span class="op">=</span> sps.csr_matrix(tfidf_new)

    tfidf_new[tfidf_new<span class="op">==</span><span class="fl">1.0</span>] <span class="op">=</span> <span class="fl">0.0</span>        
    tfidf_new <span class="op">=</span> normalize(tfidf_new, norm<span class="op">=</span><span class="st">&#39;l2&#39;</span>, axis<span class="op">=</span><span class="dv">1</span>)
    dist <span class="op">=</span> tfidf.dot(tfidf_new.T)

    res <span class="op">=</span> pd.DataFrame(dist.todense(),columns<span class="op">=</span>[<span class="st">&#39;score&#39;</span>])

    res[<span class="st">&#39;docid&#39;</span>] <span class="op">=</span> <span class="bu">range</span>(N)
    res <span class="op">=</span> res.sort_values(by<span class="op">=</span><span class="st">&#39;score&#39;</span>,ascending<span class="op">=</span><span class="va">False</span>)
    res[<span class="st">&#39;doc&#39;</span>] <span class="op">=</span> res.<span class="bu">apply</span>(<span class="kw">lambda</span> x: docs[<span class="bu">int</span>(x[<span class="st">&#39;docid&#39;</span>])],axis<span class="op">=</span><span class="dv">1</span>)
    <span class="bu">print</span> res

search(<span class="st">&quot;Galileo was a friend of mine&quot;</span>)    </code></pre></div>
<pre><code>[u&#39;galileo&#39;, u&#39;wa&#39;, u&#39;a&#39;, u&#39;friend&#39;, u&#39;of&#39;, u&#39;mine&#39;]
      score  docid     doc
1  0.087243      1  a2.txt
0  0.004841      0  a1.txt
2  0.004704      2  a3.txt
3  0.004360      3  a4.txt
7  0.003754      7  b4.txt
4  0.000000      4  b1.txt
5  0.000000      5  b2.txt
6  0.000000      6  b3.txt</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">search(<span class="st">&quot;shortest distance between two points&quot;</span>)</code></pre></div>
<pre><code>[u&#39;shortest&#39;, u&#39;distanc&#39;, u&#39;between&#39;, u&#39;two&#39;, u&#39;point&#39;]
      score  docid     doc
0  0.058233      0  a1.txt
2  0.056584      2  a3.txt
3  0.029833      3  a4.txt
7  0.025684      7  b4.txt
1  0.022051      1  a2.txt
5  0.006857      5  b2.txt
4  0.001763      4  b1.txt
6  0.000000      6  b3.txt</code></pre>
<p>Kaynaklar</p>
<p>[1] Bayramlı, <em>Dil Isleme, Python - NLTK</em>, <a href="https://burakbayramli.github.io/dersblog/sk/2016/04/dil-isleme-python-nltk.html" class="uri">https://burakbayramli.github.io/dersblog/sk/2016/04/dil-isleme-python-nltk.html</a></p>
<p>[2] Manning, <em>Introduction to NLP, Lecture Notes</em>, <a href="https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html" class="uri">https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html</a></p>
<p>[3] Bayramlı, <em>Scipy Seyrek Matrisleri (Sparse Matrices)</em>, <a href="https://burakbayramli.github.io/dersblog/sk/2016/04/scipy-seyrek-matrisler-sparse-matrices.html" class="uri">https://burakbayramli.github.io/dersblog/sk/2016/04/scipy-seyrek-matrisler-sparse-matrices.html</a></p>
<p>[4] Bayramlı, İstatistik, <em>Naive Bayes</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
