\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
Doküman Ýndekslemek, Aramak, TF-IDF

Bir doküman arayýp bulmak için popüler tekniklerden biri onu bir vektör olarak
temsil etmek. Vektörün her hücresi bir kelimeyi temsil eder ve bu vektörde, bu
ve tüm diðer dokümanlardaki kelimelerin her birinden o dokümanda kaç tane olduðu
saptanýr, hücreye o kelimeye tekabül eden yere bu sayý yazýlýr. Bu doküman
temsil yöntemine ``kelime çuvalý (bag-of-words)'' yaratmak ismi de veriliyor,
çünkü temsil yöntemi kelimelerin arasýndaki sýrayý dikkate almýyor, sadece
kelime sayýlýp vektöre yazýlýyor, bilgi "çuvala" atýlmýþ oluyor, çuval içinde
herþey birarada, sýra önemi kalmamýþ.

Altta yazar Shakespeare'in bazý eserlerini kolonda, o eserlerdeki kelimelerin
bazýlarýný satýrda gösterirsek,

\includegraphics[height=5cm]{shake.png}

Hamlet eserinde (doküman) Ceasar (Sezar) kelimesi 2 kez geçiyor. Þimdi Hamlet
kolonunun tamamýna bakarsak o vektörün tamamýný Hamlet dokümanýný temsil eden bir
vektör olarak görebiliriz, ve arama yaparken bu vektörleri kullanabiliriz.

TF

Fakat bir dokümanda geçen kelime sayýsýný direk kullanmak uygun olur mu? Mesela
bir terim (kelime) bir dokümanda 10 kez geçiyor, diðerinde 1 kez geçiyorsa evet,
kelimenin 10 kez geçtiði dokümanda bu terim daha önemlidir, ama 10 kat daha mý
önemlidir?  Hayýr. Frekansýn (yani terim sayýsýnýn) log'unu alýrsak bu doðrusal,
lineer alakayý / oraný azaltabiliriz belki, o zaman $t$ teriminin $d$
dokümanýndaki önemli / aðýrlýðý (weight),

$$ w_{t,d} = \left\{ \begin{array}{ll}
1 + \log tf_{t,d} & \textrm{ eðer }  tf_{t,d} > 0 \\
0 & \textrm { diðer }
\end{array} \right. $$

olarak gösterilebilir. 

$tf_{t,d}$: $t$ kelimesi kaç kez $d$ dokümaný içinde görülüyor.

IDF

Nadir görülen kelimeler sýk görülen kelimelere nazaran aramakta daha
faydalýdýr. Mesela içinde {\em kolonoskopi} kelimesi geçen bir doküman
düþünelim, bu nadir bir kelime, günlük konuþma, normal yazým içinde çok
kullanýlmaz, ve eðer {\em kolonoskopi} kelimesiyle bir arama yapýyorsak içinde
bu kelimenin geçtiði her doküman bizi ilgilendirir, kelime nadir olduðu için
onun içinde olduðu her doküman büyük bir ihtimalle aradýðýmýz dokümandýr. Arama
sýrasýnda bu tür nadir kelimelere daha fazla aðýrlýk verilmesini isteriz. Tüm
dokümanlarda sýk görülen kelimelere ise daha az aðýrlýk vermek daha iyi olur.

Bu tür bir aðýrlýðý tanýmlamak için önce doküman frekansýndan baþlarýz. $N$:
doküman sayýsý, $df_t$: $t$'yi içeren kaç tane doküman olduðu, $df_t \le N$
olacak þekilde. Ardýndan ters doküman frekansý (inverse document frequency)
lazým, çünkü nadir kelimeler az dokümanda, sýk kelimeler çok dokümanda olur,
birincinin aðýrlýðý fazla ikinci az olsun istiyorsak bu hesabý tersine
çevirmemiz lazým, bir bölüm iþlemi ile bunu baþarýrýz,

$$ idf_t = \log (N/df_t) $$

TF-IDF

Eh üstte bahsettiðimiz iki ölçütü birleþtirirsek ünlü TF-ÝDF ölçütünü elde
ederiz. 

$$ w_{t,d} = (1+\log tf_{t,d}) \cdot \log (N/df_t)  $$

Not olarak þunu da ekleyelim; üstteki temsil sistemi, ya da birazdan
anlatacaðýmýz arama sisteminin istatistiksel bir temeli yok. Yöntem akýllý
tahmin (heuristics) ile, deneme / yanýlma, sayýsal deneyler kullanýlarak
kararlaþtýrýlmýþ. Ýyi iþlediði görülmüþ, ve kullanýlmaya devam edilmiþ. Bir
açýdan yapay öðrenimdeki özellik yaratmak (feature extraction) yapýlan, ham
veriyi iþleyip onu daha rahat çalýþýlabilecek bir hale getirmek. 

Sorgulamak (Querying)

Bir dokümaný vektör olarak temsil ediyoruz. Þimdi bir sorgu var elimizde, bu
sorgudaki kelimeleri de TF-IDF ile bir vektöre çevirebiliriz, o zaman bu
sorgunun (vektörün) hangi dokümana daha yakýn olduðunu bulmak bize sorgulama
yeteneðini saðlar. Uzaklýk için en çok kullanýlan teknik kosinüs uzaklýðý,
benzerliðidir,

$$
\cos\theta = \frac{A \cdot B}{||A||||B||}
$$

Eðer $A,B$'yi normalize edersek, ki $||A||=||B||=1$ olacak þekilde, o zaman

$$
\cos\theta = A \cdot B
$$

yeterli olur. $A$ tek bir vektör olabilir, ya da içinde tüm dokümanlarý temsil
eden vektörlerin üst üste konulduðu bir matris olabilir. Sorgu vektörü $B$. Bu
durumda hala tek bir çarpým yeterli, ama o tek çarpým bu sefer bize sorgunun tüm
dokümanlara olan yakýnlýðýný verir.

Altta bu kavramlarýn kullanýmýný görüyoruz; daha önce [4] yazýsýnda
kullandýðýmýz veriler bunlar, Barack Obama ve Stephen Hawking'in
yazdýklarýndan alýnan bölümler. Önce indeksleme, sonra bazý kelimelerle
arama yapýyoruz. Kelimeleri sayýsal bir kolon indisine çevirmek için
anahtarlama numarasý kullanýldý (ayný yazýda bundan da bahsediliyor);
\verb!D!  kadar kolon tanýmlýyoruz, her kelime üzerinde bir hash sayýsý
hesaplayýp matematik modülosunu alýyoruz (bölümden arta kalan sayý) bu
sayýyý kolon indisi yapýyoruz. Böylece \verb!D!'den fazla kolon olamaz, ve
her kelime hala ayný / özgün kolona gider.

\begin{minted}[fontsize=\footnotesize]{python}
import nltk, string, sys      
import scipy.sparse as sps

base = "../../stat/stat_naive/data/%s"

stemmer = nltk.stem.porter.PorterStemmer()

def stem_tokens(tokens):
    return [stemmer.stem(item) for item in tokens]

D = 10000

docs = ['a1.txt','a2.txt','a3.txt','a4.txt','b1.txt','b2.txt','b3.txt','b4.txt']
N = len(docs)
A = sps.csr_matrix((N,D))
print A.shape

for i,f in enumerate(docs):
    file = base % f
    lowers = open(file).read().decode("ISO-8859-1").lower()
    tokens = nltk.word_tokenize(lowers)
    tokens = stem_tokens(tokens)    
    print i, tokens[:6]
    # kelimeler kolonda, dokumanlar satirda
    for token in tokens: A[i,hash(token) % D] += 1 
\end{minted}

\begin{verbatim}
(8, 10000)
0 [u'a', u'well-known', u'scientist', u'(', u'some', u'say']
1 [u'kepler', u',', u'and', u'the', u'italian', u',']
2 [u'time', u'in', u'such', u'a', u'manner', u'as']
3 [u'of', u'mass', u'and', u'energi', u'in', u'it']
4 [u'page', u'provid', u'a', u'manifesto', u'for', u'action']
5 [u'take', u'a', u'deep', u'breath', u'.', u'when']
6 [u'larger', u'mean', u'.', u'we', u'lose', u'elect']
7 [u'so', u'let\xe2\x80\x99', u'be', u'clear', u'.', u'the']
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
from sklearn.preprocessing import normalize

N = A.shape[0]
# sifirdan buyuk tum frekanslari 1 yap, boylece kelime bazinda
# bazinda toplam alinca o kelimenin kac diger dokumanda oldugu
# hemen hesaplanir.
A[A > 0] = 1.
idf = A.sum(axis=0)
# sadece sifir olmayan ogelerin log'unu al
idf[idf.nonzero()] = np.log(N/idf[idf.nonzero()])

tf = A.tocoo()
tf.data = 1 + np.log(tf.data)
tfidf = sps.csr_matrix(tf.multiply(idf))
tfidf = normalize(tfidf, norm='l2', axis=1)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd

def search(s):
    sm = sps.lil_matrix((1,D))
    tokens = nltk.word_tokenize(s.lower())
    tokens = stem_tokens(tokens)    
    print tokens[:20]
    for token in tokens: sm[0,hash(token) % D] += 1
    tfidf_new = sm.multiply(idf)
    tfidf_new = sps.csr_matrix(tfidf_new)

    tfidf_new[tfidf_new==1.0] = 0.0        
    tfidf_new = normalize(tfidf_new, norm='l2', axis=1)
    dist = tfidf.dot(tfidf_new.T)

    res = pd.DataFrame(dist.todense(),columns=['score'])

    res['docid'] = range(N)
    res = res.sort_values(by='score',ascending=False)
    res['doc'] = res.apply(lambda x: docs[int(x['docid'])],axis=1)
    print res

search("Galileo was a friend of mine")    
\end{minted}

\begin{verbatim}
[u'galileo', u'wa', u'a', u'friend', u'of', u'mine']
      score  docid     doc
1  0.087243      1  a2.txt
0  0.004841      0  a1.txt
2  0.004704      2  a3.txt
3  0.004360      3  a4.txt
7  0.003754      7  b4.txt
4  0.000000      4  b1.txt
5  0.000000      5  b2.txt
6  0.000000      6  b3.txt
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
search("shortest distance between two points")
\end{minted}

\begin{verbatim}
[u'shortest', u'distanc', u'between', u'two', u'point']
      score  docid     doc
0  0.058233      0  a1.txt
2  0.056584      2  a3.txt
3  0.029833      3  a4.txt
7  0.025684      7  b4.txt
1  0.022051      1  a2.txt
5  0.006857      5  b2.txt
4  0.001763      4  b1.txt
6  0.000000      6  b3.txt
\end{verbatim}


Kaynaklar

[1] Bayramlý, 
    {\em Dil Isleme, Python - NLTK}, 
    \url{https://burakbayramli.github.io/dersblog/sk/2016/04/dil-isleme-python-nltk.html}

[2] Manning, {\em Introduction to NLP, Lecture Notes}, \url{https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html}

[3] Bayramlý, 
    {\em Scipy Seyrek Matrisleri (Sparse Matrices)}, 
    \url{https://burakbayramli.github.io/dersblog/sk/2016/04/scipy-seyrek-matrisler-sparse-matrices.html}

[4] Bayramlý, Ýstatistik, {\em Naive  Bayes}

\end{document}
