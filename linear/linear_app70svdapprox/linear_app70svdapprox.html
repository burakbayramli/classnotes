<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  
  
  
</head>
<body>
<h1 id="yaklaşıksal-svd-ile-tavsiye-sistemleri">Yaklaşıksal SVD ile Tavsiye Sistemleri</h1>
<p>Geçmiş verilere bakarak bir kullanıcının hiç seyretmediği bir filme nasıl not vereceğini tahmin etmek ünlü Netflix yarışmasının konusuydu. Önceki bir yazı [14]'te benzer bir veri seti Movielens üzerinde SVD uygulayarak önce boyut azaltmıştık, azaltılmış boyut üzerinden yeni (o film için notu bilinmeyen) bir kullanıcının diğer mevcut kullanıcılara mesafesini hesaplamış, ve böylece beğeni açısından en çok benzediği diğer kullanıcıyı bulmuştuk (birkaç tane de bulunabilir). Bu kullanıcının bir film için verdiği notu yeni kullanıcı için tahmin olarak baz almıştık. SVD'yi kullanmanın bir yöntemi daha var, Netflix yarışmasında kullanılan [1] bir yaklaşım şöyle; alttaki SVD ayrıştırmasına bakalım,</p>
<div class="figure">
<img src="svdapprox_1.png" />

</div>
<p>1'inci kullanıcının 1'inci filme verdiği not üstte köyü gösterilen satırların çarpımı ile oluyor, eğer ufak harfler ve kullanıcı (user) için <span class="math inline">\(u\)</span>, film için <span class="math inline">\(i\)</span> indisini, ve <span class="math inline">\(q,p\)</span> vektörlerini <span class="math inline">\(Q,P\)</span> matrislerinin sırasıyla kolon ve satırlarını göstermek için kullanırsak, ayrıştırma sonrası beğeni değeri (önemli bir kısmı daha doğrusu) <span class="math inline">\(q_i^Tp_u\)</span> çarpımındadır. Çarpım içinde <span class="math inline">\(S\)</span>'ten gelecek eşsiz değeri (singular value) ne olacak? Şimdi formülasyonda bir değişiklik düşünelim, bu değerin çarpım dışına alındığını hayal edelim; bu değerin kabaca birkaç toplamın sonucuna dönüştüğünü düşünelim. Matematiksel olarak imkansız değil. Bu toplam terimleri, toplam oldukları için, bir baz seviyesi tanımlayabilirler. Bir kullanıcının ne kadar yanlı (bias) not verdiğini, ya da bir filmin nasıl not almaya meyıllı olduğunu modelleyebilirler (ki bu da bir yanlılık ölçüsü). Ayrıca tüm filmlere verilen notların yanlılığı da ayrı bir terim olarak gösterilebilir. Tüm bunları bir araya koyarsak, bir beğeni notunu tahmin edecek formül şöyle gösterilebilir,</p>
<p><span class="math display">\[
\hat{r}_{ui} = \mu + b_i + b_u + q_i^Tp_u
\]</span></p>
<p><span class="math inline">\(\mu\)</span> bir skalar, tüm filmlere verilen ortalamayı gösteriyor, ki tüm beğenilerin sayısal ortalaması üzerinden basit bir şekilde hızla hesaplanabilir. <span class="math inline">\(\hat{r}_{ui}\)</span>'ya bir tahmin dedik çünkü modelimizdeki vektörlerin değerlerini bulduktan sonra (eğitim verisiyle bu hesabı yapacağız) modeli kullanarak gerçek not <span class="math inline">\(r_{ui}\)</span> için bir tahmin olarak kullanılacak.</p>
<p>Yanlılık hakkında bazı örnekler vermek gerekirse, diyelim ki kullanıcı Bob not verirken yüksek seviyede oy vermeye meyıllı. Bu durumda bu kullanıcının ortalama hatta düşük oy vermesi onun bir filmden hakikaten hiç hoşlanmadığını sinyalleyebilir. Ya da bir film genellikle ortalama oy almaktadır, bu durumda ona çok iyi not veren bir kişinin bu filmi çok beğendiği ortaya çıkar. Modeldeki yanlılık parametreleri bu durumu saptayabilirler. Eğer verimizde / gerçek dünyada yanlılık var ise, modelin bu bilgiyi kullanması onun başarısını arttıracaktır.</p>
<p>Eğitim</p>
<p>Eğitim için ne yapmalı? Minimize edeceğimiz bir hedef fonksiyonu kuralım, ki çoğunlukla bu karesi alınmış hata ile olur. Mesela gerçek not <span class="math inline">\(r_{ui}\)</span> değerinden tahmin notu <span class="math inline">\(\hat{r}_{ui}\)</span>'yi çıkartıp karesini alabiliriz. Bu işlemi tüm <span class="math inline">\(u,i\)</span>'ler için yaparak sonuçları toplarız, ve bu toplamı minimize etmeye uğraşabiliriz. Yani</p>
<p><span class="math display">\[
\min_{b\star,q\star,p\star} \sum_{u,i} (r_{ui} - \hat{r}_{ui})^2 + 
\lambda (b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2)
\]</span></p>
<p><span class="math display">\[
= \min_{b\star,q\star,p\star} \sum_{u,i} (r_{ui} - \mu - b_i - b_u - q_i^Tp_u)^2 + 
\lambda (b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2)
\]</span></p>
<p>Kısaltma olarak <span class="math inline">\(e_{ui}\)</span> tanımlayalım, bu faydalı olabilir, formüldeki ilk parantez içindeki kısımda kullanmak üzere,</p>
<p><span class="math display">\[ e_{ui} := r_{ui} - \hat{r}_{ui} \]</span></p>
<p><span class="math inline">\(\lambda\)</span> ile çarpılan bölüm regülarizasyon için. İstatistik, yapay öğrenimde modelimizin aşırı uygunluk (överfitting) yapmasını engellemek için regülarizasyon kullanılır, bunun için istediğimiz değişkenlerin fazla büyümesini cezalandırırız, üstteki minimizasyon modelinde bu ceza için tüm değerlerin büyüklüğünü (magnitude) hesapladık -skalar değerlerin karesini, vektör değerlerinin kare norm'unu alarak- ve bu büyüklükleri bizim dışarıdan tanımlayacağımız bir sabitle çarpımı üzerinden minimizasyon problemine direk dahil ettik. Böylece bu büyüklükler azaltılma hedefinin parçası haline geldiler. Yani hem <span class="math inline">\(e_{ui}^2\)</span> hem de hatayı oluşturan değerlerin kendileri minimize edilecek. Bu minimizasyon sırasında bazı değişkenlerin sıfıra inip o <span class="math inline">\(u,i\)</span> için tamamen etkisiz hale gelmesi bile mümkündür (ki bu bize o parametrenin önemsiz olduğunu sinyalleyebileceği için faydalıdır).</p>
<p>Rasgele Gradyan İnişi (Stochastic Gradient Descent -SGD-)</p>
<p>Modeli nasıl minimize ederiz? Bu model konveks (convex) değil, ki konvekslik bilindiği gibi fonksiyonun düzgün bir çukur gibi olduğu problemlerdir. Böyle çukur fonksiyonlarında herhangi bir noktadan başlarsınız, gradyanı hesaplarsınız, ve bu gradyan hep optimal iniş noktasını (daha doğrusu tersini) gösterir, ve yolda giderken takılıp kalabileceğiniz yerel minimumlar mevcut değildir, ve sonunda çukur dibine ulaşılır. Bizim problemimizde <span class="math inline">\(q_i^Tp_u\)</span> var, bu değişkenlerin ikisi de bilinmiyor, ve bu çarpımın karesi alındığı için genel karesellliği (quadratic) kaybetmiş oluyoruz. Fakat yine de SGD bu problemi çözebiliyor. Bunun sebeplerini, SGD SVD'nin hikayesiyle beraber yazının sonunda bulanabilir.</p>
<p>SGD için gradyanlar lazım, her değişken için minimizasyon toplamı içindeki kısmın (bu kısma <span class="math inline">\(E\)</span> diyelim) ayrı ayrı kısmı türevini almak lazım. Mesela <span class="math inline">\(b_u\)</span> için</p>
<p><span class="math display">\[ \frac{\partial E}{\partial b_u}  = -2e_{ui} + 2 \lambda b_u
\]</span></p>
<p>Gradyan her zaman en yüksek çıkışı gösterir, o zaman hesapsal algoritma onun tersi yönüne gitmelidir. Bu gidişin adım büyüklüğünü kontrol etmek için dışarıdan bizim belirlediğimiz bir <span class="math inline">\(\gamma\)</span> sabiti ile çarpım yapabiliriz, ve bir numara daha, sabit 2 değerlerinin <span class="math inline">\(\gamma\)</span> içinde eritilebileceğini farzederek onları sileriz. Yani adım <span class="math inline">\(\gamma(e_{ui} - \lambda b_u)\)</span> haline geldi. Bir döngü içinde eski <span class="math inline">\(b_u\)</span> bulunacak, gördüğümüz yönde adım atılacak, yani adım önceki değere toplanacak, ve yeni değer elde edilecek. Diğer değişkenler için türev alıp benzer işlemleri yaparsak, sonuç şöyle,</p>
<p><span class="math display">\[ b_u \leftarrow b_u + \gamma (e_{ui} - \lambda \cdot b_u) \]</span></p>
<p><span class="math display">\[ b_i \leftarrow b_i + \gamma (e_{ui} - \lambda \cdot b_i) \]</span></p>
<p><span class="math display">\[ q_i \leftarrow q_i + \gamma (e_{ui}\cdot p_u - \lambda \cdot q_i) \]</span></p>
<p><span class="math display">\[ p_u \leftarrow p_u + \gamma (e_{ui}\cdot q_i - \lambda \cdot p_u) \]</span></p>
<p>Her değişken için başlangıç noktası rasgele olarak seçilebilir, hatta seçilmelidir; İnternet'te bu konu hakkında &quot;efendim tüm değerleri 0.1 değeri yapsanız olur'' gibi yorumlar okuyabilirsiniz, bu durumda <span class="math inline">\(q_i,p_u\)</span> değişkenlerinin tüm hücreleri aynı değerde kalır! Yani <span class="math inline">\(q_3\)</span> mesela, tamamen 0.6 değerine sahip olur, ki bu istenen bir şey olmaz. <span class="math inline">\(\gamma,\lambda\)</span> sabitleri için en iyi değerler deneme/ yanılma ya da çapraz sağlama (crossvalidation) ile bulunabilir, biz bu örnekte deneme / yanılma yöntemini seçtik.</p>
<p>Rasgelelik, aynen <em>Lojistik Regresyon</em> örneğinde olduğu gibi verinin rasgeliliğinden geliyor, her veri noktasını teker teker sırayla işliyoruz aslında fakat bu &quot;sıranın'' rasgele olduğunu farzettiğimiz için özyineli algoritmamız rasgelelik elde ediyor. Python kodu altta, eğitim için kod sadece bir kere verinin üzerinden geçiyor. Başa dönüp birkaç kere (hatta yüzlerce) veriyi işleyenler de olabiliyor.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">from</span> numpy.linalg <span class="im">import</span> linalg <span class="im">as</span> la
<span class="im">import</span> numpy <span class="im">as</span> np
<span class="im">import</span> random, pandas <span class="im">as</span> pd

<span class="kw">def</span> create_training_test(df,collim<span class="op">=</span><span class="dv">2</span>,rowlim<span class="op">=</span><span class="dv">200</span>):
    test_data <span class="op">=</span> []
    df_train <span class="op">=</span> df.copy()
    <span class="cf">for</span> u <span class="kw">in</span> <span class="bu">range</span>(df.shape[<span class="dv">0</span>]):
        row <span class="op">=</span> df.ix[u]<span class="op">;</span> idxs <span class="op">=</span> row.index[row.notnull()]
        <span class="cf">if</span> <span class="bu">len</span>(idxs) <span class="op">&gt;</span> collim:
            i <span class="op">=</span> random.choice(idxs)<span class="op">;</span> val <span class="op">=</span> df.ix[u,i]
            test_data.append([u,i,val])
            df_train.ix[u,i] <span class="op">=</span> np.nan
        <span class="cf">if</span> <span class="bu">len</span>(test_data) <span class="op">&gt;</span> rowlim: <span class="cf">break</span>
    <span class="cf">return</span> df_train, test_data

<span class="kw">def</span> ssvd(df_train,k):
    lam <span class="op">=</span> <span class="fl">0.02</span> <span class="co"># regularizasyon</span>
    gamma <span class="op">=</span> <span class="fl">0.01</span> <span class="co"># adim katsayisi</span>
    m,n <span class="op">=</span> df_train.shape
    b_u <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>m)
    b_i <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>n)
    p_u <span class="op">=</span> np.random.rand(m,k)
    q_i <span class="op">=</span> np.random.rand(k, n)
    r_ui <span class="op">=</span> np.array(df_train)
    <span class="cf">for</span> u <span class="kw">in</span> <span class="bu">range</span>(m):
        row <span class="op">=</span> df_train.ix[u]<span class="op">;</span> idxs <span class="op">=</span> row.index[row.notnull()]
        <span class="cf">for</span> i <span class="kw">in</span> idxs:
            i <span class="op">=</span> <span class="bu">int</span>(i)
            r_ui_hat <span class="op">=</span> np.dot(q_i[:,i].T,p_u[u,:])
            e_ui <span class="op">=</span> r_ui[u,i] <span class="op">-</span> r_ui_hat
            q_i[:,i] <span class="op">=</span> q_i[:,i] <span class="op">+</span> gamma <span class="op">*</span> (e_ui<span class="op">*</span>p_u[u,:].T <span class="op">-</span> lam<span class="op">*</span>q_i[:,i])
            p_u[u,:] <span class="op">=</span> p_u[u,:] <span class="op">+</span> gamma <span class="op">*</span> (e_ui<span class="op">*</span>q_i[:,i].T <span class="op">-</span> lam<span class="op">*</span>p_u[u,:])
    <span class="cf">return</span> q_i,p_u
            </code></pre></div>
<p>Kodun önemli bir özelliği şudur, boş yani <code>nan</code> değeri içeren notlar eğitim sırasında atlanır. SGD seyrek verilerle de işleyebilen bir eğitim yöntemidir. Bu durumda verinin seyrekliği (sparsıty) bizim için çok faydalı, çünkü o veri noktalarına bakılmayacak, <code>row.notnüll()</code> ile boş olmayan öğelerin indis değerlerini alıyoruz. Bilindiği üzere Movielens, Netflix verileri oldukça seyrektir, kullanıcı binlerce film içinden onlar, yüzler bağlamında not verimi yapar, geri kalan değerler boştur.</p>
<p>Pratikte Atlanabilecek Formüller</p>
<p>Üstteki formülasyon içinde genel, film ve kullanıcı yanlılıkları formül içinde belirtilmiştir. Pratikte sayısal hesap açısından bu yanlılıkları daha SVD başlamadan önce veriden çıkartmak, ve yanlılılıkları rasgele güncelleme mantığından çıkartmak daha iyi sonuç veriyor. Bu çıkartma şöyle yapılır; önce film ortalamaları hesaplanır, her kullanıcı için o kullanıcının o film ortalamasına olan farkı (offset) hesaplanır ve bu fark eğitim sırasında not olarak kullanılır. Böylece SVD tüm yanlılıklardan arınmış bir fark hesabı ile çalışır sadece, ve sayısal olarak bu daha avantajlıdır. Yoksa ilk atılan adımda fark çok büyük olacağı için bu fark gradyan inişinin yönününü ağırlıklı olarak belirleyecektir, ve bu ilk atılan adım hesabın geri kalanına aşırı baskın çıkabilecektir (dominate). Zaten SVD'nin kuzeni olan PCA'den de bildiğimiz gibi, bu yöntem ortalamadan arındırılmış (demeaned) farklarla iş yapıyor.</p>
<p>Niye Azar Azar (Incremental) Hesap?</p>
<p>Madem <span class="math inline">\(U,S,V\)</span> hesabı yapar gibi <span class="math inline">\(q_i,p_u\)</span> hesaplıyoruz, niye bu hesabı adım adım yapıyoruz?</p>
<p>Bu sorunun cevabı azar azar hesap yaparken hafızaya sadece o satırı alarak daha az bellek kullanmamız. Milyarlarca boyut ve milyarlarca satır işliyor olsaydık bu matrisin tamamını hafızaya almamız mümkün olmazdı.</p>
<p>Ayrıca olmayan verileri atlamak bu SVD hesabını bir &quot;veri tamamlama problemi'' haline getiriyor, piyasadaki neredeyse diğer tüm kütüphaneler olmayan veriyi sıfır olarak kabul eder. Bu farklı bir problemdir. Bu diğer SVD yöntemlerinde her ne kadar olan veriyi ortalasak ve sıfır değerleri sıfır averajla aynı anlama gelmeye başlasa bile, yine de olmayan veriyi sıfır kabul etmek onu atlamaktan farklıdır [12].</p>
<p>Basit bir örnek</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd
<span class="im">import</span> ssvd
d <span class="op">=</span>  np.array(
[[  <span class="fl">5.</span>,   <span class="fl">5.</span>,   <span class="fl">3.</span>,  nan,   <span class="fl">5.</span>,   <span class="fl">5.</span>],
 [  <span class="fl">5.</span>,  nan,   <span class="fl">4.</span>,  nan,   <span class="fl">4.</span>,   <span class="fl">4.</span>],
 [ nan,   <span class="fl">3.</span>,  nan,   <span class="fl">5.</span>,   <span class="fl">4.</span>,   <span class="fl">5.</span>],
 [  <span class="fl">5.</span>,   <span class="fl">4.</span>,   <span class="fl">3.</span>,   <span class="fl">3.</span>,   <span class="fl">5.</span>,   <span class="fl">5.</span>],
 [  <span class="fl">5.</span>,   <span class="fl">5.</span>,  nan,  nan,  nan,   <span class="fl">5.</span>]
])
data <span class="op">=</span> pd.DataFrame (d, columns<span class="op">=</span>[<span class="st">&#39;0&#39;</span>,<span class="st">&#39;1&#39;</span>,<span class="st">&#39;2&#39;</span>,<span class="st">&#39;3&#39;</span>,<span class="st">&#39;4&#39;</span>,<span class="st">&#39;5&#39;</span>],
       index<span class="op">=</span>[<span class="st">&#39;Ben&#39;</span>,<span class="st">&#39;Tom&#39;</span>,<span class="st">&#39;John&#39;</span>,<span class="st">&#39;Fred&#39;</span>,<span class="st">&#39;Bob&#39;</span>])
avg_movies_data <span class="op">=</span> data.mean(axis<span class="op">=</span><span class="dv">0</span>)
data_user_offset <span class="op">=</span> data.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x<span class="op">-</span>avg_movies_data, axis<span class="op">=</span><span class="dv">1</span>)
q_i,p_u <span class="op">=</span> ssvd.ssvd(data_user_offset,k<span class="op">=</span><span class="dv">3</span>)
<span class="bu">print</span> <span class="st">&#39;q_i&#39;</span>,q_i
<span class="bu">print</span> <span class="st">&#39;p_u&#39;</span>,p_u
u <span class="op">=</span> <span class="dv">4</span><span class="op">;</span> i <span class="op">=</span> <span class="dv">2</span> <span class="co"># Bob icin tahmin yapalim</span>
r_ui_hat <span class="op">=</span> np.dot(q_i[:,i].T,p_u[u,:]) <span class="op">+</span> avg_movies_data.ix[i] <span class="op">+</span> <span class="op">\</span>
           np.nan_to_num(data_user_offset.ix[u,i])
<span class="bu">print</span> r_ui_hat</code></pre></div>
<pre><code>q_i [[ 0.77564986  0.1768796   0.56224297  0.42870962  0.94723452  0.55772707]
 [ 0.74100171  0.18146607  0.07704434  0.17631144  0.48868432  0.90470742]
 [ 0.70541948  0.28729857  0.091619    0.46184649  0.64498754  0.27410096]]
p_u [[ 0.41549541  0.59784095  0.45227968]
 [ 0.7756159   0.1197948   0.2921504 ]
 [ 0.82274016  0.58432802  0.46501307]
 [ 0.1877425   0.21364635  0.1924456 ]
 [ 0.80554597  0.59254732  0.9775562 ]]
3.92146103113</code></pre>
<p>Not: Artımsal SVD için bir diğer teknik için bkz. <em>Ders 29</em>.</p>
<p>Test Etmek</p>
<p>Test verisi oluşturmak için eğitim verisinde rasgele olarak bazı notları seçtik, bunları bir kenara kaydederek onların ana matris içindeki değerini sildik (yerine <code>nan</code> koyarak), ve bir kısmı silinmiş yeni bir eğitim matrisi yarattık, <code>create_training_test</code> işlevinde bu görülebilir. Bu işlevde her kullanıcıdan sadece bir tane not verisi alıyoruz, ve bunu sadece belli bir sayıda, <code>collim</code> kadar, not vermiş kullanıcılar için yapıyoruz, ki böylece az sayıda not vermiş kullanıcıların verisini azaltmamış oluyoruz. Ayrıca belli miktarda, <code>rowlim</code> kadar test noktası elde edince iş bitti kabul ediyoruz. Test verisi yaratmak için %80-%20 gibi bir ayrım yapmadık, yani eğitim verişindeki tüm kullanıcıları ve onların neredeyse tüm verisini eğitim için kullanıyoruz.</p>
<p>Movielens verisine gelelim, [14] yazısındaki <code>movielens_prep.py</code> ile gerekli eğitim dosyası üretildiğini farzederek,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd, os
df <span class="op">=</span> pd.read_csv(<span class="st">&quot;</span><span class="sc">%s</span><span class="st">/Downloads/movielens.csv&quot;</span> <span class="op">%</span> os.environ[<span class="st">&#39;HOME&#39;</span>] ,sep<span class="op">=</span><span class="st">&#39;;&#39;</span>)
<span class="bu">print</span> df.shape
df <span class="op">=</span> df.ix[:,<span class="dv">1</span>:<span class="dv">3700</span>] <span class="co"># id kolonunu atla,</span>
df.columns <span class="op">=</span> <span class="bu">range</span>(<span class="dv">3699</span>) <span class="co"># kolon degerlerini tekrar indisle</span>
<span class="bu">print</span> df.shape</code></pre></div>
<pre><code>(6040, 3731)
(6040, 3699)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">avg_movies <span class="op">=</span> df.mean(axis<span class="op">=</span><span class="dv">0</span>)
df_user_offset <span class="op">=</span> df.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x<span class="op">-</span>avg_movies, axis<span class="op">=</span><span class="dv">1</span>)
<span class="bu">print</span> df.ix[<span class="dv">6</span>,<span class="dv">5</span>], avg_movies.ix[<span class="dv">5</span>], df_user_offset.ix[<span class="dv">6</span>,<span class="dv">5</span>]</code></pre></div>
<pre><code>4.0 3.87872340426 0.121276595745</code></pre>
<p>Eğitim ve test verisi yaratıyoruz,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> ssvd
df_train, test_data <span class="op">=</span> ssvd.create_training_test(df_user_offset,rowlim<span class="op">=</span><span class="dv">500</span>,collim<span class="op">=</span><span class="dv">300</span>)
<span class="bu">print</span> <span class="bu">len</span>(test_data)</code></pre></div>
<pre><code>501</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">q_i,p_u <span class="op">=</span> ssvd.ssvd(df_train,k<span class="op">=</span><span class="dv">8</span>)</code></pre></div>
<p>Test</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">rmse <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> n <span class="op">=</span> <span class="dv">0</span>
<span class="cf">for</span> u,i,real <span class="kw">in</span> test_data:
    r_ui_hat <span class="op">=</span> np.dot(q_i[:,i].T,p_u[u,:])
    rmse <span class="op">+=</span> (real<span class="op">-</span>r_ui_hat)<span class="op">**</span><span class="dv">2</span>
    n <span class="op">+=</span> <span class="dv">1</span>
<span class="bu">print</span> <span class="st">&quot;rmse&quot;</span>, np.sqrt(rmse <span class="op">/</span> n)</code></pre></div>
<pre><code>rmse 0.936136196897</code></pre>
<p>Sonuç fena değil.</p>
<p>Formülasyonun Hikayesi</p>
<p>SGD SVD'nin hikayesi şöyle. Yıl 2009, Netflix Yarışması [11] katılımcılarından Simon Funk (gerçek adı Brandyn Webb) SGD SVD yaklaşımını kodlayıp veri üzerinde işletince birden bire sıralamada ilk 3'e fırlar; Webb artık çok ünlü olan blog yazısında [1] yaklaşımı detayıyla paylaşıp forum'da haberini verince bu haber tam bir bomba etkisi yaratır. Pek çok kişi yaklaşımı kopyalar, hatta kazanan BellKor ürününde Webb'in SVD yaklaşımının kullanıldığı biliniyor.</p>
<p>Bu metotun keşfi hangi basamaklardan geçti? Beni meraklandıran minimizasyon formülasyonun konveks olmamasıydı -- genellikle optimizasyon problemlerinde konveksliğin mevdudiyeti aranır, çünkü bu durumda sonuca yaklaşmak (convergence) için bir garanti elde edilir. Bu durumda konvekslik yoktu. &quot;O zaman Webb nasıl rahat bir şekilde SGD kullanabildi?'' sorusunun cevabını merak ediyorduk yani. Biraz araştırınca Bottou ve LeCunn gibi araştırmacıların yazılarına ulaştık [4]. Onlara göre konvekslik olmaması yapay öğrenim araştırmacılarını korkutmamalı, eğer sayısal (empiriçally) işleyen bir algoritma var ise, teorik ispat gelene kadar bu metotun kullanılmasında sakınca yoktur.</p>
<p>Fakat böyle buluşlarda yine de bazı garantiler temel alınmış olabilir, araştırmacı tamamen balıklama atlayış yapmaz. Webb'in kendisine bu soruları sorduk ve bize buluşun hangi seviyelerden geçtiğini anlattı. Geriye sarıyoruz, Webb Netflix'den çok önce yapay sınır ağlarını araştırmaktadır, ve Sanger, Oja'nın [5,6] yayınlarını baz alarak kurduğu bir YSA için bir çözüm bulduğunu farkeder. Sayısal çözümde özdeğer/vektör bulmaya yarayan Üstel Metotun (power method) bir şeklini kullanmıştır, ki Sanger'in Genel Hebbian Algorıtmasının (GHA) üstel metot ile bağlantıları var, ve bu GHA yayınında &quot;eğitilince'' özdeğer/vektör ve PCA hesabı yapabilen bir YSA'dan bahsediliyor. Daha önemlisi GHA 1 olasılıkla (yani kesin) bu sonuçlara erişebiliyor.</p>
<p>Daha sonra Webb bu çözümü arkadaşı Gorrell ile tartışırken Gorrell ona problem formülasyonunun SVD olarak görülebileceğini söyler. Bilindiği gibi özdeğer/vektör hesabı ile SVD yakın akraba sayılır. İkili bu bağlamda birkaç yayın da yaparlar. Daha sonra Netflix yarışması başladığında Webb çözüm için gradyan baz alarak SGD kullanabileceğini farkediyor, ki SGD ile üstel metot arasında teorik bağlantı var [7]. Ve sonuç olarak SGD SVD metotu ortaya çıkıyor.</p>
<p>Tabii ki &quot;SGD SVD ne kadar SVD sayılır?'' gibi bir soru sorulabilir. Evet, regülarizasyon bazı gayrı lineerlikleri probleme sokar, zaten bu çözümü &quot;yaklaşıksal'' yapan kısım da budur. Fakat belli şartlarda, regülarizasyon olmasa çözüm tam SVD olacaktır. Bu buluşun püf noktası bu bilgide, ve üstteki teorik benzerliklerde, onları biliyor olmakta yatıyor. Eğer bunlar biliniyor ise, ve sağlam lineer cebir bilgisi ile gerektiği zaman onları ne kadar esnetebileceğimizi biliriz. Konu hakkındaki daha fazla detay [10]'da bulunabilir.</p>
<p>Tavsiye</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">movies <span class="op">=</span> pd.read_csv(<span class="st">&quot;../../stat/stat_pandas_ratings/movies.dat&quot;</span>,<span class="op">\</span>
         sep<span class="op">=</span><span class="st">&#39;::&#39;</span>,names<span class="op">=</span>[<span class="st">&#39;idx&#39;</span>,<span class="st">&#39;movie&#39;</span>,<span class="st">&#39;type&#39;</span>])
<span class="kw">def</span> eval_user(u):
    res <span class="op">=</span> {}
    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(df_user_offset.shape[<span class="dv">1</span>]):
        r_ui_hat <span class="op">=</span> np.dot(q_i[:,i].T,p_u[u,:])
        res[i] <span class="op">=</span> <span class="bu">float</span>(r_ui_hat) <span class="op">+</span> avg_movies.ix[i] <span class="op">+</span> df_user_offset.ix[u,i]
    res <span class="op">=</span> <span class="bu">sorted</span>(res.items(), key<span class="op">=</span><span class="kw">lambda</span> x:x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)

    <span class="bu">print</span> <span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">Tavsiyeler</span><span class="ch">\n\n</span><span class="st">&quot;</span>
    <span class="cf">for</span> j,(si,sm) <span class="kw">in</span> <span class="bu">enumerate</span>(res):
        <span class="bu">print</span> movies[movies[<span class="st">&#39;idx&#39;</span>] <span class="op">==</span> si][<span class="st">&#39;movie&#39;</span>]
        <span class="cf">if</span> j <span class="op">==</span> <span class="dv">10</span>: <span class="cf">break</span>

    <span class="bu">print</span> <span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">Mevcut Filmler</span><span class="ch">\n\n</span><span class="st">&quot;</span>
    row <span class="op">=</span> df.ix[u]
    idxs <span class="op">=</span> row.index[row.notnull()]
    <span class="cf">for</span> j,idx <span class="kw">in</span> <span class="bu">enumerate</span>(idxs):
        <span class="bu">print</span> movies[movies[<span class="st">&#39;idx&#39;</span>] <span class="op">==</span> idx][<span class="st">&#39;movie&#39;</span>], row[idx]
        <span class="cf">if</span> j <span class="op">==</span> <span class="dv">10</span>: <span class="cf">break</span>

eval_user(<span class="dv">300</span>)
eval_user(<span class="dv">2900</span>)</code></pre></div>
<pre><code>Tavsiyeler


3441    Frequency (2000)
Name: movie, dtype: object
105    Muppet Treasure Island (1996)
Name: movie, dtype: object
Series([], dtype: object)
2    Grumpier Old Men (1995)
Name: movie, dtype: object
4    Father of the Bride Part II (1995)
Name: movie, dtype: object
1    Jumanji (1995)
Name: movie, dtype: object
3    Waiting to Exhale (1995)
Name: movie, dtype: object
0    Toy Story (1995)
Name: movie, dtype: object
5    Heat (1995)
Name: movie, dtype: object
6    Sabrina (1995)
Name: movie, dtype: object
7    Tom and Huck (1995)
Name: movie, dtype: object


Mevcut Filmler


Series([], dtype: object) 5.0
0    Toy Story (1995)
Name: movie, dtype: object 3.0
1    Jumanji (1995)
Name: movie, dtype: object 4.0
3    Waiting to Exhale (1995)
Name: movie, dtype: object 3.0
4    Father of the Bride Part II (1995)
Name: movie, dtype: object 5.0
9    GoldenEye (1995)
Name: movie, dtype: object 4.0
15    Casino (1995)
Name: movie, dtype: object 4.0
19    Money Train (1995)
Name: movie, dtype: object 5.0
20    Get Shorty (1995)
Name: movie, dtype: object 3.0
23    Powder (1995)
Name: movie, dtype: object 4.0
30    Dangerous Minds (1995)
Name: movie, dtype: object 5.0


Tavsiyeler


Series([], dtype: object)
0    Toy Story (1995)
Name: movie, dtype: object
1    Jumanji (1995)
Name: movie, dtype: object
2    Grumpier Old Men (1995)
Name: movie, dtype: object
3    Waiting to Exhale (1995)
Name: movie, dtype: object
4    Father of the Bride Part II (1995)
Name: movie, dtype: object
5    Heat (1995)
Name: movie, dtype: object
6    Sabrina (1995)
Name: movie, dtype: object
7    Tom and Huck (1995)
Name: movie, dtype: object
8    Sudden Death (1995)
Name: movie, dtype: object
9    GoldenEye (1995)
Name: movie, dtype: object


Mevcut Filmler


19    Money Train (1995)
Name: movie, dtype: object 4.0
23    Powder (1995)
Name: movie, dtype: object 4.0
37    It Takes Two (1995)
Name: movie, dtype: object 3.0
45    How to Make an American Quilt (1995)
Name: movie, dtype: object 5.0
48    When Night Is Falling (1995)
Name: movie, dtype: object 5.0
66    Two Bits (1995)
Name: movie, dtype: object 5.0
78    Juror, The (1996)
Name: movie, dtype: object 4.0
104    Nobody Loves Me (Keiner liebt mich) (1994)
Name: movie, dtype: object 5.0
118    Race the Sun (1996)
Name: movie, dtype: object 4.0
134    From the Journals of Jean Seberg (1995)
Name: movie, dtype: object 2.0
142    Brothers McMullen, The (1995)
Name: movie, dtype: object 3.0</code></pre>
<p>Numba ve Funk SVD</p>
<p>Eğer Numba [13] kullanırsak, SVD kodunu çok daha hızlı işletebiliriz. Ayrıca Funk'ın kodlaması (ki alttaki kodu onu temel alacak) biraz daha ilginç, mesela en dış döngü özellikler (feature) geziyor, onun içindeki birkaç yüz kez yine kendi içinde olan tahmin/hata hesabını yapıyor, tüm veri seti üzerinde. Bunun için Movielens 100k verisi lazım, ardından <code>data_m100k.py</code> ile veri yaratılır,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Requires Movielens 100k data </span>
<span class="im">from</span> scipy.io <span class="im">import</span> mmread, mmwrite
<span class="im">import</span> numpy <span class="im">as</span> np, time, sys
<span class="im">from</span> numba <span class="im">import</span> jit
<span class="im">import</span> os

<span class="kw">def</span> create_user_feature_matrix(review_matrix, NUM_FEATURES):
    num_users <span class="op">=</span> review_matrix.shape[<span class="dv">0</span>]
    user_feature_matrix <span class="op">=</span> <span class="fl">1.</span><span class="op">/</span>NUM_FEATURES <span class="op">*</span> <span class="op">\</span>
        np.random.randn(NUM_FEATURES, num_users).astype(np.float32)
    <span class="cf">return</span> user_feature_matrix

<span class="kw">def</span> create_movie_feature_matrix(review_matrix, NUM_FEATURES):
    num_movies <span class="op">=</span> review_matrix.shape[<span class="dv">1</span>]
    movie_feature_matrix <span class="op">=</span> <span class="fl">1.</span><span class="op">/</span>NUM_FEATURES <span class="op">*</span> <span class="op">\</span>
        np.random.randn(NUM_FEATURES, num_movies).astype(np.float32)
    <span class="cf">return</span> movie_feature_matrix

<span class="at">@jit</span>(nopython<span class="op">=</span><span class="va">True</span>)
<span class="kw">def</span> predict_rating(user_id, movie_id, user_feature_matrix, movie_feature_matrix):
    rating <span class="op">=</span> <span class="fl">1.</span>
    <span class="cf">for</span> f <span class="kw">in</span> <span class="bu">range</span>(user_feature_matrix.shape[<span class="dv">0</span>]):
        rating <span class="op">+=</span> <span class="op">\</span>
            user_feature_matrix[f, user_id] <span class="op">*</span> <span class="op">\</span>
            movie_feature_matrix[f, movie_id]
    <span class="cf">if</span> rating <span class="op">&gt;</span> <span class="dv">5</span>: rating <span class="op">=</span> <span class="dv">5</span>
    <span class="cf">elif</span> rating <span class="op">&lt;</span> <span class="dv">1</span>: rating <span class="op">=</span> <span class="dv">1</span>
    <span class="cf">return</span> rating

<span class="at">@jit</span>(nopython<span class="op">=</span><span class="va">True</span>)
<span class="kw">def</span> sgd_inner(feature, A_row, A_col, A_data,
              user_feature_matrix, movie_feature_matrix,
              NUM_FEATURES):
    K <span class="op">=</span> <span class="fl">0.015</span>
    LEARNING_RATE <span class="op">=</span> <span class="fl">0.001</span>
    squared_error <span class="op">=</span> <span class="dv">0</span>
    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(A_data)):
        user_id <span class="op">=</span> A_row[k]
        movie_id <span class="op">=</span> A_col[k]
        rating <span class="op">=</span> A_data[k]
        p <span class="op">=</span> predict_rating(user_id,
                           movie_id,
                           user_feature_matrix,
                           movie_feature_matrix)
        err <span class="op">=</span> rating <span class="op">-</span> p            
        squared_error <span class="op">+=</span> err <span class="op">**</span> <span class="dv">2</span>
        user_feature_value <span class="op">=</span> user_feature_matrix[feature, user_id]
        movie_feature_value <span class="op">=</span> movie_feature_matrix[feature, movie_id]
        user_feature_matrix[feature, user_id] <span class="op">+=</span> <span class="op">\</span>
            LEARNING_RATE <span class="op">*</span> (err <span class="op">*</span> movie_feature_value <span class="op">-</span> K <span class="op">*</span> user_feature_value)
        movie_feature_matrix[feature, movie_id] <span class="op">+=</span> <span class="op">\</span>
            LEARNING_RATE <span class="op">*</span> (err <span class="op">*</span> user_feature_value <span class="op">-</span> K <span class="op">*</span> movie_feature_value)

    <span class="cf">return</span> squared_error

<span class="kw">def</span> calculate_features(A_row, A_col, A_data,
                       user_feature_matrix, movie_feature_matrix,
                       NUM_FEATURES):
    MIN_IMPROVEMENT <span class="op">=</span> <span class="fl">0.0001</span>
    MIN_ITERATIONS <span class="op">=</span> <span class="dv">200</span>
    rmse <span class="op">=</span> <span class="dv">0</span>
    last_rmse <span class="op">=</span> <span class="dv">0</span>
    <span class="bu">print</span> <span class="bu">len</span>(A_data)
    num_ratings <span class="op">=</span> <span class="bu">len</span>(A_data)
    <span class="cf">for</span> feature <span class="kw">in</span> <span class="bu">xrange</span>(NUM_FEATURES):
        <span class="bu">iter</span> <span class="op">=</span> <span class="dv">0</span>
        <span class="cf">while</span> (<span class="bu">iter</span> <span class="op">&lt;</span> MIN_ITERATIONS) <span class="kw">or</span>  (rmse <span class="op">&lt;</span> last_rmse <span class="op">-</span> MIN_IMPROVEMENT):
            last_rmse <span class="op">=</span> rmse
            squared_error <span class="op">=</span> sgd_inner(feature, A_row, A_col, A_data,
                                      user_feature_matrix, movie_feature_matrix,
                                      NUM_FEATURES)
            rmse <span class="op">=</span> (squared_error <span class="op">/</span> num_ratings)
            <span class="bu">iter</span> <span class="op">+=</span> <span class="dv">1</span>
        <span class="bu">print</span> (<span class="st">&#39;Squared error = </span><span class="sc">%f</span><span class="st">&#39;</span> <span class="op">%</span> squared_error)
        <span class="bu">print</span> (<span class="st">&#39;RMSE = </span><span class="sc">%f</span><span class="st">&#39;</span> <span class="op">%</span> rmse)
        <span class="bu">print</span> (<span class="st">&#39;Feature = </span><span class="sc">%d</span><span class="st">&#39;</span> <span class="op">%</span> feature)
    <span class="cf">return</span> last_rmse

<span class="kw">def</span> main():
    LAMBDA <span class="op">=</span> <span class="fl">0.02</span>
    NUM_FEATURES <span class="op">=</span> <span class="dv">30</span>

    A <span class="op">=</span> mmread(<span class="st">&#39;</span><span class="sc">%s</span><span class="st">/Downloads/A_m100k_train&#39;</span> <span class="op">%</span> os.environ[<span class="st">&#39;HOME&#39;</span>])

    user_feature_matrix <span class="op">=</span> create_user_feature_matrix(A, NUM_FEATURES)
    movie_feature_matrix <span class="op">=</span> create_movie_feature_matrix(A, NUM_FEATURES)

    A <span class="op">=</span> A.tocoo()

    rmse <span class="op">=</span> calculate_features(A.row, A.col, A.data,
                              user_feature_matrix, movie_feature_matrix,
                              NUM_FEATURES )
    <span class="bu">print</span> <span class="st">&#39;rmse&#39;</span>, rmse

    np.savetxt(<span class="st">&quot;/tmp/user_feature_matrix2.dat&quot;</span>, user_feature_matrix)
    np.savetxt(<span class="st">&quot;/tmp/movie_feature_matrix2.dat&quot;</span>, movie_feature_matrix)

<span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>: 
    main()</code></pre></div>
<p>Üstteki script'i işlettikten sonra bazı tavsiyeleri gösterebiliriz,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> pandas <span class="im">as</span> pd, os
items_file <span class="op">=</span> <span class="st">&#39;</span><span class="sc">%s</span><span class="st">/Downloads/ml-100k/u.item&#39;</span> <span class="op">%</span> os.environ[<span class="st">&#39;HOME&#39;</span>]
item_df <span class="op">=</span> pd.read_csv(items_file, sep<span class="op">=</span><span class="st">&#39;|&#39;</span>,header<span class="op">=</span><span class="va">None</span>)
item_df[<span class="st">&#39;idx&#39;</span>] <span class="op">=</span> item_df[<span class="dv">0</span>] <span class="op">-</span> <span class="dv">1</span>
item_df <span class="op">=</span> item_df.set_index(<span class="st">&#39;idx&#39;</span>)

<span class="im">from</span> scipy.io <span class="im">import</span> mmread, mmwrite
<span class="im">import</span> numpy <span class="im">as</span> np, time, sys, os
<span class="im">import</span> funk2, pandas <span class="im">as</span> pd

user_feature_matrix <span class="op">=</span> np.loadtxt(<span class="st">&quot;/tmp/user_feature_matrix2.dat&quot;</span>)
movie_feature_matrix <span class="op">=</span> np.loadtxt(<span class="st">&quot;/tmp/movie_feature_matrix2.dat&quot;</span>)

preds <span class="op">=</span> []
user_id <span class="op">=</span> <span class="dv">110</span>
<span class="cf">for</span> movie_id <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1682</span>):
    pred <span class="op">=</span> funk2.predict_rating(user_id, movie_id, user_feature_matrix, movie_feature_matrix)
    preds.append([movie_id, pred])

preds_df <span class="op">=</span> pd.DataFrame(preds,columns<span class="op">=</span>[<span class="st">&#39;movie&#39;</span>,<span class="st">&#39;score&#39;</span>])
preds_df.sort_index(by<span class="op">=</span><span class="st">&#39;score&#39;</span>,ascending<span class="op">=</span><span class="va">False</span>,inplace<span class="op">=</span><span class="va">True</span>)
preds_df[<span class="st">&#39;movie_name&#39;</span>] <span class="op">=</span> item_df[<span class="dv">1</span>]
<span class="bu">print</span> preds_df.head(<span class="dv">10</span>)</code></pre></div>
<pre><code>      movie     score                                         movie_name
1448   1448  4.600873                             Pather Panchali (1955)
407     407  4.538450                              Close Shave, A (1995)
168     168  4.424078                         Wrong Trousers, The (1993)
482     482  4.406603                                  Casablanca (1942)
99       99  4.402690                                       Fargo (1996)
11       11  4.362673                         Usual Suspects, The (1995)
319     319  4.323309  Paradise Lost: The Child Murders at Robin Hood...
49       49  4.315158                                   Star Wars (1977)
113     113  4.308009  Wallace &amp; Gromit: The Best of Aardman Animatio...
172     172  4.288836                         Princess Bride, The (1987)</code></pre>
<p>Bu kişinin seyrettiği ve en çok beğendiği filmler altta</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">A <span class="op">=</span> mmread(<span class="st">&#39;</span><span class="sc">%s</span><span class="st">/Downloads/A_m100k_train&#39;</span> <span class="op">%</span> os.environ[<span class="st">&#39;HOME&#39;</span>]).tocsc()
movies <span class="op">=</span> A[user_id,:].nonzero()[<span class="dv">1</span>]
ratings <span class="op">=</span> A[user_id,A[user_id,:].nonzero()[<span class="dv">1</span>]]
ratings <span class="op">=</span> np.ravel(ratings.todense())
likes_df <span class="op">=</span> pd.DataFrame()
likes_df[<span class="st">&#39;movie&#39;</span>] <span class="op">=</span> movies<span class="op">;</span> likes_df[<span class="st">&#39;rating&#39;</span>] <span class="op">=</span> ratings
likes_df <span class="op">=</span> likes_df.set_index(<span class="st">&#39;movie&#39;</span>)
likes_df.sort_index(by<span class="op">=</span><span class="st">&#39;rating&#39;</span>,ascending<span class="op">=</span><span class="va">False</span>,inplace<span class="op">=</span><span class="va">True</span>)
likes_df[<span class="st">&#39;movie_name&#39;</span>] <span class="op">=</span> item_df[<span class="dv">1</span>]
<span class="bu">print</span> likes_df.head(<span class="dv">10</span>)</code></pre></div>
<pre><code>       rating                     movie_name
movie                                       
301         5       L.A. Confidential (1997)
314         5               Apt Pupil (1998)
257         4                 Contact (1997)
285         4    English Patient, The (1996)
303         4           Fly Away Home (1996)
310         4  Wings of the Dove, The (1997)
353         4     Wedding Singer, The (1998)
302         3             Ulee&#39;s Gold (1997)
320         3                  Mother (1996)
304         2          Ice Storm, The (1997)</code></pre>
<p>Kaynaklar</p>
<p>[1] Funk, <em>Netflix Update: Try This at Home</em>, <a href="http://sifter.org/~simon/journal/20061211.html" class="uri">http://sifter.org/~simon/journal/20061211.html</a></p>
<p>[2] Koren, Bell, <em>Recommender Systems Handbook</em>, <a href="http://www.cs.bme.hu/nagyadat/Recommender_systems_handbook.pdf" class="uri">http://www.cs.bme.hu/nagyadat/Recommender_systems_handbook.pdf</a></p>
<p>[3] Koren, <em>MATRIX FACTORIZATION TECHNIQUES FOR RECOMMENDER SYSTEMS </em>, <a href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf">https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf</a></p>
<p>[4] LeCun, <em>Who is Afraid of Non-Convex Loss Functions?</em>, <a href="http://videolectures.net/eml07_lecun_wia" class="uri">http://videolectures.net/eml07_lecun_wia</a></p>
<p>[5] Sanger, <em>Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network </em>, <a href="http://courses.cs.washington.edu/courses/cse528/09sp/sanger_pca_nn.pdf" class="uri">http://courses.cs.washington.edu/courses/cse528/09sp/sanger_pca_nn.pdf</a> %</p>
<p>[6] Oja, <em>A Simplified Neuron Model as a Principal Component Analyzer</em>, <a href="http://users.ics.aalto.fi/oja/Oja1982.pdf" class="uri">http://users.ics.aalto.fi/oja/Oja1982.pdf</a></p>
<p>[7] Cotter, <em>Stochastic Optimization for Machine Learning</em>, <a href="http://arxiv.org/pdf/1308.3509" class="uri">http://arxiv.org/pdf/1308.3509</a></p>
<p>[8] Touchette, <em>Introduction to Numerical Computing</em>, <a href="http://www.maths.qmul.ac.uk/~wj/MTH5110/notes/MAS235_lecturenotes1.pdf" class="uri">http://www.maths.qmul.ac.uk/~wj/MTH5110/notes/MAS235_lecturenotes1.pdf</a></p>
<p>[10] Stack Exchange, <em>Gradient Descent on Non-Convex Function Works But How?</em>, <a href="http://math.stackexchange.com/questions/649701/gradient-descent-on-non-convex-function-works-but-how" class="uri">http://math.stackexchange.com/questions/649701/gradient-descent-on-non-convex-function-works-but-how</a></p>
<p>[11] Netflix, <em>Netflix Prize</em>, <a href="http://www.netflixprize.com" class="uri">http://www.netflixprize.com</a></p>
<p>[12] Gleich, <em>SVD on the Netflix matrix</em>, <a href="https://dgleich.wordpress.com/2013/10/19/svd-on-the-netflix-matrix" class="uri">https://dgleich.wordpress.com/2013/10/19/svd-on-the-netflix-matrix</a></p>
<p>[13] Bayramlı, <em>Numba, LLVM, ve SVD</em>, <a href="https://burakbayramli.github.io/dersblog/sk/2014/09/numba-llvm-ve-svd.html" class="uri">https://burakbayramli.github.io/dersblog/sk/2014/09/numba-llvm-ve-svd.html</a></p>
<p>[14] Bayramlı, Istatistik, <em>SVD, Toplu Tavsiye</em></p>
</body>
</html>
