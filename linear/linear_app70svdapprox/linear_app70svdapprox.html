<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Yaklaşıksal SVD ile Tavsiye Sistemleri</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"
  type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="yaklaşıksal-svd-ile-tavsiye-sistemleri">Yaklaşıksal SVD ile
Tavsiye Sistemleri</h1>
<p>Geçmiş verilere bakarak bir kullanıcının hiç seyretmediği bir filme
nasıl not vereceğini tahmin etmek ünlü Netflix yarışmasının konusuydu.
Önceki bir yazı [14]’te benzer bir veri seti Movielens üzerinde SVD
uygulayarak önce boyut azaltmıştık, azaltılmış boyut üzerinden yeni (o
film için notu bilinmeyen) bir kullanıcının diğer mevcut kullanıcılara
mesafesini hesaplamış, ve böylece beğeni açısından en çok benzediği
diğer kullanıcıyı bulmuştuk (birkaç tane de bulunabilir). Bu
kullanıcının bir film için verdiği notu yeni kullanıcı için tahmin
olarak baz almıştık. SVD’yi kullanmanın bir yöntemi daha var, Netflix
yarışmasında kullanılan [1] bir yaklaşım şöyle; alttaki SVD
ayrıştırmasına bakalım,</p>
<p><img src="svdapprox_1.png" /></p>
<p>1’inci kullanıcının 1’inci filme verdiği not üstte köyü gösterilen
satırların çarpımı ile oluyor, eğer ufak harfler ve kullanıcı (user)
için <span class="math inline">\(u\)</span>, film için <span
class="math inline">\(i\)</span> indisini, ve <span
class="math inline">\(q,p\)</span> vektörlerini <span
class="math inline">\(Q,P\)</span> matrislerinin sırasıyla kolon ve
satırlarını göstermek için kullanırsak, ayrıştırma sonrası beğeni değeri
(önemli bir kısmı daha doğrusu) <span
class="math inline">\(q_i^Tp_u\)</span> çarpımındadır. Çarpım içinde
<span class="math inline">\(S\)</span>’ten gelecek eşsiz değeri
(singular value) ne olacak? Şimdi formülasyonda bir değişiklik
düşünelim, bu değerin çarpım dışına alındığını hayal edelim; bu değerin
kabaca birkaç toplamın sonucuna dönüştüğünü düşünelim. Matematiksel
olarak imkansız değil. Bu toplam terimleri, toplam oldukları için, bir
baz seviyesi tanımlayabilirler. Bir kullanıcının ne kadar yanlı (bias)
not verdiğini, ya da bir filmin nasıl not almaya meyıllı olduğunu
modelleyebilirler (ki bu da bir yanlılık ölçüsü). Ayrıca tüm filmlere
verilen notların yanlılığı da ayrı bir terim olarak gösterilebilir. Tüm
bunları bir araya koyarsak, bir beğeni notunu tahmin edecek formül şöyle
gösterilebilir,</p>
<p><span class="math display">\[
\hat{r}_{ui} = \mu + b_i + b_u + q_i^Tp_u
\]</span></p>
<p><span class="math inline">\(\mu\)</span> bir skalar, tüm filmlere
verilen ortalamayı gösteriyor, ki tüm beğenilerin sayısal ortalaması
üzerinden basit bir şekilde hızla hesaplanabilir. <span
class="math inline">\(\hat{r}_{ui}\)</span>’ya bir tahmin dedik çünkü
modelimizdeki vektörlerin değerlerini bulduktan sonra (eğitim verisiyle
bu hesabı yapacağız) modeli kullanarak gerçek not <span
class="math inline">\(r_{ui}\)</span> için bir tahmin olarak
kullanılacak.</p>
<p>Yanlılık hakkında bazı örnekler vermek gerekirse, diyelim ki
kullanıcı Bob not verirken yüksek seviyede oy vermeye meyıllı. Bu
durumda bu kullanıcının ortalama hatta düşük oy vermesi onun bir filmden
hakikaten hiç hoşlanmadığını sinyalleyebilir. Ya da bir film genellikle
ortalama oy almaktadır, bu durumda ona çok iyi not veren bir kişinin bu
filmi çok beğendiği ortaya çıkar. Modeldeki yanlılık parametreleri bu
durumu saptayabilirler. Eğer verimizde / gerçek dünyada yanlılık var
ise, modelin bu bilgiyi kullanması onun başarısını arttıracaktır.</p>
<p>Eğitim</p>
<p>Eğitim için ne yapmalı? Minimize edeceğimiz bir hedef fonksiyonu
kuralım, ki çoğunlukla bu karesi alınmış hata ile olur. Mesela gerçek
not <span class="math inline">\(r_{ui}\)</span> değerinden tahmin notu
<span class="math inline">\(\hat{r}_{ui}\)</span>’yi çıkartıp karesini
alabiliriz. Bu işlemi tüm <span class="math inline">\(u,i\)</span>’ler
için yaparak sonuçları toplarız, ve bu toplamı minimize etmeye
uğraşabiliriz. Yani</p>
<p><span class="math display">\[
\min_{b\star,q\star,p\star} \sum_{u,i} (r_{ui} - \hat{r}_{ui})^2 +
\lambda (b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2)
\]</span></p>
<p><span class="math display">\[
= \min_{b\star,q\star,p\star} \sum_{u,i} (r_{ui} - \mu - b_i - b_u -
q_i^Tp_u)^2 +
\lambda (b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2)
\]</span></p>
<p>Kısaltma olarak <span class="math inline">\(e_{ui}\)</span>
tanımlayalım, bu faydalı olabilir, formüldeki ilk parantez içindeki
kısımda kullanmak üzere,</p>
<p><span class="math display">\[ e_{ui} := r_{ui} - \hat{r}_{ui}
\]</span></p>
<p><span class="math inline">\(\lambda\)</span> ile çarpılan bölüm
regülarizasyon için. İstatistik, yapay öğrenimde modelimizin aşırı
uygunluk (överfitting) yapmasını engellemek için regülarizasyon
kullanılır, bunun için istediğimiz değişkenlerin fazla büyümesini
cezalandırırız, üstteki minimizasyon modelinde bu ceza için tüm
değerlerin büyüklüğünü (magnitude) hesapladık -skalar değerlerin
karesini, vektör değerlerinin kare norm’unu alarak- ve bu büyüklükleri
bizim dışarıdan tanımlayacağımız bir sabitle çarpımı üzerinden
minimizasyon problemine direk dahil ettik. Böylece bu büyüklükler
azaltılma hedefinin parçası haline geldiler. Yani hem <span
class="math inline">\(e_{ui}^2\)</span> hem de hatayı oluşturan
değerlerin kendileri minimize edilecek. Bu minimizasyon sırasında bazı
değişkenlerin sıfıra inip o <span class="math inline">\(u,i\)</span>
için tamamen etkisiz hale gelmesi bile mümkündür (ki bu bize o
parametrenin önemsiz olduğunu sinyalleyebileceği için faydalıdır).</p>
<p>Rasgele Gradyan İnişi (Stochastic Gradient Descent -SGD-)</p>
<p>Modeli nasıl minimize ederiz? Bu model konveks (convex) değil, ki
konvekslik bilindiği gibi fonksiyonun düzgün bir çukur gibi olduğu
problemlerdir. Böyle çukur fonksiyonlarında herhangi bir noktadan
başlarsınız, gradyanı hesaplarsınız, ve bu gradyan hep optimal iniş
noktasını (daha doğrusu tersini) gösterir, ve yolda giderken takılıp
kalabileceğiniz yerel minimumlar mevcut değildir, ve sonunda çukur
dibine ulaşılır. Bizim problemimizde <span
class="math inline">\(q_i^Tp_u\)</span> var, bu değişkenlerin ikisi de
bilinmiyor, ve bu çarpımın karesi alındığı için genel karesellliği
(quadratic) kaybetmiş oluyoruz. Fakat yine de SGD bu problemi
çözebiliyor. Bunun sebeplerini, SGD SVD’nin hikayesiyle beraber yazının
sonunda bulanabilir.</p>
<p>SGD için gradyanlar lazım, her değişken için minimizasyon toplamı
içindeki kısmın (bu kısma <span class="math inline">\(E\)</span>
diyelim) ayrı ayrı kısmı türevini almak lazım. Mesela <span
class="math inline">\(b_u\)</span> için</p>
<p><span class="math display">\[ \frac{\partial E}{\partial b_u}  =
-2e_{ui} + 2 \lambda b_u
\]</span></p>
<p>Gradyan her zaman en yüksek çıkışı gösterir, o zaman hesapsal
algoritma onun tersi yönüne gitmelidir. Bu gidişin adım büyüklüğünü
kontrol etmek için dışarıdan bizim belirlediğimiz bir <span
class="math inline">\(\gamma\)</span> sabiti ile çarpım yapabiliriz, ve
bir numara daha, sabit 2 değerlerinin <span
class="math inline">\(\gamma\)</span> içinde eritilebileceğini
farzederek onları sileriz. Yani adım <span
class="math inline">\(\gamma(e_{ui} - \lambda b_u)\)</span> haline
geldi. Bir döngü içinde eski <span class="math inline">\(b_u\)</span>
bulunacak, gördüğümüz yönde adım atılacak, yani adım önceki değere
toplanacak, ve yeni değer elde edilecek. Diğer değişkenler için türev
alıp benzer işlemleri yaparsak, sonuç şöyle,</p>
<p><span class="math display">\[ b_u \leftarrow b_u + \gamma (e_{ui} -
\lambda \cdot b_u) \]</span></p>
<p><span class="math display">\[ b_i \leftarrow b_i + \gamma (e_{ui} -
\lambda \cdot b_i) \]</span></p>
<p><span class="math display">\[ q_i \leftarrow q_i + \gamma
(e_{ui}\cdot p_u - \lambda \cdot q_i) \]</span></p>
<p><span class="math display">\[ p_u \leftarrow p_u + \gamma
(e_{ui}\cdot q_i - \lambda \cdot p_u) \]</span></p>
<p>Her değişken için başlangıç noktası rasgele olarak seçilebilir, hatta
seçilmelidir; İnternet’te bu konu hakkında “efendim tüm değerleri 0.1
değeri yapsanız olur’’ gibi yorumlar okuyabilirsiniz, bu durumda <span
class="math inline">\(q_i,p_u\)</span> değişkenlerinin tüm hücreleri
aynı değerde kalır! Yani <span class="math inline">\(q_3\)</span>
mesela, tamamen 0.6 değerine sahip olur, ki bu istenen bir şey olmaz.
<span class="math inline">\(\gamma,\lambda\)</span> sabitleri için en
iyi değerler deneme/ yanılma ya da çapraz sağlama (crossvalidation) ile
bulunabilir, biz bu örnekte deneme / yanılma yöntemini seçtik.</p>
<p>Rasgelelik, aynen <em>Lojistik Regresyon</em> örneğinde olduğu gibi
verinin rasgeliliğinden geliyor, her veri noktasını teker teker sırayla
işliyoruz aslında fakat bu “sıranın’’ rasgele olduğunu farzettiğimiz
için özyineli algoritmamız rasgelelik elde ediyor. Python kodu altta,
eğitim için kod sadece bir kere verinin üzerinden geçiyor. Başa dönüp
birkaç kere (hatta yüzlerce) veriyi işleyenler de olabiliyor.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> linalg <span class="im">as</span> la</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random, pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_training_test(df,collim<span class="op">=</span><span class="dv">2</span>,rowlim<span class="op">=</span><span class="dv">200</span>):</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    test_data <span class="op">=</span> []</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    df_train <span class="op">=</span> df.copy()</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> u <span class="kw">in</span> <span class="bu">range</span>(df.shape[<span class="dv">0</span>]):</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        row <span class="op">=</span> df.iloc[u]<span class="op">;</span> idxs <span class="op">=</span> row.index[row.notnull()]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(idxs) <span class="op">&gt;</span> collim:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>            i <span class="op">=</span> random.choice(idxs)<span class="op">;</span> val <span class="op">=</span> df.iloc[u,i]</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>            test_data.append([u,i,val])</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>            df_train.iloc[u,i] <span class="op">=</span> np.nan</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(test_data) <span class="op">&gt;</span> rowlim: <span class="cf">break</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df_train, test_data</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ssvd(df_train,k):</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    lam <span class="op">=</span> <span class="fl">0.02</span> <span class="co"># regularizasyon</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    gamma <span class="op">=</span> <span class="fl">0.01</span> <span class="co"># adim katsayisi</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    m,n <span class="op">=</span> df_train.shape</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    b_u <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>m)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    b_i <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="fl">0.1</span>, size<span class="op">=</span>n)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    p_u <span class="op">=</span> np.random.rand(m,k)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    q_i <span class="op">=</span> np.random.rand(k, n)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    r_ui <span class="op">=</span> np.array(df_train)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> u <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        row <span class="op">=</span> df_train.iloc[u]<span class="op">;</span> idxs <span class="op">=</span> row.index[row.notnull()]</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> idxs:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>            i <span class="op">=</span> <span class="bu">int</span>(i)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>            r_ui_hat <span class="op">=</span> np.dot(q_i[:,i].T,p_u[u,:])</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>            e_ui <span class="op">=</span> r_ui[u,i] <span class="op">-</span> r_ui_hat</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>            q_i[:,i] <span class="op">=</span> q_i[:,i] <span class="op">+</span> gamma <span class="op">*</span> (e_ui<span class="op">*</span>p_u[u,:].T <span class="op">-</span> lam<span class="op">*</span>q_i[:,i])</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>            p_u[u,:] <span class="op">=</span> p_u[u,:] <span class="op">+</span> gamma <span class="op">*</span> (e_ui<span class="op">*</span>q_i[:,i].T <span class="op">-</span> lam<span class="op">*</span>p_u[u,:])</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_i,p_u</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>            </span></code></pre></div>
<p>Kodun önemli bir özelliği şudur, boş yani <code>nan</code> değeri
içeren notlar eğitim sırasında atlanır. SGD seyrek verilerle de
işleyebilen bir eğitim yöntemidir. Bu durumda verinin seyrekliği
(sparsıty) bizim için çok faydalı, çünkü o veri noktalarına
bakılmayacak, <code>row.notnüll()</code> ile boş olmayan öğelerin indis
değerlerini alıyoruz. Bilindiği üzere Movielens, Netflix verileri
oldukça seyrektir, kullanıcı binlerce film içinden onlar, yüzler
bağlamında not verimi yapar, geri kalan değerler boştur.</p>
<p>Pratikte Atlanabilecek Formüller</p>
<p>Üstteki formülasyon içinde genel, film ve kullanıcı yanlılıkları
formül içinde belirtilmiştir. Pratikte sayısal hesap açısından bu
yanlılıkları daha SVD başlamadan önce veriden çıkartmak, ve
yanlılılıkları rasgele güncelleme mantığından çıkartmak daha iyi sonuç
veriyor. Bu çıkartma şöyle yapılır; önce film ortalamaları hesaplanır,
her kullanıcı için o kullanıcının o film ortalamasına olan farkı
(offset) hesaplanır ve bu fark eğitim sırasında not olarak kullanılır.
Böylece SVD tüm yanlılıklardan arınmış bir fark hesabı ile çalışır
sadece, ve sayısal olarak bu daha avantajlıdır. Yoksa ilk atılan adımda
fark çok büyük olacağı için bu fark gradyan inişinin yönününü ağırlıklı
olarak belirleyecektir, ve bu ilk atılan adım hesabın geri kalanına
aşırı baskın çıkabilecektir (dominate). Zaten SVD’nin kuzeni olan
PCA’den de bildiğimiz gibi, bu yöntem ortalamadan arındırılmış
(demeaned) farklarla iş yapıyor.</p>
<p>Niye Azar Azar (Incremental) Hesap?</p>
<p>Madem <span class="math inline">\(U,S,V\)</span> hesabı yapar gibi
<span class="math inline">\(q_i,p_u\)</span> hesaplıyoruz, niye bu
hesabı adım adım yapıyoruz?</p>
<p>Bu sorunun cevabı azar azar hesap yaparken hafızaya sadece o satırı
alarak daha az bellek kullanmamız. Milyarlarca boyut ve milyarlarca
satır işliyor olsaydık bu matrisin tamamını hafızaya almamız mümkün
olmazdı.</p>
<p>Ayrıca olmayan verileri atlamak bu SVD hesabını bir “veri tamamlama
problemi’’ haline getiriyor, piyasadaki neredeyse diğer tüm kütüphaneler
olmayan veriyi sıfır olarak kabul eder. Bu farklı bir problemdir. Bu
diğer SVD yöntemlerinde her ne kadar olan veriyi ortalasak ve sıfır
değerleri sıfır averajla aynı anlama gelmeye başlasa bile, yine de
olmayan veriyi sıfır kabul etmek onu atlamaktan farklıdır [12].</p>
<p>Basit bir örnek</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ssvd</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span>  np.array(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>[[  <span class="fl">5.</span>,   <span class="fl">5.</span>,   <span class="fl">3.</span>,  np.nan,   <span class="fl">5.</span>,   <span class="fl">5.</span>],</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a> [  <span class="fl">5.</span>,  np.nan,   <span class="fl">4.</span>,  np.nan,   <span class="fl">4.</span>,   <span class="fl">4.</span>],</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a> [ np.nan,   <span class="fl">3.</span>,  np.nan,   <span class="fl">5.</span>,   <span class="fl">4.</span>,   <span class="fl">5.</span>],</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a> [  <span class="fl">5.</span>,   <span class="fl">4.</span>,   <span class="fl">3.</span>,   <span class="fl">3.</span>,   <span class="fl">5.</span>,   <span class="fl">5.</span>],</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a> [  <span class="fl">5.</span>,   <span class="fl">5.</span>,  np.nan,  np.nan,  np.nan,   <span class="fl">5.</span>]</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame (d, columns<span class="op">=</span>[<span class="st">&#39;0&#39;</span>,<span class="st">&#39;1&#39;</span>,<span class="st">&#39;2&#39;</span>,<span class="st">&#39;3&#39;</span>,<span class="st">&#39;4&#39;</span>,<span class="st">&#39;5&#39;</span>],</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>       index<span class="op">=</span>[<span class="st">&#39;Ben&#39;</span>,<span class="st">&#39;Tom&#39;</span>,<span class="st">&#39;John&#39;</span>,<span class="st">&#39;Fred&#39;</span>,<span class="st">&#39;Bob&#39;</span>])</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>avg_movies_data <span class="op">=</span> data.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>data_user_offset <span class="op">=</span> data.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x<span class="op">-</span>avg_movies_data, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>q_i,p_u <span class="op">=</span> ssvd.ssvd(data_user_offset,k<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;q_i&#39;</span>,q_i)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&#39;p_u&#39;</span>,p_u)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> <span class="dv">4</span><span class="op">;</span> i <span class="op">=</span> <span class="dv">2</span> <span class="co"># Bob icin tahmin yapalim</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>r_ui_hat <span class="op">=</span> np.dot(q_i[:,i].T,p_u[u,:]) <span class="op">+</span> avg_movies_data.iloc[i] <span class="op">+</span> <span class="op">\</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>           np.nan_to_num(data_user_offset.iloc[u,i])</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (r_ui_hat)</span></code></pre></div>
<pre><code>q_i [[ 0.77564986  0.1768796   0.56224297  0.42870962  0.94723452  0.55772707]
 [ 0.74100171  0.18146607  0.07704434  0.17631144  0.48868432  0.90470742]
 [ 0.70541948  0.28729857  0.091619    0.46184649  0.64498754  0.27410096]]
p_u [[ 0.41549541  0.59784095  0.45227968]
 [ 0.7756159   0.1197948   0.2921504 ]
 [ 0.82274016  0.58432802  0.46501307]
 [ 0.1877425   0.21364635  0.1924456 ]
 [ 0.80554597  0.59254732  0.9775562 ]]
3.92146103113</code></pre>
<p>Not: Artımsal SVD için bir diğer teknik için bkz. <em>Ders
29</em>.</p>
<p>Test Etmek</p>
<p>Test verisi oluşturmak için eğitim verisinde rasgele olarak bazı
notları seçtik, bunları bir kenara kaydederek onların ana matris
içindeki değerini sildik (yerine <code>nan</code> koyarak), ve bir kısmı
silinmiş yeni bir eğitim matrisi yarattık,
<code>create_training_test</code> işlevinde bu görülebilir. Bu işlevde
her kullanıcıdan sadece bir tane not verisi alıyoruz, ve bunu sadece
belli bir sayıda, <code>collim</code> kadar, not vermiş kullanıcılar
için yapıyoruz, ki böylece az sayıda not vermiş kullanıcıların verisini
azaltmamış oluyoruz. Ayrıca belli miktarda, <code>rowlim</code> kadar
test noktası elde edince iş bitti kabul ediyoruz. Test verisi yaratmak
için %80-%20 gibi bir ayrım yapmadık, yani eğitim verişindeki tüm
kullanıcıları ve onların neredeyse tüm verisini eğitim için
kullanıyoruz.</p>
<p>Movielens verisine gelelim, [14] yazısındaki
<code>movielens_prep.py</code> ile gerekli eğitim dosyası üretildiğini
farzederek,</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd, os</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;/opt/Downloads/ml1m/movielens.csv&quot;</span> ,sep<span class="op">=</span><span class="st">&#39;;&#39;</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df.shape)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.iloc[:,<span class="dv">1</span>:<span class="dv">3700</span>] <span class="co"># id kolonunu atla,</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>df.columns <span class="op">=</span> <span class="bu">range</span>(<span class="dv">3699</span>) <span class="co"># kolon degerlerini tekrar indisle</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df.shape)</span></code></pre></div>
<pre><code>(6040, 3731)
(6040, 3699)</code></pre>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>avg_movies <span class="op">=</span> df.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>df_user_offset <span class="op">=</span> df.<span class="bu">apply</span>(<span class="kw">lambda</span> x: x<span class="op">-</span>avg_movies, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (df.iloc[<span class="dv">6</span>,<span class="dv">5</span>], avg_movies.iloc[<span class="dv">5</span>], df_user_offset.iloc[<span class="dv">6</span>,<span class="dv">5</span>])</span></code></pre></div>
<pre><code>4.0 3.87872340426 0.121276595745</code></pre>
<p>Eğitim ve test verisi yaratıyoruz,</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> ssvd</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>df_train, test_data <span class="op">=</span> ssvd.create_training_test(df_user_offset,rowlim<span class="op">=</span><span class="dv">500</span>,collim<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> ( <span class="bu">len</span>(test_data))</span></code></pre></div>
<pre><code>501</code></pre>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>q_i,p_u <span class="op">=</span> ssvd.ssvd(df_train,k<span class="op">=</span><span class="dv">8</span>)</span></code></pre></div>
<p>Test</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> n <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> u,i,real <span class="kw">in</span> test_data:</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    r_ui_hat <span class="op">=</span> np.dot(q_i[:,i].T,p_u[u,:])</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    rmse <span class="op">+=</span> (real<span class="op">-</span>r_ui_hat)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    n <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span> (<span class="st">&quot;rmse&quot;</span>, np.sqrt(rmse <span class="op">/</span> n))</span></code></pre></div>
<pre><code>rmse 0.936136196897</code></pre>
<p>Sonuç fena değil.</p>
<p>Formülasyonun Hikayesi</p>
<p>SGD SVD’nin hikayesi şöyle. Yıl 2009, Netflix Yarışması [11]
katılımcılarından Simon Funk (gerçek adı Brandyn Webb) SGD SVD
yaklaşımını kodlayıp veri üzerinde işletince birden bire sıralamada ilk
3’e fırlar; Webb artık çok ünlü olan blog yazısında [1] yaklaşımı
detayıyla paylaşıp forum’da haberini verince bu haber tam bir bomba
etkisi yaratır. Pek çok kişi yaklaşımı kopyalar, hatta kazanan BellKor
ürününde Webb’in SVD yaklaşımının kullanıldığı biliniyor.</p>
<p>Bu metotun keşfi hangi basamaklardan geçti? Beni meraklandıran
minimizasyon formülasyonun konveks olmamasıydı – genellikle optimizasyon
problemlerinde konveksliğin mevdudiyeti aranır, çünkü bu durumda sonuca
yaklaşmak (convergence) için bir garanti elde edilir. Bu durumda
konvekslik yoktu. “O zaman Webb nasıl rahat bir şekilde SGD
kullanabildi?’’ sorusunun cevabını merak ediyorduk yani. Biraz
araştırınca Bottou ve LeCunn gibi araştırmacıların yazılarına ulaştık
[4]. Onlara göre konvekslik olmaması yapay öğrenim araştırmacılarını
korkutmamalı, eğer sayısal (empiriçally) işleyen bir algoritma var ise,
teorik ispat gelene kadar bu metotun kullanılmasında sakınca yoktur.</p>
<p>Fakat böyle buluşlarda yine de bazı garantiler temel alınmış
olabilir, araştırmacı tamamen balıklama atlayış yapmaz. Webb’in
kendisine bu soruları sorduk ve bize buluşun hangi seviyelerden
geçtiğini anlattı. Geriye sarıyoruz, Webb Netflix’den çok önce yapay
sınır ağlarını araştırmaktadır, ve Sanger, Oja’nın [5,6] yayınlarını baz
alarak kurduğu bir YSA için bir çözüm bulduğunu farkeder. Sayısal
çözümde özdeğer/vektör bulmaya yarayan Üstel Metotun (power method) bir
şeklini kullanmıştır, ki Sanger’in Genel Hebbian Algorıtmasının (GHA)
üstel metot ile bağlantıları var, ve bu GHA yayınında “eğitilince’’
özdeğer/vektör ve PCA hesabı yapabilen bir YSA’dan bahsediliyor. Daha
önemlisi GHA 1 olasılıkla (yani kesin) bu sonuçlara erişebiliyor.</p>
<p>Daha sonra Webb bu çözümü arkadaşı Gorrell ile tartışırken Gorrell
ona problem formülasyonunun SVD olarak görülebileceğini söyler.
Bilindiği gibi özdeğer/vektör hesabı ile SVD yakın akraba sayılır. İkili
bu bağlamda birkaç yayın da yaparlar. Daha sonra Netflix yarışması
başladığında Webb çözüm için gradyan baz alarak SGD kullanabileceğini
farkediyor, ki SGD ile üstel metot arasında teorik bağlantı var [7]. Ve
sonuç olarak SGD SVD metotu ortaya çıkıyor.</p>
<p>Tabii ki “SGD SVD ne kadar SVD sayılır?’’ gibi bir soru sorulabilir.
Evet, regülarizasyon bazı gayrı lineerlikleri probleme sokar, zaten bu
çözümü”yaklaşıksal’’ yapan kısım da budur. Fakat belli şartlarda,
regülarizasyon olmasa çözüm tam SVD olacaktır. Bu buluşun püf noktası bu
bilgide, ve üstteki teorik benzerliklerde, onları biliyor olmakta
yatıyor. Eğer bunlar biliniyor ise, ve sağlam lineer cebir bilgisi ile
gerektiği zaman onları ne kadar esnetebileceğimizi biliriz. Konu
hakkındaki daha fazla detay [10]’da bulunabilir.</p>
<p>Tavsiye</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>movies <span class="op">=</span> pd.read_csv(<span class="st">&quot;/opt/Downloads/ml1m/movies.dat&quot;</span>,engine<span class="op">=</span><span class="st">&#39;python&#39;</span>,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>                     sep<span class="op">=</span><span class="st">&#39;::&#39;</span>,names<span class="op">=</span>[<span class="st">&#39;idx&#39;</span>,<span class="st">&#39;movie&#39;</span>,<span class="st">&#39;type&#39;</span>],encoding<span class="op">=</span><span class="st">&#39;latin-1&#39;</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_user(u):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> {}</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(df_user_offset.shape[<span class="dv">1</span>]):</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        r_ui_hat <span class="op">=</span> np.dot(q_i[:,i].T,p_u[u,:])</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        res[i] <span class="op">=</span> <span class="bu">float</span>(r_ui_hat) <span class="op">+</span> avg_movies.iloc[i] <span class="op">+</span> df_user_offset.iloc[u,i]</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> <span class="bu">sorted</span>(res.items(), key<span class="op">=</span><span class="kw">lambda</span> x:x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">Tavsiyeler</span><span class="ch">\n\n</span><span class="st">&quot;</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j,(si,sm) <span class="kw">in</span> <span class="bu">enumerate</span>(res):</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span> (movies[movies[<span class="st">&#39;idx&#39;</span>] <span class="op">==</span> si][<span class="st">&#39;movie&#39;</span>])</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> j <span class="op">==</span> <span class="dv">10</span>: <span class="cf">break</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span> (<span class="st">&quot;</span><span class="ch">\n\n</span><span class="st">Mevcut Filmler</span><span class="ch">\n\n</span><span class="st">&quot;</span>)</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    row <span class="op">=</span> df.iloc[u]</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>    idxs <span class="op">=</span> row.index[row.notnull()]</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j,idx <span class="kw">in</span> <span class="bu">enumerate</span>(idxs):</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span> (movies[movies[<span class="st">&#39;idx&#39;</span>] <span class="op">==</span> idx][<span class="st">&#39;movie&#39;</span>], row[idx])</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> j <span class="op">==</span> <span class="dv">10</span>: <span class="cf">break</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>eval_user(<span class="dv">300</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>eval_user(<span class="dv">2900</span>)</span></code></pre></div>
<pre><code>Tavsiyeler


3441    Frequency (2000)
Name: movie, dtype: object
105    Muppet Treasure Island (1996)
Name: movie, dtype: object
Series([], dtype: object)
2    Grumpier Old Men (1995)
Name: movie, dtype: object
4    Father of the Bride Part II (1995)
Name: movie, dtype: object
1    Jumanji (1995)
Name: movie, dtype: object
3    Waiting to Exhale (1995)
Name: movie, dtype: object
0    Toy Story (1995)
Name: movie, dtype: object
5    Heat (1995)
Name: movie, dtype: object
6    Sabrina (1995)
Name: movie, dtype: object
7    Tom and Huck (1995)
Name: movie, dtype: object


Mevcut Filmler


Series([], dtype: object) 5.0
0    Toy Story (1995)
Name: movie, dtype: object 3.0
1    Jumanji (1995)
Name: movie, dtype: object 4.0
3    Waiting to Exhale (1995)
Name: movie, dtype: object 3.0
4    Father of the Bride Part II (1995)
Name: movie, dtype: object 5.0
9    GoldenEye (1995)
Name: movie, dtype: object 4.0
15    Casino (1995)
Name: movie, dtype: object 4.0
19    Money Train (1995)
Name: movie, dtype: object 5.0
20    Get Shorty (1995)
Name: movie, dtype: object 3.0
23    Powder (1995)
Name: movie, dtype: object 4.0
30    Dangerous Minds (1995)
Name: movie, dtype: object 5.0


Tavsiyeler


Series([], dtype: object)
0    Toy Story (1995)
Name: movie, dtype: object
1    Jumanji (1995)
Name: movie, dtype: object
2    Grumpier Old Men (1995)
Name: movie, dtype: object
3    Waiting to Exhale (1995)
Name: movie, dtype: object
4    Father of the Bride Part II (1995)
Name: movie, dtype: object
5    Heat (1995)
Name: movie, dtype: object
6    Sabrina (1995)
Name: movie, dtype: object
7    Tom and Huck (1995)
Name: movie, dtype: object
8    Sudden Death (1995)
Name: movie, dtype: object
9    GoldenEye (1995)
Name: movie, dtype: object


Mevcut Filmler


19    Money Train (1995)
Name: movie, dtype: object 4.0
23    Powder (1995)
Name: movie, dtype: object 4.0
37    It Takes Two (1995)
Name: movie, dtype: object 3.0
45    How to Make an American Quilt (1995)
Name: movie, dtype: object 5.0
48    When Night Is Falling (1995)
Name: movie, dtype: object 5.0
66    Two Bits (1995)
Name: movie, dtype: object 5.0
78    Juror, The (1996)
Name: movie, dtype: object 4.0
104    Nobody Loves Me (Keiner liebt mich) (1994)
Name: movie, dtype: object 5.0
118    Race the Sun (1996)
Name: movie, dtype: object 4.0
134    From the Journals of Jean Seberg (1995)
Name: movie, dtype: object 2.0
142    Brothers McMullen, The (1995)
Name: movie, dtype: object 3.0</code></pre>
<p>Kaynaklar</p>
<p>[1] Funk, <em>Netflix Update: Try This at Home</em>, <a
href="http://sifter.org/~simon/journal/20061211.html">http://sifter.org/~simon/journal/20061211.html</a></p>
<p>[2] Koren, Bell, <em>Recommender Systems Handbook</em>, <a
href="http://www.cs.bme.hu/nagyadat/Recommender_systems_handbook.pdf">http://www.cs.bme.hu/nagyadat/Recommender_systems_handbook.pdf</a></p>
<p>[3] Koren, <em>MATRIX FACTORIZATION TECHNIQUES FOR RECOMMENDER
SYSTEMS </em>, <a
href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf">https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf</a></p>
<p>[4] LeCun, <em>Who is Afraid of Non-Convex Loss Functions?</em>, <a
href="http://videolectures.net/eml07_lecun_wia">http://videolectures.net/eml07_lecun_wia</a></p>
<p>[5] Sanger, <em>Optimal Unsupervised Learning in a Single-Layer
Linear Feedforward Neural Network </em>, <a
href="http://courses.cs.washington.edu/courses/cse528/09sp/sanger_pca_nn.pdf">http://courses.cs.washington.edu/courses/cse528/09sp/sanger_pca_nn.pdf</a>
%</p>
<p>[6] Oja, <em>A Simplified Neuron Model as a Principal Component
Analyzer</em>, <a
href="http://users.ics.aalto.fi/oja/Oja1982.pdf">http://users.ics.aalto.fi/oja/Oja1982.pdf</a></p>
<p>[7] Cotter, <em>Stochastic Optimization for Machine Learning</em>, <a
href="http://arxiv.org/pdf/1308.3509">http://arxiv.org/pdf/1308.3509</a></p>
<p>[8] Touchette, <em>Introduction to Numerical Computing</em>, <a
href="http://www.maths.qmul.ac.uk/~wj/MTH5110/notes/MAS235_lecturenotes1.pdf">http://www.maths.qmul.ac.uk/~wj/MTH5110/notes/MAS235_lecturenotes1.pdf</a></p>
<p>[10] Stack Exchange, <em>Gradient Descent on Non-Convex Function
Works But How?</em>, <a
href="http://math.stackexchange.com/questions/649701/gradient-descent-on-non-convex-function-works-but-how">http://math.stackexchange.com/questions/649701/gradient-descent-on-non-convex-function-works-but-how</a></p>
<p>[11] Netflix, <em>Netflix Prize</em>, <a
href="http://www.netflixprize.com">http://www.netflixprize.com</a></p>
<p>[12] Gleich, <em>SVD on the Netflix matrix</em>, <a
href="https://dgleich.wordpress.com/2013/10/19/svd-on-the-netflix-matrix">https://dgleich.wordpress.com/2013/10/19/svd-on-the-netflix-matrix</a></p>
<p>[14] Bayramlı, Istatistik, <em>SVD, Toplu Tavsiye</em></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
