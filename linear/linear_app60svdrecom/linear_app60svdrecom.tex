\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
SVD, Toplu Tavsiye (Collaborative Filtering) 

Diyelim ki Star Trek (ST) dizisini ne kadar beðendiðini 4 tane
kullanýcý sezonlara göre iþaretlemiþ. Bu örnek veriyi alttaki gibi
gösterelim.

\begin{minted}[fontsize=\footnotesize]{python}
from pandas import *

d =  np.array(
     [[5, 5, 0, 5],
     [5, 0, 3, 4],
     [3, 4, 0, 3],
     [0, 0, 5, 3],
     [5, 4, 4, 5],
     [5, 4, 5, 5]])

data = DataFrame (d.T,
    columns=['S1','S2','S3','S4','S5','S6'],
    index=['Ben','Tom','John','Fred'])
print data
\end{minted}

\begin{verbatim}
      S1  S2  S3  S4  S5  S6
Ben    5   5   3   0   5   5
Tom    5   0   4   0   4   4
John   0   3   0   5   4   5
Fred   5   4   3   3   5   5
\end{verbatim}

Veriye göre Tom, ST dizisinin 3. sezonunu 4 seviyesinde sevmiþ. 0
deðeri o sezonun seyredilmediðini gösteriyor.

Toplu Tavsiye algoritmalarý verideki diðer kiþilerin bir ürünü, diziyi, vs. ne
kadar beðendiðinin verisinin diðer "benzer" kiþilere tavsiye olarak sunabilir,
ya da ondan önce, bir kiþinin daha almadýðý ürünü, seyretmediði sezonu,
dinlemediði müziði ne kadar beðeneceðini tahmin eder. 2006 yýlýnda yapýlan ünlü
Netflix yarýþmasýnýn amacý buydu mesela.

Peki benzerliðin kriteri nedir, ve benzerlik nelerin arasýnda ölçülür?

Benzerlik, ürün seviyesinde, ya da kiþi seviyesinde yapýlabilir. Eðer ürün
seviyesinde ise, tek bir ürün için tüm kullanýcýlarýn verdiði nota
bakýlýr. Eðer kullanýcý seviyesinde ise, tek kullanýcýnýn tüm ürünlere
verdiði beðeni notlarý vektörü kullanýlýr. 1. sezonu örnek kullanalým,o
sezonu beðenen kiþilere o sezona benzer diðer sezonlar tavsiye
edilebilir. Kiþiden hareketle, mesela John'a benzeyen diðer kiþiler
bulunarak onlarýn beðendiði ürünler John'a tavsiye edilebilir.

Ürün ya da kiþi bazýnda olsun, benzerliði hesaplamak için bir benzerlik
ölçütü oluþturmalýyýz. Genel olarak bu benzerlik ölçütünün 0 ile 1 arasýnda
deðiþen bir sayý olmasýný tercih edilir ve tavsiye mantýðýnýn geri kalaný
bu ölçütü baz alacaktýr. Elimizde beðeni notlarýný taþýyan $A,B$ vektörleri
olabilir, ve bu vektörlerin içinde beðeni notlarý olacaktýr. Vektör
içindeki sayýlarý baz alan benzerlik çeþitleri þöyledir:

Öklit Benzerliði (Euclidian Similarity)

Bu benzerlik $1 / (1+mesafe)$ olarak hesaplanýr. Mesafe karelerin
toplamýnýn karekökü (yani Öklitsel mesafe, ki isim buradan
geliyor). Bu yüzden mesafe 0 ise (yani iki "þey" arasýnda hiç mesafe
yok, birbirlerine çok yakýnlar), o zaman hesap 1 döndürür (mükemmel
benzerlik). Mesafe arttýkça bölen büyüdüðü için benzerlik sýfýra yaklaþýr. 

Pearson Benzerliði

Bu benzerliðin Öklit'ten farklýlýðý, sayý büyüklüðüne hassas olmamasýdýr.
Diyelim ki birisi her sezonu 1 ile beðenmiþ, diðeri 5 ile beðenmiþ, bu iki
vektörün Pearson benzerliðine göre birbirine eþit çýkar. Pearson -1 ile +1
arasýnda bir deðer döndürür, alttaki hesap onu normalize ederek 0 ile 1 arasýna
çeker.

Kosinüs Benzerliði (Cosine Similarity)

Ýki vektörü geometrik vektör olarak görür ve bu vektörlerin arasýnda
oluþan açýyý (daha doðrusu onun kosinüsünü) farklýlýk ölçütü olarak
kullanýr.

$$
\cos\theta = \frac{A \cdot B}{||A||||B||}
$$

\begin{minted}[fontsize=\footnotesize]{python}
from numpy import linalg as la
def euclid(inA,inB):
    return 1.0/(1.0 + la.norm(inA - inB))

def pearson(inA,inB):
    if len(inA) < 3 : return 1.0
    return 0.5+0.5*np.corrcoef(inA, inB, rowvar = 0)[0][1]

def cos_sim(inA,inB):
    num = float(np.dot(inA.T,inB))
    denom = la.norm(inA)*la.norm(inB)
    return 0.5+0.5*(num/denom)
\end{minted}

\begin{minted}[fontsize=\footnotesize]{python}
print np.array(data.ix['Fred'])
print np.array(data.ix['John'])
print np.array(data.ix['Ben'])
print pearson(data.ix['Fred'],data.ix['John'])
print pearson(data.ix['Fred'],data.ix['Ben'])
\end{minted}

\begin{verbatim}
[5 4 3 3 5 5]
[0 3 0 5 4 5]
[5 5 3 0 5 5]
0.551221949943
0.906922851283
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
print cos_sim(data.ix['Fred'],data.ix['John'])
print cos_sim(data.ix['Fred'],data.ix['Ben'])
\end{minted}

\begin{verbatim}
0.898160909799
0.977064220183
\end{verbatim}

Þimdi tavsiye mekanýðine gelelim. En basit tavsiye yöntemi, mesela
kiþi bazlý olarak, bir kiþiye en yakýn diðer kiþileri bulmak (matrisin
tamamýna bakarak) ve onlarýn beðendikleri ürünü istenilen kiþiye
tavsiye etmek. Benzerlik için üstteki ölçütlerden birini kullanmak.

Fakat belki de elimizde çok fazla ürün, ya da kullanýcý var. Bir boyut
azaltma iþlemi yapamaz mýyýz?

Evet. SVD yöntemi burada da iþimize yarar. 

$$ A = USV  $$

elde edeceðimiz için, ve $S$ içindeki en büyük deðerlere tekabül eden
$U,V$ deðerleri sýralanmýþ olarak geldiði için $U,V$'nin en baþtaki
deðerlerini almak bize "en önemli" bloklarý verir. Bu en önemli kolon
ya da satýrlarý alarak azaltýlmýþ bir boyut içinde benzerlik hesabý
yapmak iþlemlerimizi hýzlandýrýr. Bu azaltýlmýþ boyutta kümeleme
algoritmalarýný devreye sokabiliriz; $U$'nun mesela en önemli iki
kolonu bize iki boyuttaki sezon kümelerini verebilir, $V$'nin en
önemli iki (en üst) satýrý bize iki boyutta bir kiþi kümesi verebilir.

O zaman beðeni matrisi üzerinde SVD uygulayalým,

\begin{minted}[fontsize=\footnotesize]{python}
from numpy.linalg import linalg as la
U,Sigma,V=la.svd(data, full_matrices=False)
print data.shape
print U.shape, Sigma.shape, V.shape
u = U[:,:2]
vt=V[:2,:].T
print 'u', u
print 'vt', vt
print u.shape, vt.shape
\end{minted}

\begin{verbatim}
(4, 6)
(4, 4) (4,) (4, 6)
u [[-0.57098887 -0.22279713]
 [-0.4274751  -0.51723555]
 [-0.38459931  0.82462029]
 [-0.58593526  0.05319973]]
vt [[-0.44721867 -0.53728743]
 [-0.35861531  0.24605053]
 [-0.29246336 -0.40329582]
 [-0.20779151  0.67004393]
 [-0.50993331  0.05969518]
 [-0.53164501  0.18870999]]
(4, 2) (6, 2)
\end{verbatim}

degerleri elimize gecer. U ve VT matrisleri 

\begin{minted}[fontsize=\footnotesize]{python}
def label_points(d,xx,yy,style):
    for label, x, y in zip(d, xx, yy):
        plt.annotate(
            label, 
            xy = (x, y), xytext = style,
            textcoords = 'offset points', ha = 'right', va = 'bottom',
            bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),
            arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))

plt.plot(u[:,0],u[:,1],'r.')
label_points(data.index, u[:, 0], u[:, 1],style=(-10, 30))
plt.plot(vt[:,0],vt[:,1],'b.')
label_points(data.columns, vt[:, 0], vt[:, 1],style=(20, 20))
plt.savefig('svdrecom_1.png')
\end{minted}

\includegraphics[height=6cm]{svdrecom_1.png}

Çok güzel! SVD bize ürün bazýnda sezon 5 ve 6'nin bir küme
oluþturduðunu, Ben ve Fred'in de kiþi bazýnda ayrý bir küme olduðunu
gösterdi.

Azaltýlmýþ boyutlarý nasýl kullanýrýz? Yeni bir kiþiyi (mesela Bob)
ele alýnca, bu kiþinin verisini öncelikle aynen diðer verilerin
indirgendiði gibi azaltýlmýþ boyuta "indirgememiz" gerekiyor. Çünkü
artýk iþlem yaptýðýmýz boyut orasý. Peki bu indirgemeyi nasýl yaparýz?
SVD genel formülünü hatýrlarsak,

$$ A = USV $$

Azaltilmis ortamda

$$ A = U_k S_k V_k $$

Diyelim ki gitmek istediðimiz nokta azaltýlmýþ $U$, o zaman $U_k$'yi tek
baþýna býrakalým (dikkat, mesela $V$'nin tersini aldýk, fakat bir matrisin
tersini almak için o matrisin kare matris olmasý gerekir, eðer kare
deðilse, ters alma iþlemi taklit ters alma iþlemi -pseudoinverse- ile
gerçekleþtirilir, daha fazla detay için [6])

$$ A V_k^{-1} = U_k S V_k V_k^{-1} $$

$U_k,V_k$ matrisleri birimdik (orthonormal), o zaman $V_k^{-1}V_k = I$
olacak, yani yokolacak

$$ A V_k^{-1} = U_k S  $$

Benzer þekilde

$$  A V_k^{-1} S^{-1} = U_k $$

Çok fazla ters alma iþlemi var, her iki tarafýn devriðini alalým

$$ (S^{-1})^T (V_k^{-1})^T A^T = U_k^T $$

$V_k^{-1} = V_k^T$ olduðunu biliyoruz. Nasýl? Çünkü $ V_k^TV_k = I $, ayný
þekilde $ V_k^{-1}V_k = I $. Ters alma iþleminin özgünlüðü (üniqueness)
sebebiyle $V_k^{-1} = V_k^T$ olmak zorundadýr $\Box$

Demek ki üstteki formül devriðin devriðini almak demektir, yani tekrar baþa
dönmüþ oluyoruz, demek ki $V_k$ deðiþmeden kalýyor

$$ (S^{-1})^T V_k A^T = U_k^T $$

$S$ ise köþegen matris, onun tersi yine köþegen, köþegen matrisin devriði
yine kendisi

$$ S^{-1} V_k A^T = U_k^T $$

Bazý kod ispatlarý, $u$'nun birimdik olmasý:

\begin{minted}[fontsize=\footnotesize]{python}
print np.dot(u.T,u)
\end{minted}

\begin{verbatim}
[[  1.00000000e+00   4.83147593e-18]
 [  4.83147593e-18   1.00000000e+00]]
\end{verbatim}

Doðal olarak \verb!1e-17! gibi bir sayý sýfýra çok yakýn, yani sýfýr kabul
edilebilir. Devrik ve tersin ayný olduðunu gösterelim: Ýki matrisi birbirinden
çýkartýp, çok küçük bir sayýdan büyüklüðe göre filtreleme yapalým, ve sonuç
içinde bir tane bile True olup olmadýðýný kontrol edelim,

\begin{minted}[fontsize=\footnotesize]{python}
print not any(U.T-la.inv(U) > 1e-15)
\end{minted}

\begin{verbatim}
True
\end{verbatim}

Yeni Bob verisi 

\begin{minted}[fontsize=\footnotesize]{python}
bob = np.array([5,5,0,0,0,5]) 
\end{minted}

O zaman 

\begin{minted}[fontsize=\footnotesize]{python}
print bob.T.shape
print u.shape
S_k = np.eye(2)*Sigma[:2]
bob_2d = np.dot(np.dot(la.inv(S_k),vt.T),bob.T)
print bob_2d
\end{minted}

\begin{verbatim}
(6,)
(4, 2)
[-0.37752201 -0.08020351]
\end{verbatim}

Not: \verb!bob.T! üstteki formüldeki $A^T$ yerine geçecek; formülü tekrar
düzenlerken $A$ üzerinden iþlem yaptýk, fakat formülü ``$A$'ya eklenen
herhangi bir yeni satýr'' olarak ta görebiliriz, ki bu örneðimizde Bob'un
verisi olurdu. 

Üstte \verb!eye! ve \verb!Sigma! ile ufak bir takla attýk, bunun sebebi
\verb!svd! çaðrýsýndan gelen \verb!Sigma!  sonucunun bir vektör olmasý ama
üstteki iþlem için köþegen bir "matrise" ihtiyacýmýz olmasý. Eðer birim
(identity) matrisini alýp onu \verb!Sigma! ile çarparsak, bu köþegen
matrisi elde ederiz.

Þimdi mesela kosinüs benzerliði kullanarak bu izdüþümlenmiþ yeni
vektörün hangi diðer vektörlere benzediðini bulalým.

\begin{minted}[fontsize=\footnotesize]{python}
for i,user in enumerate(u):
   print data.index[i],cos_sim(user,bob_2d)
\end{minted}

\begin{verbatim}
Ben 0.993397525045
Tom 0.891664622942
John 0.612561691287
Fred 0.977685793579
\end{verbatim}

Sonuca göre yeni kullanýcý Bob, en çok Ben ve Fred'e benziyor. Sonuca
eriþtik! Artýk bu iki kullanýcýnýn yüksek not verdiði ama Bob'un hiç
not vermediði sezonlarý alýp Bob'a tavsiye olarak sunabiliriz.

SVD ile Veriyi Oluþturmak

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd
import numpy.linalg as lin
import numpy as np
import scipy.sparse.linalg as lin
import scipy.sparse as sps

d =  np.array(
[[ 5.,  5.,  3.,  np.nan,  5., 5.],
 [ 5.,  np.nan,  4.,  np.nan,  4., 4.],
 [ np.nan,  3.,  np.nan,  5.,  4., 5.],
 [ 5.,  4.,  3.,  3.,  5., 5.],
 [ 5.,  5.,  np.nan,  np.nan,  np.nan, 5.]
])
users = ['Ben','Tom','John','Fred','Bob']
seasons = ['0','1','2','3','4','5']
data = pd.DataFrame (d, columns=seasons,index=users)
print data
avg_movies_data = data.mean(axis=0)
print avg_movies_data
data_user_offset = data.apply(lambda x: x-avg_movies_data, axis=1)
A = sps.coo_matrix(np.nan_to_num(np.array(data_user_offset)))
U,S,VT = lin.svds(A,k=3)
def predict(u,i):
    offset = np.dot(U[u,:],VT[:,i]) 
    r_ui_hat = offset + avg_movies_data.ix[i] 
    return r_ui_hat, offset

print 'Bob', predict(users.index('Bob'),2)
print 'Tom', predict(users.index('Tom'),1)
\end{minted}

\begin{verbatim}
       0   1   2   3   4  5
Ben    5   5   3 NaN   5  5
Tom    5 NaN   4 NaN   4  4
John NaN   3 NaN   5   4  5
Fred   5   4   3   3   5  5
Bob    5   5 NaN NaN NaN  5
0    5.000000
1    4.250000
2    3.333333
3    4.000000
4    4.500000
5    4.800000
dtype: float64
Bob (3.3115641365499888, -0.021769196783344661)
Tom (4.295419370813935, 0.045419370813934629)
\end{verbatim}

Alternatif Yöntem

Bir diðer yöntem [1] yeni Bob verisi $y$'yi alýp

$$ z = VV^Ty $$

olarak $z$'ye çevirmek. Bu durumda aslýnda cebirsel olarak hiçbir þey
yapmamýþ oluyoruz,

$$ z = VV^Ty = Iy = y$$

ve iteratif sayýsal çoðu algoritmanýn temelini de bu oluþturuyor. Kavramsal
olarak $y$'yi alýp $V$ uzayýna ``yansýtýyoruz''. Daha kavramsal olarak kullanýcý
seçimlerini temsil eden veri için $V$ bir ``kordinat sistemi'' oluþturmuþtur
(SVD'nin doðal sonucu olarak) ve her veri noktasý bu kordinat sistemi, bu bazýn
vektörlerinin bir kombinasyonu olarak temsil edilebilir durumdadýr (SVD için
kullanýlan veriden bahsediyoruz). Bu durumda yeni veriyi oraya yansýtmak doðal
bir iþlemdir. Tabii yansýtýp sonra geri geliyoruz, yani baþlangýçtaki boyutlara
/ hale dönüyoruz, bu olurken ayný zamanda Bob verisinin boþ noktalarý en makul
tahminlerle ``doldurulmuþ'' oluyor.

\begin{minted}[fontsize=\footnotesize]{python}
from numpy.linalg import linalg as la
U,Sigma,V=la.svd(data, full_matrices=False)
print data.shape
print U.shape, Sigma.shape, V.shape
u = U[:,:2]
vt=V[:2,:].T
print data
print 'bob', bob
y = bob
for i in range(3):
    z = np.dot(vt,np.dot(vt.T,y))
    print z
    z[y>0] = y[y>0]
print z
\end{minted}

\begin{verbatim}
(4, 6)
(4, 4) (4,) (4, 6)
      S1  S2  S3  S4  S5  S6
Ben    5   5   3   0   5   5
Tom    5   0   4   0   4   4
John   0   3   0   5   4   5
Fred   5   4   3   3   5   5
bob [5 5 0 0 0 5]
[ 3.26615993  2.27206826  2.16256132  1.04609626  3.37952362  3.45858088]
[ 3.26615993  2.27206826  2.16256132  1.04609626  3.37952362  3.45858088]
[ 3.26615993  2.27206826  2.16256132  1.04609626  3.37952362  3.45858088]
[ 5.          5.          2.16256132  1.04609626  3.37952362  5.        ]
\end{verbatim}

Sonuca göre Bob büyük ihtimalle S5'i sevecektir, not tahminleri arasýnda en
yüksek puan orada tahmin edilmiþ, ki bu daha önceki Ben ve Fred benzerlik
tahminleri ile uyumlu. 

Not: Döngüde $z$'nin hep ayný satýr olmasý kafa karýþýklýðý yaratmasýn, bu
çok ufak bir veri seti, daha büyük veri setlerdinde bu deðiþim
görülecektir. 

Ýteratif iþlem sözde kod (pseudocode) olarak,

Algoritma \verb!imputed_svd!
\begin{enumerate}
  \item $\code{while }$ $z$'deki deðiþim azalýncaya kadar (convergence)
  \item $z = VV^Ty$ 
  \item  $y$'nin ilk halindeki bilinen noktalarý alýp $z$'ye kopyala
\end{enumerate}

En son projemizde üstteki iþlemin en iyi sonuçlar verdiðini gözlemledik. 

Movielens 1M Verisi

Bu veri seti 6000 kullanýcý tarafýndan yaklaþýk 4000 tane filme
verilen not / derece (rating) verisini içeriyor, 1 milyon tane not
verilmiþ, yani 4000 * 6000 = 24 milyon olasýlýk içinde sadece 1 milyon
veri noktasý dolu. Bu oldukça seyrek bir matris demektir.

Verinin ham hali diðer ders notlarýmýzý içeren üst dizinlerde var, veriyi
SVD ile kullanýlýr hale getirmek için bu dizindeki \verb!movielens_prep.py!
adlý script kullanýlýr. Ýþlem bitince \verb!movielens.csv! adlý bir dosya
script'te görülen yere yazýlacak. Bu dosyada olmayan derecelendirmeler,
verilmemiþ notlar boþ olacaktýr. Bu boþluklarý sýfýrlarsak, seyrek matrisi
o noktalarý atlar. Ardýndan bu seyrek matris üzerinde seyrek SVD
iþletilebilir. Bu normal SVD'den daha hýzlý iþleyecektir.

Tavsiye kodlamamýz için yazýnýn baþýnda anlatýlan tekniði kullanacaðýz, film
verisi üzerinde boyut azaltýlmasý yapýlacak, benzer kullanýcý bulunacak, ve
herhangi bir yeni kullanýcý / film kombinasyonu için bu diðer benzer
kullanýcýnýn o filme verdiði not baz alýnacak.

Veriyi eðitim ve test olarak iki parçaya böleceðiz. SVD eðitim bölümü
üzerinde iþletilecek.

Bu baðlamda, önemli bir diðer konu eksik veri noktalarýnýn SVD
sonuçlarýný nasýl etkileyeceði. Sonuçta eksik yerler \verb!nan!,
oradan sýfýr yapýlýp ardýndan seyrek matris kodlamasý üzerinden
"atlanýyor" olabilir, fakat bu deðerler atlanýyor (yani hýzlý
iþleniyor, depolanýyor) olsa bile, onlarýn sýfýr olmasýnýn bir anlamý
yok mudur? Evet vardýr. Not bakýmýndan sýfýr da bir not'tur, ve bu
sebeple sonuçlarý istenmeyen biçimde etkileyebilir.

O zaman mevcut veriyi öyle bir deðiþtirelim ki verilmemiþ notlar, yani
sýfýr deðerleri sonucu fazla deðiþtirmesin.

Bunu yapmanýn yollarýndan biri her film için bir ortalama not deðeri
hesaplamak, ve bu ortalama deðeri o filme verilen tüm not
deðerlerinden çýkartmaktýr. Bu iþleme "sýfýr çevresinde merkezlemek"
ismi de verilir, hakikaten mesela film j için ortalama 3 ise, 5 deðeri
2, 3 deðeri sýfýr, 2 deðeri -1 haline gelecektir. Bu bir ilerlemedir
çünkü ortalama 3 deðeri zaten bizim için "önemsiz" bir deðerdir,
tavsiye problemi baðlamýnda bizim en çok ilgilendiðimiz sevilen
filmler, ve sevilmeyen filmler. Bu deðerler sýrasýyla artý ve eksi
deðerlere dönüþecekler, ve SVD bu farklýlýðý matematiksel olarak
kullanabilme yeteneðine sahip.

Altta Pandas \verb!mean! çaðrýsý ile bu iþlemin yapýldýðýný görüyoruz, dikkat,
Pandas dataframe içinde \verb!nan! deðerleri olacaktýr, ve Pandas bu deðerleri
atlamasý gerektiðini bilir, yani bu deðerler ortalamaya etki etmez. Ardýndan
merkezleme iþlemi eðitim verisi üzerinde uygulanýyor.

\begin{minted}[fontsize=\footnotesize]{python}
import pandas as pd, os
import scipy.sparse as sps
df = pd.read_csv("%s/Downloads/movielens.csv" % os.environ['HOME'] ,sep=';')
print df.shape
df = df.ix[:,1:] # id kolonunu atla
df = df.ix[:,:3700] # sadece filmleri al
df_train = df.copy().ix[:5000,:]
df_test = df.copy().ix[5001:,:]
df_train[np.isnan(df_train)] = 0.0
movie_avg_rating = np.array(df_train.mean(axis=0))
df_train = df_train - movie_avg_rating
dfs_train = sps.coo_matrix(df_train)

df_train = np.array(df_train)
df_test = np.array(df_test)

print df_train.shape
print df_test.shape

__top_k__ = 10
import scipy.sparse.linalg as slin
import scipy.linalg as la
U,Sigma,V=slin.svds(dfs_train,k=__top_k__)
print U.shape, Sigma.shape, V.shape
Sigma = np.diag(Sigma)
\end{minted}

\begin{verbatim}
(6040, 3731)
(5001, 3700)
(1039, 3700)
(5001, 10) (10,) (10, 3700)
\end{verbatim}

Altta test verisi üzerinde satýr satýr ilerliyoruz, ve her satýr (test
kullanýcýsý) içinde film film ilerliyoruz. "Verilmiþ bir not" arýyoruz
(çoðunlukla not verilmemiþ oluyor çünkü), ve bulduðumuz zaman artýk
elimizde test edebileceðimiz bir þey var, o notu "sýfýrlayýp" vektörün
geri kalanýný azaltýlmýþ boyuta yansýtýyoruz, ve sonra o boyuttaki tüm
diðer $U$ vektörleri içinde arama yapýyoruz, en yakýn diðer
kullanýcýyý buluyoruz ve onun bu filme verdiði notu tahminimiz olarak
kullanýyoruz.

Altta eðer bulunan diðer kullanýcý o filme not vermemiþse, basitleþtirme
amaçlý olarak, o filmi atladýk. Gerçek dünya þartlarýnda filme not vermiþ
ve yakýn olan (en yakýn olmasa da) ikinci, üçüncü kullanýcýlar bulunup
onlarýn notu kullanýlabilir. Hatta en yakýn k tane kullanýcýnýn ortalamasý
alýnabilir (o kullanýcýlar kNN gibi bir metotla bulunur belki), vs.

\begin{minted}[fontsize=\footnotesize]{python}
def euclid(inA,inB):
    return 1.0/(1.0 + la.norm(inA - inB))
    
rmse = 0; n = 0
for i,test_row in enumerate(df_test):
    for j, test_val in enumerate(test_row):
        # nan olmayan bir not buluncaya kadar ara
        if np.isnan(test_val): continue	
        # bulduk, test satirini tamamen kopyala ve bulunan notu silerek
        # onu nan / sifir haline getir cunku yansitma (projection) oncesi
        # o notu 'bilmiyormus gibi' yapmamiz lazim. 
	curr = test_row.copy()
        curr[j] = np.nan
        curr[np.isnan(curr)] = 0.

	proj_row = np.dot(np.dot(la.inv(Sigma),V),curr)

	sims = np.array(map(lambda x: euclid(x, proj_row), U[:,:__top_k__]))
	isim = np.argmax(sims)

	# eger bulunan kullanici o filme not vermemisse atla
	if np.isnan(df.ix[isim, j]): continue

	# egitim verisinde notlar sifir etrafinda ortalanmis, tekrar
	# normal haline dondur
	est = df_train[isim, j]+movie_avg_rating[j]

	# gercek not
	real = df_test[i, j]

	print i, 'icin en yakin', isim, 'urun',j, 'icin oy', est, 'gercek', real
        rmse += (real-est)**2
        n += 1
	break # her kullanici icin tek film test et
    if i == 20: break # 20 kullanici test et

print "rmse", np.sqrt(rmse / n)
\end{minted}

\begin{verbatim}
0 icin en yakin 1903 urun 144 icin oy 5.0 gercek 5.0
1 icin en yakin 239 urun 144 icin oy 5.0 gercek 5.0
2 icin en yakin 2045 urun 844 icin oy 4.0 gercek 4.0
3 icin en yakin 4636 urun 0 icin oy 3.0 gercek 4.0
4 icin en yakin 139 urun 845 icin oy 4.0 gercek 5.0
5 icin en yakin 427 urun 1107 icin oy 4.0 gercek 5.0
6 icin en yakin 3620 urun 31 icin oy 4.0 gercek 4.0
7 icin en yakin 1870 urun 0 icin oy 4.0 gercek 3.0
8 icin en yakin 4816 urun 106 icin oy 5.0 gercek 5.0
9 icin en yakin 3511 urun 0 icin oy 3.0 gercek 4.0
10 icin en yakin 3973 urun 1212 icin oy 5.0 gercek 4.0
11 icin en yakin 2554 urun 287 icin oy 4.0 gercek 5.0
12 icin en yakin 4733 urun 31 icin oy 4.0 gercek 3.0
13 icin en yakin 2339 urun 9 icin oy 4.0 gercek 3.0
14 icin en yakin 3036 urun 10 icin oy 4.0 gercek 3.0
15 icin en yakin 2748 urun 253 icin oy 5.0 gercek 5.0
16 icin en yakin 450 urun 16 icin oy 4.0 gercek 4.0
17 icin en yakin 1133 urun 9 icin oy 5.0 gercek 2.0
18 icin en yakin 3037 urun 253 icin oy 5.0 gercek 4.0
19 icin en yakin 1266 urun 107 icin oy 3.0 gercek 3.0
20 icin en yakin 537 urun 253 icin oy 5.0 gercek 5.0
rmse 0.975900072949
\end{verbatim}

Sonuç fena deðil. Tavsiye programlarýnda RMSE 0.9 civarý iyi olarak
bilinir, Netflix yarýþmasýnda [3] mesela kazanan algoritma RMSE 0.85'e
eriþmiþtir.

Kaynaklar

[1] Grigorik, {\em SVD Recommendation System in Ruby}, \url{http://www.igvita.com/2007/01/15/svd-recommendation-system-in-ruby}

[2] Harrington, P., {\em Machine Learning in Action}

[3] Wikipedia, {\em Netflix Prize}, \url{http://en.wikipedia.org/wiki/Netflix_Prize}

[4] Stack Exchange, {\em How do I use the SVD in collaborative filtering?}, \url{http://stats.stackexchange.com/questions/31096/how-do-i-use-the-svd-in-collaborative-filtering}

[5] Anand, {\em MORE ON LINEAR STRUCTURE IN DATA, AND SINGULAR VALUE
  DECOMPOSITION}, url{https://anandoka.wordpress.com/tag/imputed-svd}

[6] Bayramli, Lineer Cebir, {\em Ders 33}

\end{document}
