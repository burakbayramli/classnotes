<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
        cancel: ["Extension","cancel"], cancelto: ["Extension","cancel"]
      });
    });
    </script>  
   
  <title>Paralel Lineer Cebir Temeli</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="header">
</div>
<h1 id="paralel-lineer-cebir-temeli">Paralel Lineer Cebir Temeli</h1>
<p>Satırsal <span class="math inline">\(A^TA\)</span></p>
<p>[5]'te tek makinalı ortamda matris çarpımının nasıl yapılacağını, ve nasıl görülecebileğini anlattık. Satır bakış açısı, kolon bakış açısı işlendi. Peki parallel bir ortamda hangi matematik bizi ilgilendirmeli? Mesela <span class="math inline">\(A^TA\)</span>'yi ele alalım. Bu çarpım oldukça önemli çünkü başka sonuçlar için de kullanılabiliyor, Eşsiz Değer Ayrıştırması (Singular Value Decomposition -SVD-) bunlardan biri.</p>
<p>Büyük Veri ve paralel işlem bağlamında <span class="math inline">\(A^TA\)</span>'nin önemli şurada; eğer <span class="math inline">\(A\)</span> matrisi &quot;uzun boylu ve zayıf'' ise, yani çok miktarda satırı ama az miktarda kolonu var ise, <span class="math inline">\(m \times d\)</span> diyelim, <span class="math inline">\(A^TA\)</span> çarpımı bize <span class="math inline">\(d \times d\)</span> boyutunda ufak bir matris verir. Eğer SVD hesabını bu matris üzerinden yapabilirsek, işimizi kolaylaştırmış oluruz.</p>
<p>Satırsal olarak <span class="math inline">\(A^TA\)</span> hesabı yapmak için satır satır gezerken her satırın kendisi ile dışsal çarpımını (outer product) alıp sonuçları toplamak yeterli [9]. İşlemi daha sözel tarif eden bir açıklama [3]'de bulunabilir.</p>
<p><span class="math inline">\(A^TA\)</span> ve SVD</p>
<p><span class="math inline">\(A^TA\)</span> açılımını yapalım [7], bir <span class="math inline">\(A\)</span> için SVD ayrıştırması</p>
<p><span class="math display">\[
A = U \Sigma V^T
\]</span></p>
<p>olduğuna göre</p>
<p><span class="math display">\[
A^TA =  (U \Sigma V^T)^T  U \Sigma V^T
\]</span></p>
<p>olacaktır, devam edersek,</p>
<p><span class="math display">\[
= V \Sigma U^T U \Sigma V^T 
\]</span></p>
<p><span class="math inline">\(U^T U = I\)</span> olduğu için geri kalanlar</p>
<p><span class="math display">\[
A^TA = V \Sigma^2 V^T 
\]</span></p>
<p>Peki eşitliğin sağından <span class="math inline">\(V\)</span>'yi nasıl çıkartırız? Bir dikgen matris çarpı köşegen bir matris çarpı o dikgen matrisin devriği bize bir şeyi hatırlatıyor mu? Evet, bu bir özdeğer / özvektör hesabına benziyor, [2]'de görüldüğü gibi <span class="math inline">\(A=S\Lambda S^{-1}\)</span> ayrıştırması da var (birimdik matrislerde tersi alma işleminin devrik ile aynı şey olduğunu unutmayalım). O zaman <span class="math inline">\(A^TA\)</span>'nin öz hesabını yaparsak oradaki özvektör bize <span class="math inline">\(V\)</span> matrisini verecektir.</p>
<p><span class="math inline">\(U\)</span>'yi elde etmek için basit matris çarpımları yeterli,</p>
<p><span class="math display">\[
A = U \Sigma V^T \to U = A V \Sigma^{-1}
\]</span></p>
<p>Yani <span class="math inline">\(V\)</span> elde edildikten sonra onunla <span class="math inline">\(A\)</span>'yi çarpıyoruz, sonra <span class="math inline">\(\Sigma^{-1}\)</span> ile. <span class="math inline">\(A\)</span> ne kadar büyük olursa olsun onu bir vektör <span class="math inline">\(V\)</span> ile çarpmak hızlı işler, <span class="math inline">\(\Sigma\)</span> ise köşegen, seyrek bir matristir, onun çarpımı da basittir.</p>
<p>Bu işlemlerin paralel versiyonları için [8, 6] kaynaklarına bakılabilir.</p>
<p>SVD ve öz hesapların benzerliği için alttaki koda bakılabilir,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy.linalg <span class="im">as</span> lin
<span class="im">import</span> pandas <span class="im">as</span> pd

k <span class="op">=</span> <span class="dv">7</span> <span class="co"># izdusum uzayinin boyutlari</span>
df <span class="op">=</span> pd.read_csv(<span class="st">&quot;../linear_app10rndsvd/w1.dat&quot;</span>,sep<span class="op">=</span><span class="st">&#39;;&#39;</span>,header<span class="op">=</span><span class="va">None</span>)
A <span class="op">=</span> np.array(df)[:,<span class="dv">1</span>:]
<span class="bu">print</span> (A.shape)</code></pre></div>
<pre><code>(71, 30)</code></pre>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">ATA <span class="op">=</span> np.dot(A.T,A)
<span class="bu">eval</span>,evec <span class="op">=</span> lin.eig(ATA)
u,s,v <span class="op">=</span> lin.svd(A)
<span class="bu">print</span> (evec.shape)
<span class="bu">print</span> (v.shape)</code></pre></div>
<pre><code>(30, 30)
(30, 30)</code></pre>
<p>Matrisler birbirine değersel olarak ne kadar yakın, kontrol edelim,</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="bu">print</span> ( np.mean(np.<span class="bu">abs</span>(evec) <span class="op">-</span> np.<span class="bu">abs</span>(v.T)))</code></pre></div>
<pre><code>0.005230468666628483
-2.7259159635502234e-13</code></pre>
<p>Fark çok ufak (<code>abs</code> ile mutlak değer kullandık çünkü bazen tüm bir kolon diğerinin eksi hali olabiliyor).</p>
<p>SVD İçin QR</p>
<p>QR ile SVD yapmak mumkundur. QR ayrıştırması [4] kolonların hepsi bilindiği gibi birbirine dik (orthogonal) birim vektörler olan bir <span class="math inline">\(Q\)</span> matrisi ve üst üçgensel (upper triangular) bir <span class="math inline">\(R\)</span> matrisi oluşturur. Ayrıştırmanın <span class="math inline">\(A^TA\)</span> ile bağlantısı nedir? Eğer <span class="math inline">\(A\)</span> yerine onun ayrıştırmasını <span class="math inline">\(QR\)</span> koyarsak,</p>
<p><span class="math display">\[
C = A^TA = (QR)^T (QR) = R^T Q^T QR
\]</span></p>
<p>Tum <span class="math inline">\(Q\)</span> vektorleri birbirine dik, ve birim vektorler ise, <span class="math inline">\(Q^T Q\)</span> birim matrisi <span class="math inline">\(I\)</span> olur. O zaman</p>
<p><span class="math display">\[
C = R^T Q^T QR = R^T R
\]</span></p>
<p>Yani</p>
<p><span class="math display">\[
C = R^TR
\]</span></p>
<p>Peki <span class="math inline">\(A^TA\)</span> hesaplayıp (böylece <span class="math inline">\(R^TR\)</span>'yi elde edince) onun içinden <span class="math inline">\(R\)</span>'yi nasıl çekip çıkartırız? Şimdi Cholesky ayrıştırması kullanmanın zamanı. Cholesky ayrıştırması (herhangi bir simetrik pozitif kesin <span class="math inline">\(C\)</span> matrisi üzerinde)</p>
<p><span class="math display">\[C = LL^T\]</span></p>
<p>olarak bilinir, yani bir matris alt üçgensel (lower triangular -ki L harfi oradan geliyor-) <span class="math inline">\(L\)</span> matrisine ve onun devriği olan üst üçgensel <span class="math inline">\(L^T\)</span>'nin çarpımına ayrıştırılır. Elimizde <span class="math inline">\(R^TR\)</span> var, ve ona benzer <span class="math inline">\(LL^T\)</span> var, <span class="math inline">\(R\)</span> bilindiği gibi üst üçgensel, <span class="math inline">\(L\)</span> alt üçgensel, <span class="math inline">\(L^T\)</span> ve <span class="math inline">\(R\)</span> birbirine eşit demek ki. Yani <span class="math inline">\(A^TA\)</span> üzerinde sayısal hesap kütüphenimzin Cholesky çağrısı kullanmak bize <span class="math inline">\(QR\)</span>'in <span class="math inline">\(R\)</span>'sini verir.</p>
<p>Şu anda akla şu soru gelebilir: madem kütüphane çağrısı yaptık, niye <span class="math inline">\(A\)</span> üzerinde kütüphenimizin <span class="math inline">\(QR\)</span> çağrısını kullanmıyoruz?</p>
<p>Cevap Büyük Veri argümanında saklı. Bu ortamda uğraşılan verilerde <span class="math inline">\(A\)</span> matrisi <span class="math inline">\(m \times n\)</span> boyutlarındadır, ve <span class="math inline">\(m\)</span> milyonlar, hatta milyarlarca satır olabilir. Şimdilik <span class="math inline">\(m &gt;&gt; n\)</span> olduğunu farzedelim, yani <span class="math inline">\(m\)</span>, <span class="math inline">\(n\)</span>'den &quot;çok, çok büyük&quot;, yani &quot;boyut kolonlarının&quot;, ki <span class="math inline">\(n\)</span>, sayısı binler ya da onbinlerde. Bu gayet tipik bir senaryo aslında, ölçüm noktaları (boyutlar) var, ama çok fazla değil, diğer yandan o ölçümler için milyonlarca veri noktası toplanmış. Tipik bir aşırı belirtilmiş (överdetermined) sistem - ki en az kareler (least squares) gibi yaklaşımların temel aldığı sistemler bunlardır, eldeki denklem sayısından daha fazla ölçüm noktası vardır. Bu arada en az karelerden bahsettik, <span class="math inline">\(QR\)</span>'in kullanıldığı alanlardan biri en az karelerin çözümüdür.</p>
<p>Argümana devam ediyoruz, kütüphane <code>qr</code> çağrısını <span class="math inline">\(A\)</span> üzerinde yaparsak, <span class="math inline">\(m \times n\)</span> gibi devasa bir matris üzerinde işlem yapmak gerekir. Ama <span class="math inline">\(A^TA\)</span> üzerinde işlem (Cholesky) yaparsak, ki bu çarpımın boyutu <span class="math inline">\(n \times m \cdot m \times n = n \times n\)</span>, yani çok daha ufak bir matristir. <span class="math inline">\(A^TA\)</span>'in işlem bedeli çok ufak, birazdan anlatacağımız yöntem sayesinde bu bedel <span class="math inline">\(O(m)\)</span>.</p>
<p>SVD</p>
<p>Simdi <span class="math inline">\(QR\)</span> sonuçlarını kullanarak SVD hesaplamaya gelelim. SVD bize ne verir?</p>
<p><span class="math display">\[ A = U \Sigma V^T \]</span></p>
<p><span class="math inline">\(U\)</span> ve <span class="math inline">\(V^T\)</span> dikgen (orthogonal) matrislerdir, <span class="math inline">\(\Sigma\)</span> sadece köşegeni boyunca değerleri olan bir matristir. Daha fazla detay için bkz [4]. Şimdi <span class="math inline">\(A = QR\)</span> yerine koyalım,</p>
<p><span class="math display">\[ QR =  U \Sigma V^T \]</span></p>
<p><span class="math display">\[ R = Q^T U \Sigma V^T \]</span></p>
<p>Bu son formüledeki <span class="math inline">\(Q^TU\)</span> ibaresi, iki dikgen matrisin çarpımıdır. Lineer Cebir kurallarına göre iki dikgen matrisin çarpımı bir diğer ortogonal matristir. Bu yeni dikgen matrise <span class="math inline">\(U_R\)</span> adı verelim, o zaman</p>
<p><span class="math display">\[ R = U_R \Sigma V^T \]</span></p>
<p>Bu son formül bize bir şeyler söylüyor. <span class="math inline">\(R\)</span>'nin SVD üzerinden ayrıştırılabileceğini söylüyor ve bu ayrıştırma sonrası ele geçen <span class="math inline">\(U_R,V^T\)</span> ve <span class="math inline">\(\Sigma\)</span> köşegen matrisleridir! Bu çok önemli bir sonuç. Bu ayrıştırmanın sonucu <span class="math inline">\(A\)</span>'nin ki ile birbirine çok benziyor, tek fark <span class="math inline">\(U\)</span> ile <span class="math inline">\(U_R\)</span>. Bu iki matris arasındaki geçiş şöyle:</p>
<p><span class="math display">\[ U_R = Q^T U \]</span></p>
<p><span class="math display">\[ U = QU_R \]</span></p>
<div class="figure">
<img src="ur.png" />

</div>
<p>Bu demektir ki eğer <span class="math inline">\(R\)</span> üzerinde kütüphanemizin <code>svd</code> çağrısını kullanırsak (ki <span class="math inline">\(R\)</span> nispeten ufak olduğu için bu ucuz olur) ele geçen <span class="math inline">\(U_R\)</span>'i alıp, <span class="math inline">\(Q\)</span> ile çarparsak, <span class="math inline">\(A\)</span> ayrıştırmasının <span class="math inline">\(U\)</span>'sunu elde ederiz! <span class="math inline">\(Q\)</span> ile paralel yapılabilir.</p>
<p>Tekrar paylaşmak gerekirse paralelleştirme teknikleri [6,8] yazılarında.</p>
<p>Kaynaklar</p>
<p>[1] Benson, A., <em>Tall-and-skinny Matrix Computations in MapReduce</em></p>
<p>[2] Bayramlı, <em>Lineer Cebir Ders 22</em>, <a href="https://burakbayramli.github.io/dersblog/linear/linear_22/ders_22.html" class="uri">https://burakbayramli.github.io/dersblog/linear/linear_22/ders_22.html</a></p>
<p>[3] Bayramlı, <a href="https://burakbayramli.github.io/dersblog/sk/2022/11/paralel-lineer-cebir.html" class="uri">https://burakbayramli.github.io/dersblog/sk/2022/11/paralel-lineer-cebir.html</a></p>
<p>[4] Bayramlı, Lineer Cebir, <em>Ders 29</em></p>
<p>[5] Bayramlı, Lineer Cebir, <em>Matris Çarpımı, Ders 1</em></p>
<p>[6] [Lineer Cebir ve Hadoop](https://burakbayramli.github.io/dersblog/sk/2015/03/lineer-cebir-hadoop.html)</p>
<p>[7] Zadeh, <em>CME 323: Distributed Algorithms and Optimization, Lecture 17</em>, <a href="https://stanford.edu/~rezab/classes/cme323/S17/" class="uri">https://stanford.edu/~rezab/classes/cme323/S17/</a></p>
<p>[8] Bayramlı, <em>Paralel Lineer Cebir</em>, <a href="https://burakbayramli.github.io/dersblog/sk/2022/11/paralel-lineer-cebir.html" class="uri">https://burakbayramli.github.io/dersblog/sk/2022/11/paralel-lineer-cebir.html</a></p>
<p>[9] Gundersen, <em>Matrix Multiplication as the Sum of Outer Products</em>, <a href="https://gregorygundersen.com/blog/2020/07/17/matmul/" class="uri">https://gregorygundersen.com/blog/2020/07/17/matmul/</a></p>
<p>
  <a href="..">Yukarı</a>
</p>
</body>
</html>
