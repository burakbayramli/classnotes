\documentclass[12pt,fleqn]{article}\usepackage{../../common}
\begin{document}
SVD ile Kümeleme, Benzerlik

Eþsiz Deðer Ayrýþtýrma (Singular Value Decomposition -SVD-) ile bir veri
madenciliði örneði göreceðiz. [6]'da SVD'nin matematiðini iþledik. SVD bir
matris $A$ üzerinde ayrýþtýrma yapar, ve $A$ herhangi boyutta, türde bir
matris olabilir.

\includegraphics[height=7cm]{svd_1.png}

Ayrýþtýrma $m \times n$ boyutlu matrisi $A=CWF$ olarak ayrýþtýrýr, burada $C$,
ana matris ile ayný miktarda satýra sahiptir, $F$ ayný miktarda kolona
sahiptir. Ayrýþtýrma sonrasý $A$'nin kertesi (rank) $r$ ortaya çýkar, eðer tüm
$A$ kolonlarý birbirinden baðýmsýz ise, o zaman $r=m$ olacaktýr, ama kolonlarýn
bazýlarý mesela ayný ölçümü deðiþik katlarda tekrarlýyor ise, o zaman matriste
eþsizlik vardýr, ve bu durumda $r < m$ olur, ve ortadaki $W$ matrisi $r \times
r$ olduðu için beklenenden daha ufak boyutlarda olabilir.

Ayrýca SVD, $W$ çaprazýndaki özdeðerleri büyüklük sýrasýna göre dizer, ve her
özdeðere tekabül eden özvektörler de ona göre sýraya dizilmiþ olacaktýr, ve SVD
tamamlanýnca mesela "en büyük 10" özdeðere ait olan $CWF$ deðerlerini alýp,
diðerlerini atmayý da seçebiliriz, yani kerte üzerinden yapýlan "eleme" üstüne
bir eleme de kendimiz yapabiliriz. Bu elemeyi yapabilmemizin mantýðý þöyle;
küçük özdeðerlerin çarptýðý özvektörlerin nihai toplama daha az etki ettiði
söylenebilir, ve bu "gürültüyü" elemek sonucu deðiþtirmeyecektir. Ayrýca bu
elemeyi yaparak bir tür boyut azaltma (dimensionality reduction) iþlemini de
ayný zamanda baþarmýþ oluruz.

Ayrýþtýrmanýn Anlamlarý

Bir ayrýþtýrmayý deðiþik þekillerde görmek mümkündür. Bunlardan önemli
birisi çizit bakýþ açýsýdýr (graph interpretation). Çizit bilindiði gibi
düðümler ve onlar arasýndaki ayrýtlardan (edges) oluþur. Bir çizit matris
formunda temsil edilebilir, satýr / kolon kesiþimi iki düðüm arasýndaki
ayrýtýn aðýrlýðýný, ya da varlýðýný (1 ve 0 üzerinden) temsil edecektir. Bu
durumda SVD sonucunda elde edilen $CWF$, bize iki düðüm arasý geçiþli
(bipartite) çiziti üç düðüm arasý geçiþli (tripartite) çizite çevrilmiþ
halde geri verir. Ve bu yeni çizitde en fazla $r$ tane geçiþ noktalarý
(waystations) oluþmuþtur, üstte bahsettiðimiz eleme ile geçiþler daha da
azaltýlabilir.

Þimdi, bu geçiþ noktalarýna olan $C$'nin ``baðlanma þekli'', "baðlanma kuvveti",
ek kümeleme basamaðý tarafýndan kullanýlabilir. Bu "azaltýlmýþ" geçiþin
üzerindeki her iþlem / ona yapýlan her referans kümeleme için bir ipucudur. Bunu
görmek için örnek zaman serilerinin SVD sonrasý elde edilen $C$ (örnekte
\verb!u!) matrisinin ilk iki kolonunu bile grafiklemek yeterlidir.

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.linalg as lin
data = np.genfromtxt("synthetic_control.data", dtype=float)

# before norm, and take only 10 data points
data = data[:,0:10]

print data.shape

# show the mean, and std of the first time series
print data[0,:]
print np.mean(data[0,:], axis=0)
print np.std(data[0,:], axis=0)

# normalize
data -= np.mean(data, axis=0)
data /= np.std(data, axis=0)

# after norm
print data[0,:]

u,s,v = lin.svd(data, full_matrices=False)
print 'svd'
print u.shape
print s
print v.shape

plt.plot(u[:,0], u[:,1], '.')
plt.savefig('svd_3.png')
\end{minted}

\begin{verbatim}
(600, 10)
[ 28.7812  34.4632  31.3381  31.2834  28.9207  33.7596  25.3969  27.7849
  35.2479  27.1159]
30.40918
3.16894521278
[-0.35501371  0.85457443 -0.10641642 -0.16202975 -0.51986031  0.56762802
 -1.19371757 -0.29304061  1.27639519 -0.2095089 ]
svd
(600, 10)
[ 48.29293361  30.97232928  24.52860861  20.63081553  20.0940039
  17.52035809  16.48932523  16.03796372  15.41270426  14.27678793]
(10, 10)
\end{verbatim}

\includegraphics[height=6cm]{svd_3.png}

Görüldüðü gibi net bir þekilde iki tane küme ortaya çýktý. Bu kümeler yazýnýn
baþýndaki iki ayrý zaman serisi öbeklerine tekabül ediyorlar.

O zaman serilerini ayýrtetmek için ne yaparýz? Üstteki veriler üzerinde kmeans
iþletebilirdik, ya da kabaca bakýyoruz, dikey olarak -0.025 seviyesinde bir
çizgi ayýraç olarak görülebilir. Numpy filtreleme tekniði

\verb!u[:,0] < -0.025!

bize ana veri üzerinde uygulanabilecek \verb!True! ve \verb!False!  deðerleri
verir, bunlarý alarak ana veriye filtrele olarak uygularýz,

\verb!data[u[:,0] < -0.025]!

ve mesela birinci kümeye ait zaman serilerini bulabiliriz. 

Kontrol etmek için ilk 3 kolonun deðerlerini üç boyutta grafikleyelim.

\begin{minted}[fontsize=\footnotesize]{python}
from mpl_toolkits.mplot3d import Axes3D
import scipy.linalg as lin

data = np.genfromtxt("synthetic_control.data", dtype=float)

data = data[:,0:10]

print data.shape

data -= np.mean(data, axis=0)
data /= np.std(data, axis=0)

u,s,v = lin.svd(data)
print 'svd'
print u.shape
print s
print v.shape

fig = plt.figure()
ax = Axes3D(fig)
ax.plot(u[:,0], u[:,1], u[:,2],',', zs=0, zdir='z', label='zs=0, zdir=z')
plt.savefig('svd_4.png')
\end{minted}

\begin{verbatim}
(600, 10)
svd
(600, 600)
[ 48.29293361  30.97232928  24.52860861  20.63081553  20.0940039
  17.52035809  16.48932523  16.03796372  15.41270426  14.27678793]
(10, 10)
\end{verbatim}

\includegraphics[height=6cm]{svd_4.png}

Yine iki tane küme olduðunu görüyoruz. 

Kelime Vektorleri [5]

Diyelim ki elimizde üç tane cümle var. Bu cümlelere dayanarak bir kelimenin
vektörsel temsilini bulmak istiyoruz. 

\includegraphics[width=20em]{svd_7.png}

Matris içindeki sayýlar her kelimenin bir diðeri ile beraber kaç kere ayný
cümlede olduðuna (cooccurence) göre oluþturuldu. Mesela ``I'' ile ``like''
kelimesi beraber 2 kere çýkmýþ, bu matriste 1,2 ve 2,1 kordinatlarýnda
görülüyor.  O zaman bu matrise bir kelimenin satýrsal ya da kolonsal
temsiline bakarak o kelimenin vektörsel halini bulabiliriz. Mesela
``enjoy'' icin bu $\left[\begin{array}{cccccccc} 1&0&0&0&0&1&0 \end{array}\right]$.

Fakat gerçek uygulamalarda bu þekilde bir temsil performans ve depolama
açýsýndan bedeli olabilir; eðer eldeki kelime sayýsý 1 milyon ise bu matris
1 milyon x 1 milyon öðeye ihtiyaç duyar. 

Çözüm: boyut azaltmak. SVD bu iþ için biçilmiþ kaftan. 

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.linalg as lin
words = ["I", "like", "enjoy",
         "deep", "learning", "NLP", "flying", "."]
X = np.array([[0,2,1,0,0,0,0,0],
              [2,0,0,1,0,1,0,0],
              [1,0,0,0,0,0,1,0],
              [0,1,0,0,1,0,0,0],
              [0,0,0,1,0,0,0,1],
              [0,1,0,0,0,0,0,1],
              [0,0,1,0,0,0,0,1],
              [0,0,0,0,1,1,1,0]])

U, s, Vh = lin.svd(X, full_matrices=False)
print U.shape, s.shape, Vh.shape

for i in xrange(len(words)):
    plt.text(U[i,0], U[i,1], words[i])

plt.ylim(-0.8,0.8)
plt.xlim(-0.8,0.2)              
plt.savefig('svd_8.png')
\end{minted}

\begin{verbatim}
(8, 8) (8,) (8, 8)
\end{verbatim}

\includegraphics[width=20em]{svd_8.png}

$U$'nun ilk iki kolonunu grafikledik çünkü en büyük iki eþsiz deðere
tekabül eden kolonlar bunlar, yani en ``önemli'' deðerler orada. 

En önemli kolonlarý bulduk, o zaman diyebiliriz ki bu iki kolon üzerinden
bir kelimenin vektörsel temsilini de bulmuþ olduk. Bu temsil eskisine göre
daha küçük, ve özetleme açýsýndan daha kuvvetli. Artýk kelimelerin
birbirine yakýnlýðý, benzerliði gibi hesaplar bu vektör üzerinden
yapýlabilir. 

Üstteki grafik yakýnlýk açýsýndan bazý anlamsal yapýyý göstermeye baþladý
bile: mesela ``like'' ve ``enjoy'' birbirine yakýn, bu mantýklý çünkü ikisi
de birinin yaptýðý þeyler. Diðer yandan ``learning'' kelimesine en yakýn
``flying'' bu da mantýklý, her iki kelime de cümle sonlarýnda ortaya
çýkýyorlar ve hedef kelimeler. 

Gerçek uygulamalar için bazý taklalar: 

Ýngilizce'nin yapýsý sebebiyle sürekli görülen ama çok anlam katmayan bazý
kelimeler var, mesela ``the'', ``he'', ``has'' gibi. Bu kelimeler direk
sayýlýrsa bu sayý çok yüksek. Çözüm, belli bir sayý üstünü saymamak, ya da
onlarý tamamen devreden çýkartmak.

Word2Vec

Yapay Sinir Aðlarý (YSA) literatüründe duyulan word2vec aslýnda üstteki
vektörsel temsilin baþka bir yoldan öðrenilmesinden ibaret. YSA yaklaþýmý
ile beraber olma sayýsý hesaplanmadan vektörsel temsil direk öðreniliyor,
bunun için her kelime için o kelime yakýnýndaki (bir pencere içindeki)
diðer kelimeler tahmin edilmeye uðraþýlýyor, daha doðrusu hedef fonksiyon
budur, ve eðitim verisine bakýlarak bu tahmindeki baþarý geriye yayýlým
(backpropagation -backprop-) ile düzeltilerek arttýrýlýyor. 

Word2Vec'in insaný þaþýrtabilen bazý ilginç özellikleri var: mesela çok
büyük veriler üzerinden vektörler hesaplandýktan sonra mesela kral
vektörünü alýp ondan erkek vektörünü çýkartýyorsunuz, ve kadýn vektörünü
toplayorsunuz ve kraliçe vektörünü elde ediyorsunuz (ona yakýn bir vektörü
en azýndan). Ýlginç deðil mi? Bu keþif pek çok araþtýrmacýya ``vay canýna''
dedirtirdi, tabii ki bunun istatistiki sebepleri var, bu konuya bakanlar da
oldu, detaylar için [5, 18:50]. 

Örnek

Þimdi biraz daha deðiþik bir probleme bakalým, bu sefer bir grup kelimeyi
birbirlerine benzerlikleri (ya da uzaklýðý) üzerinden kümelemeye uðraþacaðýz.

Benzerlik, Levenhstein mesafesi adlý ölçüt [2] üzerinden olacak. Matrisimiz her
kelimenin her diðer kelime ile arasýndaki uzaklýðý veren bir matris olmalý, eðer
100 kelime var ise, bu matris 100 x 100 boyutlarýnda olacak. SVD sonrasý elde
edilen \verb!u! üzerinde kmeans iþleteceðiz, ve kümeleri bulacaðýz. Ayrýca her
küme için bir ``temsilci'' seçebilmek için kmeans'in bize verdiði küme ortasý
kordinatýnýn en yakýn olduðu kelimeyi çekip çýkartacaðýz, ve onu temsilci olarak
alacaðýz.

Kelime mesafesi olarak

\inputminted[fontsize=\footnotesize]{python}{leven.py}

\begin{minted}[fontsize=\footnotesize]{python}
import leven
s1 = "pizza"
s2 = "pioazza"   
distance = leven.levenshtein(s1, s2)       
print 'The Levenshtein-Distance of ',s1, ' and ', s2, ' is ', distance

s1 = "hamburger"
s2 = "haemmurger"   
distance = leven.levenshtein(s1, s2)       
print 'The Levenshtein-Distance of ',s1, ' and ', s2, ' is ', distance
\end{minted}

\begin{verbatim}
The Levenshtein-Distance of  pizza  and  pioazza  is  2
The Levenshtein-Distance of  hamburger  and  haemmurger  is  2
\end{verbatim}

\begin{minted}[fontsize=\footnotesize]{python}
import scipy.linalg as lin
from sklearn.cluster import KMeans
import itertools

words = np.array(
    ['the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',
     'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',
     'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they', 'we',
     'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one', 'all',
     'would', 'there', 'their', 'what', 'so', 'up', 'out', 'if',
     'about', 'who', 'get', 'which', 'go', 'me', 'when', 'make',
     'can', 'like', 'time', 'no', 'just', 'him', 'know', 'take',
     'people', 'into', 'year', 'your', 'good', 'some', 'could',
     'them', 'see', 'other', 'than', 'then', 'now', 'look',
     'only', 'come', 'its', 'over', 'think', 'also', 'back',
     'after', 'use', 'two', 'how', 'our', 'work', 'first', 'well',
     'way', 'even', 'new', 'want', 'because', 'any', 'these',
     'give', 'day', 'most', 'us'])

print "calculating distances..."

(dim,) = words.shape

f = lambda (x,y): leven.levenshtein(x,y)

res=np.fromiter(itertools.imap(f, itertools.product(words, words)),
                dtype=np.uint8)
A = np.reshape(res,(dim,dim))

print "svd..."

u,s,v = lin.svd(A, full_matrices=False)

print u.shape
print s.shape
print s[:10]
print v.shape

data = u[:,0:8]
k=KMeans(init='k-means++', n_clusters=25, n_init=10)
k.fit(data)
centroids = k.cluster_centers_
labels = k.labels_
print labels[:10]

def dist(x,y):   
    return np.sqrt(np.sum((x-y)**2, axis=1))
    
print "clusters, centroid points.."
for i,c in enumerate(centroids):
    idx = np.argmin(dist(c,data[labels==i]))
    print words[labels==i][idx]
    print words[labels==i]
    
plt.plot(centroids[:,0],centroids[:,1],'x')
plt.hold(True)
plt.plot(u[:,0], u[:,1], '.')
plt.savefig('svd_5.png')

from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = Axes3D(fig)
ax.plot(u[:,0], u[:,1], u[:,2],'.', zs=0,
        zdir='z', label='zs=0, zdir=z')
plt.savefig('svd_6.png')
\end{minted}

\begin{verbatim}
calculating distances...
svd...
(100, 100)
(100,)
[ 357.98820225   46.49125611   32.1352688    23.80316426   21.48889925
   17.53558749   17.2577475    15.08233454   13.60531866   12.78642892]
(100, 100)
[ 0 12 20 17  4 23  3  7  9 14]
clusters, centroid points..
she
['the' 'she']
even
['one' 'get' 'year' 'over' 'after' 'even' 'new']
not
['for' 'not' 'on' 'you' 'who' 'into' 'now' 'how']
if
['in' 'it' 'his' 'if' 'him' 'its']
any
['and' 'say' 'an' 'all' 'can' 'back' 'way' 'want' 'any' 'day']
most
['about' 'just' 'also' 'first' 'most']
come
['people' 'some' 'come']
that
['that' 'what' 'than']
only
['only' 'well']
have
['have' 'make' 'take']
with
['with' 'will' 'which']
like
['like' 'time' 'give']
be
['be' 'he' 'we' 'me' 'see' 'use']
could
['would' 'could']
I
['I' 'by' 'my']
this
['this' 'think']
good
['from' 'know' 'good' 'look' 'work']
our
['of' 'or' 'out' 'your' 'our']
other
['her' 'there' 'their' 'other']
because
['because']
do
['to' 'do' 'so' 'go' 'no' 'two']
up
['but' 'up' 'us']
these
['these']
at
['a' 'as' 'at']
they
['they' 'when' 'them' 'then']
\end{verbatim}

\includegraphics[height=6cm]{svd_5.png}

\includegraphics[height=6cm]{svd_6.png}

Bu tekniðin uygulanabileceði daha pek çok alan var. Mesela her dokümanýn
içindeki belli kelimelerin sayýlarý kolonlarda (her kolon özel bir kelimeye
tekabül edecek þekilde), ve dökümanlarýn kendisi satýrlarda olacak þekilde bir
matrisimiz olsaydý, SVD bu matris üzerinde de bir kümeleme için
kullanýlabilirdi. Bu örnekte ``kaç tane kelime olduðu'' gibi bir ölçüt vardýr
(daha önce kelimelerin birbirine uzaklýðýný kullandýk), ama teknik yine de ise
yarar.

Kaynaklar

Not: \verb!np.fromiter .. itertools.imap! kullanýmýnýn tarifi için [4].

[1] Alcock, {\em Synthetic Control Chart Time Series}, \url{kdd.ics.uci.edu/databases/synthetic_control/synthetic_control.data.html}

[2] Bayramlý, 
    {\em Kelime Benzerligi - Levenshtein Mesafesi}, 
    \url{https://burakbayramli.github.io/dersblog/sk/2012/07/kelime-benzerligi-levenshtein-mesafesi.html}

[3] Skillicorn, D., {\em Understanding Complex Datasets Data Mining with Matrix Decompositions}

[4] Bayramlý, 
    {\em Dongu Yazmamak, Fonksiyonel Diller, Python}, 
    \url{https://burakbayramli.github.io/dersblog/sk/2012/07/dongu-yazmamak-fonksiyonel-diller-python.html}

[5] Socher, {\em CS224d, Deep Learning for Natural Language Processing,
  Lecture 2}, \url{https://www.youtube.com/watch?v=T8tQZChniMk}

[6] Bayramli, Lineer Cebir, {\em Ders 29}

\end{document}
